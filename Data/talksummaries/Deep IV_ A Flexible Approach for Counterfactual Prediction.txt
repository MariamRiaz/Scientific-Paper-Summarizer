6	24	First, imagine that price varies in the training data because the airline gradually increases prices as a plane fills.
7	64	Around holidays, more people want to fly and hence planes become fuller leading to higher prices.
9	73	A direct ML approach might incorrectly predict that if the airline were to increase prices at other times in the year they would also observe increased sales, whereas the true relationship between price and sales is surely negative.
19	36	Depending on the timescale over which such randomization were conducted, it could also hurt the airline’s long-term interests (because it could be perceived as unfair) and could fail if passengers were able to hide their identities (e.g., by logging in from a different computer if they didn’t like the price).
20	19	The alternative is to work with observational data, but doing so requires explicit assumptions about the causal structure of the DGP (Bottou et al., 2013).
23	40	We can do without this assumption and get around both types of selection if we can identify one or more instrumental variables (IVs) that only affect treatment assignment and not the outcome variable.
24	44	In our airline example, the cost of fuel could be such an instrument: its variation is independent of demand for airline tickets and it affects sales only via ticket prices.
29	50	Most IV applications make use of a two-stage least squares procedure (2SLS; e.g., Angrist & Pischke, 2008) that applies a model of linear and homogeneous treatment effects (e.g., all airline customers must have the same price sensitivity).
50	26	Define the counterfactual prediction function h (p, x) ≡ g (p, x) +E[e|x], (2) which is the conditional expectation of y given the observables p and x, holding the distribution of e constant as p is changed.
51	15	Note that we condition only on x and not p in the term E[e|x]; this term is typically nonzero, but it will remain constant under arbitrary changes to our policy variable p.2 Thus h (p, x) is the structural equation that we estimate.
54	16	useful because to evaluate policy options (e.g. changing the ticket price from p0 to p1) we can look at the difference in outcomes h (p1, x)−h (p0, x) = g (p1, x)− g (p0, x).
78	18	Specifically, to minimize `2 loss given n data points and given a function space H (which may not include the true h ), we solve min ĥ∈H n∑ t=1 ( yt − ∫ ĥ (p, xt)dF(p|xt,zt) )2 .
80	17	So the DeepIV procedure has two stages: a first stage density estimation procedure to estimate F̂(p|x,z) and a second that optimizes the loss function described in Equation (6).
87	18	This model is known as a mixture density network, as detailed in §5.6 of Bishop (2006).
92	23	We optimize network parameters θ to minimize the integral loss function in Equation (6) over training data D of size T = |D| from the joint DGPD, L(D;θ) = |D|−1 ∑ t ( yt − ∫ hθ(p, xt)dF̂φ(p|xt,zt) )2 .
94	54	We use stochastic gradient descent (SGD; see algorithms in, e.g., Duchi et al., 2011; Kingma & Ba, 2014) to train the network weights.
112	14	This objective is easy to implement in practice as it just involves drawing samples during training.
126	70	Fortunately, both steps in our Deep IV procedure can be validated by simply evaluating the respective losses on held out data.
131	59	Each stage is evaluated in turn, with second stage validation using the best-possible network as selected in the first stage.
134	41	Without validation, a sufficiently powerful model will perfectly overfit by approximating the conditional distribution F(p|zi) with a point mass at pi.
142	60	We evaluated our approach on both simulated and real data.
143	42	We used simulations to assess DeepIV’s ability to recover an underlying counterfactual function both in a lowdimensional domain with informative features and in a highdimensional domain with features consisting of pixels of a handwritten image.
145	20	We also considered a real-world dataset where ground truth was not available, showing that we could replicate the findings of a previous study in a dramatically more automatic fashion.
147	29	We assume that there are 7 customer types s ∈ {1, ...,7} that each exhibit different levels of price sensitivity.
148	74	We model the holiday effect on sales by letting the customer’s price sensitivity vary continuously throughout the year according to a complex non-linear function, ψt = 2 ( (t−5)4/600 + exp [ −4(t−5)2 ] + t/10−2 ) .
154	58	To evaluate the model, we consider the counterfactual question, “What would sales have been if prices had been changed to p′?” Thus the price in our test set is set deterministically over a fixed grid of price values that spans the range of training set prices.
156	42	Low dimensional domain We evaluated structural mean square error (MSE) while varying both the number of training examples and ρ, the correlation between e and p. In addition to Deep IV, we considered a regular feed-forward network (FFNet) with the same architecture as our outcome network, a non-parametric IV polynomial kernel regression (NonPar, Darolles et al., 2011) using Hayfield et al. (2008)’s R implementation, and standard two-stage least squares (2SLS).
160	88	For 1000 data points, NonPar’s mean performance was better than 2SLS but failed to match DeepIV.
162	51	Adding regularized polynomial basis functions to 2SLS (2SLS(poly)) gives some empirical improvements in performance over 2SLS on larger datasets but the procedure is not causally valid because it violates 2SLS’s linearity assumptions.
163	22	Both forms of 2SLS performed far better than FFNet which did a good job of estimating h (t, s, p)+E[e|p] but a terrible job of recovering the true counterfactual.
