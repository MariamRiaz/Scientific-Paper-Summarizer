0	1	Hierarchical clustering (HC) is a widely used data analysis tool, ubiquitous in information retrieval, data mining, and machine learning (see a survey by (Berkhin, 2006)).
1	68	This clustering technique represents a given dataset as a binary tree; each leaf represents an individual data point and each internal node represents a cluster on the leaves of its descendants.
2	28	HC has become the most popular method for gene expression data analysis (Eisen et al., 1998), and also has been used in the analysis of social networks (Leskovec et al., 2014; Mann et al., 2008), bioinformatics (Diez et al., 2015), image and text classification (Steinbach et al., 2000), and even in analysis of financial markets (Tumminello et al., 2010).
3	26	It is attractive because it provides richer information at all levels of granularity simultaneously, compared to more traditional flat clustering approaches like k-means.
4	28	Recently, (Dasgupta, 2016) formulated HC as a combinatorial optimization problem, giving a principled way to compare the performance of different HC algorithms.
5	35	This optimization viewpoint has since received a lot of attention (Roy & Pokutta, 2016; Charikar & Chatziafratis, 2017; CohenAddad et al., 2017; Moseley & Wang, 2017; Cohen-Addad et al., 2018) that has led not only to the development of new algorithms but also to theoretical justifications for the observed success of popular algorithms (e.g. average-linkage).
6	21	However, in real applications of clustering, the user often has background knowledge about the data that may not be captured by the input to the clustering algorithm.
8	41	Very recently, “semi-supervised” versions of HC that incorporate additional constraints have been studied (Vikram & Dasgupta, 2016), where the natural form of such constraints is triplet (or “must link before”) constraints ab|c1: these require that valid solutions contain a sub-cluster with a, b together and c previously separated from them.2 Such triplet constraints, as we show later, can encode more general structural constraints in the form of rooted subtrees.
9	29	Surprisingly, such simple triplet constraints already pose significant challenges for bottom-up linkage methods.
10	25	Our work is motivated by applying the optimization lens to study the interaction of hierarchical clustering algorithms with structural constraints.
11	31	Constraints can be fairly naturally incorporated into top-down (i.e. divisive) algorithms for hierarchical clustering; but can we establish guarantees on the quality of the solution they produce?
12	74	Another issue is that incorporating constraints from multiple experts may lead to a conflicting set of constraints; can the optimization viewpoint of hierarchical clustering still help us obtain good solutions even in the presence of infeasible constraints?
13	30	Finally, different objective functions for HC have been studied in the literature; do algorithms designed for these objectives behave similarly in the presence of constraints?
14	12	To the best of our knowledge, this is the first work to propose a unified approach for constrained HC through the lens of optimization and to give provable approximation guarantees for a collection of fast and simple top-down algorithms that have been used for unconstrained HC in practice (e.g. community detection in social networks (Mann et al., 2008)).
15	27	Background on Optimization View of HC.
16	3	(Dasgupta, 2016) introduced a natural optimization framework for HC.
17	47	Given a weighted graph G(V,E,w) and pairwise similarities wij ≥ 0 between the n data points i, j ∈ V , the goal is to find a hierarchical tree T ∗ such that T ∗ = arg min all trees T ∑ (i,j)∈E wij · |Tij | (1) where Tij is the subtree rooted at the lowest common ancestor of i, j in T and |Tij | is the number of leaves it contains.3 We denote (1) as similarity-HC.
20	7	In particular, if there is an underlying ground-truth hierarchical structure in the data, then T ∗ can recover the ground-truth.
21	19	Also, both objectives are NP-hard to optimize, so the focus is on approximation algorithms.
22	4	i) We design algorithms that take into account both the geometry of the data, in the form of similarities, and the structural constraints imposed by the users.
23	28	Our algorithms emerge as the natural extensions of Dasgupta’s original recursive sparsest cut algorithm and the recursive balanced cut suggested in (Charikar & Chatziafratis, 2017).
24	11	We generalize previous analyses to handle constraints and we prove an O(kαn)-approximation guarantee4, thus surprisingly matching the best approximation guarantee of the unconstrained HC problem for constantly many constraints.
28	2	iii) We then change gears and study the dissimilarity-HC objective.
30	14	Specifically, the (locally) densest cut heuristic performs poorly even if there is only one triplet constraint, blowing up its approximation factor to O(n).
31	12	Moreover, we improve upon the state-ofthe-art in (Cohen-Addad et al., 2018), by showing a simple randomized partitioning is a 23 -approximation algorithm.
32	36	We also give a deterministic local-search algorithm with the same worst-case guarantee.
