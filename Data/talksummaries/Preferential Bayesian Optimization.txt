0	80	Let g : X → < be a well-behaved black-box function defined on a bounded subset X ⊆ <q.
1	56	We are interested in solving the global optimization problem of finding xmin = argmin x∈X g(x).
2	100	(1) We assume that g is not directly accessible and that queries to g can only be done in pairs of points or duels [x,x′] ∈ X × X from which we obtain binary feedback {0, 1} that represents whether or not x is preferred over x′ (has lower value)1.
3	19	We will consider that x is the winner of the duel if the output is {1} and that x′ wins the duel if the output is {0}.
4	15	The goal here is to find xmin by reducing as much as possible the number of queried duels.
5	30	Our setup is different to the one typically used in BO where direct feedback from g in the domain is available (Jones, 2001; Snoek et al., 2012).
6	37	In our context, the objective is a latent object that is only accessible via indirect observations.
7	27	However, although the scenario described in this work has not received a wider attention, there exist a variety of real wold scenarios in which the objective function needs to be optimized via preferential returns.
8	56	Most cases involve modeling latent human preferences, such as web design via A/B testing, the use of recommender systems (Brusilovsky et al., 2007) or the ranking of game players skills (Herbrich et al., 2007).
9	56	In prospect theory, the models used are based on comparisons with some reference point, as it has been demonstrated that humans are better at evaluating differences rather than absolute magnitudes (Kahneman and Tversky, 1979).
10	14	Optimization methods for pairwise preferences have been studied in the armed-bandits context (Yuea et al., 2012).
20	18	To select new duels, standard acquisition functions like the Expected Improvement (EI) (Mockus, 1977) are extended on top of a GP model with likelihood for duels.
43	37	Note that for any duel [x,x′] in which g(x) ≤ g(x′) it holds that πf ([x,x′]) ≥ 0.5. πf is therefore a preference function that fully specifies the problem.
44	13	We introduce here the concept of normalised Copeland score, already used in the literature of raking methods (Zoghi et al., 2015a), as S(x) = Vol(X )−1 ∫ X I{πf ([x,x′])≥0.5}dx ′, where Vol(X ) = ∫ X dx ′ is a normalizing constant that bounds S(x) in the interval [0, 1].
46	13	Instead of the Copeland score, in this work we use a soft version of it, in which the probability function πf is integrated over X without further truncation.
49	15	It is straightforward to see that if xc is a Condorcet winner with respect to the soft-Copeland score, it is a global minimum of f in X : the integral in (3) takes maximum value for points x ∈ X such that f([x,x′]) = g(x′) − g(x) > 0 for all x′, which only occurs if xc is a minimum of f .
50	20	This implies that if by observing the results of a set of duels we can learn the preference function πf the optimization problem of finding the minimum of f can be addressed by finding the Condorcet winner of the Copeland score.
66	12	The important message here is, however, that given data from the locations and result of the duels we can learn the preference function πf by taking into account the correlations across the duels, which makes the approach to be very data efficient compared to bandits scenarios where correlations are ignored.
113	76	In this section we propose an alternative acquisition function that is fast to compute and explicitly balances exploration and exploitation.
116	14	Step 1, selecting x: First, generate a sample f̃ from the model using continuous Thompson sampling 2 and compute the associated soft-Copland’s score by integrating over X .
121	17	See Figure 3 for an illustration of this effect.
122	11	Step 2, selecting x′: Given xnext the second element of the duel is selected as the location that maximizes the variance of σ(f?)
148	17	The outcome of a pairwise comparison is generated as described in Section 2, i.e., the outcome is drawn from a Bernoulli distribution of which the sample probability is computed according to equation (2).
159	27	Therefore, we present plots of #iterations versus g(xc).
170	19	Figure 5 shows the performance of the compared methods, which is consistent across the four plots: IBO shows a poor performance, due to the combination of the greedy nature of the acquisitions and the poor calibration of the model.
177	12	The results of the Sparring method are shown for the Forrester function but are omitted in the rest of the plots (the number of used evaluations used is smaller than the numbers of the arms and therefore no real learning can happen within the budget).
178	21	However, in Figure 6 we show the comparison between Sparring and PBO-DTS for an horizon in which the bandit method has almost converged.
180	35	We have explored a new framework, PBO, for optimizing black-box functions in which only preferential returns are available.
184	41	In comparison with other alternatives out of the PBO framework, such as IBO or other bandit methods, DTS shows the state-ofthe-art performance.
185	15	There exist several future extensions of our current approach like tackling the existing limitation on the dimension of the input space, which is doubled with respect to the original dimensionality of the problem.
