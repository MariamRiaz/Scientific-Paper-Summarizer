30	20	Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data.
32	29	Even though Spanish is not resource-poor we simulate this by using only English annotated data.
33	22	Xiao and Guo (2013) proposed a cross-lingual log-bilinear document model to learn distributed representations of words, which can capture both the semantic similarities of words across languages and the predictive information with respect to the classification task.
35	108	Zhou et al. (2014) employed aligned sentences in the BWE learning process, but in the sentiment classification process only representations in the source language are used for training, and representations in the target language are used for predicting labels.
36	24	An important weakness of these three works was that aligned sentences were required.
37	12	Some work has trained sentiment-specific BWEs using annotated sentiment information in both languages (Zhou et al., 2015, 2016), which is desirable, but this is not applicable to our scenario.
49	22	In contrast, our delightfully simple approach to create high-quality BWEs for the medical domain requires only monolingual data.
56	38	The delightfully simple method adapts general domain BWEs in a way that preserves the semantic knowledge from general domain data and leverages monolingual domain specific data to create domain-specific BWEs.
59	22	To create domain adapted BWEs, we first train MWEs (monolingual word embeddings) in both languages and then map those into the same space using post-hoc mapping (Mikolov et al., 2013b).
150	31	As the second approach we use a classifier based system proposed by Heyman et al. (2017).
180	19	The probability of going from sample A to B is proportional to the cosine similarity of their embeddings.
181	32	To maximize the number of correct cycles, two loss functions are employed: Walker loss and Visit loss.
184	34	Visit loss encourages cycles to visit all unlabeled samples, rather than just those which are the most similar to labeled samples.
189	27	Due to the fact that we initialize the embedding layers for both classifiers with BWEs the models are able to make some correct cycles at the beginning of the training and improve them later on.
202	11	The RepLab dataset contains tweets from 4 topics: automotive, banking, university, music.
209	18	On the other hand, adding labeled Spanish data caused just a slight increase comparing to semisup with Subtitle+22M tweets based BWEs (0.59%), while in case of Subtitle+BACKGROUND we got significant additional improvement (2.61%).
211	10	It can also be seen that the target-aware system outperformed the target-ignorant system using additional labeled target-language data.
214	43	We achieved high accuracy on the Spanish test set by using only English training data.
219	70	For the first, BNC, we generate a general unlabeled set using English words from the BNC lexicon and generate 10 pairs out of each word by using the 5 most similar Dutch words based on the corresponding BWEs and 5 random Dutch words.
220	132	For the second scenario, medical, we generate an in-domain unlabeled set by generating for each English word in the medical lexicon the 3 most similar Dutch words based on BWEs and for each of these we use the 5 most similar English words (ignoring the words which are in the original medical lexicon) and 5 negative words.
222	11	Results in Table 5 show that adding semisup to the classifier further increases performance for BLI as well.
223	10	For the baseline system, when using only in-domain text for creating BWEs, only the medical unlabeled set was effective, general domain word pairs could not be exploited due to the lack of general semantic knowledge in the BWE model.
226	71	These results show that adapted BWEs are needed to exploit unlabeled data well which leads to an impressive overall 3.71 increase compared with the best result in previous work (Heyman et al., 2017), by using only unlabeled data.
227	26	Bilingual word embeddings trained on general domain data yield poor results in out-of-domain tasks.
228	34	We presented experiments on two different low-resource task/domain combinations.
229	13	Our delightfully simple task independent method to adapt BWEs to a specific domain uses unlabeled monolingual data only.
230	26	We showed that with the support of adapted BWEs the performance of offthe-shelf methods can be increased for both crosslingual Twitter sentiment classification and medical bilingual lexicon induction.
231	23	Furthermore, by adapting the broadly applicable semi-supervised approach of HaÌˆusser et al. (2017) (which until now has only been applied in computer vision) we were able to effectively exploit unlabeled data to further improve performance.
232	13	We showed that, when also using high-quality adapted BWEs, the performance of the semi-supervised systems can be significantly increased by using unlabeled data at classifier training time.
233	34	In addition, CLSC results are competitive with a system that uses targetlanguage labeled data, even when we use no such target-language labeled data.
