{"sections": [{"heading": "1. Introduction", "text": "Clustering is a widely used technique with applications in machine learning, statistics, speech processing, computer vision. It consists in grouping a set of examples so that \u201csimilar\u201d examples are in the same cluster while \u201cdissimilar\u201d examples are in different clusters. In most applications, the vector representation of examples is given as input and\n1Department of Computer Science, University of Toronto, Toronto, Canada 2CIFAR Senior Fellow. Correspondence to: Marc T. Law <law@cs.toronto.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\na key step is then to determine an appropriate similarity metric so that similar and dissimilar objects can be easily identified. In some cases, experts with domain knowledge may help determine an appropriate distance metric. However, in high-dimensional problems, determining an effective metric becomes increasingly difficult even for an expert, and standard metrics such as the Euclidean distance can lead to very poor results.\nMany approaches that learn an appropriate similarity metric (Xing et al., 2002; Lajugie et al., 2014; Law et al., 2016) or a nonlinear embedding function (Schroff et al., 2015; Sohn, 2016; Song et al., 2017) in a supervised way have thus been proposed. They assume the availability of a training dataset that \u201cshares the same metric\u201d as the test dataset that they want to partition (e.g. the datasets represent the same concepts such as birds species or car models). As both datasets share the same metric, the model learned from training examples is expected to correctly compare test examples. The approaches can roughly be divided into four groups based on two criteria: semisupervised/supervised setting and shallow/deep architecture. In the semi-supervised setting (Xing et al., 2002; Chopra et al., 2005), the training data is given as a small set of example pairs that are expected to be in the same or different clusters. On the other hand, supervised approaches (Lajugie et al., 2014; Law et al., 2016) assume the availability of labeled datasets for which the ground truth partitions are provided during training. A model is then learned so that some clustering algorithm will produce a partition similar to the ground truth partition on the training dataset. The supervised clustering setting can be seen as the spe-\ncial case of semi-supervised setting where all the pairwise similarity relations between training examples are given. In particular, supervised clustering is a specific classification problem where the model is learned so that the representations of training examples are closer to the representative vector of their category than to the representative vector of any other category. In this paper, we propose a novel metric learning approach whose rationale is illustrated in Fig. 1. In particular, we leverage deep nonlinear and hierarchical architectures to learn complex representations that better reflect similarity relations among examples.\nMany deep metric learning approaches (Schroff et al., 2015; Song et al., 2016) extend the ideas introduced in the shallow metric learning literature (Xing et al., 2002; Weinberger et al., 2006) to learn deep neural networks on large datasets. The main difficulty is how to implement these ideas so that they are scalable and avoid memory bottleneck. For instance, many approaches have proposed hard negative mining strategies to limit the number of training constraints. A deep supervised clustering approach was proposed in (Song et al., 2017): to compute its gradient, the approach uses an iterative greedy algorithm whose algorithmic complexity is high.\nContributions: In this paper, we propose a metric learning framework that optimizes an embedding function so that the learned representations of similar examples are grouped into the same cluster, and dissimilar examples are in different clusters. To this end, we relax the problem of partitioning a dataset with Bregman divergences (Banerjee et al., 2005), and we formulate our problem so that the gradient is efficient to compute. In particular, the gradient can be expressed in closed-form, and the algorithmic complexity to compute it is linear in the size of the training mini-batch and quadratic in the representation dimensionality, which is better than the complexity of existing iterative methods. Our method is also simple to implement and obtains stateof-the-art performance on standard datasets."}, {"heading": "2. Preliminaries", "text": "In this section, we provide some technical background about clustering, and set up the notation throughout. We reformulate in matrix form the problem of clustering a set of examples with Bregman divergences (Banerjee et al., 2005) and write it as an optimization problem w.r.t. one variable in Eq. (2).\nNotation: We note \u3008A,B\u3009 := tr(AB>), the Frobenius inner product where A and B are real-valued matrices; and \u2016A\u2016 := \u221a tr(AA>), the Frobenius norm of A. 1 is the vector of all ones with appropriate dimensionality. A\u2020 is the Moore-Penrose pseudoinverse of A. ri(F) is the relative interior of the set F .\nData: We consider that we are given a set of n examples f1, \u00b7 \u00b7 \u00b7 , fn \u2208 F which may be represented as a single matrix F = [f1, \u00b7 \u00b7 \u00b7 , fn]> \u2208 Fn. In the following, we consider that F \u2286 Rd, and thus Fn \u2286 Rn\u00d7d.\nBregman divergence: Any Bregman divergence d\u03c6 : F \u00d7 ri(F)\u2192 [0,+\u221e) is defined as d\u03c6(x,y) = \u03c6(x)\u2212 \u03c6(y)\u2212 \u3008x \u2212 y,\u2207\u03c6(y)\u3009 where \u03c6 : F \u2192 R is a continuouslydifferentiable and strictly convex function, \u2207\u03c6(y) \u2208 Rd represents the gradient vector of \u03c6 at y. The most commonly used Bregman divergences for clustering are the squared Euclidean distance defined with \u03c6(x) = \u2016x\u201622, the KL-divergence (Dhillon et al., 2003) or the Itakura-Saito distance (Buzo et al., 1980). We refer the reader to (Banerjee et al., 2005; Nielsen & Nock, 2009) for details.\nClustering: Partitioning the n observations in F = [f1, \u00b7 \u00b7 \u00b7 , fn]> \u2208 Fn into k clusters is equivalent to determining an assignment matrix Y\u0302 \u2208 {0, 1}n\u00d7k such that Y\u0302ic = 1 if fi is assigned to cluster c and 0 otherwise. In this paper, we assume that each example is assigned to one and only one cluster (i.e. the sum of each row of Y\u0302 is 1) and that there is no empty cluster (which corresponds to adding the constraint rank(Y\u0302 ) = k). Therefore, Y\u0302 is in the following set of assignment matrices:\nYn\u00d7k := {Y\u0302 \u2208 {0, 1}n\u00d7k : Y\u0302 1 = 1, rank(Y\u0302 ) = k}\nWe assume as in (Banerjee et al., 2005) that each cluster c is represented by a single representative vector zc \u2208 ri(F) and that an example fi is assigned to cluster c only if \u2200b 6= c, d\u03c6(fi, zc) \u2264 d\u03c6(fi, zb) where d\u03c6 is the chosen Bregman divergence. To simplify the notations, we consider that zc \u2208 F , and we concatenate the k representative vectors into a single matrix Z = [z1, \u00b7 \u00b7 \u00b7 , zk]> \u2208 Fk. The problem of partitioning the n examples in F \u2208 Fn with d\u03c6 can then be formulated as minimizing the energy function:\nmin Y\u0302 \u2208Yn\u00d7k,Z\u2208Fk n\u2211 i=1 k\u2211 c=1 Y\u0302ic \u00b7 d\u03c6(fi, zc) (1)\n= min Y\u0302 \u2208Yn\u00d7k,Z\u2208Fk\n1>\u03a6(F )\u22121>\u03a6(Y\u0302 Z)\u2212\u3008F\u2212Y\u0302 Z,\u2207\u03a6(Y\u0302 Z)\u3009\nwhere we have used the definition of Bregman divergences and we note: \u2200A = [a1, \u00b7 \u00b7 \u00b7 ,an]> \u2208 Fn, \u03a6(A) = [\u03c6(a1), \u00b7 \u00b7 \u00b7 , \u03c6(an)]> \u2208 Rn is the concatenation into a single vector of the different \u03c6(ai) \u2208 R, and \u2207\u03a6(A) = [\u2207\u03c6(a1), \u00b7 \u00b7 \u00b7 ,\u2207\u03c6(an)]> \u2208 Rn\u00d7d is the concatenation into a single matrix of the different gradients\u2207\u03c6(ai) \u2208 Rd.\nBanerjee et al. (2005) demonstrated that, for any value of Y\u0302 \u2208 Yn\u00d7k, the minimizer of zc in Eq. (1) is unique and is the mean vector of all the examples in F assigned to cluster c if and only if d\u03c6 is a Bregman divergence. In matrix form, this means that Z = (Y\u0302 >Y\u0302 )\u22121Y\u0302 >F = Y\u0302 \u2020F\nis the unique global minimizer of Z in Eq. (1). We then define the set P = {Y\u0302 Y\u0302 \u2020 : Y\u0302 \u2208 Yn\u00d7k} and rewrite Eq. (1) as a minimization problem w.r.t. one variable:\nmin C\u0302\u2208P\n1>\u03a6(F )\u2212 1>\u03a6(C\u0302F )\u2212 \u3008F \u2212 C\u0302F,\u2207\u03a6(C\u0302F )\u3009 (2)\nAs an illustration, if d\u03c6 is the squared Euclidean distance, then Eq. (2) reduces to: \u2016F\u20162 \u2212 \u2016C\u0302F\u20162 \u2212 \u3008F \u2212 C\u0302F, 2C\u0302F \u3009 = \u2016F\u20162 +\u2016C\u0302F\u20162\u22122\u3008F, C\u0302F \u3009 = \u2016F \u2212 C\u0302F\u20162. We then obtain the formulation mentioned in (Lajugie et al., 2014)[Section 2.2] of the usual kmeans algorithm:\nmin C\u0302\u2208P \u2016F \u2212 C\u0302F\u20162 \u21d4 max C\u0302\u2208P \u3008C\u0302, FF>\u3009 (3)\nby using the fact that all the matrices in P are orthogonal projection matrices (i.e. symmetric and idempotent): we then have \u2200C\u0302 \u2208 P, \u2016C\u0302F\u20162 = tr(C\u03022FF>) = tr(C\u0302FF>)."}, {"heading": "3. Deep Spectral Clustering Learning", "text": "In this section, we introduce our method that we call Deep Spectral Clustering Learning (DSCL). We first relax the clustering problem in Eq. (2) and consider the set of solutions of the relaxed problem as the prediction function of our model. Then, we present our large margin supervised clustering problem and its efficient solver. Finally, we explain the connection of our approach with spectral clustering learning."}, {"heading": "3.1. Constraint Relaxation", "text": "Optimizing Eq. (2) is a NP-hard problem (Aloise et al., 2009), we then approximate it. Following (Shi & Malik, 2000; Zha et al., 2001; Ng et al., 2002; Peng & Wei, 2007), we now present a spectral relaxation of this problem. In particular, we extend the domain of Eq. (2) so that the set of solutions of the resulting problem can be formulated in a convenient way.\nWe propose to relax the constraint C\u0302 \u2208 P in Eq. (2) with the constraint C\u0302 \u2208 Cn,k where the set Cn,k includes P and is defined as the set of n \u00d7 n rank-k orthogonal projection matrices Cn,k := {C\u0302 \u2208 Rn\u00d7n : C\u03022 = C\u0302, C\u0302> = C\u0302, rank(C\u0302) = k}. The set of solutions of the resulting relaxed version of Eq. (2) can then be formulated:\nf(F ) := arg max C\u0302\u2208Cn,k\n1>\u03a6(C\u0302F ) + \u3008F \u2212 C\u0302F,\u2207\u03a6(C\u0302F )\u3009 (4)\nwhere the terms that do not depend on C\u0302 are omitted.\nThe solutions in Eq. (4) can be found in closed-form. Let us note s = rank(F ), we show in the supplementary material that if s \u2264 k, then the set of solutions of Eq. (4) is f(F ) = {C\u0302 \u2208 Cn,k : C\u0302F = F} = {C\u0302 \u2208 Cn,k : C\u0302 = FF \u2020 +\nV V >, V V > \u2208 Cn,(k\u2212s), V V >F = 0}. In particular, if s = k, then V V > = 0 and we have f(F ) = {FF \u2020}.\nIn the following, we consider only the case where the dimensionality d of training examples is not greater than k so that the property s \u2264 k is satisfied. Nonetheless, if s > k, the set of solutions can be considered as f(F ) = {FF \u2020}."}, {"heading": "3.2. Structured output prediction", "text": "In the following, we consider that we are given n examples xi \u2208 X (e.g. n images) and an embedding function \u03d5\u03b8 : X \u2192 F whose set of parameters is \u03b8 and such that \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, \u03d5\u03b8(xi) = fi (e.g. fi can be the output of a convolutional neural network). All these representations are concatenated into a single matrix F = [f1, \u00b7 \u00b7 \u00b7 , fn]> \u2208 Fn. We are also given a ground truth assignment matrix Y \u2208 Yn\u00d7k which indicates the desired partition for the n examples. In other words, letC = Y Y \u2020 \u2208 P be the ground truth clustering matrix of F , we would like to learn the embedding function \u03d5\u03b8 so that the matrix predicted with the (relaxed) clustering problem f(F ) \u2286 Cn,k in Eq. (4) is as close as possible to the ground truth clustering matrix C.\nDifferent evaluation metrics such as purity, rand index and normalized mutual information exist to evaluate clustering (see (Manning et al., 2008) [Chapter 16.3] for details). As in (Hubert & Arabie, 1985; Bach & Jordan, 2004; Lajugie et al., 2014; Law et al., 2016), we choose the Frobenius norm which has many advantages. As explained in (Lajugie et al., 2014)[Section 3.2], unlike rand index, the Frobenius norm between clustering (orthogonal projection) matrices takes into account the size of the different clusters (i.e. its value is not dominated by the performance on the largest clusters) and thus optimizes intra-class variance that is a rescaled indicator."}, {"heading": "3.2.1. PROBLEM FORMULATION", "text": "In the following, we consider that the matrix F is a variable (that depends on \u03d5\u03b8). The problem of minimizing the discrepancy between the ground truth clustering C \u2208 P of the matrix F and the (relaxed) clustering C\u0302 predicted with f(F ) in Eq. (4) can be formulated as the following empirical risk minimization problem:\nmin F\u2208Fn max C\u0302\u2208f(F )\n\u2016C \u2212 C\u0302\u20162 (5)\nAs all the matrices in f(F ) have the same rank, we can rewrite Eq. (5): \u2016C \u2212 C\u0302\u20162 = \u2016C\u20162 + \u2016C\u0302\u20162 \u2212 2 tr(CC\u0302) where \u2016C\u0302\u20162 = tr(C\u0302) = rank(C\u0302) = k is a constant. Eq. (5) is then equivalent to the problem:\nmax F\u2208Fn min C\u0302\u2208f(F ) tr(CC\u0302) (6)\nAlgorithm 1 Deep Spectral Clustering Learning (DSCL) input : Set of training examples (e.g. images) in X , embedding function \u03d5\u03b8 , number of iterations \u03c4 , learning rates \u03b71, \u00b7 \u00b7 \u00b7 , \u03b7\u03c4 1: for iteration t = 1 to \u03c4 do 2: Randomly sample n training examples x1, \u00b7 \u00b7 \u00b7 ,xn \u2208 X 3: Create representation matrix F \u2190 [f1, \u00b7 \u00b7 \u00b7 , fn]> \u2208 Fn s.t. \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, fi = \u03d5\u03b8(xi) 4: Create rescaled gradientG\u2190 (I\u2212FF \u2020)C(F \u2020)> (see Eq. (9)) whereC = Y Y \u2020 is the desired partition matrix of the n examples 5: Update the set of parameters \u03b8 of \u03d5\u03b8 by exploiting the rescaled gradient G and perform gradient step with learning rate \u03b7t 6: end for\nwhich is naturally lower bounded by (see details in supplementary material):\nmax F\u2208Fn min C\u0302\u2208f(F ) tr(CC\u0302FF \u2020) = max F\u2208Fn tr(CFF \u2020) (7)\nwhere we use the fact that \u2200C\u0302 \u2208 f(F ), C\u0302FF \u2020 = FF \u2020. We note that Eq. (7) equals Eq. (6) if rank(F ) = k.\nThe difficulty of optimizing the problem in Eq. (7) is that it depends on both F and F \u2020. If we assume that rank(F ) is a constant (with F not necessarily full rank) in Eq. (7), then the problem is differentiable (Golub & Pereyra, 1973) and the gradient of Eq. (7) w.r.t. F is:\n\u2207F = 2(I \u2212 FF \u2020)C(F \u2020)> (8)\nwhere I is the identity matrix. Details on the gradient can be found in the supplementary material. Our matrix F is always full rank in our experiments1, so the constant rank condition along the iterations is satisfied."}, {"heading": "3.2.2. LOW ALGORITHMIC COMPLEXITY", "text": "We now show that in addition to having a closed-form expression, computing our gradient\u2207F is efficient: the complexity to compute it is linear in n and quadratic in d.\nWe note C = Y Y \u2020 \u2208 P the ground truth partition matrix where the matrix Y \u2208 Yn\u00d7k is given: let yc be the c-th column of Y , then the c-th column of (Y \u2020)> can be written 1 max{1,y>c 1}\nyc. The complexity to compute (Y \u2020)> is linear in n due to the sparsity of Y . (Y \u2020)> is also sparse (i.e. it contains n nonzero elements). We can then write \u2207F2 as:\nG := \u2207F 2\n= ( Y \u2212 F [F \u2020Y ] ) [F \u2020(Y \u2020)>]> (9)\nwhere [\u00b7] indicates d\u00d7k matrices which are computed efficiently due to the sparsity of Y . The complexity to compute F \u2020 is O(ndmin{n, d}) (i.e. O(nd2) as we assume d \u2264 n)."}, {"heading": "3.2.3. DIRECT LOSS MINIMIZATION", "text": "Our method computes in closed-form the gradient of the structured output prediction problem in Eq. (5) when rank(F ) = k, and its algorithmic complexity is low.\n1In our experiments, since the number of rows of F is greater than its number of colums, F has full column rank.\nTo the best of our knowledge, although we exploit classic spectral relaxation results, our method is the first approach that includes the closed-form solution of the relaxed kmeans problem (see Section 3.1) within a large margin method for structured output. Even Mahalanobis metric learning methods (Lajugie et al., 2014) cannot use such a simplification due to the nature of their model, and the formulation of their subgradient is then different.\nIncluding the closed-form solution of the relaxed kmeans algorithm results in a simple problem formulation (see Eq. (7)). The resulting large margin optimization problem is easy to optimize as we do not have to perform loss-augmented inference during training."}, {"heading": "3.3. Learning deep models", "text": "Our method can be used to learn neural networks with conventional gradient-based methods by exploiting chain rule. Our approach is illustrated in Algorithm 1.\nIn detail, we note a mini-batch matrix F = [f1, \u00b7 \u00b7 \u00b7 , fn]> \u2208 Fn the concatenation into a single matrix of the n different embedding representations \u03d5\u03b8(xi) = fi, where, for instance, \u03d5\u03b8 : X \u2192 F is a neural network embedding function and xi \u2208 X is an image. We can rewrite Eq. (7) as a minimization problem by defining our loss function as a function of the mini-batch representation matrix:\nLoss(F ) := k \u2212 tr(CFF \u2020) (10)\nwhich is nonnegative and where C = Y Y \u2020 \u2208 P is the ground truth clustering matrix of F . The neural network \u03d5\u03b8 is then learned via backpropagation as illustrated in Algorithm 1 (i.e. using stochastic gradient descent with rescaled gradient \u2212G where G is defined in Eq. (9)).\nWe note that the structure of the learned neural network is not limited to the case where F is Rd. For instance, if the goal is to partition probability distributions with KL-divergence, the set F can be constrained to be the ddimensional simplex {x \u2208 [0, 1]d : x>1 = 1} by using a softmax regression at the last layer of the neural network.\nRegression problem: It is worth noting that\u2207F is also the gradient w.r.t. F of the nonlinear least squares problem:\nmax F\u2208Fn \u22121 2 \u2016FF \u2020 \u2212 C\u20162 (11)\nwhere we assume that the rank of F is constant (otherwise, the problem is not differentiable). Our solver can then be seen as a gradient-based solver for the nonlinear regression problem that iteratively decreases the distance \u2016FF \u2020\u2212C\u2016. In particular, the spectral relaxation proposed in Section 3.1 allows to write the set of predicted clusterings f(F ) as a function of FF \u2020, and the choice of the Frobenius norm to compare clusterings makes our problem similar to a least squares problem.\nGiven the least squares formulation of Eq. (11), one can see that our problem focuses more on the similarity between the matrices C and FF \u2020 than between C and the individual matrix F . Our problem can then be seen as a spectral clustering algorithm as explained in the following."}, {"heading": "3.4. Connection with spectral clustering", "text": "We now explain how our proposed method can be seen as learning spectral clustering in a supervised way.\nAs explained in (Bach & Jordan, 2004; Von Luxburg, 2007), spectral clustering does not refer to one particular method but to a family of methods (e.g. (Shi & Malik, 2000)) that partition a dataset by exploiting the leading eigenvectors of a similarity matrix. They rely on the eigenstructure of a similarity matrix to partition examples into disjoint clusters, with examples in the same cluster having high similarity and examples in different clusters having low similarity. In our case, as can be seen in Eq. (7), the (kernel) similarity matrix is K = FF \u2020 = UU> where U \u2208 Rn\u00d7s is a matrix whose columns are the left-singular vectors of F corresponding to its nonzero singular values and where s = rank(F ). By definition, these left-singular vectors of F are also the s leading eigenvectors of K.\nIt is worth noting that our approach is different from classic spectral clustering approaches that perform clustering by exploiting some Laplacian matrix of the similarity matrix. Our approach directly performs clustering by exploiting the leading eigenvectors of the similarity matrix K.\nBy increasing the value tr(CFF \u2020) = tr(CUU>) at each backpropagation iteration, the leading eigenvectors of UU> = K become more similar to the leading eigenvectors of C. As C and K = UU> are both orthogonal projection matrices, in the ideal case, tr(CUU>) is maximized when the column space of one of the two matrices C or (UU>) is included in the column space of the other matrix. In particular, if rank(C) = rank(UU>), then tr(CUU>) is maximized iff C = UU> (Fan, 1949). In this ideal case, we have arg maxC\u0302\u2208P\u3008C\u0302, UU>\u3009 = {C}, which corresponds to the solution of Eq. (3) when replacing F by U ; in other words, partitioning the rows of U with kmeans will return the desired clustering matrix C.\nIt is then clear that comparing the similarity between the\nrows of U (i.e. using spectral clustering) is at least as relevant as comparing the rows of F to partition the dataset. In our experiments, our method obtains better performance when the left-singular vectors of the test set matrix are used for partitioning rather than the learned features."}, {"heading": "4. Experiments", "text": "The goal of metric learning is to learn a metric that can be used to compare new examples, possibly from categories that were not in the training dataset. In this context, the model is learned on a training dataset and tested on a dataset that shares the same \u201csemantical\u201d metric and represents similar concepts. This allows to evaluate the generalization ability of the model. Following the experimental protocol described in (Song et al., 2016; 2017), we evaluate our method on the following fine-grained datasets and use the exact same train/test splits:\n\u2022 The Caltech-UCSD Birds (CUB-200-2001) dataset (Wah et al., 2011) is composed of 11,788 images of birds from 200 different species/categories. We split the first 100 categories for training (5,864 images) and the rest for test (5,924 images).\n\u2022 The CARS196 dataset (Krause et al., 2013) is composed of 16,185 images of cars from 196 model categories. The first 98 categories are used for training (8,054 images), the rest for test (8,131 images).\n\u2022 The Stanford Online Product (Song et al., 2016) dataset is composed of 120,253 images from 22,634 online product categories. It is partitioned into 59,551 images from 11,318 categories for training and 60,502 images from 11,316 categories for test.\nIn these experiments, the categories for training and the categories for testing are disjoint although they belong to the same context (i.e. they all represent birds, cars or products). This makes the problem challenging as deep models may overfit on the training categories2. In the same way as the baselines, we then perform early stopping."}, {"heading": "4.1. Implementation details", "text": "We closely follow the experimental setup described in (Song et al., 2016; 2017). In particular, we implemented our method with the Tensorflow package (Abadi et al., 2016) and used the Inception (Szegedy et al., 2015) network with batch normalization (Ioffe & Szegedy, 2015) pretrained on ImageNet/ILSVRC 2012-CLS (Russakovsky et al., 2015), we fine-tuned the network on the training datasets. We perform two types of fine-tuning:\n2For instance, our model obtains more than 90% performance for all the evaluation metrics on the training categories after 1000 iterations.\nAlgorithm 2 DSCL Normalized Spectral Clustering input : Test set Ft \u2208 Fnt , nt is the number of test examples 1: Create Mt \u2208 Rnt\u00d7d by mean centering Ft 2: Create r = rank(Mt) 3: Create Ut = [u1, \u00b7 \u00b7 \u00b7 ,unt ]> \u2208 Rnt\u00d7r s.t. MtM \u2020 t = UtU > t\n4: Create T = [t1, \u00b7 \u00b7 \u00b7 , tnt ]> \u2208 Rnt\u00d7r s.t. \u2200i, ti = 1\u2016ui\u20162ui 5: partition the rows of T into k clusters with kmeans\n\u2022 end-to-end: we fine-tune the model and update the parameters of all the layers of the neural network during backpropagation. In this case, we perform 100 iterations of gradient descent.\n\u2022 last layer: we freeze all the parameters of the neural network (pretrained on ImageNet) except those in the last layer. Only the parameters in the last layer of the model are updated. We perform 200 iterations of gradient descent.\nIn both cases, we remove the softmax function at the end of the last layer.\nThe images are first resized to square size (256\u00d7 256) and cropped at 227 \u00d7 227. For the dataset augmentation, we use a random horizontal mirroring for training and a single center crop for test. As in (Song et al., 2017) and unlike (Sohn, 2016), we use a single crop per image.\nWe ran our experiments on a single Tesla P100 GPU with 16GB RAM, and used a standard Stochastic Gradient optimizer. Our batch size is set to n = o\u00d7p = 18\u00d770 = 1260, our method backpropagates the loss in Eq. (10) for all the examples in the batch. As Inception is a large model and to fit into memory, we iteratively compute submatrices Fi \u2208 Fo and concatenate them into a single matrix F = [F>1 , \u00b7 \u00b7 \u00b7 , F>p ]> \u2208 Fn. Our method then computes the gradient matrix G = [G>1 , \u00b7 \u00b7 \u00b7 , G>p ]> \u2208 Rn\u00d7d defined in Eq. (9), and minimizes the loss function \u3008F,\u2212G\u3009 =\u2211p i=1\u3008Fi,\u2212Gi\u3009 with gradient descent where \u2212G is fixed.\nDifferent embedding dimensionality values d \u2208 {64, 128, 256, 512} are tested in (Song et al., 2016), it is reported that d does not play a crucial role. In our case, we then set our dimensionality d to be equal to the number of categories k for the Birds and Cars datasets.\nFor the Products dataset which contains more than 10k categories, we observed as in (Sohn, 2016) that the larger the value of d, the better the results. We then set d = 512 in order to be fair with the other models and randomly subsample 512 training categories."}, {"heading": "4.2. Partitioning a test dataset", "text": "Partitioning a test dataset Xt = [x1, \u00b7 \u00b7 \u00b7 ,xnt ]> \u2208 Xnt (where xi \u2208 X is an image in these experiments, and nt is the number of test examples) is done by first computing the test representation matrix Ft = [\u03d5\u03b8(x1), \u00b7 \u00b7 \u00b7 , \u03d5\u03b8(xnt)]> \u2208\nFnt where \u03d5\u03b8 is the learned embedding function, and then applying a partition algorithm. The partition algorithm can either be a standard clustering algorithm as described in (Banerjee et al., 2005), or can exploit the connection of our method with spectral clustering as explained in Section 3.4.\nThe spectral clustering (SC) algorithm that we use, which is inspired by (Ng et al., 2002), is illustrated in Algorithm 2: we first mean center Ft (the mean of each column of the resulting matrix Mt is zero), then we extract the matrix Ut that contains the leading left-singular vectors of the resulting matrix Mt, the rows of Ut are then `2-normalized and partitioned with the usual kmeans algorithm.\nTo test our method without spectral clustering, we `2- normalize the output representations as done in (Song et al., 2017) and then apply the usual kmeans algorithm with the squared Euclidean distance."}, {"heading": "4.3. Quantitative results", "text": "We compare our method to current state-of-the-art metric learning approaches in Tables 1 to 3 by evaluating the Normalized Mutual Information (NMI) and Recall@K performances. In particular, Recall@1 is a useful metric in zeroshot learning contexts, it allows to assign the category of a test image to the category of its nearest neighbor, it then evaluates the generalization performance of a model to new similar concepts. The scores for the following baselines (Schroff et al., 2015; Song et al., 2016; Sohn, 2016; Song et al., 2017) are reported from (Song et al., 2017) where the methods are tested in a similar setup. As explained in the related work (Section 5), the NMI-based approach (Song et al., 2017) is the only baseline that is explicitly learned to optimize a clustering criterion.\nWe also report the performance of the vanilla GoogLeNet/Inception features pretrained on ImageNet, and GoogLeNet features fine-tuned for classification with softmax regression. In both cases, we report the results obtained with Spectral Clustering (SC) as illustrated in Algorithm 2, and without SC (as explained in the last paragraph of Section 4.2). We report the scores for `2-normalized features as they obtain better performance than unnormalized features in our experiments.\nRecognition performance: Our spectral clustering approach obtains state-of-the-art Recall@K performance on all the datasets. Only the method in (Song et al., 2017) obtains better NMI performance on Birds and Products, this can be expected as the model in (Song et al., 2017) is specifically learned to optimize the NMI evaluation metric. Our choice to optimize the Frobenius norm which allows to compute a gradient in closed-form then seems to be a good trade-off for scalability as it obtains competitive NMI results and state-of-the-art Recall@K performance.\nThe usual kmeans algorithm applied on the representations of our learned model obtains worse results than our proposed spectral clustering (SC) approach. It obtains better performance than most baselines on Birds and Cars, but also poor results on the Products dataset. This may be explained by the fact that there are only 5.3 images per category in this dataset, which is 10 times less than for the other datasets. Some categories also contain only 2 images, which is hard to generalize on. Our approach is then more appropriate when the categories are large (i.e. more than 50 images) than when there are many (very) small clusters.\nWe also note that when our model updates only the last layer during fine-tuning, it obtains state-of-the-art performance on Birds and Cars, but not as good as our fully learned model on the Products dataset. The strong results on the former two is likely due to their small size (fewer than 10k training images); which makes learning all the layers prone to overfitting. Learning in all layers seems beneficial on larger datasets such as Products.\nWe observe that most baselines (Schroff et al., 2015; Sohn, 2016; Song et al., 2017) and our spectral method obtain comparable results on the Products dataset. There is then not a clear way to learn deep models in contexts with small categories. On the other hand, there is a huge gap in Recall@K performance on the other datasets between approaches that optimize clustering and approaches that do not. The largest performance gap is observed on the Cars dataset which contains the largest number of images per category (more than 80).\nIt is also worth noting that unlike usual softmax regression for classification that promotes centroids to be one-hot vectors in the ideal case, our approach takes as input the current representations and tries to group similar examples together without fixing the desired centroids. It can then be easily combined with other approaches.\nTraining time: Once the matrix representation of the minibatch F \u2208 Fn has been computed, computing the gradient G takes 1 second (with n = 1280 and d = 100). Since we backpropagate our loss for all the examples in the minibatch, each iteration takes about 50 seconds due to the large architecture of Inception. Nevertheless, multiple GPUs can be used in parallel to speed up training.\nQualitative results: t-SNE (Van Der Maaten, 2014) plots are available in the supplementary material."}, {"heading": "5. Related work", "text": "As explained in Section 1, many approaches have been proposed to learn a similarity metric (Xing et al., 2002; BarHillel et al., 2005; Lajugie et al., 2014; Law et al., 2016; 2017b) or a nonlinear embedding function (Schroff et al., 2015; Song et al., 2017) optimized to perform clustering. However, most of them belong to the semi-supervised setting (Xing et al., 2002; Bar-Hillel et al., 2005; Schroff et al., 2015), i.e. they are designed to learn from small sets of pairwise or triplet-wise relations and do not account for the global clustering performance on the training dataset. In pairwise approaches, the model is given pairs of examples (xi,xj) which are either similar (e.g. the examples are in the same category) or dissimilar (e.g. the examples are in different categories), the model is then learned so that the distances between representations of similar objects are smaller than the distances between dissimilar objects.\nIn the triplet-wise approach (Schultz & Joachims, 2003; Weinberger et al., 2006), triplets of examples (xi,x+i ,x \u2212 i ) are provided and the model is learned so that the distance between the representations of the pair (xi,x+i ) is smaller than for the pair (xi,x\u2212i ). We focus on the supervised clustering setting which considers all the possible similar and dissimilar pairs and takes into account the global clustering structure of the training dataset (here a mini-batch).\nIn the shallow metric learning literature, (Lajugie et al., 2014; Law et al., 2016) learn a linear transformation in the supervised clustering setting so that the partition obtained when using kmeans on a dataset is as close as possible to the desired partition. However, they both consider that the data representation is fixed and are limited by the complexity of their linear model. Moreover, their solvers are very different from ours. Lajugie et al. (2014) propose an extension of the structural SVM (Tsochantaridis et al., 2005) for Mahalanobis distances, and their projected subgradient method thus has high complexity. Law et al. (2016) propose a closed-form solver but the method is limited to the case where there is a single training dataset and they do not provide gradient-based strategies to optimize other types of models such as neural networks.\nIn the deep learning literature, Song et al. (2016) proposed to approximate a loss function that considers all the positive and negative pairs. To this end, they iteratively randomly sample a few similar pairs, and then actively add their difficult neighbors to the training mini-batch. This idea is similar to the idea of active set of constraints used in (Weinberger & Saul, 2009; Law et al., 2017a) to limit the number of active constraints (i.e. the number of constraints that have nonzero subgradients) and be able to optimize over large numbers of triplets. Although their approach considers most of the similarity relations, it is not optimized to group all the similar examples into the same unique cluster. Indeed, each category can be divided in multiple subclusters as explained in (Song et al., 2017).\nSohn (2016) proposed to tackle the problem of slow convergence of triplet-wise approaches (caused by hard negative mining) by optimizing losses over (n + 1)-tuplets. In particular, an efficient batch construction method is proposed to require 2n examples instead of the na\u0131\u0308ve (n+ 1)n to build n tuplets of length (n + 1). The loss function recruits multiple distances between dissimilar examples from different categories and approximates the ideal loss that minimizes the distances between similar examples while maximizing the distances between dissimilar examples. This approach considers the global structure of the representation of mini-batches better than triplet-wise approaches. However, although it does take into account most distances in the mini-batch, it does not explicitly optimize the model so that it obtains good clustering performance.\nIn the deep metric learning literature, the most similar approach to ours is (Song et al., 2017). They select for each category one unique example that will be the medoid (i.e. representative example). The choice of the medoids is not discussed and may be problematic if there is noise in the labels. In contrast, our centroids are learned so that they are the mean vectors of the training examples. Indeed, our set of centroids is written Z = Y\u0302 \u2020F where F \u2208 Fn is the representation of our mini-batch and Y\u0302 is implicitly included in the formulation of the set Cn,k. Our optimization problem then learns a data representation such that training examples are projected close to their respective centroids. Moreover, the gradient in (Song et al., 2017) requires an iterative greedy algorithm and each iteration of their greedy algorithm has higher complexity than the complexity of computing our gradient (we set the dimensionality d of our learned representations to be k). Each iteration of the greedy algorithm is linear in the size of the mini-batch and cubic in the number of categories in the mini-batch.\nDeep learning was also used in (Ionescu et al., 2015) to learn a convolutional neural network optimized for clustering. However, it is applied to unsupervised image segmentation with normalized cuts (Shi & Malik, 2000). We are interested in this paper in the supervised clustering setting where the ground truth partition is provided. Another deep clustering approach was proposed in (Hershey et al., 2016). However, their model is not optimized to be robust to the size of the different clusters as discussed in Section 3.2."}, {"heading": "6. Conclusion", "text": "We have presented a novel deep learning approach optimized for the supervised clustering task. Our method is simple to implement and scalable thanks to its low algorithmic complexity. It also obtains state-of-the-art recall@K performance on different standard fine-grained datasets. Future work includes improving our proposed solver by exploiting the results in (Ionescu et al., 2015) which take into account the structure of the neural network instead of using a standard stochastic gradient descent solver.\nAcknowledgments: We thank David Duvenaud, Stavros Tsogkas, Dimitris Vlitas and the anonymous reviewers for their helpful comments. This work was supported by Samsung and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government."}], "year": 2017, "references": [{"title": "Np-hardness of euclidean sum-of-squares clustering", "authors": ["Aloise", "Daniel", "Deshpande", "Amit", "Hansen", "Pierre", "Popat", "Preyas"], "venue": "Machine learning,", "year": 2009}, {"title": "Learning spectral clustering", "authors": ["Bach", "Francis", "Jordan", "Michael"], "venue": "NIPS, 16:305\u2013312,", "year": 2004}, {"title": "Clustering with bregman divergences", "authors": ["Banerjee", "Arindam", "Merugu", "Srujana", "Dhillon", "Inderjit S", "Ghosh", "Joydeep"], "venue": "Journal of machine learning research,", "year": 2005}, {"title": "Learning a Mahalanobis metric from equivalence constraints", "authors": ["Bar-Hillel", "Aharon", "Hertz", "Tomer", "Shental", "Noam", "Weinshall", "Daphna"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "Speech coding based upon vector quantization", "authors": ["Buzo", "Andr\u00e9s", "A Gray", "R Gray", "Markel", "John"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "year": 1980}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "authors": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "year": 2005}, {"title": "A divisive information-theoretic feature clustering algorithm for text classification", "authors": ["Dhillon", "Inderjit S", "Mallela", "Subramanyam", "Kumar", "Rahul"], "venue": "Journal of machine learning research,", "year": 2003}, {"title": "On a theorem of weyl concerning eigenvalues of linear transformations i", "authors": ["Fan", "Ky"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "year": 1949}, {"title": "The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate", "authors": ["Golub", "Gene H", "Pereyra", "Victor"], "venue": "SIAM Journal on numerical analysis,", "year": 1973}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "authors": ["Hershey", "John R", "Chen", "Zhuo", "Le Roux", "Jonathan", "Watanabe", "Shinji"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2016}, {"title": "Comparing partitions", "authors": ["Hubert", "Lawrence", "Arabie", "Phipps"], "venue": "Journal of classification,", "year": 1985}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "year": 2015}, {"title": "Matrix backpropagation for deep networks with structured layers", "authors": ["Ionescu", "Catalin", "Vantzos", "Orestis", "Sminchisescu", "Cristian"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "year": 2015}, {"title": "3d object representations for fine-grained categorization", "authors": ["Krause", "Jonathan", "Stark", "Michael", "Deng", "Jia", "Fei-Fei", "Li"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "year": 2013}, {"title": "Large margin metric learning for constrained partitioning problems", "authors": ["R. Lajugie", "F. Bach", "S. Arlot"], "venue": "Proc. International Conference on Machine Learning,", "year": 2014}, {"title": "Closed-form training of mahalanobis distance for supervised clustering", "authors": ["Law", "Marc Teva", "Yu", "Yaoliang", "Cord", "Matthieu", "Xing", "Eric Poe"], "venue": "In CVPR", "year": 2016}, {"title": "Learning a distance metric from relative comparisons between quadruplets of images", "authors": ["Law", "Marc Teva", "Thome", "Nicolas", "Cord", "Matthieu"], "year": 2017}, {"title": "Efficient multiple instance metric learning using weakly supervised data", "authors": ["Law", "Marc Teva", "Yu", "Yaoliang", "Urtasun", "Raquel", "Zemel", "Richard", "Xing", "Eric Poe"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "year": 2017}, {"title": "Introduction to information retrieval, volume 1", "authors": ["Manning", "Christopher D", "Raghavan", "Prabhakar", "Sch\u00fctze", "Hinrich"], "year": 2008}, {"title": "On spectral clustering: Analysis and an algorithm", "authors": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In NIPS,", "year": 2002}, {"title": "Sided and symmetrized bregman centroids", "authors": ["Nielsen", "Frank", "Nock", "Richard"], "venue": "IEEE transactions on Information Theory,", "year": 2009}, {"title": "Approximating k-means-type clustering via semidefinite programming", "authors": ["J. Peng", "Y. Wei"], "venue": "SIAM Journal on Optimization,", "year": 2007}, {"title": "Facenet: A unified embedding for face recognition and clustering", "authors": ["Schroff", "Florian", "Kalenichenko", "Dmitry", "Philbin", "James"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2015}, {"title": "Learning a distance metric from relative comparisons", "authors": ["Schultz", "Matthew", "Joachims", "Thorsten"], "venue": "In NIPS,", "year": 2003}, {"title": "Normalized cuts and image segmentation", "authors": ["Shi", "Jianbo", "Malik", "Jitendra"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "year": 2000}, {"title": "Improved deep metric learning with multiclass n-pair loss objective", "authors": ["Sohn", "Kihyuk"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Deep metric learning via lifted structured feature embedding", "authors": ["Song", "Hyun Oh", "Xiang", "Yu", "Jegelka", "Stefanie", "Savarese", "Silvio"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Deep metric learning via facility location", "authors": ["Song", "Hyun Oh", "Jegelka", "Stefanie", "Rathod", "Vivek", "Murphy", "Kevin"], "venue": "In Computer Vision and Pattern Recognition", "year": 2017}, {"title": "Large margin methods for structured and interdependent output variables", "authors": ["Tsochantaridis", "Ioannis", "Joachims", "Thorsten", "Hofmann", "Thomas", "Altun", "Yasemin"], "venue": "Journal of machine learning research,", "year": 2005}, {"title": "Accelerating t-sne using treebased algorithms", "authors": ["Van Der Maaten", "Laurens"], "venue": "Journal of machine learning research,", "year": 2014}, {"title": "A tutorial on spectral clustering", "authors": ["Von Luxburg", "Ulrike"], "venue": "Statistics and computing,", "year": 2007}, {"title": "The caltech-ucsd birds-2002011", "authors": ["Wah", "Catherine", "Branson", "Steve", "Welinder", "Peter", "Perona", "Pietro", "Belongie", "Serge"], "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "authors": ["Weinberger", "Kilian Q", "Saul", "Lawrence K"], "venue": "JMLR, 10:207\u2013244,", "year": 2009}, {"title": "Distance metric learning for large margin nearest neighbor classification", "authors": ["Weinberger", "Kilian Q", "Blitzer", "John", "Saul", "Lawrence"], "venue": "Advances in neural information processing systems,", "year": 2006}, {"title": "Distance metric learning with application to clustering with side-information", "authors": ["Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart", "Ng", "Andrew Y"], "venue": "In NIPS, pp", "year": 2002}, {"title": "Spectral relaxation for k-means clustering", "authors": ["Zha", "Hongyuan", "He", "Xiaofeng", "Ding", "Chris", "Gu", "Ming", "Simon", "Horst D"], "venue": "In NIPS, pp", "year": 2001}], "id": "SP:d592ad48973b2bb0e268d3b404fb7262d56587b1", "authors": [{"name": "Marc T. Law", "affiliations": []}, {"name": "Raquel Urtasun", "affiliations": []}, {"name": "Richard S. Zemel", "affiliations": []}], "abstractText": "Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall@K performance.", "title": "Deep Spectral Clustering Learning"}