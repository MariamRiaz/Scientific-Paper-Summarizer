0	99	In practice one often encounters multi-class classification problem with a large number of classes.
1	18	For example, applications in image classification (Russakovsky et al., 2015) and language modeling (Mikolov et al., 2010) usually have tens to hundreds of thousands of classes.
2	20	Under such cases, training the standard softmax logistic or one-against-all models becomes impractical.
6	81	NCE reduces the problem of multi-class classification to binary classification problem, which discriminates between a target class distribution and a noise distribution and a few noise classes are sampled as a representation of the entire noise space.
8	42	For example, a power-raised unigram distribution has been shown to be effective in language models (Mikolov et al., 2013; Ji et al., 2015; Mnih & Teh, 2012).
16	18	In these methods, a tree structure is defined over the classes which are treated as leaves.
19	25	Then, the multi-class classification problem is reduced to solving a number of small local models defined by a tree, which typically admits a logarithmic complexity on the total number of classes.
24	18	Our approach is complementary to these tree classifiers, because we study the orthogonal issue of consistent class sampling, which in principle can be combined with many of these tree methods.
28	56	Given a data point x (without observing y), we select a small number of competitive candidates as a set Cx.
29	29	Then, we sample the remaining classes, which are treated as noises, to represent the entire noise space in the large normalization factor.
31	15	We show that CANE is consistent and its computation using stochastic gradient method is independent of the class size K. Moreover, the statistical variance of the CANE estimator can approach that of the maximum likelihood estimator (MLE) of the softmax logistic regression when Cx can cover the target class y with high probability.
33	17	We then describe two concrete algorithms: the first one is a generic stochastic optimization procedure for CANE; the second one employs a tree structure with leaves as classes to enable fast beam search for candidate selection.
41	23	Therefore, we propose to find a small subset of classes as a candidate set Cx ⊂ {1, · · · ,K} and treat the classes outside Cx as noises, so that we can focus on the small set Cx instead of the entire K classes.
45	32	That is, we replace the partial summation ∑ j∈Nx e sj(x,θ) in the denominator of Eq.
47	35	Thus, the denominator ∑K k′=1 e sk′ (x,θ) will be approximated as ∑ k′∈Cx e sk′ (x,θ) + esj(x,θ)/qx(j).
50	18	(5) consists of two summations over both the data points and the classes in the noise set Nx.
73	15	Under the same assumption used in Theorem 2, as n → ∞, √ n(θ̂ − θ∗) follows the asymptotic normal distribution: √ n(θ̂ − θ∗) d−→ N(0, [Ex∇M∇>]−1), (6) where M = ∑ j∈Nx qx(j) [ diag (uj)− 1 p(K,x) + ∑ k∈Cx p(k,x) + p(j,x) qx(j) uju > j ] , uj = ( p(i1,x), · · · , p(i|Cx|,x)︸ ︷︷ ︸ The candidate part , 0, · · · , p(j,x)/qx(j), · · · , 0︸ ︷︷ ︸ The noise part )> , for j = j1, · · · , j|Nx|, ∇ = diag ([ ∇θsi1 (x,θ), · · · ,∇θsi|Cx| (x,θ),∇θsj1 (x,θ), · · · ,∇θsj|Nx| (x,θ) ]>) .
75	30	As we will see in the next corollary, if one can successfully choose the candidate set Cx so that it covers the observed label y with high probability, then the difference between the statistical variance of CANE and that of Eq.
90	15	3: Initialize θ; 4: for every sampled example do 5: Receive example (x, y); 6: Find the candidate set Cx; 7: if y ∈ Cx then 8: Sample Nn noises outside Cx according to q and denote the selected noise set as Tx; 9: θ ← θ + η∇θR̂ with∇θR̂ given by ∇θsy(x,θ)− 1 |Tx| ∑ j∈Tx (7) ∑k′∈Cx esk′ (x,θ)∇θsk′(x,θ) + esj(x,θ)qx(j) ∇θsj(x,θ)∑ k′∈Cx e sk′ (x,θ) + e sj(x,θ) qx(j)  ; 10: else 11: θ ← θ + η∇θR̂ with∇θR̂ given by ∇θsy(x,θ)−∑ k′∈Cx e sk′ (x,θ)∇θsk′(x,θ) + e sy(x,θ) qx(y) ∇θsy(x,θ)∑ k′∈Cx e sk′ (x,θ) + e sy(x,θ) qx(y) ; (8) 12: end if 13: end for with gradient∇θR̂ given by Eq.
94	18	An attractive strategy is to use a tree defined on the classes, because one can perform fast heuristic search algorithms based on a tree structure to prune the uncompetitive classes.
98	50	1 uses a binary tree over K = 8 labels as example while any tree structure can be used for selecting Cx.
131	18	Therefore, Algorithm 2 has a complexity of O((2Nc +Nn) logbK) which is logarithmic with respect to K. The term logbK is from the tree structure used in this specific candidate selection method, so it does not conflict with the complexity of the general Algorithm 1, which is independent of K. Another advantage of the Beam Tree algorithm is that it allows fast predictions and can naturally output the top-J predictions using beam search.
141	36	At each iteration of Algorithm 2, we route the example by selecting the subset with the largest score (in place of beam search) and then sample the candidates from the subset according to some distribution.
158	15	We evaluate the CANE method in various applications in this section, including both multi-class classification problems and neural language modeling.
160	22	The competitors include the standard softmax, the NCE (Mnih & Kavukcuoglu, 2013; Mnih & Teh, 2012), the BlackOut (Ji et al., 2015), the hierarchical softmax (HSM) (Morin & Bengio, 2005), the Filter Tree (Beygelzimer et al., 2009) implemented in Vowpal-Wabbit (VW, a learning platform)2, the LOMTree (Choromanska & Langford, 2015) in VW and the Recall Tree (Daume III et al., 2017) in VW.
207	38	Generally, when the number of selected candidates / noises decrease, the test perplexities of all the methods increase on both datasets, while the performance degradation of CANE is not obvious.
210	51	We showed that CANE is consistent and the computation using SGD is always efficient (that is, independent of the class size K).
211	50	Moreover, the new estimator has low statistical variance approaching that of the softmax logistic regression, if the observed class label belongs to the candidate set with high probability.
212	16	Empirical results demonstrated that CANE is effective for speeding up both training and prediction in multi-class classification problems and CANE is effective in neural language modeling.
213	51	We note that this work employs a fixed distribution (i.e., the uniform distribution) to sample noises in CANE.
