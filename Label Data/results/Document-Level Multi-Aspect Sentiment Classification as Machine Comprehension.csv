0,1,label2,summary_sentences
"People vary widely both in their linguistic preferences when producing language and in their ability to understand specific natural-language expressions, depending on what they know about the domain, their age and cognitive capacity, and many other factors.",1 Introduction,[0],[0]
"It has long been recognized that effective NLG systems should therefore adapt to the current user, in order to generate language which works well for them.",1 Introduction,[0],[0]
"This adaptation needs to address all levels of the NLG pipeline, including discourse planning (Paris, 1988), sentence planning (Walker et al., 2007), and RE generation (Janarthanam and Lemon, 2014), and depends on many features of the user, including level of expertise and language proficiency, age, and gender.
",1 Introduction,[0],[0]
Existing techniques for adapting the output of an NLG system have shortcomings which limit their practical usefulness.,1 Introduction,[0],[0]
"Some systems need user-specific information in training (Ferreira and Paraboni, 2014) and therefore cannot generalize to unseen users.",1 Introduction,[0],[0]
"Other systems assume that each user in the training data is annotated with their group, which allows them to learn a model from the data of each group.",1 Introduction,[0],[0]
"However, hand-designed user groups
may not reflect the true variability of the data, and may therefore inhibit the system’s ability to flexibly adapt to new users.
",1 Introduction,[0],[0]
"In this paper, we present a user adaptation model for NLG systems which induces user groups from training data in which these groups were not annotated.",1 Introduction,[0],[0]
"At training time, we probabilistically assign users to groups and learn the language preferences for each group.",1 Introduction,[0],[0]
"At evaluation time, we assume that our system has a chance to interact with each new user repeatedly – e.g., in the context of a dialogue system.",1 Introduction,[0],[0]
"It will then calculate an increasingly accurate estimate of the user’s group membership based on observable behavior, and use it to generate utterances that are suitable to the user’s true group.
",1 Introduction,[0],[0]
We evaluate our model on two tasks involving the generation of referring expressions (RE).,1 Introduction,[0],[0]
"First, we predict the use of spatial relations in humanlike REs in the GRE3D domain (Viethen and Dale, 2010) using a log-linear production model in the spirit of Ferreira and Paraboni (2014).",1 Introduction,[0],[0]
"Second, we predict the comprehension of generated REs, in a synthetic dataset based on data from the GIVE Challenge domain (Striegnitz et al., 2011) with the log-linear comprehension model of Engonopoulos et al. (2013).",1 Introduction,[0],[0]
"In both cases, we show that our model discovers user groups in the training data and infers the group of unseen users with high confidence after only a few interactions during testing.",1 Introduction,[0],[0]
"In the GRE3D domain, our system outperformed a strong baseline which used demographic information for the users.",1 Introduction,[0],[0]
Differences between individual users have a substantial impact on language comprehension.,2 Related Work,[0],[0]
"Factors that play a role include level of expertise and spatial ability (Benyon and Murray, 1993); age (Häuser et al., 2017); gender (Dräger and Koller,
ar X
iv :1
80 6.
",2 Related Work,[0],[0]
"05 94
7v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
5 Ju
n 20
18
2012); or language proficiency (Koller et al., 2010).
",2 Related Work,[0],[0]
Individual differences are also reflected in the way people produce language.,2 Related Work,[0],[0]
"Viethen and Dale (2008) present a corpus study of human-produced REs (GRE3D3) for simple visual scenes, where they note two clearly distinguishable groups of speakers, one that always uses a spatial relation and one that never does.",2 Related Work,[0],[0]
Ferreira and Paraboni (2014) show that a model using speaker-specific information outperforms a generic model in predicting the attributes used by a speaker when producing an RE.,2 Related Work,[0],[0]
"However, their system needs to have seen the particular speaker in training, while our system can dynamically adapt to unseen users.",2 Related Work,[0],[0]
"Ferreira and Paraboni (2017) also demonstrate that splitting speakers in predefined groups and training each group separately improves the human likeness of REs compared to training individual user models.
",2 Related Work,[0],[0]
"The ability to adapt to the comprehension and production preferences of a user is especially important in the context of a dialog system, where there are multiple chances of interacting with the same user.",2 Related Work,[0],[0]
Some methods adapt to dialog system users by explicitly modeling the users’ knowledge state.,2 Related Work,[0],[0]
"An early example is Paris (1988); she selects a discourse plan for a user, depending on their level of domain knowledge ranging between novice and expert, but provides no mechanism for inferring the group to which the user belongs.",2 Related Work,[0],[0]
"Rosenblum and Moore (1993) try to infer what knowledge a user possesses during dialogue, based on the questions they ask.",2 Related Work,[0],[0]
Janarthanam and Lemon (2014) adapt to unseen users by using reinforcement learning with simulated users to make a system able to adjust to the level of the user’s knowledge.,2 Related Work,[0],[0]
"They use five predefined groups from which they generate the simulated users’ behavior, but do not assign real users to these groups.",2 Related Work,[0],[0]
"Our system makes no assumptions about the user’s knowledge and does not need to train with simulated users, or use any kind of information-seeking moves; we instead rely on the groups that are discovered in training and dynamically assign new, unseen users, based only on their observable behavior in the dialog.
",2 Related Work,[0],[0]
"Another example of a user-adapting dialog component is SPaRKy (Walker et al., 2007), a trainable sentence planner that can tailor sentence plans to individual users’ preferences.",2 Related Work,[0],[0]
"This requires training on separate data for each user; in contrast to this, we leverage the similarities between users and can take advantage of the full training data.",2 Related Work,[0],[0]
We start with a basic model of the way in which people produce and comprehend language.,3 Log-linear models for NLG in dialog,[0],[0]
"In order to generalize over production and comprehension, we will simply say that a human language user exhibits a certain behavior b among a range of possible behaviors, in response to a stimulus s.",3 Log-linear models for NLG in dialog,[0],[0]
"The behavior of a speaker is the utterance b they produce in order to achieve a communicative goal s; the behavior of a listener is the meaning b which they assign to the utterance s they hear.
",3 Log-linear models for NLG in dialog,[0],[0]
"Given this terminology, we define a basic loglinear model (Berger et al., 1996) of language use as follows:
P (b|s; ρ) = exp(ρ · φ(b, s))∑ b′",3 Log-linear models for NLG in dialog,[0],[0]
"exp(ρ · φ(b′, s))
",3 Log-linear models for NLG in dialog,[0],[0]
"(1)
where ρ is a real-valued parameter vector of length n and φ(b, s) is a vector of real-valued feature functions f1, ..., fn over behaviors and stimuli.",3 Log-linear models for NLG in dialog,[0],[0]
"The parameters can be trained by maximum-likelihood estimation from a corpus of observations (b, s).",3 Log-linear models for NLG in dialog,[0],[0]
"In addition to maximum-likelihood training it is possible to include some prior probability distribution, which expresses our belief about the probability of any parameter vector and which is generally used for regularization.",3 Log-linear models for NLG in dialog,[0],[0]
"The latter case is referred to as a posteriori training, which selects the value of ρ that maximizes the product of the parameter probability and the probability of the data.
",3 Log-linear models for NLG in dialog,[0],[0]
"In this paper, we focus on the use of such models in the context of the NLG module of a dialogue system, and more specifically on the generation of referring expressions (REs).",3 Log-linear models for NLG in dialog,[0],[0]
"Using (1) as a comprehension model, Engonopoulos et al. (2013) developed an RE generation model in which the stimulus s = (r, c) consists of an RE r and a visual context c of the GIVE Challenge (Striegnitz et al., 2011), as illustrated in Fig. 1.",3 Log-linear models for NLG in dialog,[0],[0]
The behavior is the object b in the visual scene to which the user will resolve the RE.,3 Log-linear models for NLG in dialog,[0],[0]
"Thus for instance, when we consider the RE r =“the blue button” in the context of Fig. 1, the log-linear model may assign a higher probability to the button on the right than to the one in the background.",3 Log-linear models for NLG in dialog,[0],[0]
"Engonopoulos and Koller (2014) develop an algorithm for generating the RE r which maximizes P (b∗|s; ρ), where b∗ is the intended referent in this setting.
",3 Log-linear models for NLG in dialog,[0],[0]
"Conversely, log-linear models can also be used to directly capture how a human speaker would refer to an object in a given scene.",3 Log-linear models for NLG in dialog,[0],[0]
"In this case, the stimulus s = (a, c) consists of the target object a and
the visual context c, and the behavior b is the RE.",3 Log-linear models for NLG in dialog,[0],[0]
"We follow Ferreira and Paraboni (2014) in training individual models for the different attributes which can be used in the RE (e.g., that a is a button; that it is blue; that the RE contains a binary relation such as “to the right of”), such that we can simply represent b as a binary choice b ∈ {1,−1} between whether a particular attribute should be used in the RE or not.",3 Log-linear models for NLG in dialog,[0],[0]
"We can then implement an analog of Ferreira’s model in terms of (1) by using feature functions φ(b, a, c) = b · φ′(a, c), where φ′(a, c) corresponds to their context features, which do not capture any speaker-specific information.",3 Log-linear models for NLG in dialog,[0],[0]
"As discussed above, a user-agnostic model such as (1) does not do justice to the variability of language comprehension and production across different speakers and listeners.",4 Log-linear models with user groups,[0],[0]
We will therefore extend it to a model which distinguishes different user groups.,4 Log-linear models with user groups,[0],[0]
We will not try to model why1 users behave differently.,4 Log-linear models with user groups,[0],[0]
"Instead our model sorts users into groups simply based on the way in which they respond to stimuli, in the sense of Section 3, and implements this by giving each group g its own parameter vector ρ(g).",4 Log-linear models with user groups,[0],[0]
"As a theoretical example, Group 1 might contain users who reliably comprehend REs which use colors (“the green button”), whereas Group 2 might contain users who more easily understand relational REs (“the button next to the lamp”).",4 Log-linear models with user groups,[0],[0]
"These groups are then discovered at training time.
",4 Log-linear models with user groups,[0],[0]
"When our trained NLG system starts interacting with an unseen user u, it will infer the group to which u belongs based on u’s observed responses to previous stimuli.",4 Log-linear models with user groups,[0],[0]
"Thus as the dialogue with u unfolds, the system will have an increasingly pre-
1E.g., in the sense of explicitly modeling sociolects or the difference between novice system users vs. experts.
cise estimate of the group to which u belongs, and will thus be able to generate language which is increasingly well-tailored to this particular user.",4 Log-linear models with user groups,[0],[0]
"We assume training data D = {(bi, si, ui)}i which contains stimuli si together with the behaviors bi which the users ui exhibited in response to si.",4.1 Generative story,[0],[0]
"We write D(u) = {(bu1 , su1), . . .",4.1 Generative story,[0],[0]
"(buN , suN )} for the data points for each user",4.1 Generative story,[0],[0]
"u.
",4.1 Generative story,[0],[0]
"The generative story we use is illustrated in Fig. 2; observable variables are shaded gray, unobserved variables and parameters to be set in training are shaded white and externally set hyperparameters have no circle around them.",4.1 Generative story,[0],[0]
"Arrows indicate which variables and parameters influence the probability distribution of other variables.
",4.1 Generative story,[0],[0]
"We assume that each user belongs to a group g ∈ {1, . . .",4.1 Generative story,[0],[0]
",K}, where the number K of groups is fixed beforehand based on, e.g., held out data.",4.1 Generative story,[0],[0]
"A group g is assigned to u at random from the distribution
P (g|π) = exp(πg)∑K g′=1 exp(πg′)
(2)
",4.1 Generative story,[0],[0]
"Here π ∈ RK is a vector of weights, which defines how probable each group is a-priori.
",4.1 Generative story,[0],[0]
"We replace the single parameter vector ρ of (1) with group-specific parameters vectors ρ(g), thus obtaining a potentially different log-linear model P ( b|s; ρ(g) ) for each group.",4.1 Generative story,[0],[0]
"After assigning a group, our model generates responses bu1 , . . .",4.1 Generative story,[0],[0]
", b u N at
random from P ( b|s; ρ(g) ) , based on the group specific parameter vector and the stimuli su1 , . . .",4.1 Generative story,[0],[0]
", s u N .",4.1 Generative story,[0],[0]
This accounts for the generation of the data.,4.1 Generative story,[0],[0]
"We model the parameter vectors π ∈ RK , and ρ(g) ∈",4.1 Generative story,[0],[0]
"Rn for every 1 ≤ g ≤ K as drawn from
P (D; θ) = ∏ u∈U K∑ g=1",4.1 Generative story,[0],[0]
"P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ) · N (π|0, σ(π)) · K∏",4.1 Generative story,[0],[0]
g=1 N,4.1 Generative story,[0],[0]
"( ρ(g)|0, σ(ρ) )",4.1 Generative story,[0],[0]
"(3) L(θ) =
∑ u∈U log K∑",4.1 Generative story,[0],[0]
g=1,4.1 Generative story,[0],[0]
P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ),4.1 Generative story,[0],[0]
"(4)
AL(θ) = ∑ u∈U K∑ g=1
P (g|D(u); θ(i−1)) ·",4.1 Generative story,[0],[0]
logP (g|π) +,4.1 Generative story,[0],[0]
"∑
d∈Du
logP ( bd|sd; ρ(g) )",4.1 Generative story,[0],[0]
"(5)
normal distributions N (0, σ(π)), and N (0, σ(ρ)), which are centered at 0 with externally given variances and no covariance between parameters.",4.1 Generative story,[0],[0]
This has the effect of making parameter choices close to zero more probable.,4.1 Generative story,[0],[0]
"Consequently, our models are unlikely to contain large weights for features that only occurred a few times or which are only helpful for a few examples.",4.1 Generative story,[0],[0]
"This should reduce the risk of overfitting the training set.
",4.1 Generative story,[0],[0]
The equation for the full probability of the data and a specific parameter setting is given in (3).,4.1 Generative story,[0],[0]
"The left bracket contains the likelihood of the data, while the right bracket contains the prior probability of the parameters.",4.1 Generative story,[0],[0]
Once we have set values θ =,4.2 Predicting user behavior,[0],[0]
"(π, ρ(1), . . .",4.2 Predicting user behavior,[0],[0]
", ρ(K))",4.2 Predicting user behavior,[0],[0]
"for all the parameters, we want to predict what behavior b a user u will exhibit in response to a stimulus s. If we encounter a completely new user u, the prior user group distribution from (2) gives the probability that this user belongs to each group.",4.2 Predicting user behavior,[0],[0]
"We combine this with the group-specific log-linear behavior models to obtain the distribution:
P (b|s; θ) = K∑ g=1 P ( b|s; ρ(g) )",4.2 Predicting user behavior,[0],[0]
"· P (g|π) (6)
",4.2 Predicting user behavior,[0],[0]
"Thus, we have a group-aware replacement for (1).",4.2 Predicting user behavior,[0],[0]
"Furthermore, in the interactive setting of a dialogue system, we may have multiple opportunities to interact with the same user u. We can then develop a more precise estimate of u’s group based on their responses to previous stimuli.",4.2 Predicting user behavior,[0],[0]
Say that we have made the previous observations D(u) =,4.2 Predicting user behavior,[0],[0]
"{〈s1, b1〉, . . .",4.2 Predicting user behavior,[0],[0]
", 〈sN , bN 〉} for user u.",4.2 Predicting user behavior,[0],[0]
"Then we can use Bayes’ theorem to calculate a posterior estimate for u’s group membership:
P ( g|D(u); θ ) ∝",4.2 Predicting user behavior,[0],[0]
P ( D(u)|ρ(g) ) ·,4.2 Predicting user behavior,[0],[0]
"P (g|π) (7)
",4.2 Predicting user behavior,[0],[0]
This posterior balances whether a group is likely in general against whether members of that group behave as u does.,4.2 Predicting user behavior,[0],[0]
"We can use Pu(g) = P ( g|D(u); θ ) as our new estimate for the group membership probabilities for u and replace (6) with: P ( b|s,D(u); θ ) =
K∑ g=1 P ( b|s; ρ(g) ) ·",4.2 Predicting user behavior,[0],[0]
"Pu(g) (8)
for the next interaction with u. An NLG system can therefore adapt to each new user over time.",4.2 Predicting user behavior,[0],[0]
"Before the first interaction with u, it has no specific information about u and models u’s behavior based on (6).",4.2 Predicting user behavior,[0],[0]
"As the system interacts with u repeatedly, it collects observationsD(u) about u’s behavior.",4.2 Predicting user behavior,[0],[0]
"This allows it to calculate an increasingly accurate posterior Pu(g) = P ( g|D(u); θ ) of u’s group membership, and thus generate utterances which are more and more suitable to u using (8).",4.2 Predicting user behavior,[0],[0]
"So far we have not discussed how to find settings for the parameters θ = π, ρ(1), . . .",5 Training,[0],[0]
", ρ(K), which define our probability model.",5 Training,[0],[0]
"The key challenge for training is the fact that we want to be able to train while treating the assignment of users to groups as unobserved.
",5 Training,[0],[0]
"We will use a maximum a posteriori estimate for θ, i.e., the setting which maximizes (3) when D is our training set.",5 Training,[0],[0]
"We will first discuss how to pick parameters to maximize only the left part of (3), i.e., the data likelihood, since this is the part that involves unobserved variables.",5 Training,[0],[0]
We will then discuss handling the parameter prior in section 5.2.,5 Training,[0],[0]
"Gradient descent based methods (Nocedal and Wright, 2006) exist for finding the parameter settings which maximize the likelihood for log-linear
models, under the conditions that all relevant variables are observed in the training data.",5.1 Expectation Maximization,[0],[0]
"If group assignments were given, gradient computations, and therefore gradient based maximization, would be straightforward for our model.",5.1 Expectation Maximization,[0],[0]
"One algorithm specifically designed to solve maximization problems with unknown variables by reducing them to the case where all variables are observed, is the expectation maximization (EM) algorithm (Neal and Hinton, 1999).",5.1 Expectation Maximization,[0],[0]
"Instead of maximizing the data likelihood from (3) directly, EM equivalently maximizes the log-likelihood, given in (4).",5.1 Expectation Maximization,[0],[0]
"It helps us deal with unobserved variables by introducing “pseudo-observations” based on the expected frequency of the unobserved variables.
",5.1 Expectation Maximization,[0],[0]
"EM is an iterative algorithm which produces a sequence of parameter settings θ(1), . . .",5.1 Expectation Maximization,[0],[0]
", θ(n).",5.1 Expectation Maximization,[0],[0]
Each will achieve a larger value for (4).,5.1 Expectation Maximization,[0],[0]
Each new setting is generated in two steps: (1) an lower bound on the log-likelhood is generate and (2) the new parameter setting is found by optimizing this lower bound.,5.1 Expectation Maximization,[0],[0]
"To find the lower bound we compute the probability for every possible value the unobserved variables could have had, based on the observed variables and the parameter setting θ(i−1) from the last iteration step.",5.1 Expectation Maximization,[0],[0]
"Then the lower bound essentially assumes that each assignment was seen with a frequency equal to these probabilities - these are the “pseudo-observations”.
",5.1 Expectation Maximization,[0],[0]
In our model the unobserved variables are the assignments of users to groups.,5.1 Expectation Maximization,[0],[0]
"The probability of seeing each user u assigned to a group, given all the data D(u) and the model parameters from the last iteration θ(i−1), is simply the posterior group membership probability P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
The lower bound is then given by (5).,5.1 Expectation Maximization,[0],[0]
"This is the sum of the log probabilities of the data points under each group model, weighted by P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
We can now use gradient descent techniques to optimize this lower bound.,5.1 Expectation Maximization,[0],[0]
To fully implement EM we need a way to maximize (5).,5.1.1 Maximizing the Lower Bound,[0],[0]
"This can be achieved with gradient based methods such as L-BFGS (Nocedal and Wright, 2006).",5.1.1 Maximizing the Lower Bound,[0],[0]
Here the gradient refers to the vector of all partial derivatives of the function with respect to each dimension of θ.,5.1.1 Maximizing the Lower Bound,[0],[0]
"We therefore need to calculate these partial derivatives.
",5.1.1 Maximizing the Lower Bound,[0],[0]
There are existing implementations of the gradient computations our base model such as in Engonopoulos et al. (2013).,5.1.1 Maximizing the Lower Bound,[0],[0]
"The gradients of (5)
for each of the ρ(g) is simply the gradient for the base model on each datapoint d weighted by P ( g|D(u); θ(i−1) )",5.1.1 Maximizing the Lower Bound,[0],[0]
"if d ∈ Du, i.e., the probability that the user u from which the datapoint originates belongs to group g. We can therefore compute the gradients needed for each ρ(g) by using implementations developed for the base model.
",5.1.1 Maximizing the Lower Bound,[0],[0]
"We also need gradients for the parameters in π, which are only used in our extended model.",5.1.1 Maximizing the Lower Bound,[0],[0]
"We can use the rules for computing derivatives to find, for each dimension g:
∂UL(θ) ∂πg = ∑ u∈U Pu(g)− exp (πg)∑K g′=1 exp ( πg′ )
where Pu(g) = P ( g|D(u); θ(i−1) ) .",5.1.1 Maximizing the Lower Bound,[0],[0]
Using these gradients we can use L-BFGS to maximize the lower bound and implement the EM iteration.,5.1.1 Maximizing the Lower Bound,[0],[0]
So far we have discussed maximization only for the likelihood without accounting for the prior probabilities for every parameter.,5.2 Handling the Parameter Prior,[0],[0]
"To obtain our full training objective we add the log of the right hand side of (3):
log N (π|0, σ(π)) · K∏",5.2 Handling the Parameter Prior,[0],[0]
g=1 N,5.2 Handling the Parameter Prior,[0],[0]
"( ρ(g)|0, σ(ρ) )",5.2 Handling the Parameter Prior,[0],[0]
"i.e., the parameter prior, to (4) and (5).",5.2 Handling the Parameter Prior,[0],[0]
The gradient contribution from these priors can be computed with standard techniques.,5.2 Handling the Parameter Prior,[0],[0]
"We can now implement an EM loop, which maximizes (3) as follows: we randomly pick an initial value θ(0) for all parameters.",5.3 Training Iteration,[0],[0]
Then we repeatedly compute the P ( g|D(u); θ(i−1) ) values and maximize the lower bound using L-BFGS to find θ(i).,5.3 Training Iteration,[0],[0]
This EM iteration is guaranteed to eventually converge towards a local optimum of our objective function.,5.3 Training Iteration,[0],[0]
"Once change in the objective falls below a pre-defined threshold, we keep the final θ setting.
",5.3 Training Iteration,[0],[0]
"For our implementation we make a small improvement to the approach: L-BFGS is itself an iterative algorithm and instead of running it until convergence every time we need to find a new θ(i), we only let it take a few steps.",5.3 Training Iteration,[0],[0]
"Even if we just took a single L-BFGS step in each iteration, we would still obtain a correct algorithm (Neal and
Hinton, 1999) and this has the advantage that we do not spend time trying to find a θ(i) which is a good fit for the likely poor group assignments P ( g|D(u); θ(i−1) )",5.3 Training Iteration,[0],[0]
we obtain from early parameter estimates.,5.3 Training Iteration,[0],[0]
Our model can be used in any component of a dialog system for which a prediction of the user’s behavior is needed.,6 Evaluation,[0],[0]
"In this work, we evaluate it in two NLG-related prediction tasks: RE production and RE comprehension.",6 Evaluation,[0],[0]
In both cases we evaluate the ability of our model to predict the user’s behavior given a stimulus.,6 Evaluation,[0],[0]
"We expect our user-group model to gradually improve its prediction accuracy compared to a generic baseline without user groups as it sees more observations from a given user.
",6 Evaluation,[0],[0]
In all experiments described below we set the prior variances σγ = 1.0 and σπ = 0.3 after trying out values between 0.1 and 10 on the training data of the comprehension experiment.,6 Evaluation,[0],[0]
"Task The task of RE generation can be split in two steps: attribute selection, the selection of the visual attributes to be used in the RE such as color, size, relation to other objects and surface realization, the generation of a full natural language expression.",6.1 RE production,[0],[0]
"We focus here on attribute selection: given a visual scene and a target object, we want to predict the set of attributes of the target object that a human speaker would use in order to describe it.",6.1 RE production,[0],[0]
"Here we treat attribute selection in terms of individual classification decisions on whether to use each attribute, as described in Section 3.",6.1 RE production,[0],[0]
"More specifically, we focus on predicting whether the speaker will use a spatial relation to another object (“landmark”).",6.1 RE production,[0],[0]
"Our motivation for choosing this attribute stems from the fact that previous authors (Viethen and Dale, 2008; Ferreira and Paraboni, 2014) have found substantial variation between different users with respect to their preference towards using spatial relations.
",6.1 RE production,[0],[0]
"Data We use the GRE3D3 dataset of humanproduced REs (Viethen and Dale, 2010), which contains 630 descriptions for 10 scenes collected from 63 users, each describing the same target object in each scene.",6.1 RE production,[0],[0]
35% of the descriptions in this corpus use a spatial relation.,6.1 RE production,[0],[0]
"An example of such a scene can be seen in Fig. 3.
",6.1 RE production,[0],[0]
"Models We use two baselines for comparison:
Basic: The state-of-the-art model on this task with this dataset, under the assumption that users are seen in training, is presented in Ferreira and Paraboni (2014).",6.1 RE production,[0],[0]
"They define context features such as type of relation between the target object and its landmark, number of object of the same color or size, etc., then train an SVM classifier to predict the use of each attribute.",6.1 RE production,[0],[0]
"We recast their model in terms of a log-linear model with the same features, to make it fit with the setup of Section 3.
Basic++: Ferreira and Paraboni (2014) also take speaker features into account.",6.1 RE production,[0],[0]
"We do not use speaker identity and the speaker’s attribute frequency vector, because we only evaluate on unseen users.",6.1 RE production,[0],[0]
"We do use their other speaker features (age, gender), together with Basic’s context features; this gives us a strong baseline which is aware of manually annotated user group characteristics.
",6.1 RE production,[0],[0]
"We compare these baselines to our Group model for values of K between 1 and 10, using the exact same features as Basic.",6.1 RE production,[0],[0]
"We do not use the speaker features of Basic++, because we do not want to rely on manually annotated groups.",6.1 RE production,[0],[0]
"Note that our results are not directly comparable with those of Ferreira and Paraboni (2014), because of a different training-test split: their model requires having seen speakers in training, while we explicitly want to test our model’s ability to generalize to unseen users.
",6.1 RE production,[0],[0]
"Experimental setup We evaluate using crossvalidation, splitting the folds so that all speakers we see in testing are previously unseen in training.",6.1 RE production,[0],[0]
We use 9 folds in order to have folds of the same size (each containing 70 descriptions coming from 7 speakers).,6.1 RE production,[0],[0]
At each iteration we train on 8 folds and test on the 9th.,6.1 RE production,[0],[0]
"At test time, we process each test instance iteratively: we first predict for each instance whether the user uwould use a spatial relation or not and test our prediction; we then add the
actual observation from the corpus to the set D(u) of observations for this particular user, in order to update our estimate about their group membership.
",6.1 RE production,[0],[0]
"Results Figure 4 shows the test F1-score (microaveraged over all folds) as we increase the number of groups, compared to the baselines.",6.1 RE production,[0],[0]
"For our Group models, these are averaged over all interactions with the user.",6.1 RE production,[0],[0]
"Our model gets F1-scores between 0.69 and 0.76 for all values ofK > 1, outperforming both Basic (0.22) and Basic++ (0.23).
",6.1 RE production,[0],[0]
"In order to take a closer look at our model’s behavior, we also show the accuracy of our model as it observes more instances at test time.",6.1 RE production,[0],[0]
We compare the model with K = 3 groups against the two baselines.,6.1 RE production,[0],[0]
"Figure 5 shows that the group model’s F1-score increases dramatically after the first two observations and then stays high throughout the test phase, always outperforming both baselines by at least 0.37 F1-score points after the first observation.",6.1 RE production,[0],[0]
The baseline models of course are not expected to improve with time; fluctuations are due to differences between the visual scenes.,6.1 RE production,[0],[0]
"In the same figure, we plot the evolution of the entropy of the group model’s posterior distribution over the groups (see (7)).",6.1 RE production,[0],[0]
"As expected, the model is highly uncertain at the beginning of the test phase about which group the user belongs to, then gets more and more certain as the set D(u) of observations from that user grows.",6.1 RE production,[0],[0]
Task Our next task is to predict the referent to which a user will resolve an RE in the context of a visual scene.,6.2 RE comprehension,[0],[0]
"Our model is given a stimulus s = (r, c) consisting of an instruction containing an RE r and a visual context c and outputs a probability distribution over all possible referents b.",6.2 RE comprehension,[0],[0]
"Such a model can be used by a probabilistic RE generator to select an RE which is highly likely to be correctly understood by the user or predict potential
misunderstandings (see Section 3).
",6.2 RE comprehension,[0],[0]
Data We use the GIVE-2.5 corpus for training and the GIVE-2 corpus for testing our model (the same used by Engonopoulos et al. (2013)).,6.2 RE comprehension,[0],[0]
These contain recorded observations of dialog systems giving instructions to users who play a game in a 3D environment.,6.2 RE comprehension,[0],[0]
"Each instruction contains an RE r, which is recorded in the data together with the visual context c at the time the instruction was given.",6.2 RE comprehension,[0],[0]
The object b which the user understood as the referent of the RE is inferred by the immediately subsequent action of the user.,6.2 RE comprehension,[0],[0]
"In total, we extracted 2927 observations by 403 users from GIVE-25 and 5074 observations by 563 users from GIVE-2.
",6.2 RE comprehension,[0],[0]
Experimental setup We follow the training method described in Section 3.,6.2 RE comprehension,[0],[0]
"At test time, we present the observations from each user in the order they occur in the test data; for each stimulus, we ask our models to predict the referent a which the user understood to be the referent of the RE, and compare with the recorded observation.",6.2 RE comprehension,[0],[0]
"We subsequently add the recorded observation to the dataset for the user and continue.
",6.2 RE comprehension,[0],[0]
"Models As a baseline, we use the Basic model described in Section 3, with the features of the “semantic” model of Engonopoulos et al. (2013).",6.2 RE comprehension,[0],[0]
"Those features capture information about the objects in the visual scene (e.g. salience) and some basic semantic properties of the RE (e.g. color, position).",6.2 RE comprehension,[0],[0]
"We use those features for our Group model as well, and evaluate for K between 1 and 10.
Results on GIVE data Basic had a test accuracy of 72.70%, which was almost identical with the accuracy of our best Group model for K = 6 (72.78%).",6.2 RE comprehension,[0],[0]
This indicates that our group model does not differentiate between users.,6.2 RE comprehension,[0],[0]
"Indeed, after training, the 6-group model assigns 81% prior probabil-
ity to one of the groups, and effectively gets stuck with this assignment while testing; the mean entropy of the posterior group distribution only falls from an initial 1.1 to 0.7 after 10 observations.
",6.2 RE comprehension,[0],[0]
We speculate that the reason behind this is that the features we use are not sensitive enough to capture the differences between the users in this data.,6.2 RE comprehension,[0],[0]
"Since our model relies completely on observable behavior, it also relies on the ability of the features to make relevant distinctions between users.
",6.2 RE comprehension,[0],[0]
"Results on synthetic data In order to test this hypothesis, we made a synthetic dataset based on the GIVE datasets with 1000 instances from 100 users, in the following way: for each user, we randomly selected 10 scenes from GIVE-2, and replaced the target the user selected, so that half of the users always select the target with the highest visual salience, and the other half always select the one with the lowest.",6.2 RE comprehension,[0],[0]
"Our aim was to test whether our model is capable of identifying groups when they are clearly present in the data and exhibit differences which our features are able to capture.
",6.2 RE comprehension,[0],[0]
We evaluated the same models in a 2-fold crossvalidation.,6.2 RE comprehension,[0],[0]
Figure 6 shows the prediction accuracy for Basic and the Group models for K from 1 to 10.,6.2 RE comprehension,[0],[0]
"All models for K > 1 clearly outperform the baseline model: the 2-group model gets 62.3% vs 28.6% averaged over all test examples, while adding more than two groups does not further improve the accuracy.",6.2 RE comprehension,[0],[0]
We also show in Figure 7 the evolution of the accuracy asD(u) grows: the Group model with K = 2 reaches a 64% testing accuracy after seeing two observations from the same user.,6.2 RE comprehension,[0],[0]
"In the same figure, the entropy of the posterior distribution over groups (see production experiment) falls towards zero as D(u) grows.",6.2 RE comprehension,[0],[0]
"These results show that our model is capable of correctly assigning a user to the group they belong to, once the features are adequate for distinguishing between different user behaviors.",6.2 RE comprehension,[0],[0]
"Our model was shown to be successful in discovering groups of users with respect to their behavior, within datasets which present discernible user variation.",6.3 Discussion,[0],[0]
"In particular, if all listeners are influenced in a similar way by e.g. the visual salience of an object, then the group model cannot learn different weights for the visual salience feature; if this happens for all available features, there are effectively no groups for our model to discover.
",6.3 Discussion,[0],[0]
"Once the groups have been discovered, our model can then very quickly distinguish between them at test time.",6.3 Discussion,[0],[0]
This is reflected in the steep performance improvement even after the first user observation in both the real data experiment in 6.1 and the synthetic data experiment in 6.2.,6.3 Discussion,[0],[0]
"We have presented a probabilistic model for NLG which predicts the behavior of individual users of a dialog system by dynamically assigning them to user groups, which were discovered during training2.",7 Conclusion,[0],[0]
"We showed for two separate NLG-related tasks, RE production and RE comprehension, how our model, after being trained with data that is not annotated with user groups, can quickly adapt to unseen users as it gets more observations from them in the course of a dialog and makes increasingly accurate predictions about their behavior.
",7 Conclusion,[0],[0]
"Although in this work we apply our model to tasks related to NLG, nothing hinges on this choice; it can also be applied to any other dialog-related prediction task where user variation plays a role.",7 Conclusion,[0],[0]
"In the future, we will also try to apply the basic principles of our user group approach to more sophisticated underlying models, such as neural networks.
2Our code and data is available in https://bit.ly/ 2jIu1Vm",7 Conclusion,[0],[0]
We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups.,abstractText,[0],[0]
"In contrast to previous work, these user groups are not specified beforehand, but learned in training.",abstractText,[0],[0]
"We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.",abstractText,[0],[0]
Discovering User Groups for Natural Language Generation,title,[0],[0]
"As originally defined by Pearl (1988), Bayesian networks express joint distributions over finite sets of random variables as products of conditional distributions.",1. Introduction,[0],[0]
"Probabilistic programming languages (PPLs) (Koller et al., 1997; Milch et al., 2005a; Goodman et al., 2008; Wood et al., 2014b) apply the same idea to potentially infinite sets of variables with general dependency structures.",1. Introduction,[0],[0]
"Thanks to their expressive power, PPLs have been used to solve many real-world applications, including Captcha (Le et al., 2017), seismic monitoring (Arora et al., 2013), 3D pose estimation (Kulkarni et al., 2015), generating design suggestions (Ritchie et al., 2015), concept learning (Lake et al., 2015), and cognitive science applications (Stuhlmüller & Goodman, 2014).
",1. Introduction,[0],[0]
"In practical applications, we often have to deal with a mix-
1University of California, Berkeley 2Arizona State University 3Vicarious Inc. 4Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Yi Wu <jxwuyi@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
ture of continuous and discrete random variables.",1. Introduction,[0],[0]
"Existing PPLs support both discrete and continuous random variables, but not discrete-continuous mixtures, i.e., variables whose distributions combine discrete and continuous elements.",1. Introduction,[0],[0]
"Such variables are fairly common in practical applications: sensors that have thresholded limits, e.g. thermometers, weighing scales, speedometers, pressure gauges; or a hybrid sensor that can report a either real value or an error condition.",1. Introduction,[0],[0]
"The occurrence of such variables has been noted in many other applications from a wide range of scientific domains (Kharchenko et al., 2014; Pierson & Yau, 2015; Gao et al., 2017).
",1. Introduction,[0],[0]
"Many PPLs have a restricted syntax that forces the expressed random variables to be either discrete or continuous, including WebPPL (Goodman & Stuhlmüller, 2014), Edward (Tran et al., 2016), Figaro (Pfeffer, 2009) and Stan (Carpenter et al., 2016).",1. Introduction,[0],[0]
"Even for PPLs whose syntax allows for mixtures of discrete and continuous variables, such as BLOG (Milch et al., 2005a), Church (Goodman, 2013), Venture (Mansinghka et al., 2014) and Anglican (Wood et al., 2014a), the underlying semantics of these PPLs implicitly assumes the random variables are not mixtures.",1. Introduction,[0],[0]
"Moreover, the inference algorithms associated with the semantics inherit the same assumption and can produce incorrect results when discrete-continuous mixtures are used.
",1. Introduction,[0],[0]
"Consider the following GPA example: a two-variable Bayes net Nationality → GPA where the nationality follows a binary distribution
P (Nationality = USA) = P (Nationality = India)",1. Introduction,[0],[0]
"= 0.5
and the conditional probabilities are discrete-continuous mixtures
GPA|Nationality = USA ∼0.01 · 1 {GPA = 4}+ 0.99 · Unif(0, 4),
GPA|Nationality = India ∼0.01 · 1 {GPA = 10}+ 0.99 · Unif(0, 10).
",1. Introduction,[0],[0]
This is a typical scenario in practice because many top students have perfect GPAs.,1. Introduction,[0],[0]
Now suppose we observe a student with a GPA of 4.0.,1. Introduction,[0],[0]
Where do they come from?,1. Introduction,[0],[0]
"If the student is Indian, the probability of any singleton set {g}
where 0 <",1. Introduction,[0],[0]
"g < 10 is zero, as this range has a probability density.",1. Introduction,[0],[0]
"On the other hand if the student is American, the set {4} has the probability 0.01.",1. Introduction,[0],[0]
"Thus, by Bayes theorem, P (Nationality = USA|GPA = 4) = 1, which means the student must be from the USA.
",1. Introduction,[0],[0]
"However, if we run the default Bayesian inference algorithm for this problem in PPLs, e.g., the standard importance sampling algorithm (Milch et al., 2005b), a sample that picks India receives a density weight of 0.99/10.0 = 0.099, whereas one that picks USA receives a discrete-mass weight of 0.01.",1. Introduction,[0],[0]
"Since the algorithm does not distinguish probability density and mass, it will conclude that the student is very probably from India, which is far from the truth.
",1. Introduction,[0],[0]
"We can fix the GPA example by considering a density weight infinitely smaller than a discrete-mass weight (Nitti et al., 2016; Tolpin et al., 2016).",1. Introduction,[0],[0]
"However, the situation becomes more complicated when involving more than one evidence variable, e.g., GPAs over multiple semesters for students who may study in both countries.",1. Introduction,[0],[0]
Vector-valued variables also cause problems—does a point mass in three dimensions count more or less than a point mass in two dimensions?,1. Introduction,[0],[0]
"These practical issues motivate the following two tasks:
• Inherit all the existing properties of PPL semantics and extend it to handle random variables with mixed discrete and continuous distributions;
• Design provably correct inference algorithms for the extended semantics.
",1. Introduction,[0],[0]
"In this paper, we carry out all these two tasks and implement the extended semantics as well as the new algorithms in a widely used PPL, Bayesian Logic (BLOG) (Milch et al., 2005a).",1. Introduction,[0],[0]
Measure-Theoretical Bayesian Nets (MTBNs) Measure theory can be applied to handle discrete-continuous mixtures or even more abstract measures.,1.1. Main Contributions,[0],[0]
"In this paper, we define a generalization of Bayesian networks called measure-theoretic Bayesian networks (MTBNs) and prove that every MTBN represents a unique measure on the input space.",1.1. Main Contributions,[0],[0]
"We then show how MTBNs can provide a more general semantic foundation for PPLs.
More concretely, MTBNs support (1) random variables with infinitely (even uncountably) many parents, (2) random variables valued in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed), (3) establishment of conditional independencies implied by an infinite graph, and (4) open-universe semantics in terms of the possible worlds in the vocabulary of the model.
",1.1. Main Contributions,[0],[0]
Inference Algorithms,1.1. Main Contributions,[0],[0]
"We propose a provably correct inference algorithm, lexicographic likelihood weighting (LLW), for general MTBNs with discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
"In addition, we propose LPF, a particle-filtering variant of LLW for sequential Monte Carlo (SMC) inference on state-space models.
",1.1. Main Contributions,[0],[0]
"Incorporating MTBNs into an existing PPL We incorporate MTBNs into BLOG with simple modifications and then define the generalized BLOG language, measuretheoretic BLOG, which formally supports arbitrary distributions, including discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
We prove that every generalized BLOG model corresponds to a unique MTBN.,1.1. Main Contributions,[0],[0]
"Thus, all the desired theoretical properties of MTBNs can be carried to measure-theoretic BLOG.",1.1. Main Contributions,[0],[0]
We also implement the LLW and LPF algorithms in the backend of measure-theoretic BLOG and use three representative examples to show their effectiveness.,1.1. Main Contributions,[0],[0]
This paper is organized as follows.,1.2. Organization,[0],[0]
We first discuss related work in Section 2.,1.2. Organization,[0],[0]
"In Section 3, we formally define measure-theoretic Bayesian nets and study their theoretical properties.",1.2. Organization,[0],[0]
Section 4 describes the LLW and LPF inference algorithms for MTBNs with discrete-continuous mixtures and establishes their correctness.,1.2. Organization,[0],[0]
"In Section 5, we introduce the measure-theoretic extension of BLOG and study its theoretical foundations for defining probabilistic models.",1.2. Organization,[0],[0]
"In Section 6, we empirically validate the generalized BLOG system and the new inference algorithms on three representative examples.",1.2. Organization,[0],[0]
"The motivating GPA example has been also discussed as a special case under some other PPL systems (Tolpin et al., 2016; Nitti et al., 2016).",2. Related Work,[0],[0]
Tolpin et al. (2016) and Nitti et al. (2016) proposed different solutions specific to this example but did not address the general problems of representation and inference with random variables with mixtures of discrete and continuous distributions.,2. Related Work,[0],[0]
"In contrast, we present a general formulation with provably correct inference algorithms.
",2. Related Work,[0],[0]
"Our approach builds upon the foundations of the BLOG probabilistic programming language (Milch, 2006).",2. Related Work,[0],[0]
We use a measure theoretic formulation to generalize the syntax and semantics of BLOG to random variables that may have infinitely many parents and mixed continuous and discrete distributions.,2. Related Work,[0],[0]
"The BLP framework Kersting & De Raedt (2007) unifies logic programming with probability models, but requires each random variable to be influenced by a finite set of random variables in order to define the semantics.",2. Related Work,[0],[0]
"This amounts to requiring only finitely many ances-
tors of each random variable.",2. Related Work,[0],[0]
Choi et al. (2010) present an algorithm for carrying out lifted inference over models with purely continuous random variables.,2. Related Work,[0],[0]
"They also require parfactors to be functions over finitely many random variables, thus limiting the set of influencing variables for each node to be finite.",2. Related Work,[0],[0]
Gutmann et al. (2011a) also define densities over finite dimensional vectors.,2. Related Work,[0],[0]
"In a relatively more general formulation (Gutmann et al., 2011b) define the distribution of each random variable using a definite clause, which corresponds to the limitation that each random variable (either discrete or continuous) has finitely many parents.",2. Related Work,[0],[0]
Frameworks building on Markov networks also have similar restrictions.,2. Related Work,[0],[0]
"Wang & Domingos (2008) only consider networks of finitely many random variables, which can have either discrete or continuous distributions.",2. Related Work,[0],[0]
"Singla & Domingos (2007) extend Markov logic to infinite (non-hybrid) domains, provided that each random variable has only finitely many influencing random variables.
",2. Related Work,[0],[0]
"In contrast, our approach not only allows models with arbitrarily many random variables with mixed discrete and continuous distributions, but each random variable can also have arbitrarily many parents as long as all ancestor chains are finite (but unbounded).",2. Related Work,[0],[0]
"The presented work constitutes a rigorous framework for expressing probability models with the broadest range of cardinalities (uncountably infinite parent sets) and nature of random variables (discrete, mixed, and even arbitrary measure spaces), with clear semantics in terms of first-order possible worlds and the generalization of conditional independences on such models.
",2. Related Work,[0],[0]
"Lastly, there are also other works using measure-theoretic approaches to analyze the semantics properties of probabilistic programs but with different emphases, such as the commutativity (Staton, 2017), design choices for monad structures (Ramsey, 2016) and computing a disintegration (Shan & Ramsey, 2017).",2. Related Work,[0],[0]
"In this section, we introduce measure-theoretic Bayesian networks (MTBNs) and prove that an MTBN represents a unique measure with desired theoretical properties.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We assume familiarity with measure-theoretic approaches to probability theory.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Some background is included in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A.
We begin with some necessary definitions of graph theory.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.1.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A digraph G is a pair G = (V,E) of a set of vertices V , of any cardinality, and a set of directed edges E ⊆ V × V .",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The notation u→ v denotes (u, v) ∈ E, and u 7→ v denotes the existence of a path from u to v in G.
Definition 3.2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"A vertex v ∈ V is a root vertex if there are no incoming edges to it, i.e., there is no u ∈ V such that u → v. Let pa(v)",3. Measure-Theoretic Bayesian Networks,[0],[0]
"= {u ∈ V : u → v} denote the set of parents of a vertex v ∈ V , and nd(v) =",3. Measure-Theoretic Bayesian Networks,[0],[0]
"{u ∈ V : not v 7→
u} denote its set of non-descendants.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.3.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A well-founded digraph (V,E) is one with no countably infinite ancestor chain v0 ← v1 ← v2 ← . . .",3. Measure-Theoretic Bayesian Networks,[0],[0]
".
",3. Measure-Theoretic Bayesian Networks,[0],[0]
This is the natural generalization of a finite directed acyclic graph to the infinite case.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Now we are ready to give the key definition of this paper.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.4.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A measure-theoretic Bayesian network M = (V,E, {Xv}v∈V , {Kv}v∈V ) consists of (a) a wellfounded digraph (V,E) of any cardinality, (b) an arbitrary measurable space Xv for each v ∈ V , and (c) a probability kernel Kv from ∏ u∈pa(v) Xu to Xv for each v ∈ V .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"By definition, MTBNs allow us to define very general and abstract models with the following two major benefits:
1.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables with infinitely (even uncountably) many parents because MTBN is defined on a well-founded digraph.
2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Next, we related MTBN to a probability measure.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Fix an MTBN M = (V,E, {Xv}v∈V , {Kv}v∈V ).",3. Measure-Theoretic Bayesian Networks,[0],[0]
For U ⊆ V let XU = ∏ u∈U Xu be the product measurable space over variables u ∈ U .,3. Measure-Theoretic Bayesian Networks,[0],[0]
"With this notation, Kv is a kernel from Xpa(v) to Xv.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Whenever W ⊆ U let πUW : XU → XW denote the projection map.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Let XV be our base measurable space upon which we will consider different probability measures µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Let Xv for v ∈ V denote both the underlying set of Xv and the random variable given by the projection πV{v}, and XU for U ⊆ V the underlying space of XU and the random variable given by the projection πVU .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.5.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a measure µ on XV , if for all v ∈ V :
• Xv is conditionally independent of its non-descendants Xnd(v) given its parents Xpa(v).",3. Measure-Theoretic Bayesian Networks,[0],[0]
"• Kv(Xpa(v), A) = Pµ[Xv ∈ A|Xpa(v)] holds almost surely for any A ∈ Xv, i.e., Kv is a version of the conditional distribution of Xv given its parents.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Def. 3.5 captures the generalization of the local properties of Bayes networks – conditional independence and conditional distributions defined by parent-child relationships.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Here we assume the conditional probability exists and is unique.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"This is a mild condition because this holds as long as the probability space is regular (Kallenberg, 2002).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The next theorem shows that MTBNs are well-defined.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Theorem 3.6.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a unique measure µ on XV .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof of theorem 3.6 requires several intermediate results and is presented in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
B.,3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof proceeds by first defining a projective family of measures.,3. Measure-Theoretic Bayesian Networks,[0],[0]
This gives a way to recursively construct our measure µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
We then define a notion of consistency such that every consistent projective family constructs a measure that M represents.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Lastly, we give an explicit characterization of the unique consistent projective family, and thus of the unique measure M represents.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We introduce the lexicographic likelihood weighting (LLW) algorithm for provably correct inference on MTBNs.,4. Generalized Inference Algorithms,[0],[0]
We also present lexicographic particle filter (LPF) for statespace models by adapting LLW for the sequential Monte Carlo (SMC) framework.,4. Generalized Inference Algorithms,[0],[0]
"Suppose we have an MTBN with finitely many random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XN , and that, without loss of generality, we observe real-valued random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XM for M < N as evidence.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose the distribution of Xi given its parents Xpa(i) is a mixture between a density fi(xi|xpa(i)) with respect to the Lebesgue measure and a discrete distribution Fi(xi|xpa(i)), i.e., for any > 0, we have P (Xi ∈",4.1. Lexicographic likelihood weighting,[0],[0]
"[xi − , xi]|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"=∑ x∈[xi− ,xi] Fi(xi|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
+ ∫,4.1. Lexicographic likelihood weighting,[0],[0]
xi xi− fi(x|Xpa(i)),4.1. Lexicographic likelihood weighting,[0],[0]
dx.,4.1. Lexicographic likelihood weighting,[0],[0]
This implies that Fi(xi|xpa(i)) is nonzero for at most countably many values,4.1. Lexicographic likelihood weighting,[0],[0]
xi.,4.1. Lexicographic likelihood weighting,[0],[0]
"If Fi is nonzero for finitely many points, it can be represented by a list of those points and their values.
",4.1. Lexicographic likelihood weighting,[0],[0]
"Lexicographic Likelihood Weighting (LLW) extends the classical likelihood weighting (Milch et al., 2005b) to this setting.",4.1. Lexicographic likelihood weighting,[0],[0]
"It visits each node of the graph in topological order, sampling those variables that are not observed, and accumulating a weight for those that are observed.",4.1. Lexicographic likelihood weighting,[0],[0]
"In particular, at an evidence variable Xi we update a tuple (d,w) of the number of densities and a weight, initially (0, 1), by:
(d,w)← { (d,wFi(xi|xpa(i))) Fi(xi|xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"> 0, (d+ 1, wfi(xi|xpa(i)))",4.1. Lexicographic likelihood weighting,[0],[0]
"otherwise.
(1)
Finally, having K samples x(1), . .",4.1. Lexicographic likelihood weighting,[0],[0]
.,4.1. Lexicographic likelihood weighting,[0],[0]
", x(K) by this process and accordingly a tuple (d(i), w(i)) for each sample x(i), let d∗ = mini:w(i) 6=0 d
(i) and estimate E[f(X)|X1:M ] by∑ {i:d(i)=d∗} w
(i) f(x(i))∑",4.1. Lexicographic likelihood weighting,[0],[0]
{i:d(i)=d∗} w (i) .,4.1. Lexicographic likelihood weighting,[0],[0]
"(2)
The algorithm is summarised in Alg. 1",4.1. Lexicographic likelihood weighting,[0],[0]
The next theorem shows this procedure is consistent.,4.1. Lexicographic likelihood weighting,[0],[0]
Theorem 4.1.,4.1. Lexicographic likelihood weighting,[0],[0]
"LLW is consistent: (2) converges almost surely to E[f(X)|X1:M ].
",4.1. Lexicographic likelihood weighting,[0],[0]
"Algorithm 1 Lexicographic Likelihood Weighting Require: densities f , masses F , evidences E, and K.
for i = 1 . .",4.1. Lexicographic likelihood weighting,[0],[0]
.K,4.1. Lexicographic likelihood weighting,[0],[0]
"do sample all the ancestors of E from prior compute (d(i), w(i)) by Eq.",4.1. Lexicographic likelihood weighting,[0],[0]
(1) end for d?,4.1. Lexicographic likelihood weighting,[0],[0]
"← mini:w(i) 6=0 d(i)
",4.1. Lexicographic likelihood weighting,[0],[0]
"Return (∑
i:d(i)=d? w (i)f(x(i))
) /",4.1. Lexicographic likelihood weighting,[0],[0]
(∑ i:d(i)=d?,4.1. Lexicographic likelihood weighting,[0],[0]
"w (i) )
",4.1. Lexicographic likelihood weighting,[0],[0]
"In order to prove Theorem 4.1, the main technique we adopt is to use a more restricted algorithm, the Iterative Refinement Likelihood Weighting (IRLW) as a reference.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose we want to approximate the posterior distribution of an X -valued random variable X conditional on a Yvalued random variable Y , for arbitrary measure spaces X and Y .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In general, there is no notion of a probability density of Y given X for weighing samples.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If, however, we could make a discrete approximation Yt of Y then we could weight samples by the probability P",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
[Yt = yt|X].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If we increase the accuracy of the approximation with the number of samples, this should converge in the limit.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"We show this is possible, if we are careful about how we approximate:
Definition 4.2.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
An approximation scheme for a measurable space Y consists of a measurable spaceA and measurable approximation functions αi :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y → A for i = 1, 2, . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
and αji :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
A → A for i < j such that αj ◦ α,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"i = αi and y can be measurably recovered from the subsequence αt(y), αt+1(y), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"for any t > 0.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"When Y is a real-valued variable we will use the approximation scheme αn(y) = 2−nd2nye where dre denotes the ceiling of r, i.e., the smallest integer no smaller than it.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Observe in this case that P (αn(Y ) = αn(y)),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= P (αn(y)− 2−n < Y ≤ αn(y)) which we can compute from the CDF of Y .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Lemma 4.3.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"IfX,Y are real-valued random variables with E |X| <∞, then limi→∞ E[X|αi(Y )]",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= E[X|Y ].
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Proof.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Let Fi = σ(αi(Y )),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
be the sigma algebra generated by αi(Y ).,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Whenever i ≤,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
we have αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= (αj ◦αji )(Y ) and so Fi ⊆ Fj .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"This means E[X|αi(Y )] = E[X|Fi] is a martingale, so we can use martingale convergence results.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In particular, since E |X| <∞
E[X|Fi]→ E[X|F∞] a.s. and in L1, where F∞ =",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
⋃,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
i Fi is the sigma-algebra generated by {αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
": i ∈ N} (see Theorem 7.23 in (Kallenberg, 2002)).
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y is a measurable function of the sequence (α1(Y ), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"),",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
as limi→∞ αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= Y , and so σ(Y ) ⊆ F∞. By definition
the sequence is a measurable function of Y , and so F∞ ⊆ σ(Y ), and so E[X|F∞] = E[X|Y ] giving our result.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Iterative refinement likelihood weighting (IRLW) samples x(1), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
", x(K)",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
from the prior,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"and evaluates:
∑K i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i))f(x(i))∑K
i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i)) (3)
Using Lemma 4.3, G.12, and G.13, we can show IRLW is consistent.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Theorem 4.4.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
IRLW is consistent: (3) converges almost surely to E[f(X)|Y ].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Now we are ready to prove Theorem 4.1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
Proof of Theorem 4.1.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
We prove the theorem for evidence variables that are leaves It is straightforward to extend the proof when the evidence variables are non-leaf nodes.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Let x be a sample produced by the algorithm with number of densities and weight (d,w).",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
With In = ∏ i=1...,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"M (αn(xi)− 2−n, αn(xi)] a 2−n-cube around x1:M we have
lim n→∞ P (X1:M ∈ In|XM+1:N = xM+1:N )",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"w 2−dn = 1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Using In as an approximation scheme by Def. 4.2, the numerator in the above limit is the weight used by IRLW.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"But given the above limit, using w 2−dn as the weight will give the same result in the limit.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Then if we have K samples, in the limit of n→∞ only those samples x(i) with minimal d(i) will contribute to the estimation, and up to normalization they will contribute weight w(i) to the estimation.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"We now consider inference in a special class of highdimensional models known as state-space models, and show how LLW can be adapted to avoid the curse of dimensionality when used with such models.",4.2. Lexicographic particle filter,[0],[0]
A state-space model (SSM) consists of latent states {Xt}0≤t≤T and the observations {Yt}0≤t≤T with a special dependency structure where pa(Yt) =,4.2. Lexicographic particle filter,[0],[0]
"Xt and pa(Xt) = Xt−1 for 0 < t ≤ T .
SMC methods (Doucet et al., 2001), also knowns as particle filters, are a widely used class of methods for inference on SSMs.",4.2. Lexicographic particle filter,[0],[0]
"Given the observed variables {Yt}0≤t≤T , the posterior distribution P (Xt|Y0:t) is approximated by a set of K particles where each particle x(k)t represents a sample of {Xi}0≤i≤t.",4.2. Lexicographic particle filter,[0],[0]
"Particles are propagated forward through the transition model P (Xt|Xt−1) and resampled at each time step t according to the weight of each particle, which is defined by the likelihood of observation Yt.
",4.2. Lexicographic particle filter,[0],[0]
"Algorithm 2 Lexicographic Particle Filter (LPF) Require: densities f , masses F , evidences Y , and K
for t = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
", T do for k = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
",K do x
(k) t ← sample from transition
compute (d(k), w(k)) by Eq. 4 end for d?",4.2. Lexicographic particle filter,[0],[0]
← mink:w(k) 6=0 d(k) ∀k : d(k) >,4.2. Lexicographic particle filter,[0],[0]
"d?, w(k) ← 0 Output ( w(k)f(x (k) t ) ) / (∑ k w (k) ) resample particles according to w(k)
end for
In the MTBN setting, the distribution of Yt1 given its parent Xt can be a mixture of density ft(yt|xt) and a discrete distribution Ft(yt|xt).",4.2. Lexicographic particle filter,[0],[0]
"Hence, the resampling step in a particle filter should be accordingly modified: following the idea from LLW, when computing the weight of a particle, we enumerate all the observations yt,",4.2. Lexicographic particle filter,[0],[0]
"i at time step t and again update a tuple (d,w), initially (0,1), by
(d,w)← { (d,wFt(yt,i|xt)) Ft(yt,i|xt) > 0, (d+ 1, wft(yt,i|xt))",4.2. Lexicographic particle filter,[0],[0]
otherwise.,4.2. Lexicographic particle filter,[0],[0]
"(4)
We discard all those particles with a non-minimum d value and then perform the normal resampling step.",4.2. Lexicographic particle filter,[0],[0]
"We call this algorithm lexicographical particle filter (LPF), which is summarized in Alg. 2.
",4.2. Lexicographic particle filter,[0],[0]
The following theorem guarantees the correctness of LPF.,4.2. Lexicographic particle filter,[0],[0]
"Its Proof easily follows the analysis for LLW and the classical proof of particle filtering based on importance sampling.
",4.2. Lexicographic particle filter,[0],[0]
Theorem 4.5.,4.2. Lexicographic particle filter,[0],[0]
LPF is consistent: the outputs of Alg.,4.2. Lexicographic particle filter,[0],[0]
2 converges almost surely to {E[f(Xt)|Y0:t]}0≤t≤T .,4.2. Lexicographic particle filter,[0],[0]
In Section 3 and Section 4 we provided the theoretical foundation of MTBN and general inference algorithms.,5. Generalized Probabilistic Programming Languages,[0],[0]
This section describes how to incorporate MTBN into a practical PPL.,5. Generalized Probabilistic Programming Languages,[0],[0]
"We focus on a widely used open-universe PPL, BLOG (Milch, 2006).",5. Generalized Probabilistic Programming Languages,[0],[0]
"We define the generalized BLOG language, the measure-theoretic BLOG, and prove that every well-formed measure-theoretic BLOG model corresponds to a unique MTBN.",5. Generalized Probabilistic Programming Languages,[0],[0]
"Note that our approach also applies to other PPLs2.
",5. Generalized Probabilistic Programming Languages,[0],[0]
1There can be multiple variables observed.,5. Generalized Probabilistic Programming Languages,[0],[0]
"Here the notation Yt denotes {Yt,i}i for conciseness.
",5. Generalized Probabilistic Programming Languages,[0],[0]
"2It has been shown that BLOG has equivalent semantics to other PPLs (Wu et al., 2014; McAllester et al., 2008).",5. Generalized Probabilistic Programming Languages,[0],[0]
Figure 1.,16 query Nationality(David) = USA;,[0],[0]
"A BLOG code for the GPA example.
",16 query Nationality(David) = USA;,[0],[0]
"We begin with a brief description of the core syntax of BLOG, with particular emphasis on (1) number statements, which are critical for expressing open-universe models3, and (2) new syntax for expressing MTBNs, i.e., the Mix distribution.",16 query Nationality(David) = USA;,[0],[0]
Further description of BLOG’s syntax can be found in Li & Russell (2013).,16 query Nationality(David) = USA;,[0],[0]
Fig. 1 shows a BLOG model with measure-theoretic extensions for a multi-student GPA example.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 1 declares two types, Applicant and Country.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 2 defines 3 distinct countries with keyword distinct, New Zealand, India and USA.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Lines 3 to 5 define a number statement, which states that the number of US applicants follows a Poisson distribution with a higher mean than those from New Zealand or India.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 defines an origin function, which maps the object being generated to the arguments that were used in the number statement that was responsible for generating it.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Here Nationality maps applicants to their nationalities.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 and 13 define two random variables by keyword random.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 to 12 state that the GPA of an applicant is distributed as a mixture of weighted discrete and continuous distributions.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"For US applicants, the range of values 0",5.1. Syntax of measure-theoretic BLOG,[0],[0]
< GPA < 4 follows a truncated Gaussian with bounds 0 and 4 (line 9).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
The probability mass outside the range is attributed to the corresponding bounds: P (GPA = 0),5.1. Syntax of measure-theoretic BLOG,[0],[0]
= P,5.1. Syntax of measure-theoretic BLOG,[0],[0]
(,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA = 4) = 10−4 (line 10).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA distributions for other countries are specified similarly.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Line 13 defines a random applicant David.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 15 states that the David’s GPA is observed to be 4 and we query in line 16 whether David is from USA.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Number Statement (line 3 to 5) Fig. 2 shows the syntax of a number statement for Typei.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In this specification, gj are origin functions (discussed below); ȳj are tuples of arguments drawn from x̄ = x1, . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; ϕj are first-order formulas with free variables ȳj ; ēj are tuples of expressions
3The specialized syntax in BLOG to express models with infinite number of variables.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"over a subset of x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; and cj(ēj) specify kernels κj : Π{Xτe :e∈ēj}Xe → N where τe is the type of the expression e.
#Typei(g1 = x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", gk = xk) ∼",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"if ϕ1(ȳ1) then c1(ē1)
else if ϕ2(ȳ2) then c2(ē2)
.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
". .
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"else cm(ēm);
ments can be recovered using the origin functions gj , each of which is declared as:
origin Typej gj(Typei),
where Typej is the type of the argument xj in the number statement of Typei where gj was used.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"The value of the jth variable used in the number statement that generated u, an element of the universe, is given by gj(u).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 in Fig. 1 is an example of origin function.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Mixture Distribution (line 9 to 12),5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In measure-theoretic BLOG, we introduce a new distribution, the mixture distribution (e.g., lines 9-10 in Fig. 1).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"A mixture distribution is specified as:
Mix({c1(ē1)→ w1(ē′), . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", ck(ēk)→ wk(ē′)}), where ci are arbitrary distributions, and wi’s are arbitrary real valued functions that sum to 1 for every possible assignment to their arguments: ∀ē′ ∑ i wi(ē
′) =",5.1. Syntax of measure-theoretic BLOG,[0],[0]
1.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Note that in our implementation of measure-theoretical BLOG, we only allow a Mix distribution to express a mixture of densities and masses for simplifying the system design, although it still possible to express the same semantics without Mix.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
In this section we present the semantics of measure-theoretic BLOG and its theoretical properties.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Every BLOG model implicitly defines a first-order vocabulary consisting of the set of functions and types mentioned in the model.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"BLOG’s semantics are based on the standard, open-universe semantics of first-order logic.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We first define the set of all possible elements that may be generated for a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.1.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of possible elements UM for a BLOG model M with types {τ1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", τk} is ⋃ j∈N{Uj}, where
• U0 = 〈U01 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U0k 〉, U0j = {cj : cj is a distinct τi constant inM} • Ui+1",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= 〈U i+11 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U i+1 k 〉, where U i+1m = U",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"im ∪
{uν,ū,m : ν(x̄) is a number statement of type τm, ū is a tuple of elements of the type of x̄ from U i, m ∈ N}
Def.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"5.1 allows us to define the set of random variables corresponding to a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.2.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of basic random variables for a BLOG modelM, BRV (M), consists of:
• for each number statement ν(x̄), a number variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vν,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] over the standard measurable space N, where ū is of the type of x̄. • for each function f(x̄) and tuple ū from UM of the type of x̄, a function application variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vf [ū] with the measurable space XVf,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] = Xτf , where Xτf is the measurable space corresponding to τf , the return type of f .
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We now define the space of consistent assignments to random variables.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.3.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"An instantiation σ of the basic RVs defined by a BLOG modelM is consistent if and only if:
•",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"For every element uν,v̄,i used in an assignment of the form",5.2. Semantics of measure-theoretic BLOG,[0],[0]
σ(Vf [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
= w or σ(Vν [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= m > 0, σ(Vν",5.2. Semantics of measure-theoretic BLOG,[0],[0]
[v̄]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
≥,5.2. Semantics of measure-theoretic BLOG,[0],[0]
i;,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• For every fixed function symbol f with the interpretation f̃ , σ(Vf [ū]) = f̃(ū); and • For every element uν,ū=〈u1,...,um〉,i, generated by the number statement ν, with origin functions g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm, for every gj ∈ {g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm}, σ(Vgj [uν,ū,i]) = uj .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"That is, origin functions give correct inverse maps.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Lemma 5.4.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Every consistent assignment σ to the basic RVs forM defines a unique possible world in the vocabulary ofM.
The proof of Lemma 5.4 is in Appx.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"F. In the following definition, we use the notation e[ū/x̄] to denote a substitution of every occurrence of the variable xi with ui in the expression e. For any BLOG modelM, let V (M) = BRV (M); for each v ∈ V , Xv is the measurable space corresponding to v. Let E(M) consist of the following edges for every number statement or function application statement of the form s(x̄):
• The edge (Vg[w̄], Vs[ū])",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"if g is a function symbol in M such that g(ȳ) appears in s(x̄), and either g(w̄) = g(ȳ)[ū/x̄] or an occurrence of g(ȳ) in s(x̄) uses quantified variables z1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", zn, ū′ is a tuple of elements of the type of z̄ and g(w̄) = g(ȳ)[ū/x̄][ū′/z̄].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• The edge (Vν [v̄], Vs[ū]), for element uν,v̄,i ∈ ū.
Note that the first set of edges defined in E(M) above may include infinitely many parents for Vs[ū].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Let the dependency statement in the BLOG model M corresponding to a number or function variable Vs[f̄ ] be s. Let expr(s) be the set of expressions used in s.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Each such statement then defines in a straightforward manner, a kernel Ks(ū) : Xexpr(s(ū))",5.2. Semantics of measure-theoretic BLOG,[0],[0]
→ XVs[ū].,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"In order ensure consistent assignments, we include a special value null ∈",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Xτ for each τ,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Figure 3.,9 query hasFakeCoin;,[0],[0]
"BLOG code for the Scale example
inM, and require that Ks(ū)(σ(pa(Vs[ū])), {null}c) = 0",9 query hasFakeCoin;,[0],[0]
whenever σ violates the first condition of consistent assignments (Def. 5.3).,9 query hasFakeCoin;,[0],[0]
"In other words, all the local kernels ensure are locally consistent: variables involving an object uν,ū,i get a non-null assignment only if the assignment to its number statement represents the generation of at least i objects (σ(Vν(ū))",9 query hasFakeCoin;,[0],[0]
≥ i).,9 query hasFakeCoin;,[0],[0]
"Each kernel of the formKs(ū) can be transformed into a kernel Kpa(Vs[ū]) from its parent vertices (representing basic random variables) by composing the kernels determining the truth value of each expression e ∈ expr(v) in terms of the basic random variables, with the kernel KeVs[ū].",9 query hasFakeCoin;,[0],[0]
Let κ(M) = {Kpa(Vs[ū]) : Vs[ū] ∈ BRV (M)}.,9 query hasFakeCoin;,[0],[0]
Definition 5.5.,9 query hasFakeCoin;,[0],[0]
"The MTBN M for a BLOG model M is defined using V = V (M), E = E(M), the set of measurable spaces {Xv : v ∈ BRV (M)} and the kernels for each vertex given by κ(M).
",9 query hasFakeCoin;,[0],[0]
"By Thm. 3.6, we have the main result of this section, which provides the theoretical foundation for the generalized BLOG language:
Theorem 5.6.",9 query hasFakeCoin;,[0],[0]
"If the MTBNM for a BLOG model is a wellfounded digraph, thenM represents a unique measure µ on XBRV (M).",9 query hasFakeCoin;,[0],[0]
"We implemented the measure-theoretic extension of BLOG and evaluated our inference algorithms on three models where naive algorithms fail: (1) the GPA model (GPA); (2) the noisy scale model (Scale); and (3) a SSM, the aircraft tracking model (Aircraft-Tracking).",6. Experiment Results,[0],[0]
"The implementation is based on BLOG’s C++ compiler (Wu et al., 2016).
",6. Experiment Results,[0],[0]
GPA model: Fig. 1 presents the BLOG code for the GPA example as explained in Sec. 5.,6. Experiment Results,[0],[0]
"Since the GPA of David is exactly 4, Bayes rule implies that David must be from USA.",6. Experiment Results,[0],[0]
"We evaluate LLW and the naive LW on this model in Fig 4(a), where the naive LW converges to an incorrect posterior.
",6. Experiment Results,[0],[0]
Scale model:,6. Experiment Results,[0],[0]
"In the noisy scale example (Fig. 3), we have an even number of coins and there might be a fake coin among them (Line 4).",6. Experiment Results,[0],[0]
The fake coin will be slightly heavier than a normal coin (Line 2-3).,6. Experiment Results,[0],[0]
We divide the coins into two halves and place them onto a noisy scale.,6. Experiment Results,[0],[0]
"When there is no fake coin, the scale always balances (Line 7).
",6. Experiment Results,[0],[0]
"When there is a fake coin, the scale will noisily reflect the weight difference with standard deviation σ",6. Experiment Results,[0],[0]
(sigma in Line 6).,6. Experiment Results,[0],[0]
Now we observe that the scale is balanced (Line 8),6. Experiment Results,[0],[0]
and we would like to infer whether a fake coin exists.,6. Experiment Results,[0],[0]
We again compare LLW against the naive LW with different choices of the σ parameter in Fig. 4(b).,6. Experiment Results,[0],[0]
"Since the scale is precisely balanced, there must not be a fake coin.",6. Experiment Results,[0],[0]
"LLW always produces the correct answer but naive LW converges to different incorrect posteriors for different values of σ; as σ increases, naive LWs result approaches the true posterior.
",6. Experiment Results,[0],[0]
Aircraft-Tracking model: Fig. 5 shows a simplified BLOG model for the aircraft tracking example.,6. Experiment Results,[0],[0]
"In this state-space model, we have N = 6 radar points (Line 1) and a single aircraft to track.",6. Experiment Results,[0],[0]
Both the radars and the aircraft are considered as points on a 2D plane.,6. Experiment Results,[0],[0]
The prior of the aircraft movement is a Gaussian process (Line 3 to 6).,6. Experiment Results,[0],[0]
"Each radar r has an effective range radius(r): if the aircraft is within the range, the radar will noisily measure the distance from the aircraft to its own location (Line 13); if the aircraft is out of range, the radar will almost surely just output its radius (Line 10 to 11).",6. Experiment Results,[0],[0]
Now we observe the measurements from all the radar points for T time steps and we want to infer the location of the aircraft.,6. Experiment Results,[0],[0]
"With the measure-theoretic extension, a generalized BLOG program is more expressive for modeling truncated sensors: if a radar outputs exactly its radius, we can surely infer that the aircraft must be out of the effective range of this radar.",6. Experiment Results,[0],[0]
"However, this information cannot be captured by the original BLOG language.",6. Experiment Results,[0],[0]
"To illustrate this case, we manually generated a synthesis dataset of T = 8 time steps4 and evaluated LPF against the naive particle filter with different numbers of particles in Fig. 4(c).",6. Experiment Results,[0],[0]
We take the mean of the samples from all the particles as the predicted aircraft location.,6. Experiment Results,[0],[0]
"Since we know the ground truth, we measure the average mean square error between the true location and the prediction.",6. Experiment Results,[0],[0]
"LPF accurately predicts the
4The full BLOG programs with complete data are available at https://goo.gl/f7qLwy.",6. Experiment Results,[0],[0]
Figure 5.,18 query Y(t) for Timestep t;,[0],[0]
"BLOG code for the Aircraft-Tracking example
true locations while the naive PF converges to the incorrect results.",18 query Y(t) for Timestep t;,[0],[0]
"We presented a new formalization, measure-theoretic Bayesian networks, for generalizing the semantics of PPLs to include random variables with mixtures of discrete and continuous distributions.",7. Conclusion,[0],[0]
"We developed provably correct inference algorithms for such random variables and incorporated MTBNs into a widely used PPL, BLOG.",7. Conclusion,[0],[0]
"We believe that together with the foundational inference algorithms, our proposed rigorous framework will facilitate the development of powerful techniques for probabilistic reasoning in practical applications from a much wider range of scientific areas.",7. Conclusion,[0],[0]
"This work is supported by the DARPA PPAML program, contract FA8750-14-C-0011.",Acknowledgment,[0],[0]
"Simon S. Du is funded by NSF grant IIS1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.",Acknowledgment,[0],[0]
"Despite the recent successes of probabilistic programming languages (PPLs) in AI applications, PPLs offer only limited support for random variables whose distributions combine discrete and continuous elements.",abstractText,[0],[0]
We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure spaces.,abstractText,[0],[0]
"We develop two new general sampling algorithms that are provably correct under the MTBN framework: the lexicographic likelihood weighting (LLW) for general MTBNs and the lexicographic particle filter (LPF), a specialized algorithm for statespace models.",abstractText,[0],[0]
"We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through representative examples.",abstractText,[0],[0]
Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2315–2325, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment).",1 Introduction,[0],[0]
"It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015).",1 Introduction,[0],[0]
"Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences.",1 Introduction,[0],[0]
"Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation.
",1 Introduction,[0],[0]
∗These authors contribute equally to this paper.,1 Introduction,[0],[0]
"†To whom correspondence should be addressed.
",1 Introduction,[0],[0]
"Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential.",1 Introduction,[0],[0]
"Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space.",1 Introduction,[0],[0]
Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations.,1 Introduction,[0],[0]
"Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013).
",1 Introduction,[0],[0]
Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs).,1 Introduction,[0],[0]
"CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees).",1 Introduction,[0],[0]
"RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree.",1 Introduction,[0],[0]
"However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009).",1 Introduction,[0],[0]
"(CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.)
",1 Introduction,[0],[0]
"A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).
",1 Introduction,[0],[0]
"In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).1 Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees.",1 Introduction,[0],[0]
"The model variants are denoted as c-TBCNN and d-TBCNN, respectively.",1 Introduction,[0],[0]
"The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire
1The model of tree-based convolution was firstly proposed to process program source code in our (unpublished) previous work (Mou et al., 2014).
2315
parse tree of a sentence; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension.",1 Introduction,[0],[0]
"One merit of such architecture is that all features, along the tree, have short propagation paths to the output layer, and hence structural information can be learned effectively.
",1 Introduction,[0],[0]
"TBCNNs are evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments.",1 Introduction,[0],[0]
"To understand how TBCNNs work, we also visualize the network by plotting the convolution process.",1 Introduction,[0],[0]
We make our code and results available on our project website.2,1 Introduction,[0],[0]
"In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling.",2 Background and Related Work,[0],[0]
"Convolutional neural networks (CNNs), early used for image processing (LeCun, 1995), turn out to be effective with natural languages as well.",2.1 Convolutional Neural Networks,[0],[0]
"Figure 1a depicts a classic convolution process on a sentence (Collobert and Weston, 2008).",2.1 Convolutional Neural Networks,[0],[0]
"A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features.",2.1 Convolutional Neural Networks,[0],[0]
"Let t be the window size, and x1, · · · ,",2.1 Convolutional Neural Networks,[0],[0]
xt ∈ Rne be ne-dimensional word embeddings.,2.1 Convolutional Neural Networks,[0],[0]
"The output of convolution, evaluated at the current position, is
y = f (W ·",2.1 Convolutional Neural Networks,[0],[0]
[x1; · · · ; xt] + b) where y ∈ Rnc (nc is the number of feature detectors).,2.1 Convolutional Neural Networks,[0],[0]
"W ∈ Rnc×(t·ne) and b ∈ Rnc are parame-
2https://sites.google.com/site/tbcnnsentence/
ters; f is the activation function.",2.1 Convolutional Neural Networks,[0],[0]
Semicolons represent column vector concatenation.,2.1 Convolutional Neural Networks,[0],[0]
"After convolution, the extracted features are pooled to a fixedsize vector for classification.
",2.1 Convolutional Neural Networks,[0],[0]
Convolution can extract neighboring information effectively.,2.1 Convolutional Neural Networks,[0],[0]
"However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related.",2.1 Convolutional Neural Networks,[0],[0]
Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers.,2.1 Convolutional Neural Networks,[0],[0]
Similar CNNs include Kim (2014) and Hu et al. (2014).,2.1 Convolutional Neural Networks,[0],[0]
"All these models are “flat,” by which we mean no structural information is used explicitly.",2.1 Convolutional Neural Networks,[0],[0]
"Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees.",2.2 Recursive Neural Networks,[0],[0]
"In the original version, RNN is built upon a binarized constituency tree.",2.2 Recursive Neural Networks,[0],[0]
"Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings.",2.2 Recursive Neural Networks,[0],[0]
"Non-leaf nodes are sentence constituents, coded by child nodes recursively.",2.2 Recursive Neural Networks,[0],[0]
"Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2.",2.2 Recursive Neural Networks,[0],[0]
"The parent’s representation is composited by
p = f(W ·",2.2 Recursive Neural Networks,[0],[0]
[c1; c2] + b) (1) where W and b are parameters.,2.2 Recursive Neural Networks,[0],[0]
"This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b).
",2.2 Recursive Neural Networks,[0],[0]
"Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014).",2.2 Recursive Neural Networks,[0],[0]
Irsoy and Cardie (2014) build deep RNNs to enhance information interaction.,2.2 Recursive Neural Networks,[0],[0]
"Im-
provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013).",2.2 Recursive Neural Networks,[0],[0]
"They are more suitable for capturing logical information in sentences, such as negation and exclamation.
",2.2 Recursive Neural Networks,[0],[0]
One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss.,2.2 Recursive Neural Networks,[0],[0]
"Thus, RNNs bury illuminating information under a complicated neural architecture.",2.2 Recursive Neural Networks,[0],[0]
"Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009).",2.2 Recursive Neural Networks,[0],[0]
"Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).
",2.2 Recursive Neural Networks,[0],[0]
Recurrent networks.,2.2 Recursive Neural Networks,[0],[0]
"A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree.",2.2 Recursive Neural Networks,[0],[0]
"In such models, meaningful tree structures are also lost, similar to CNNs.",2.2 Recursive Neural Networks,[0],[0]
This section introduces the proposed tree-based convolutional neural networks (TBCNNs).,3 Tree-based Convolution,[0],[0]
"Figure 1c depicts the convolution process on a tree.
",3 Tree-based Convolution,[0],[0]
"First, a sentence is converted to a parse tree, either a constituency or dependency tree.",3 Tree-based Convolution,[0],[0]
The corresponding model variants are denoted as c-TBCNN and d-TBCNN.,3 Tree-based Convolution,[0],[0]
"Each node in the tree is represented as a distributed, real-valued vector.
",3 Tree-based Convolution,[0],[0]
"Then, we design a set of fixed-depth subtree feature detectors, called the tree-based convolution window.",3 Tree-based Convolution,[0],[0]
"The window slides over the entire tree to extract structural information of the sentence, illustrated by a dashed triangle in Figure 1c.",3 Tree-based Convolution,[0],[0]
"Formally, let us assume we have t nodes in the convolution window, x1, · · · ,xt, each represented as an ne-dimensional vector.",3 Tree-based Convolution,[0],[0]
Let nc be the number of feature detectors.,3 Tree-based Convolution,[0],[0]
"The output of the tree-based convolution window, evaluated at the current subtree, is given by the following generic equation.
",3 Tree-based Convolution,[0],[0]
"y = f
( t∑
i=1
Wi ·xi + b )
(2)
where Wi ∈ Rnc×ne is the weight parameter associated with node xi; b ∈ Rnc is the bias term.
",3 Tree-based Convolution,[0],[0]
"Extracted features are thereafter packed into one or more fixed-size vectors by max pooling,
that is, the maximum value in each dimension is taken.",3 Tree-based Convolution,[0],[0]
"Finally, we add a fully connected hidden layer, and a softmax output layer.
",3 Tree-based Convolution,[0],[0]
"From the designed architecture (Figure 1c), we see that our TBCNN models allow short propagation paths between the output layer and any position in the tree.",3 Tree-based Convolution,[0],[0]
"Therefore structural feature learning becomes effective.
",3 Tree-based Convolution,[0],[0]
Several main technical points in tree-based convolution include: (1) How can we represent hidden nodes as vectors in constituency trees?,3 Tree-based Convolution,[0],[0]
"(2) How can we determine weights, Wi, for dependency trees, where nodes may have different numbers of children?",3 Tree-based Convolution,[0],[0]
"(3) How can we pool varying sized and shaped features to fixed-size vectors?
",3 Tree-based Convolution,[0],[0]
"In the rest of this section, we explain model variants in detail.",3 Tree-based Convolution,[0],[0]
"Particularly, Subsections 3.1 and 3.2 address the first and second problems; Subsection 3.3 deals with the third problem by introducing several pooling heuristics.",3 Tree-based Convolution,[0],[0]
Subsection 3.4 presents our training objective.,3 Tree-based Convolution,[0],[0]
"Figure 2a illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase.",3.1 c-TBCNN,[0],[0]
"Sentences are parsed by the Stanford parser;3 further, constituency trees are binarized for simplicity.
",3.1 c-TBCNN,[0],[0]
One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings.,3.1 c-TBCNN,[0],[0]
"Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b).",3.1 c-TBCNN,[0],[0]
"After pretraining, vector representations of nodes are fixed.
",3.1 c-TBCNN,[0],[0]
"We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children cl and cr, their vector representations denoted as p, cl, and cr.",3.1 c-TBCNN,[0],[0]
"The convolution equation, specific for c-TBCNN, is
y = f ( W (c)p ·p +W (c)l ·cl +W (c)r ·cr + b(c) ) where W (c)p , W (c) l , and W (c) r are weights associated with the parent and its child nodes.",3.1 c-TBCNN,[0],[0]
Superscript (c) indicates that the weights are for cTBCNN.,3.1 c-TBCNN,[0],[0]
"For leaf nodes, which do not have children, we set cl and cr to be 0.
3http://nlp.stanford.edu/software/lex-parser.shtml
Tree-based convolution windows can be extended to arbitrary depths straightforwardly.",3.1 c-TBCNN,[0],[0]
"The complexity is exponential to the depth of the window, but linear to the number of nodes.",3.1 c-TBCNN,[0],[0]
"Hence, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time.",3.1 c-TBCNN,[0],[0]
"In our experiments, we use convolution windows of depth 2.",3.1 c-TBCNN,[0],[0]
Dependency trees are another representation of sentence structures.,3.2 d-TBCNN,[0],[0]
The nature of dependency representation leads to d-TBCNN’s major difference from traditional convolution: there exist nodes with different numbers of child nodes.,3.2 d-TBCNN,[0],[0]
"This causes trouble if we associate weight parameters according to positions in the window, which is standard for traditional convolution, e.g., Collobert and Weston (2008) or c-TBCNN.
",3.2 d-TBCNN,[0],[0]
"To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsubj) rather than positions.",3.2 d-TBCNN,[0],[0]
"We believe this strategy makes much sense because dependency types (de Marneffe et al., 2006) reflect the relationship between a governing word and its child words.",3.2 d-TBCNN,[0],[0]
"To be concrete, the generic convolution formula (Equation 2) for d-TBCNN becomes
y = f ( W (d)p ·p + n∑ i=1",3.2 d-TBCNN,[0],[0]
"W (d) r[ci] ·ci + b(d) )
where W (d)p is the weight parameter for the parent p (governing word); W (d)r[ci] is the weight for child ci, who has grammatical relationship r[ci]
to its parent,",3.2 d-TBCNN,[0],[0]
p. Superscript (d) indicates the parameters are for d-TBCNN.,3.2 d-TBCNN,[0],[0]
"Note that we keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix.
",3.2 d-TBCNN,[0],[0]
Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN.,3.2 d-TBCNN,[0],[0]
"As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also have topologies varying in size and shape.",3.3 Pooling Heuristics,[0],[0]
"Dynamic pooling (Socher et al., 2011a) is a common technique for dealing with
this problem.",3.3 Pooling Heuristics,[0],[0]
We propose several heuristics for pooling along a tree structure.,3.3 Pooling Heuristics,[0],[0]
Our generic design criteria for pooling include: (1) Nodes that are pooled to one slot should be “neighboring” from some viewpoint.,3.3 Pooling Heuristics,[0],[0]
"(2) Each slot should have similar numbers of nodes, in expectation, that are pooled to it.",3.3 Pooling Heuristics,[0],[0]
"Thus, (approximately) equal amount of information is aggregated along different parts of the tree.",3.3 Pooling Heuristics,[0],[0]
"Following the above intuition, we propose pooling heuristics as follows.
",3.3 Pooling Heuristics,[0],[0]
•,3.3 Pooling Heuristics,[0],[0]
Global pooling.,3.3 Pooling Heuristics,[0],[0]
"All features are pooled to one vector, shown in Figure 3a.",3.3 Pooling Heuristics,[0],[0]
We take the maximum value in each dimension.,3.3 Pooling Heuristics,[0],[0]
"This simple heuristic is applicable to any structure, including c-TBCNN and d-TBCNN.",3.3 Pooling Heuristics,[0],[0]
• 3-slot pooling for c-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"To preserve
more information over different parts of constituency trees, we propose 3-slot pooling (Figure 3b).",3.3 Pooling Heuristics,[0],[0]
"If a tree has maximum depth d, we pool nodes of less than α · d layers to a TOP slot (α is set to 0.6); lower nodes are pooled to slots LOWER LEFT or LOWER",3.3 Pooling Heuristics,[0],[0]
RIGHT according to their relative position with respect to the root node.,3.3 Pooling Heuristics,[0],[0]
"For a constituency tree, it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time.",3.3 Pooling Heuristics,[0],[0]
"Therefore, we regard 3-slot pooling for c-TBCNN is a “hard mechanism” temporarily.",3.3 Pooling Heuristics,[0],[0]
Further improvement can be addressed in future work.,3.3 Pooling Heuristics,[0],[0]
• k-slot pooling for d-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"Different from
constituency trees, nodes in dependency trees are one-one corresponding to words in a sentence.",3.3 Pooling Heuristics,[0],[0]
"Thus, a total order on features (after convolution) can be defined according to their corresponding word orders.",3.3 Pooling Heuristics,[0],[0]
"For kslot pooling, we can adopt an “equal allocation” strategy, shown in Figure 3c.",3.3 Pooling Heuristics,[0],[0]
"Let i be the position of a word in a sentence (i = 1, 2, · · · , n).",3.3 Pooling Heuristics,[0],[0]
"Its extracted feature vector is pooled to the j-th slot, if
(j − 1) n",3.3 Pooling Heuristics,[0],[0]
k ≤,3.3 Pooling Heuristics,[0],[0]
"i ≤ j n k
We assess the efficacy of pooling quantitatively in Section 4.3.1.",3.3 Pooling Heuristics,[0],[0]
"As we shall see by the experimental results, complicated pooling methods do preserve more information along tree structures to some extent, but the effect is not large.",3.3 Pooling Heuristics,[0],[0]
TBCNNs are not very sensitive to pooling methods.,3.3 Pooling Heuristics,[0],[0]
"After pooling, information is packed into one or more fixed-size vectors (slots).",3.4 Training Objective,[0],[0]
"We add a hidden layer, and then a softmax layer to predict the probability of each target label in a classification task.",3.4 Training Objective,[0],[0]
"The error function of a sample is the standard cross entropy loss, i.e., J = −∑ci=1 ti log yi, where t is the ground truth (one-hot represented), y the output by softmax, and c the number of classes.",3.4 Training Objective,[0],[0]
"To regularize our model, we apply both `2 penalty and dropout (Srivastava et al., 2014).",3.4 Training Objective,[0],[0]
Training details are further presented in Section 4.1 and 4.2.,3.4 Training Objective,[0],[0]
"In this section, we evaluate our models with two tasks, sentiment analysis and question classification.",4 Experimental Results,[0],[0]
We also conduct quantitative and qualitative model analysis in Subsection 4.3.,4 Experimental Results,[0],[0]
Sentiment analysis is a widely studied task for discriminative sentence modeling.,4.1.1 The Task and Dataset,[0],[0]
"The Stanford sentiment treebank4 consists of more than 10,000 movie reviews.",4.1.1 The Task and Dataset,[0],[0]
"Two settings are considered for sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive, positive, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative).",4.1.1 The Task and Dataset,[0],[0]
"Some examples are shown in
4http://nlp.stanford.edu/sentiment/
Table 1.",4.1.1 The Task and Dataset,[0],[0]
"We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction.",4.1.1 The Task and Dataset,[0],[0]
"Binary classification does not contain the neutral class.
",4.1.1 The Task and Dataset,[0],[0]
"In the dataset, phrases (sub-sentences) are also tagged with sentiment labels.",4.1.1 The Task and Dataset,[0],[0]
RNNs deal with them naturally during the recursive process.,4.1.1 The Task and Dataset,[0],[0]
"We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014).",4.1.1 The Task and Dataset,[0],[0]
"The training set therefore has more than 150,000 entries in total.",4.1.1 The Task and Dataset,[0],[0]
"For validating and testing, only whole sentences (root labels) are considered in our experiments.
",4.1.1 The Task and Dataset,[0],[0]
Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing.,4.1.1 The Task and Dataset,[0],[0]
"This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation.",4.1.2 Training Details,[0],[0]
"c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters.",4.1.2 Training Details,[0],[0]
"c-TBCNN’s settings can be found on our website.
",4.1.2 Training Details,[0],[0]
"In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer.",4.1.2 Training Details,[0],[0]
"Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus.",4.1.2 Training Details,[0],[0]
2- slot pooling is applied for d-TBCNN.,4.1.2 Training Details,[0],[0]
"(c-TBCNN uses 3-slot pooling.)
",4.1.2 Training Details,[0],[0]
"To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200.",4.1.2 Training Details,[0],[0]
"We use ReLU (Nair and Hinton, 2010) as the activation function .
",4.1.2 Training Details,[0],[0]
"For regularization, we add `2 penalty for weights with a coefficient of 10−5.",4.1.2 Training Details,[0],[0]
"Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings.",4.1.2 Training Details,[0],[0]
"All hidden layers are dropped out by 50%, and embeddings 40%.",4.1.2 Training Details,[0],[0]
Table 2 compares our models to state-of-the-art results in the task of sentiment analysis.,4.1.3 Performance,[0],[0]
"For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015).",4.1.3 Performance,[0],[0]
c-TBCNN is slightly worse.,4.1.3 Performance,[0],[0]
"It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model).
",4.1.3 Performance,[0],[0]
"Regarding 2-class prediction, we adopted a simple strategy in Irsoy and Cardie (2014),5 where the 5-class network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes.",4.1.3 Performance,[0],[0]
(The neutral class is discarded as in other studies.),4.1.3 Performance,[0],[0]
"This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position.",4.1.3 Performance,[0],[0]
"Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list.
",4.1.3 Performance,[0],[0]
"In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently outperform RNNs (Socher et al., 2011b) to a large extent (50.4–51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than 10%.",4.1.3 Performance,[0],[0]
"Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs.
",4.1.3 Performance,[0],[0]
We also observe d-TBCNN achieves higher performance than c-TBCNN.,4.1.3 Performance,[0],[0]
This suggests that compact tree expressiveness is more important than integrating global information in this task.,4.1.3 Performance,[0],[0]
We further evaluate TBCNN models on a question classification task.6,4.2 Question Classification,[0],[0]
The dataset contains 5452 annotated sentences plus 500 test samples in TREC 10.,4.2 Question Classification,[0],[0]
"We also use the standard split, like Silva et al. (2011).",4.2 Question Classification,[0],[0]
"Target labels contain 6 classes, namely abbreviation, entity, description, human, location, and numeric.",4.2 Question Classification,[0],[0]
"Some examples are also shown in Table 1.
",4.2 Question Classification,[0],[0]
"We chose this task to evaluate our models because the number of training samples is rather small, so that we can know TBCNNs’ performance when applied to datasets of different sizes.",4.2 Question Classification,[0],[0]
"To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively.",4.2 Question Classification,[0],[0]
"We do not back-propagate gradient to embeddings in this
5Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification.",4.2 Question Classification,[0],[0]
We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model.,4.2 Question Classification,[0],[0]
"Our website reviews some related work and provides more discussions.
",4.2 Question Classification,[0],[0]
"6http://cogcomp.cs.illinois.edu/Data/QA/QC/
task.",4.2 Question Classification,[0],[0]
"Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%.
",4.2 Question Classification,[0],[0]
Table 3 compares our models to various other methods.,4.2 Question Classification,[0],[0]
"The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011).",4.2 Question Classification,[0],[0]
Their method utilizes more than 10k features and 60 hand-coded rules.,4.2 Question Classification,[0],[0]
"On the contrary, our TBCNN models do not use a single human-engineered feature or rule.",4.2 Question Classification,[0],[0]
"Despite this, c-TBCNN achieves similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to 96%.",4.2 Question Classification,[0],[0]
"To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.
",4.2 Question Classification,[0],[0]
"The result also shows that both c-TBCNN and d-TBCNN reduce the error rate to a large extent, compared with other neural architectures in this task.",4.2 Question Classification,[0],[0]
"In this part, we analyze our models quantitatively and qualitatively in several aspects, shedding some light on the mechanism of TBCNNs.",4.3 Model Analysis,[0],[0]
The extracted features by tree-based convolution have topologies varying in size and shape.,4.3.1 The Effect of Pooling,[0],[0]
We propose in Section 3.3 several heuristics for pooling.,4.3.1 The Effect of Pooling,[0],[0]
"This subsection aims to provide a fair comparison among these pooling methods.
",4.3.1 The Effect of Pooling,[0],[0]
One reasonable protocol for comparison is to tune all hyperparameters for each setting and compare the highest accuracy.,4.3.1 The Effect of Pooling,[0],[0]
"This methodology, however, is too time-consuming, and depends largely on the quality of hyperparameter tuning.",4.3.1 The Effect of Pooling,[0],[0]
An alternative is to predefine a set of sensible hyperparameters and report the accuracy under the same setting.,4.3.1 The Effect of Pooling,[0],[0]
"In this experiment, we chose the latter protocol, where hidden layers are all 300- dimensional; no `2 penalty is added.",4.3.1 The Effect of Pooling,[0],[0]
Each configuration was run five times with different random initializations.,4.3.1 The Effect of Pooling,[0],[0]
"We summarize the mean and standard deviation in Table 4.
",4.3.1 The Effect of Pooling,[0],[0]
"As the results imply, complicated pooling is better than global pooling to some degree for both model variants.",4.3.1 The Effect of Pooling,[0],[0]
"But the effect is not strong; our models are not that sensitive to pooling methods, which mainly serve as a necessity for dealing with varying-structure data.",4.3.1 The Effect of Pooling,[0],[0]
"In our experiments, we apply 3-slot pooling for c-TBCNN and 2-slot pooling for d-TBCNN.
",4.3.1 The Effect of Pooling,[0],[0]
"Comparing with other studies in the literature, we also notice that pooling is very effective and efficient in information gathering.",4.3.1 The Effect of Pooling,[0],[0]
"Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification.",4.3.1 The Effect of Pooling,[0],[0]
Our TBCNNs are typically trained within 25 epochs.,4.3.1 The Effect of Pooling,[0],[0]
We analyze how sentence lengths affect our models.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"Sentences are split into 7 groups by length, with granularity 5.",4.3.2 The Effect of Sentence Lengths,[0],[0]
A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457.,4.3.2 The Effect of Sentence Lengths,[0],[0]
Figure 4 presents accuracies versus lengths in TBCNNs.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b).",4.3.2 The Effect of Sentence Lengths,[0],[0]
"Thus, we think our reimplementation is fair and that the comparison is sensible.
",4.3.2 The Effect of Sentence Lengths,[0],[0]
We observe that c-TBCNN and d-TBCNN yield very similar behaviors.,4.3.2 The Effect of Sentence Lengths,[0],[0]
They consistently outperform the RNN in all scenarios.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"This result confirms our theoretical analysis in Section 2—for long sentences, the propagation paths in RNNs are deep, causing RNNs’ difficulty in information processing.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"By contrast, our models explore structural information more effectively with
tree-based convolution.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"As information from any part of the tree can propagate to the output layer with short paths, TBCNNs are more capable for sentence modeling, especially for long sentences.",4.3.2 The Effect of Sentence Lengths,[0],[0]
Visualization is important to understanding the mechanism of neural networks.,4.3.3 Visualization,[0],[0]
"For TBCNNs, we would like to see how the extracted features (after convolution) are further processed by the max pooling layer, and ultimately related to the supervised task.
",4.3.3 Visualization,[0],[0]
"To show this, we trace back where the max pooling layer’s features come from.",4.3.3 Visualization,[0],[0]
"For each dimension, the pooling layer chooses the maximum value from the nodes that are pooled to it.",4.3.3 Visualization,[0],[0]
"Thus, we can count the fraction in which a node’s features are gathered by pooling.",4.3.3 Visualization,[0],[0]
"Intuitively, if a node’s features are more related to the task, the fraction tends to be larger, and vice versa.
",4.3.3 Visualization,[0],[0]
"Figure 5 illustrates an example processed by dTBCNN in the task of sentiment analysis.7 Here, we applied global pooling because information tracing is more sensible with one pooling slot.",4.3.3 Visualization,[0],[0]
"As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest.",4.3.3 Visualization,[0],[0]
"The 2-layer windows corresponding to “visual will impress viewers,” “the stunning dreamlike visual,” say, are discriminative to the sentence’s sentiment.",4.3.3 Visualization,[0],[0]
"Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling.",4.3.3 Visualization,[0],[0]
"On the other hand, words like the, will, even are known as stop words (Fox, 1989).",4.3.3 Visualization,[0],[0]
"They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered.",4.3.3 Visualization,[0],[0]
"Such results are consistent with human intuition.
",4.3.3 Visualization,[0],[0]
We further observe that tree-based convolution does integrate information of different words in the window.,4.3.3 Visualization,[0],[0]
"For example, the word stunning appears in two windows: (a) the window “stunning” itself, and (b) the window of “the stunning dreamlike visual,” with root node visual, stunning acting as a child.",4.3.3 Visualization,[0],[0]
"We see that Window b is more relevant to the ultimate sentiment than Window a, with fractions 0.19 versus 0.07, even though the root visual itself is neutral in sentiment.",4.3.3 Visualization,[0],[0]
"In fact,
7We only have space to present one example in the paper.",4.3.3 Visualization,[0],[0]
This example was not chosen deliberately.,4.3.3 Visualization,[0],[0]
"Similar traits can be found through out the entire gallery, available on our website.",4.3.3 Visualization,[0],[0]
"Also, we only present d-TBCNN, noticing that dependency trees are intrinsically more suitable for visualization since we know the “meaning” of every node.
",4.3.3 Visualization,[0],[0]
"Window a has a larger fraction than the sum of its children’s (the windows of “the,” “stunning,” and “dreamlike”).",4.3.3 Visualization,[0],[0]
"In this paper, we proposed a novel neural discriminative sentence model based on sentence parsing structures.",5 Conclusion,[0],[0]
"Our model can be built upon either constituency trees (denoted as c-TBCNN) or dependency trees (d-TBCNN).
",5 Conclusion,[0],[0]
Both variants have achieved high performance in sentiment analysis and question classification.,5 Conclusion,[0],[0]
"d-TBCNN is slightly better than c-TBCNN in our experiments, and has outperformed previous stateof-the-art results in both tasks.",5 Conclusion,[0],[0]
"The results show that tree-based convolution can capture sentences’ structural information effectively, which is useful for sentence modeling.",5 Conclusion,[0],[0]
This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015 and 91318301.,Acknowledgments,[0],[0]
This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling.,abstractText,[0],[0]
Our model leverages either constituency trees or dependency trees of sentences.,abstractText,[0],[0]
"The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling.",abstractText,[0],[0]
"Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction.",abstractText,[0],[0]
We evaluate our models on two tasks: sentiment analysis and question classification.,abstractText,[0],[0]
"In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering.",abstractText,[0],[0]
"We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.",abstractText,[0],[0]
Discriminative Neural Sentence Modeling by Tree-Based Convolution,title,[0],[0]
Representation learning remains an outstanding research problem in machine learning and computer vision.,1. Introduction,[0],[0]
"Recently there is a rising interest in disentangled representations, in which each component of learned features refers to a semantically meaningful concept.",1. Introduction,[0],[0]
"In the example of video sequence modelling, an ideal disentangled representation would be able to separate time-independent concepts (e.g. the identity of the object in the scene) from dynamical information (e.g. the time-varying position and the orientation or pose of that object).",1. Introduction,[0],[0]
"Such disentangled represen-
1University of Cambridge, UK 2Disney Research, Los Angeles, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Yingzhen Li<yl494@cam.ac.uk>, Stephan Mandt <stephan.mandt@disneyresearch.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tations would open new efficient ways of compression and style manipulation, among other applications.
",1. Introduction,[0],[0]
"Recent work has investigated disentangled representation learning for images within the framework of variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014).",1. Introduction,[0],[0]
"Some of them, e.g. the β-VAE method (Higgins et al., 2016), proposed new objective functions/training techniques that encourage disentanglement.",1. Introduction,[0],[0]
"On the other hand, network architecture designs that directly enforce factored representations have also been explored by e.g. Siddharth et al. (2017); Bouchacourt et al. (2017).",1. Introduction,[0],[0]
"These two types of approaches are often mixed together, e.g. the infoGAN approach (Chen et al., 2016) partitioned the latent space and proposed adding a mutual information regularisation term to the vanilla GAN loss.",1. Introduction,[0],[0]
"Mathieu et al. (2016) also partitioned the encoding space into style and content components, and performed adversarial training to encourage the datapoints from the same class to have similar content representations, but diverse style features.
",1. Introduction,[0],[0]
Less research has been conducted for unsupervised learning of disentangled representations of sequences.,1. Introduction,[0],[0]
"For video sequence modelling, Villegas et al. (2017) and Denton & Birodkar (2017) utilised different networks to encode the content and dynamics information separately, and trained the auto-encoders with a combination of reconstruction loss and GAN loss.",1. Introduction,[0],[0]
"Structured (Johnson et al., 2016) and Factorised VAEs (Deng et al., 2017) used hierarchical priors to learn more interpretable latent variables.",1. Introduction,[0],[0]
Hsu et al. (2017) designed a structured VAE in the context of speech recognition.,1. Introduction,[0],[0]
Their VAE architecture is trained using a combination of the standard variational lower bound and a discriminative regulariser to further encourage disentanglement.,1. Introduction,[0],[0]
"More related work is discussed in Section 3.
",1. Introduction,[0],[0]
"In this paper, we propose a generative model for unsupervised structured sequence modelling, such as video or audio.",1. Introduction,[0],[0]
"We show that, in contrast to previous approaches, a disentangled representation can be achieved by a careful design of the probabilistic graphical model.",1. Introduction,[0],[0]
"In the proposed architecture, we explicitly use a latent variable to represent content, i.e., information that is invariant through the sequence, and a set of latent variables associated to each frame to represent dynamical information, such as pose and position.",1. Introduction,[0],[0]
"Com-
pared to the mentioned previous models that usually predict future frames conditioned on the observed sequences, we focus on learning the distribution of the video/audio content and dynamics to enable sequence generation without conditioning.",1. Introduction,[0],[0]
"Therefore our model can also generalise to unseen sequences, which is confirmed by our experiments.",1. Introduction,[0],[0]
"In more detail, our contributions are as follows:
• Controlled generation.",1. Introduction,[0],[0]
Our architecture allows us to approximately control for content and dynamics when generating videos.,1. Introduction,[0],[0]
"We can generate random dynamics for fixed content, and random content for fixed dynamics.",1. Introduction,[0],[0]
"This gives us a controlled way of manipulating a video/audio sequence, such as swapping the identity of moving objects or the voice of a speaker.
",1. Introduction,[0],[0]
• Efficient encoding.,1. Introduction,[0],[0]
Our representation is more data efficient than encoding a video frame by frame.,1. Introduction,[0],[0]
"By factoring out a separate variable that encodes content, our dynamical latent variables can have smaller dimensions.",1. Introduction,[0],[0]
"This may be promising when it comes to end-to-end neural video encoding methods.
",1. Introduction,[0],[0]
"• We design a new metric that allow us to verify disentanglement of the latent variables, by investigating the stability of an object classifier over time.
",1. Introduction,[0],[0]
"• We give empirical evidence, based on video data of a physics simulator, that for long sequences, a stochastic transition model generates more realistic dynamics.
",1. Introduction,[0],[0]
The paper is structured as follows.,1. Introduction,[0],[0]
Section 2 introduces the generative model and the problem setting.,1. Introduction,[0],[0]
Section 3 discusses related work.,1. Introduction,[0],[0]
Section 4 presents three experiments on video and speech data.,1. Introduction,[0],[0]
"Finally, Section 5 concludes the paper and discusses future research directions.",1. Introduction,[0],[0]
"Let x1:T = (x1,x2, ...,xT ) denote a high dimensional sequence, such as a video with T consecutive frames.",2. The model,[0],[0]
"Also, assume the data distribution of the training sequences is pdata(x1:T ).",2. The model,[0],[0]
"In this paper, we model the observed data with a latent variable model that separates the representation of time-invariant concepts (e.g. object identities) from those of time-varying concepts (e.g. pose information).
",2. The model,[0],[0]
Generative model.,2. The model,[0],[0]
"Consider the following probabilistic model, which is also visualised in Figure 1:
pθ(x1:T , z1:T ,f) = pθ(f) T∏ t=1 pθ(zt|z<t)pθ(xt|zt,f).
",2. The model,[0],[0]
(1) We use the convention that z0 = 0.,2. The model,[0],[0]
The generation of frame xt at time t depends on the corresponding latent variables zt and f .,2. The model,[0],[0]
"θ are model parameters.
",2. The model,[0],[0]
"Ideally, f will be capable of modelling global aspects of the whole sequence which are time-invariant, while zt will encode time-varying features.",2. The model,[0],[0]
"This separation may be achieved when choosing the dimensionality of zt to be small enough, thus reserving zt only for time-dependent features while compressing everything else into f .",2. The model,[0],[0]
"In the context of video encodings, zt would thus encode a “morphing transformation”, which encodes how a frame at time t is morphed into a frame at time t+ 1.
Inference models.",2. The model,[0],[0]
"We use variational inference to learn an approximate posterior over latent variables given data (Jordan et al., 1999).",2. The model,[0],[0]
"This involves an approximating distribution q. We train the generative model with the VAE algorithm (Kingma & Welling, 2013):
max θ,φ
EpD(x1:T )",2. The model,[0],[0]
"[ Eqφ [ log pθ(x1:T , z1:T ,f)
qφ(z1:T ,f |x1:T )
",2. The model,[0],[0]
]] .,2. The model,[0],[0]
"(2)
To quantify the effect of the architecture of q on the learned generative model, we test with two types of q factorisation structures as follows.
",2. The model,[0],[0]
"The first architecture constructs a factorised q distribution
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )",2. The model,[0],[0]
"T∏
t=1
qφ(zt|xt) (3)
",2. The model,[0],[0]
as the amortised variational distribution.,2. The model,[0],[0]
We refer to this as “factorised q” in the experiments section.,2. The model,[0],[0]
This factorization assumes that content features are approximately independent of motion features.,2. The model,[0],[0]
"Furthermore, note that the distribution over content features is conditioned on the entire time series, whereas the dynamical features are only conditioned on the individual frames.
",2. The model,[0],[0]
"The second encoder assumes that the variational posterior of z1:T depends on f , and the q distribution has the following architecture:
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )qφ(z1:T |f ,x1:T ), (4)
and the distribution q(z1:T |f ,x1:T ) is conditioned on the entire time series.",2. The model,[0],[0]
"It can be implemented by e.g. a bidirectional LSTM (Graves & Schmidhuber, 2005) conditioned on f , followed by an RNN taking the bi-LSTM hidden states as the inputs.",2. The model,[0],[0]
We provide a visualisation of the corresponding computation graph in the appendix.,2. The model,[0],[0]
This encoder is referred to as “full q”.,2. The model,[0],[0]
"The idea behind the structured approximation is that content may affect dynamics: in video, the shape of objects may be informative about their motion patterns, thus z1:T is conditionally dependent on f .",2. The model,[0],[0]
"The architectures of the generative model and both encoders are visualised in Figure 1.
",2. The model,[0],[0]
Unconditional generation.,2. The model,[0],[0]
"After training, one can use the generative model to synthesise video or audio sequences
by sampling the latent variables from the prior and decoding them.",2. The model,[0],[0]
"Furthermore, the proposed generative model allows generation of multiple sequences entailing the same global information (e.g. the same object in a video sequence), simply by fixing f ∼ p(f), sampling different zk1:T ∼ p(z1:T ), k = 1, ...,K, and generating the observations xkt ∼ p(xt|zkt ,f).",2. The model,[0],[0]
"Generating sequences with similar dynamics is done analogously, by fixing z1:T ∼ p(z1:T ) and sampling fk, k = 1, ...K from the prior.
",2. The model,[0],[0]
Conditional generation.,2. The model,[0],[0]
"Together with the encoder, the model also allows conditional generation of sequences.",2. The model,[0],[0]
"As an example, given a video sequence x1:T as reference, one can manipulate the latent variables and generate new sequences preserving either the object identity or the pose/movement information.",2. The model,[0],[0]
"This is done by conditioning on f ∼ q(f |x1:T ) for a given x1:T then randomising z1:T from the prior, or the other way around.
",2. The model,[0],[0]
Feature swapping.,2. The model,[0],[0]
One might also want to generate a new video sequence with the object identity and pose information encoded from different sequence.,2. The model,[0],[0]
"Given two sequences xa1:T and x b 1:T , the synthesis process first infers the latent variables fa ∼ q(f |xa1:T ) and zb1:T ∼ q(z1:T |xb1:T )1, then produces a new sequence by sampling xnewt ∼ p(xt|zbt ,fa).",2. The model,[0],[0]
"This allows us to control both the content and the dynamics of the generated sequence, which can be applied to e.g. conversion of voice of the speaker in a speech sequence.",2. The model,[0],[0]
Research on learning disentangled representation has mainly focused on two aspects: the training objective and the generative model architecture.,3. Related work,[0],[0]
"Regarding the loss function design for VAE models, Higgins et al. (2016) propose the β-VAE by scaling up the KL[q(z|x)||p(z)] term in the variational lower-bound with β > 1 to encourage learning of independent attributes (as the prior p(z) is usually factorised).",3. Related work,[0],[0]
"While the β-VAE has been shown effective in learning better representations for natural images and might be able to further improve the performance of our model, we do not
1For the full q encoder",3. Related work,[0],[0]
"it also requires f b ∼ q(f |xb1:T ).
",3. Related work,[0],[0]
"test this recipe here to demonstrate that disentanglement can be achieved by a careful model design.
",3. Related work,[0],[0]
"For sequence modelling, a number of prior publications have extended VAE to video and speech data (Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014; Chung et al., 2015).",3. Related work,[0],[0]
"These models, although being able to generate realistic sequences, do not explicitly disentangle the representation of time-invariant and time-dependent information.",3. Related work,[0],[0]
"Thus it is inconvenient for these models to perform tasks such as controlled generation and feature swapping.
",3. Related work,[0],[0]
"For GAN-like models, both Villegas et al. (2017) and Denton & Birodkar (2017) proposed an auto-encoder architecture for next frame prediction, with two separate encoders responsible for content and pose information at each time step.",3. Related work,[0],[0]
"While in Villegas et al. (2017), the pose information is extracted from the difference between two consecutive frames xt−1 and xt, Denton & Birodkar (2017) directly encoded xt for both pose and content, and further designed a training objective to encourage learning of disentangled representations.",3. Related work,[0],[0]
"On the other hand, Vondrick et al. (2016) used a spatio-temporal convolutional architecture to disentangle a video scene’s foreground from its background.",3. Related work,[0],[0]
"Although it has successfully achieved disentanglement, we note that the time-invariant information in this model is predefined to represent the background, rather than learned from the data automatically.",3. Related work,[0],[0]
"Also this architecture is suitable for video sequences only, unlike our model which can be applied to any type of sequential data.
",3. Related work,[0],[0]
"Very recent work (Hsu et al., 2017) introduced the factorised hierarchical variational auto-encoder (FHVAE) for unsupervised learning of disentangled representation of speech data.",3. Related work,[0],[0]
"Given a speech sequence that has been partitioned into segments {xn1:T }Nn=1, FHVAE models the joint distribution of {xn1:T }Nn=1 and latent variables as follows:
p({xn1:T , zn1 , zn2 },µ2) = p(µ2) N∏
n=1
p(xn1:T , z n 1 , z n 2 |µ2),
p(xn1:T , z n 1 , z n 2 |µ2) = p(zn1 )p(zn2",3. Related work,[0],[0]
"|µ2)p(xn1:T |zn1 , zn2 ).
",3. Related work,[0],[0]
"Here the zn2 variable has a hierarchical prior p(z n 2 |µ2) = N (µ2, σ2I), p(µ2) = N (0, λI).",3. Related work,[0],[0]
"The authors showed that by having different prior structures for zn1 and z n 2 , it allows the model to encode with zn2 speech sequence-level
attributes (e.g. pitch of a speaker), and other residual information with zn1 .",3. Related work,[0],[0]
"A discriminative training objective (see discussions in Section 4.2) is added to the variational lowerbound, which has been shown to further improve the quality of the disentangled representation.",3. Related work,[0],[0]
"Our model can also benefit from the usage of hierarchical prior distributions, e.g. fn ∼ p(f |µ2),µ2 ∼ p(µ2), and we leave the investigation to future work.",3. Related work,[0],[0]
We carried out experiments both on video data (Section 4.1) as well as speech data (Section 4.2).,4. Experiments,[0],[0]
"In both setups, we find strong evidence that our model learns an approximately disentangled representation that allows for conditional generation and feature swapping.",4. Experiments,[0],[0]
We further investigated the efficiency for encoding long sequences with a stochastic transition model in Section 4.3.,4. Experiments,[0],[0]
The detailed model architectures of the networks used in each experiment are reported in the appendix.,4. Experiments,[0],[0]
"We present an initial test of the proposed VAE architecture on a dataset of video game “sprites”, i.e. animated cartoon characters whose clothing, pose, hairstyle, and skin color we can fully control.",4.1. Video sequence: Sprites,[0],[0]
"This dataset comes from an open-source video game project called Liberated Pixel Cup2, and has been also considered in Reed et al. (2015); Mathieu et al. (2016) for image processing experiments.",4.1. Video sequence: Sprites,[0],[0]
"Our experiments show that static attributes such as hair color and clothing are well preserved over time for randomly generated videos.
",4.1. Video sequence: Sprites,[0],[0]
Data and preprocessing.,4.1. Video sequence: Sprites,[0],[0]
"We downloaded and selected the online available sprite sheets3, and organised them into 4 attribute categories (skin color, tops, pants and hairstyle) and 9 action categories (walking, casting spells and slashing, each with three viewing angles).",4.1. Video sequence: Sprites,[0],[0]
"In order to avoid a combinatorial explosion problem, each of the attribute categories contains 6 possible variants (see Figure 2), therefore it leads to 64 = 1296 unique characters in total.",4.1. Video sequence: Sprites,[0],[0]
We used 1000 of them for training/validation and the rest of them for testing.,4.1. Video sequence: Sprites,[0],[0]
The resulting dataset consists of sequences with T = 8 frames of dimension 64× 64.,4.1. Video sequence: Sprites,[0],[0]
Note here we did not use the labels for training the generative model.,4.1. Video sequence: Sprites,[0],[0]
"Instead these labels on the data frames are used to train a classifier that is later deployed to produce quantitative evaluations on the VAE, see below.
",4.1. Video sequence: Sprites,[0],[0]
Qualitative analysis.,4.1. Video sequence: Sprites,[0],[0]
We start with a qualitative evaluation of our VAE architecture.,4.1. Video sequence: Sprites,[0],[0]
"Figure 3 shows both re-
2http://lpc.opengameart.org/",4.1. Video sequence: Sprites,[0],[0]
"3https://github.com/jrconway3/
Universal-LPC-spritesheet
constructed as well as generated video sequences from our model.",4.1. Video sequence: Sprites,[0],[0]
Each panel shows three video sequences with time running from left to right.,4.1. Video sequence: Sprites,[0],[0]
"Panel (a) shows parts of the original data from the test set, and (b) shows its reconstruction.
",4.1. Video sequence: Sprites,[0],[0]
The sequences visualised in panel (c) are generated using zt ∼ q(zt|xt) but f ∼ p(f).,4.1. Video sequence: Sprites,[0],[0]
"Hence, the dynamics are imposed by the encoder, but the identity is sampled from the prior.",4.1. Video sequence: Sprites,[0],[0]
"We see that panel (c) reveals the same motion patterns as (a), but has different character identities.",4.1. Video sequence: Sprites,[0],[0]
"Conversely, in panel (d) we take the identity from the encoder, but sample the dynamics from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Panel (d) reveals the same characters as (a), but different motion patterns.
",4.1. Video sequence: Sprites,[0],[0]
Panels (e) and (f) focus on feature swapping.,4.1. Video sequence: Sprites,[0],[0]
"In (e), the frames are constructed by computing zt ∼ q(zt|xt) on one input sequence but f encoded on another input sequence.",4.1. Video sequence: Sprites,[0],[0]
"These panels demonstrate that the encoder and the decoder have learned a factored representation for content and pose.
",4.1. Video sequence: Sprites,[0],[0]
"Panels (g) and (h) focus on conditional generation, showing randomly generated sequences that share the same f or z1:T samples from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Thus, in panel (g) we see the same character performing different actions, and in (h) different characters performing the same motion.",4.1. Video sequence: Sprites,[0],[0]
"This again illustrates that the prior model disentangles the representation.
",4.1. Video sequence: Sprites,[0],[0]
Quantitative analysis.,4.1. Video sequence: Sprites,[0],[0]
"Next we perform quantitative evaluations of the generative model, using a classifier trained on the labelled frames.",4.1. Video sequence: Sprites,[0],[0]
"Empirically, we find that the fully factorized and structured inference networks produce almost identical results here, presumably because in this dataset the object identity and pose information are truly independent.",4.1. Video sequence: Sprites,[0],[0]
"Therefore we only report results on the fully factorised q distribution case.
",4.1. Video sequence: Sprites,[0],[0]
The first evaluation task considers reconstructing the test sequences with encoded f and randomly sampled zt (in the same way as to produce panel (d) in Figure 3).,4.1. Video sequence: Sprites,[0],[0]
Then we compare the classifier outputs on both the original frames and the reconstructed frames.,4.1. Video sequence: Sprites,[0],[0]
"If the character’s identity is preserved over time, the classifier should produce identical probability vectors on the data frames and the reconstructed frames (denoted as pdata and precon respectively).
",4.1. Video sequence: Sprites,[0],[0]
We evaluate the similarity between the original and reconstructed sequences both in terms of the disagreement of the predicted class labels maxi[precon(i)] 6= maxi[pdata(i)] and the KL-divergence KL[precon||pdata].,4.1. Video sequence: Sprites,[0],[0]
We also compute the two metrics on the action predictions using reconstructed sequences with randomised f and inferred zt.,4.1. Video sequence: Sprites,[0],[0]
The results in Table 1 indicate that the learned representation is indeed factorised.,4.1. Video sequence: Sprites,[0],[0]
"For example, in the fix-f generation test, only 4% out of 296× 9 data-reconstruction frame pairs contain characters whose generated skin color differs from the rest, where in the case of hairstyle preservation the disagreement rate is only 0.06%.",4.1. Video sequence: Sprites,[0],[0]
The KL metric is also much smaller than the KL-divergence KL[prandom||pdata],4.1. Video sequence: Sprites,[0],[0]
"where prandom = (1/Nclass, ..., 1/Nclass), indicating that our result is significant.
",4.1. Video sequence: Sprites,[0],[0]
"In the second evaluation, we test whether static attributes of generated sequences, such as clothing or hair style, are preserved over time.",4.1. Video sequence: Sprites,[0],[0]
"We sample 200 video sequences from the generator, using the same f but different latent dynamics z1:T .",4.1. Video sequence: Sprites,[0],[0]
We use the trained classifier to predict both the attributes and the action classes for each of the generated frames.,4.1. Video sequence: Sprites,[0],[0]
"Results are shown in Figure 4(a), where we plot the prediction of the classifiers for each frame over time.",4.1. Video sequence: Sprites,[0],[0]
"For example, the trajectory curve in the “skin color” panel in Figure 4(a) corresponds to the skin color attribute classification results for frames x1:T of a generated video sequence.",4.1. Video sequence: Sprites,[0],[0]
"We repeat this process 5 times with different f samples,
where each f corresponds to one color.
",4.1. Video sequence: Sprites,[0],[0]
"It becomes evident that those lines with the same color are clustered together, confirming that f mainly controls the generation of time-invariant attributes.",4.1. Video sequence: Sprites,[0],[0]
"Also, most character attributes are preserved over time, e.g. for the attribute “tops”, the trajectories are mostly straight lines.",4.1. Video sequence: Sprites,[0],[0]
"However, some of the trajectories for the attributes drift away from the majority class.",4.1. Video sequence: Sprites,[0],[0]
"We conjecture that this is due of the mass-covering behaviour of (approximate) maximum likelihood training, which makes the trained model generate characters that do not exist in the dataset.",4.1. Video sequence: Sprites,[0],[0]
"Indeed the middle row of panel (c) in Figure 3 contains a character with an unseen hairstyle, showing that our model is able to generalise beyond the training set.",4.1. Video sequence: Sprites,[0],[0]
"On the other hand, the sampling process returns sequences with diverse actions as depicted in the action panel, meaning that f contains little information regarding the video dynamics.
",4.1. Video sequence: Sprites,[0],[0]
"We performed similar tests on sequence generations with shared latent dynamics z1:T but different f , shown in Figure 4(b).",4.1. Video sequence: Sprites,[0],[0]
"The experiment is repeated 5 times as well, and again trajectories with the same color encoding correspond to videos generated with the same z1:T (but different f ).",4.1. Video sequence: Sprites,[0],[0]
Here we also observe diverse trajectories for the attribute categories.,4.1. Video sequence: Sprites,[0],[0]
"In contrast, the characters’ actions are mostly the same.",4.1. Video sequence: Sprites,[0],[0]
These two test results again indicate that the model has successfully learned disentangled representations of character identities and actions.,4.1. Video sequence: Sprites,[0],[0]
"Interestingly we observe multi-modalities in the action domain for the generated sequences, e.g. the trajectories in the action panel of Figure 4(b) are jumping between different levels.",4.1. Video sequence: Sprites,[0],[0]
We also visualise in Figure 5 generated sequences of the “turning” action that is not present in the dataset.,4.1. Video sequence: Sprites,[0],[0]
It again shows that the trained model generalises to unseen cases.,4.1. Video sequence: Sprites,[0],[0]
We also experiment on audio sequence data.,4.2. Speech data: TIMIT,[0],[0]
Our disentangled representation allows us to convert speaker identities into each other while conditioning on the content of the speech.,4.2. Speech data: TIMIT,[0],[0]
"We also show that our model gives rise to speaker verification, where we outperform a recent probabilistic baseline model.
(a) Trajectory plots on the generated sequences with shared f .
(b) Trajectory plots on the generated sequences with shared z1:T .
",4.2. Speech data: TIMIT,[0],[0]
Figure 4.,4.2. Speech data: TIMIT,[0],[0]
"Classification test on the generated video sequences with shared f (top) or shared z1:T (bottom), respectively.",4.2. Speech data: TIMIT,[0],[0]
The experiment is repeated 5 times and depicted by different color coding.,4.2. Speech data: TIMIT,[0],[0]
"The x and y axes are time and the class id of the attributes, respectively.
",4.2. Speech data: TIMIT,[0],[0]
Figure 5.,4.2. Speech data: TIMIT,[0],[0]
Visualising multi-modality in action space.,4.2. Speech data: TIMIT,[0],[0]
"In this case the characters turn from left to right, and this action sequence is not observed in data.
Data and preprocessing.",4.2. Speech data: TIMIT,[0],[0]
"The TIMIT data (Garofolo et al., 1993) contains broadband 16kHz recordings of phonetically-balanced read speech.",4.2. Speech data: TIMIT,[0],[0]
A total of 6300 utterances (5.4 hours) are presented with 10 sentences from each of the 630 speakers (70% male and 30% female).,4.2. Speech data: TIMIT,[0],[0]
"We follow Hsu et al. (2017) for data pre-processing: the raw speech waveforms are first split into sub-sequences of 200ms, and then preprocessed with sparse fast Fourier transform to obtain a 200 dimensional log-magnitude spectrum, computed every 10ms.",4.2. Speech data: TIMIT,[0],[0]
"This implies T = 20 for the observation x1:T .
",4.2. Speech data: TIMIT,[0],[0]
Qualitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We perform voice conversion experiments to demonstrate the disentanglement of the learned representation.,4.2. Speech data: TIMIT,[0],[0]
The goal here is to convert male voice to female voice (and vice versa) with the speech content being preserved.,4.2. Speech data: TIMIT,[0],[0]
"Assuming that f has learned the representation of speaker’s identity, the conversion can be done by first encoding two sequences xmale1:T and x female 1:T with q to obtain representations {fmale, zmale1:T } and {f female, zfemale1:T }, then construct the converted sequence by feeding f female and zmale1:T to the decoder p(xt|zt,f).",4.2. Speech data: TIMIT,[0],[0]
Figure 6 shows the reconstructed spectrogram after the swapping process of the f features.,4.2. Speech data: TIMIT,[0],[0]
"We also provide the reconstructed speech waveforms using the Griffin-Lim algorithm (Griffin & Lim, 1984) in the appendix.
",4.2. Speech data: TIMIT,[0],[0]
The experiments show that the harmonics of the converted speech sequences shifted to higher frequency in the “male to female” test and vice versa.,4.2. Speech data: TIMIT,[0],[0]
"Also the pitch (the red arrow in Figure 6 indicating the fundamental frequency, i.e. the first harmonic) of the converted sequence (b) is close to the pitch of (c), same as for the comparison between (d) and (a).",4.2. Speech data: TIMIT,[0],[0]
"By an informal listening test of the speech sequence pairs (a, d) and (b, c), we confirm that the speech content is preserved.",4.2. Speech data: TIMIT,[0],[0]
"These results show that our model is successfully applied to speech sequences for learning disentangled representations.
",4.2. Speech data: TIMIT,[0],[0]
Quantitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We further follow Hsu et al. (2017) to use speaker verification for quantitative evaluation.,4.2. Speech data: TIMIT,[0],[0]
"Speaker verification is the process of verifying the claimed identity of a speaker, usually by comparing the “features” wtest of the test utterance xtest1:",4.2. Speech data: TIMIT,[0],[0]
T1 with those of the target utterance xtarget1:T2 from the claimed identity.,4.2. Speech data: TIMIT,[0],[0]
"The claimed identity is confirmed if the cosine similarity cos(wtest,wtarget) is grater than a given threshold (Dehak et al., 2009).",4.2. Speech data: TIMIT,[0],[0]
By varying ∈,4.2. Speech data: TIMIT,[0],[0]
"[0, 1], we report the verification performance in terms of equal error rate (EER), where the false rejection rate equals the false acceptance rate.
",4.2. Speech data: TIMIT,[0],[0]
The extraction of the “features” is crucial for the performance of this speaker verification system.,4.2. Speech data: TIMIT,[0],[0]
"Given a speech sequence containing N segments {x(n)1:T }Nn=1, we constructed two types of “features”, one by computing µf as the mean
of q(f (n)|x(n)1:T ) across the segments, and the other by extracting the mean µzt of q(zt|x1:T ) and averaging them across both time T and segments.",4.2. Speech data: TIMIT,[0],[0]
"In formulas,
µf = 1
N N∑ n=1 µfn , µfn = Eq(fn|xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[f n],
µz = 1
TN T∑ t=1 N∑ n=1 µznt , µznt = Eq(znt |xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[z n t ].
We also include two baseline results from Hsu et al. (2017): one used the i-vector method (Dehak et al., 2011) for feature extraction, and the other one used µ1 and µ2 (analogous to µz and µf in our case) from a trained FHVAE model on Mel-scale filter bank (FBank) features.
",4.2. Speech data: TIMIT,[0],[0]
"The test data were created from the test set of TIMIT, containing 24 unique speakers and 18,336 pairs for verification.",4.2. Speech data: TIMIT,[0],[0]
Table 2 presents the EER results of the proposed model and baselines.4,4.2. Speech data: TIMIT,[0],[0]
"It is clear that the µf feature performs significantly better than the i-vector method, indicating that the f variable has learned to represent a speaker’s identity.",4.2. Speech data: TIMIT,[0],[0]
"On the other hand, using µz as the features returns considerably worse EER rates compared to the i-vector method and µf feature.",4.2. Speech data: TIMIT,[0],[0]
"This is good, as it indicates that the z variables contain less information about the speaker’s identity, again validating the success of disentangling time-variant and time-independent information.",4.2. Speech data: TIMIT,[0],[0]
"Note that the EER results for µz get worse when using the full q encoder, and in the 64 dimensional feature case the verification performance of µf improves slightly.",4.2. Speech data: TIMIT,[0],[0]
"This also shows that for real-world data it is useful to use a structured inference network to further improve the quality of disentangled representation.
",4.2. Speech data: TIMIT,[0],[0]
Our results are competitive with (or slightly better than) the FHVAE results (α = 0) reported in Hsu et al. (2017).,4.2. Speech data: TIMIT,[0],[0]
The better results for FHVAE (α = 10) is obtained by adding a discriminative training objective (scaled by α) to the variational lower-bound.,4.2. Speech data: TIMIT,[0],[0]
"In a nutshell, the timeinvariant information in FHVAE is encoded in a latent variable zn2 ∼ p(zn2 |µ2), and the discriminative objective encourages zn2 encoded from a segment of one sequence to be close to the corresponding µ2 while far away from µ2 of other sequences.",4.2. Speech data: TIMIT,[0],[0]
"However, we do not test this idea here because (1) our goal is to demonstrate that the proposed architecture is a minimalistic framework for learning disentangled representations of sequential data; (2) this discriminative objective is specifically designed for hierarchical VAE, and in general the assumption behind it might not always be true (consider encoding two speech sequences coming from the same speaker).",4.2. Speech data: TIMIT,[0],[0]
"Similar ideas for discriminative training have been considered in e.g. Mathieu et al. (2016), but that discriminative objective can only be applied
4 Hsu et al. (2017) did not provide the EER results for α = 0 and µ1 in the 16 dimension case.
to two sequences that are known to entail different timeinvariant information (e.g. two sequences with different labels), which implicitly uses supervisions.",4.2. Speech data: TIMIT,[0],[0]
"Nevertheless, a better design for the discriminative objective without supervision can further improve the disentanglement of the learned representations, and we leave it to future work.",4.2. Speech data: TIMIT,[0],[0]
"Lastly, although not a main focus of the paper, we show that the usage of a stochastic transition model for the prior leads to more realistic dynamics of the generated sequence.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we consider another class of models:
p(x1:T , z,f) = p(f)p(z) T∏ t=1 p(xt|z,f).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The parameters of p(xt|z,f) are defined by a neural network NN(ht,f), with ht computed by a deterministic RNN conditioned on",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"z. We experiment with two types of deterministic dynamics, with the graphical model visualised in appendix.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The first model uses an LSTM with z as the initial state: h0 = z, ht = LSTM(ht−1).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
In later experiments we refer this dynamics as LSTM-f as the latent variable z is forward propagated in a deterministic way.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The second one deploys an LSTM conditioned on z (i.e. h0 = 0,ht = LSTM(ht−1, z)), therefore we refer it as LSTM-c. This is identical to the transition dynamics used in the FHVAE model (Hsu et al., 2017).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we refer to our model as the ’stochastic’ model (Eq. 1).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The LSTM models encodes temporal information in a global latent variable z. Therefore, small differences/errors in z will accumulate over time, which may result in unrealistic long-time dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"In contrast, the stochastic model (Eq. 1) keeps track of the time-varying aspects of xt in zt
for every t, making the reconstruction to be time-local and therefore much easier.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Therefore, the stochastic model is better suited if the sequences are long and complex.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We give empirical evidence to support this claim.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Data preprocessing & hyper-parameters.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We follow Fraccaro et al. (2017) to simulate video sequences of a ball (or a square) bouncing inside an irregular polygon using Pymunk.5,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The irregular shape was chosen because it induces chaotic dynamics, meaning that small deviations from the initial position and velocity of the ball will create exponentially diverging trajectories at long times.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
This makes memorizing the dynamics of a prototypical sequence challenging.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We randomly sampled the initial position and velocity of the ball, but did not apply any force to the ball, except for the fully elastic collisions with the walls.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We generated 5,000 sequences in total (1000 for test), each of them containing T = 30 frames with a resolution of 32×32.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the deterministic LSTMs, we fix the dimensionality of zt to 64, and set ht and the LSTM internal states to be 512 dimensions.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The latent variable dimensionality of the stochastic dynamics is dim(zt) = 16.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Qualitative & quantitative analyses.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We consider both reconstruction and missing data imputation tasks for the learned generative models.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the latter and for T = 30, the models observe the first t < T frames of a sequence and predict the remaining T − t frames using the prior dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We visualise in Figure 7 the ground truth, recon-
5http://www.pymunk.org/en/latest/.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For simplicity we disabled rotation of the square when hitting the wall, by setting the inertia to infinity.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"structed, and predicted sequences (t = 20) from all models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We further consider average fraction of incorrectly reconstructed/predicted pixels as a quantitative metric, to evaluate how well the ground-truth dynamics is recovered given consecutive missing frames.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The result is reported in Figure 8.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The stochastic model outperforms the deterministic models both qualitatively and quantitatively.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The shape of the ball is better preserved over time, and the trajectories are more physical.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"This explains the lower errors of the stochastic model, and the advantage is significant when the number of missing frames is small.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Our experiments give evidence that the stochastic model is better suited to modelling long, complex sequences when compared to the deterministic dynamical models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We expect that a better design for the stochastic transition dynamics, e.g. by combining deep neural networks with well-studied linear dynamical systems (Krishnan et al., 2015; Fraccaro et al., 2016; Karl et al., 2016; Johnson et al., 2016; Krishnan et al., 2017; Fraccaro et al., 2017), can further enhance the quality of the learned representations.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We presented a minimalistic generative model for learning disentangled representations of high-dimensional time series.,5. Conclusions and outlook,[0],[0]
"Our model consists of a global latent variable for content features, and a stochastic RNN with time-local latent variables for dynamical features.",5. Conclusions and outlook,[0],[0]
The model is trained using standard amortized variational inference.,5. Conclusions and outlook,[0],[0]
We carried out experiments both on video and audio data.,5. Conclusions and outlook,[0],[0]
"Our approach allows us to perform full and conditional generation, as well as feature swapping, such as voice conversion and video content manipulation.",5. Conclusions and outlook,[0],[0]
"We also showed that a stochastic transition model generally outperforms a deterministic one.
",5. Conclusions and outlook,[0],[0]
Future work may investigate whether a similar model applies to more complex video and audio sequences.,5. Conclusions and outlook,[0],[0]
"Also, disentangling may further be improved by additional crossentropy terms, or discriminative training.",5. Conclusions and outlook,[0],[0]
A promising avenue of research is to explore the usage of this architecture for neural compression.,5. Conclusions and outlook,[0],[0]
"An advantage of the model is that it separates dynamical from static features, allowing the latent space for the dynamical part to be low-dimensional.",5. Conclusions and outlook,[0],[0]
"We thank Robert Bamler, Rich Turner, Jeremy Wong and Yu Wang for discussions and feedback on the manuscript.",Acknowledgements,[0],[0]
We also thank Wei-Ning Hsu for helping reproduce the FHVAE experiments.,Acknowledgements,[0],[0]
Yingzhen Li thanks Schlumberger Foundation FFTF fellowship for supporting her PhD study.,Acknowledgements,[0],[0]
"We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio.",abstractText,[0],[0]
"Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content).",abstractText,[0],[0]
This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features.,abstractText,[0],[0]
"In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping.",abstractText,[0],[0]
"For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics.",abstractText,[0],[0]
"Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.",abstractText,[0],[0]
Disentangled Sequential Autoencoder,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
In this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (M3Ns) as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1",text,[0],[0]
"Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.",1 Introduction,[0],[0]
"There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by
1Our source code is available at http://code.google.com/p/disfluency-detection/downloads/list
the following words.",1 Introduction,[0],[0]
"An example is shown below that includes edited words and filler words.
",1 Introduction,[0],[0]
"I want a flight to Boston︸ ︷︷ ︸ edited uh I mean︸ ︷︷ ︸ filler to Denver
Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection.
",1 Introduction,[0],[0]
Many models have been evaluated for this task.,1 Introduction,[0],[0]
Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection.,1 Introduction,[0],[0]
They showed that CRFs significantly outperformed Maximum Entropy models and HMMs.,1 Introduction,[0],[0]
"Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001).",1 Introduction,[0],[0]
Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking.,1 Introduction,[0],[0]
They obtained the best reported F-score of 83.8% on the Switchboard corpus.,1 Introduction,[0],[0]
"Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.
",1 Introduction,[0],[0]
"From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006).",1 Introduction,[0],[0]
"Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage.",1 Introduction,[0],[0]
"They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.
820
In this paper, we propose a multi-step stacked learning approach for disfluency detection.",1 Introduction,[0],[0]
"In our method, we first perform filler word detection, then edited word detection.",1 Introduction,[0],[0]
"In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step.",1 Introduction,[0],[0]
"We also include a new type of features, called inbetween features, and incorporate them into the last step.",1 Introduction,[0],[0]
"For edited word detection, we use Max-Margin Markov Networks (M3Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance.",1 Introduction,[0],[0]
"On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection.",1 Introduction,[0],[0]
"Weighted M3Ns
We use a sequence labeling model for edit detection.",2 Balancing Precision and Recall Using,[0],[0]
"Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).",2 Balancing Precision and Recall Using,[0],[0]
"For example, the previous sentence is represented as:
I/",2 Balancing Precision and Recall Using,[0],[0]
O want/O a/O flight/,2 Balancing Precision and Recall Using,[0],[0]
O to/BE Boston/EE uh/,2 Balancing Precision and Recall Using,[0],[0]
O I/,2 Balancing Precision and Recall Using,[0],[0]
"O mean/O to/O Denver/O
We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words:
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
F = 2× P ×R
P + R
There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006).",2 Balancing Precision and Recall Using,[0],[0]
"Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.
",2 Balancing Precision and Recall Using,[0],[0]
"In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary
results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward).
",2 Balancing Precision and Recall Using,[0],[0]
"The learning task for M3Ns can be represented as follows:
min α
1 2 C∥ ∑ x,y αx,y∆f(x, y)∥22 + ∑ x,y αx,yL(x, y)
s.t. ∑
y
αx,y = 1 ∀x
αx,y ≥ 0, ∀x, y
The above shows the dual form for training M3Ns, where x is the observation of a training sample, y ∈ Y is a label.",2 Balancing Precision and Recall Using,[0],[0]
"α is the parameter needed to be optimized, C > 0 is the regularization parameter.",2 Balancing Precision and Recall Using,[0],[0]
"∆f(x, y) is the residual feature vector: f(x, ỹ)",2 Balancing Precision and Recall Using,[0],[0]
"− f(x, y), where ỹ is the true label of x. L(x, y) is the loss function.",2 Balancing Precision and Recall Using,[0],[0]
"Taskar et al. (2004) used un-weighted hamming loss, which is the number of incorrect components: L(x, y) = ∑ t δ(yt, ỹt), where δ(a, b) is the binary indicator function (it is 0 if a = b).",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we use the weighted hamming loss:
L(x, y) = ∑
t
v(yt, ỹt)δ(yt, ỹt)
where v(yt, ỹt) is the weighted loss for the error when ỹt is mislabeled as yt.",2 Balancing Precision and Recall Using,[0],[0]
Such a weighted loss function allows us to balance the model’s precision and recall rates.,2 Balancing Precision and Recall Using,[0],[0]
"For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we tune the weight matrix v using the development dataset.",2 Balancing Precision and Recall Using,[0],[0]
"Rather than just using the above M3Ns with some features, in this paper we propose to use stacked learning to incorporate gradually refined n-gram features.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Stacked learning is a meta-learning approach (Cohen and de Carvalho, 2005).",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Its idea is to use two
(or more) levels of predictors, where the outputs of the low level predictors are incorporated as features into the next level predictors.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
It has the advantage of incorporating non-local features as well as nonlinear classifiers.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In our task, we do not just use the classifier’s output (a word is an edited word or not) as a feature, rather we use such output to remove the disfluencies and extract new n-gram features for the subsequent stacked classifiers.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
We use 10 fold cross validation to train the low level predictors.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
The following describes the three steps in our approach.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In the first step, we automatically detect filler words.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since filler words often occur immediately after edited words (before the corrected words), we expect that removing them will make rough copy detection easy.",3.1 Step 1: Filler Word Detection,[0],[0]
"For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency.",3.1 Step 1: Filler Word Detection,[0],[0]
"Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.
",3.1 Step 1: Filler Word Detection,[0],[0]
"For filler word detection, similar to edited word detection, we define 5 labels: BP , IP , EP , SP , O. We use un-weighted hamming loss to learn M3Ns for this task.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since for filler word detection, our performance metric is not F-measure, but just the overall accuracy in order to generate cleaned text for subsequent n-gram features, we did not use the weighted hamming hoss for this.",3.1 Step 1: Filler Word Detection,[0],[0]
The features we used are listed in Table 1.,3.1 Step 1: Filler Word Detection,[0],[0]
All n-grams are extracted from the original text.,3.1 Step 1: Filler Word Detection,[0],[0]
"In the second step, edited words are detected using M3Ns with the weighted-hamming loss.",3.2 Step 2: Edited Word Detection,[0],[0]
The features we used are listed in Table 2.,3.2 Step 2: Edited Word Detection,[0],[0]
All n-grams in the first step are also used here.,3.2 Step 2: Edited Word Detection,[0],[0]
"Besides that, word n-grams, POS n-grams and logic",3.2 Step 2: Edited Word Detection,[0],[0]
n,3.2 Step 2: Edited Word Detection,[0],[0]
-grams extracted from filler word removed text are included.,3.2 Step 2: Edited Word Detection,[0],[0]
"Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words.",3.2 Step 2: Edited Word Detection,[0],[0]
"In this step, we use n-gram features extracted from the text after removing edit disfluencies based on
the previous step.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"According to our analysis of the errors produced by step 2, we observed that many errors occurred at the boundaries of the disfluencies, and the word bigrams after removing the edited words are unnatural.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"The following is an example:
• Ref: The new type is prettier than what their/SE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"• Sys: The new type is prettier than what/BE their/EE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Using the system’s prediction, we would have bigram than they, which is odd.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Usually, the pronoun following than is accusative case.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We expect adding n-gram features derived from the cleaned-up sentences would allow the new classifier to fix such hypothesis.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"This kind of n-gram features is similar to the language models used in (Zwarts and Johnson,
2011).",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"They have the benefit of measuring the fluency of the cleaned text.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"For example,
• Coordinates: they ca n′t decide which are the good aspects and which are the bad aspects
• Rough Copies: it/",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"BE ′s/IE a/IE pleasure/IE to/EE it s good to get outside
To distinguish the rough copies and the coordinate examples shown above, we analyze the training data statistically.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We extract all the pieces lying between identical word bigrams AB . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
AB.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
The observation is that coordinates are often longer than edited sequences.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
Hence we introduce the in-between features for each word.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We also used other patterns such as A . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
A and ABC . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"ABC, but they are too noisy or infrequent and do not yield much performance gain.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
Table 3 lists the feature templates used in this last step.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).",4.1 Experimental Setup,[0],[0]
"We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and
such kind of information is not available (Johnson and Charniak, 2004).
",4.1 Experimental Setup,[0],[0]
We tuned the weight matrix for hamming loss on the development dataset using simple grid search.,4.1 Experimental Setup,[0],[0]
"The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.",4.1 Experimental Setup,[0],[0]
The optimal weight matrix is shown in Table 4.,4.1 Experimental Setup,[0],[0]
"Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with ·E will be mapped to the class of edited words.",4.1 Experimental Setup,[0],[0]
"We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning.",4.2 Results,[0],[0]
"For each model, we tuned the parameters on the development data:",4.2 Results,[0],[0]
"Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1.",4.2 Results,[0],[0]
"For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.",4.2 Results,[0],[0]
"Regularization penalty is C = 0.1 and iteration number is 30.
",4.2 Results,[0],[0]
Table 5 shows the results using different models and features.,4.2 Results,[0],[0]
The baseline models use only the ngrams features extracted from the original text.,4.2 Results,[0],[0]
"We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models.",4.2 Results,[0],[0]
"Regarding the features, the gradually added n-gram features have consistent improvement for all models.",4.2 Results,[0],[0]
"Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words.",4.2 Results,[0],[0]
"In our analysis, we also noticed that the in-between fea-
tures yield about 1% improvement in F-score for all models (the gain of step 3 over step 2 is because of the in-between features and the new n-gram features extracted from the text after removing previously detected edited words).",4.2 Results,[0],[0]
"We performed McNemar’s test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value < 0.001).",4.2 Results,[0],[0]
"There are no significant differences among CRFs, AP and PA.",4.2 Results,[0],[0]
"Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value < 0.001).
",4.2 Results,[0],[0]
"We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6.",4.2 Results,[0],[0]
We achieved the best F-score.,4.2 Results,[0],[0]
"The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models.",4.2 Results,[0],[0]
"In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.",5 Conclusion,[0],[0]
The first level removes the filler words providing new ngrams for the second level to remove edited words.,5 Conclusion,[0],[0]
"The
third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.",5 Conclusion,[0],[0]
"To minimize the F-loss approximately, we modified the hamming loss in M3Ns.",5 Conclusion,[0],[0]
"Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources.",5 Conclusion,[0],[0]
We thank three anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
This work is partly supported by DARPA under Contract No. HR0011-12-C-0016 and FA8750-13-2-0041.,Acknowledgments,[0],[0]
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.,Acknowledgments,[0],[0]
"In this paper, we propose a multi-step stacked learning model for disfluency detection.",abstractText,[0],[0]
Our method incorporates refined n-gram features step by step from different word sequences.,abstractText,[0],[0]
"First, we detect filler words.",abstractText,[0],[0]
"Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text.",abstractText,[0],[0]
"In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection.",abstractText,[0],[0]
We use Max-Margin Markov Networks (MNs) as the classifier with the weighted hamming loss to balance precision and recall.,abstractText,[0],[0]
Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and MNs with weighted hamming loss can significantly improve the performance.,abstractText,[0],[0]
Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1,abstractText,[0],[0]
Disfluency Detection Using Multi-step Stacked Learning,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",text,[0],[0]
"In relation extraction, one challenge that is faced when building a machine learning system is the generation of training examples.",1 Introduction,[0],[0]
"One common technique for coping with this difficulty is distant supervision (Mintz et al., 2009) which assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way.",1 Introduction,[0],[0]
"Figure 1 shows an example of the auto-
matic labeling of data through distant supervision.",1 Introduction,[0],[0]
"In this example, Apple and Steve Jobs are two related entities in Freebase1.",1 Introduction,[0],[0]
All sentences that contain these two entities are selected as training instances.,1 Introduction,[0],[0]
The distant supervision strategy is an effective method of automatically labeling training data.,1 Introduction,[0],[0]
"However, it has two major shortcomings when used for relation extraction.
",1 Introduction,[0],[0]
"First, the distant supervision assumption is too strong and causes the wrong label problem.",1 Introduction,[0],[0]
A sentence that mentions two entities does not necessarily express their relation in a knowledge base.,1 Introduction,[0],[0]
It is possible that these two entities may simply share the same topic.,1 Introduction,[0],[0]
"For instance, the upper sentence indeed expresses the “company/founders” relation in Figure 1.",1 Introduction,[0],[0]
"The lower sentence, however, does not express this relation but is still selected as a training instance.",1 Introduction,[0],[0]
"This will hinder the performance of a model trained on such noisy data.
",1 Introduction,[0],[0]
"Second, previous methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision.",1 Introduction,[0],[0]
These features are often derived from preexisting Natural Language Processing (NLP) tools.,1 Introduction,[0],[0]
"Since errors inevitably exist in NLP tools, the use of traditional features leads to error propagation or accumulation.",1 Introduction,[0],[0]
"Distant supervised relation extraction generally ad-
1http://www.freebase.com/
1753
dresses corpora from the Web, including many informal texts.",1 Introduction,[0],[0]
Figure 2 shows the sentence length distribution of a benchmark distant supervision dataset that was developed by Riedel et al. (2010).,1 Introduction,[0],[0]
Approximately half of the sentences are longer than 40 words.,1 Introduction,[0],[0]
McDonald and Nivre (2007) showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length.,1 Introduction,[0],[0]
"Therefore, when using traditional features, the problem of error propagation or accumulation will not only exist, it will grow more serious.
",1 Introduction,[0],[0]
"In this paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address the two problems described above.",1 Introduction,[0],[0]
"To address the first problem, distant supervised relation extraction is treated as a multi-instance problem similar to previous studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).",1 Introduction,[0],[0]
"In multi-instance problem, the training set consists of many bags, and each contains many instances.",1 Introduction,[0],[0]
"The labels of the bags are known; however, the labels of the instances in the bags are unknown.",1 Introduction,[0],[0]
We design an objective function at the bag level.,1 Introduction,[0],[0]
"In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem.
",1 Introduction,[0],[0]
"To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014).",1 Introduction,[0],[0]
"Our proposal is an extension of Zeng et al. (2014), in which a single max pooling operation is utilized to determine the most significant features.",1 Introduction,[0],[0]
"Although this operation has been shown to be effective for textual feature representation (Collobert et al., 2011; Kim, 2014), it reduces the size of the
hidden layers too rapidly and cannot capture the structural information between two entities (Graham, 2014).",1 Introduction,[0],[0]
"For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them.",1 Introduction,[0],[0]
Several approaches have employed manually crafted features that attempt to model such structural information.,1 Introduction,[0],[0]
These approaches usually consider both internal and external contexts.,1 Introduction,[0],[0]
A sentence is inherently divided into three segments according to the two given entities.,1 Introduction,[0],[0]
"The internal context includes the characters inside the two entities, and the external context involves the characters around the two entities (Zhang et al., 2006).",1 Introduction,[0],[0]
"Clearly, single max pooling is not sufficient to capture such structural information.",1 Introduction,[0],[0]
"To capture structural and other latent information, we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer.",1 Introduction,[0],[0]
The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence.,1 Introduction,[0],[0]
"Thus, it is expected to exhibit superior performance compared with traditional methods.
",1 Introduction,[0],[0]
"The contributions of this paper can be summarized as follows.
",1 Introduction,[0],[0]
• We explore the feasibility of performing distant supervised relation extraction without hand-designed features.,1 Introduction,[0],[0]
"PCNNS are proposed to automatically learn features without complicated NLP preprocessing.
",1 Introduction,[0],[0]
"• To address the wrong label problem, we develop innovative solutions that incorporate multi-instance learning into the PCNNS for distant supervised relation extraction.
",1 Introduction,[0],[0]
"• In the proposed network, we devise a piecewise max pooling layer, which aims to capture structural information between two entities.",1 Introduction,[0],[0]
Relation extraction is one of the most important topics in NLP.,2 Related Work,[0],[0]
"Many approaches to relation extraction have been developed, such as bootstrapping, unsupervised relation discovery and supervised classification.",2 Related Work,[0],[0]
"Supervised approaches are the most commonly used methods for relation
extraction and yield relatively high performance (Bunescu and Mooney, 2006; Zelenko et al., 2003; Zhou et al., 2005).",2 Related Work,[0],[0]
"In the supervised paradigm, relation extraction is considered to be a multi-class classification problem and may suffer from a lack of labeled data for training.",2 Related Work,[0],[0]
"To address this problem, Mintz et al. (2009) adopted Freebase to perform distant supervision.",2 Related Work,[0],[0]
"As described in Section 1, the algorithm for training data generation is sometimes faced with the wrong label problem.",2 Related Work,[0],[0]
"To address this shortcoming, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) developed the relaxed distant supervision assumption for multi-instance learning.",2 Related Work,[0],[0]
"The term ‘multiinstance learning was coined by (Dietterich et al., 1997) while investigating the problem of predicting drug activity.",2 Related Work,[0],[0]
"In multi-instance learning, the uncertainty of instance labels can be taken into account.",2 Related Work,[0],[0]
"The focus of multi-instance learning is to discriminate among the bags.
",2 Related Work,[0],[0]
These methods have been shown to be effective for relation extraction.,2 Related Work,[0],[0]
"However, their performance depends strongly on the quality of the designed features.",2 Related Work,[0],[0]
Most existing studies have concentrated on extracting features to identify the relations between two entities.,2 Related Work,[0],[0]
Previous methods can be generally categorized into two types: feature-based methods and kernel-based methods.,2 Related Work,[0],[0]
"In feature-based methods, a diverse set of strategies is exploited to convert classification clues (e.g., sequences, parse trees) into feature vectors (Kambhatla, 2004; Suchanek et al., 2006).",2 Related Work,[0],[0]
Feature-based methods suffer from the necessity of selecting a suitable feature set when converting structured representations into feature vectors.,2 Related Work,[0],[0]
"Kernel-based methods provide a natural alternative to exploit rich representations of input classification clues, such as syntactic parse trees.",2 Related Work,[0],[0]
Kernelbased methods enable the use of a large set of features without needing to extract them explicitly.,2 Related Work,[0],[0]
"Several kernels have been proposed, such as the convolution tree kernel (Qian et al., 2008), the subsequence kernel (Bunescu and Mooney, 2006) and the dependency tree kernel (Bunescu and Mooney, 2005).
",2 Related Work,[0],[0]
"Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using existing NLP tools.",2 Related Work,[0],[0]
"With the recent revival of interest in neural networks, many researchers have investigated the possibility of using neural networks to automatically learn features (Socher et
al., 2012; Zeng et al., 2014).",2 Related Work,[0],[0]
"Inspired by Zeng et al. (2014), we propose the use of PCNNs with multi-instance learning to automatically learn features for distant supervised relation extraction.",2 Related Work,[0],[0]
Dietterich et al. (1997) suggested that the design of multi-instance modifications for neural networks is a particularly interesting topic.,2 Related Work,[0],[0]
Zhang and Zhou (2006) successfully incorporated multiinstance learning into traditional Backpropagation (BP) and Radial Basis Function (RBF) networks and optimized these networks by minimizing a sum-of-squares error function.,2 Related Work,[0],[0]
"In contrast to their method, we define the objective function based on the cross-entropy principle.",2 Related Work,[0],[0]
Distant supervised relation extraction is formulated as multi-instance problem.,3 Methodology,[0],[0]
"In this section, we present innovative solutions that incorporate multi-instance learning into a convolutional neural network to fulfill this task.",3 Methodology,[0],[0]
PCNNs are proposed for the automatic learning of features without complicated NLP preprocessing.,3 Methodology,[0],[0]
Figure 3 shows our neural network architecture for distant supervised relation extraction.,3 Methodology,[0],[0]
It illustrates the procedure that handles one instance of a bag.,3 Methodology,[0],[0]
"This procedure includes four main parts: Vector Representation, Convolution, Piecewise Max Pooling and Softmax Output.",3 Methodology,[0],[0]
We describe these parts in detail below.,3 Methodology,[0],[0]
The inputs of our network are raw word tokens.,3.1 Vector Representation,[0],[0]
"When using neural networks, we typically transform word tokens into low-dimensional vectors.",3.1 Vector Representation,[0],[0]
"In our method, each input word token is transformed into a vector by looking up pre-trained word embeddings.",3.1 Vector Representation,[0],[0]
"Moreover, we use position features (PFs) to specify entity pairs, which are also transformed into vectors by looking up position embeddings.",3.1 Vector Representation,[0],[0]
Word embeddings are distributed representations of words that map each word in a text to a ‘k’dimensional real-valued vector.,3.1.1 Word Embeddings,[0],[0]
"They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014).",3.1.1 Word Embeddings,[0],[0]
"Using word embeddings that have been trained a priori has become common practice for
enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014).
",3.1.1 Word Embeddings,[0],[0]
A common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm.,3.1.1 Word Embeddings,[0],[0]
"Recent research (Erhan et al., 2010) has shown that neural networks can converge to better local minima when they are initialized with word embeddings.",3.1.1 Word Embeddings,[0],[0]
Word embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled text.,3.1.1 Word Embeddings,[0],[0]
"Researchers have proposed several methods of training word embeddings (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013).",3.1.1 Word Embeddings,[0],[0]
"In this paper, we use the Skip-gram model (Mikolov et al., 2013) to train word embeddings.",3.1.1 Word Embeddings,[0],[0]
"In relation extraction, we focus on assigning labels to entity pairs.",3.1.2 Position Embeddings,[0],[0]
"Similar to Zeng et al. (2014), we use PFs to specify entity pairs.",3.1.2 Position Embeddings,[0],[0]
A PF is defined as the combination of the relative distances from the current word to e1 and e2.,3.1.2 Position Embeddings,[0],[0]
"For instance, in the following example, the relative distances from son to e1 (Kojo Annan) and e2 (Kofi Annan) are 3 and -2, respectively.
...",3.1.2 Position Embeddings,[0],[0]
"hired Kojo Annan , the son of Kofi Annan , in ...3 -2 Two position embedding matrixes (PF1 and PF2) are randomly initialized.",3.1.2 Position Embeddings,[0],[0]
We then transform the relative distances into real valued vectors by looking up the position embedding matrixes.,3.1.2 Position Embeddings,[0],[0]
"In the example shown in Figure 3, it is assumed that
the size of the word embedding is dw = 4 and that the size of the position embedding is dp = 1.",3.1.2 Position Embeddings,[0],[0]
"In combined word embeddings and position embeddings, the vector representation part transforms an instance into a matrix S ∈ Rs×d, where s is the sentence length and d = dw + dp ∗ 2.",3.1.2 Position Embeddings,[0],[0]
The matrix S is subsequently fed into the convolution part.,3.1.2 Position Embeddings,[0],[0]
"In relation extraction, an input sentence that is marked as containing the target entities corresponds only to a relation type; it does not predict labels for each word.",3.2 Convolution,[0],[0]
"Thus, it might be necessary to utilize all local features and perform this prediction globally.",3.2 Convolution,[0],[0]
"When using a neural network, the convolution approach is a natural means of merging all these features (Collobert et al., 2011).
",3.2 Convolution,[0],[0]
"Convolution is an operation between a vector of weights, w, and a vector of inputs that is treated as a sequence q.",3.2 Convolution,[0],[0]
The weights matrix w is regarded as the filter for the convolution.,3.2 Convolution,[0],[0]
"In the example shown in Figure 3, we assume that the length of the filter is w (w = 3);",3.2 Convolution,[0],[0]
"thus, w ∈ Rm (m = w∗d).",3.2 Convolution,[0],[0]
"We consider S to be a sequence {q1,q2, · · · ,qs}, where qi ∈ Rd.",3.2 Convolution,[0],[0]
"In general, let qi:j refer to the concatenation of qi to qj .",3.2 Convolution,[0],[0]
"The convolution operation involves taking the dot product of w with each w-gram in the sequence q to obtain another sequence c ∈ Rs+w−1:
cj = wqj−w+1:j (1)
where the index j ranges from 1 to s+w−1.",3.2 Convolution,[0],[0]
"Outof-range input values qi, where i < 1 or i > s, are
taken to be zero.",3.2 Convolution,[0],[0]
The ability to capture different features typically requires the use of multiple filters (or feature maps) in the convolution.,3.2 Convolution,[0],[0]
"Under the assumption that we use n filters (W = {w1,w2, · · · ,wn}), the convolution operation can be expressed as follows:
cij = wiqj−w+1:j 1 ≤ i ≤ n",3.2 Convolution,[0],[0]
(2),3.2 Convolution,[0],[0]
"The convolution result is a matrix C = {c1, c2, · · · , cn} ∈ Rn×(s+w−1).",3.2 Convolution,[0],[0]
Figure 3 shows an example in which we use 3 different filters in the convolution procedure.,3.2 Convolution,[0],[0]
The size of the convolution output matrix C ∈ Rn×(s+w−1) depends on the number of tokens s in the sentence that is fed into the network.,3.3 Piecewise Max Pooling,[0],[0]
"To apply subsequent layers, the features that are extracted by the convolution layer must be combined such that they are independent of the sentence length.",3.3 Piecewise Max Pooling,[0],[0]
"In traditional Convolution Neural Networks (CNNs), max pooling operations are often applied for this purpose (Collobert et al., 2011; Zeng et al., 2014).",3.3 Piecewise Max Pooling,[0],[0]
This type of pooling scheme naturally addresses variable sentence lengths.,3.3 Piecewise Max Pooling,[0],[0]
"The idea is to capture the most significant features (with the highest values) in each feature map.
",3.3 Piecewise Max Pooling,[0],[0]
"However, despite the widespread use of single max pooling, this approach is insufficient for relation extraction.",3.3 Piecewise Max Pooling,[0],[0]
"As described in the first section, single max pooling reduces the size of the hidden layers too rapidly and is too coarse to capture finegrained features for relation extraction.",3.3 Piecewise Max Pooling,[0],[0]
"In addition, single max pooling is not sufficient to capture the structural information between two entities.",3.3 Piecewise Max Pooling,[0],[0]
"In relation extraction, an input sentence can be divided into three segments based on the two selected entities.",3.3 Piecewise Max Pooling,[0],[0]
"Therefore, we propose a piecewise max pooling procedure that returns the maximum value in each segment instead of a single maximum value.",3.3 Piecewise Max Pooling,[0],[0]
"As shown in Figure 3, the output of each convolutional filter ci is divided into three segments {ci1, ci2, ci3} by Kojo Annan and Kofi Annan.",3.3 Piecewise Max Pooling,[0],[0]
"The piecewise max pooling procedure can be expressed as follows:
pij = max(cij) 1 ≤ i ≤ n, 1 ≤ j ≤ 3 (3) For the output of each convolutional filter, we can obtain a 3-dimensional vector pi = {pi1, pi2, pi3}.",3.3 Piecewise Max Pooling,[0],[0]
"We then concatenate all vectors
p1:n and apply a non-linear function, such as the hyperbolic tangent.",3.3 Piecewise Max Pooling,[0],[0]
"Finally, the piecewise max pooling procedure outputs a vector:
g = tanh(p1:n) (4)
where g ∈ R3n.",3.3 Piecewise Max Pooling,[0],[0]
The size of g is fixed and is no longer related to the sentence length.,3.3 Piecewise Max Pooling,[0],[0]
"To compute the confidence of each relation, the feature vector g is fed into a softmax classifier.
",3.4 Softmax Output,[0],[0]
"o = W1g + b (5)
W1 ∈ Rn1×3n is the transformation matrix, and o ∈ Rn1 is the final output of the network, where n1 is equal to the number of possible relation types for the relation extraction system.
",3.4 Softmax Output,[0],[0]
"We employ dropout (Hinton et al., 2012) on the penultimate layer for regularization.",3.4 Softmax Output,[0],[0]
Dropout prevents the co-adaptation of hidden units by randomly dropping out a proportion p of the hidden units during forward computing.,3.4 Softmax Output,[0],[0]
"We first apply a “masking” operation (g◦r) on g, where r is a vector of Bernoulli random variables with probability p of being 1. Eq.(5)",3.4 Softmax Output,[0],[0]
"becomes:
o = W1(g ◦ r) + b (6)
",3.4 Softmax Output,[0],[0]
Each output can then be interpreted as the confidence score of the corresponding relation.,3.4 Softmax Output,[0],[0]
This score can be interpreted as a conditional probability by applying a softmax operation (see Section 3.5).,3.4 Softmax Output,[0],[0]
"In the test procedure, the learned weight vectors are scaled by p such that Ŵ1 = pW1 and are used (without dropout) to score unseen instances.",3.4 Softmax Output,[0],[0]
"In order to alleviate the wrong label problem, we use multi-instance learning for PCNNs.",3.5 Multi-instance Learning,[0],[0]
"The PCNNs-based relation extraction can be stated as a quintuple θ = (E,PF1,PF2,W,W1)2.",3.5 Multi-instance Learning,[0],[0]
The input to the network is a bag.,3.5 Multi-instance Learning,[0],[0]
"Suppose that there are T bags {M1, M2, · · · ,MT } and that the i-th bag contains qi instances Mi = {m1i ,m2i , · · · , mqii }.",3.5 Multi-instance Learning,[0],[0]
The objective of multi-instance learning is to predict the labels of the unseen bags.,3.5 Multi-instance Learning,[0],[0]
"In this paper, all instances in a bag are considered independently.",3.5 Multi-instance Learning,[0],[0]
"Given an input instance mji , the network with the parameter θ outputs a vector o, where the r-th component or corresponds to the score associated
2E represents the word embeddings.
",3.5 Multi-instance Learning,[0],[0]
Algorithm 1 Multi-instance learning 1: Initialize θ.,3.5 Multi-instance Learning,[0],[0]
"Partition the bags into mini-
batches of size bs. 2: Randomly choose a mini-batch, and feed the
bags into the network one by one.",3.5 Multi-instance Learning,[0],[0]
3: Find the j-th instance mji (1 ≤,3.5 Multi-instance Learning,[0],[0]
"i ≤ bs) in each
bag according to Eq. (9).",3.5 Multi-instance Learning,[0],[0]
"4: Update θ based on the gradients of mji (1 ≤
i ≤ bs) via Adadelta.",3.5 Multi-instance Learning,[0],[0]
"5: Repeat steps 2-4 until either convergence or
the maximum number of epochs is reached.
with relation r.",3.5 Multi-instance Learning,[0],[0]
"To obtain the conditional probability p(r|m, θ), we apply a softmax operation over all relation types:
p(r|mji ; θ) = eor n1∑ k=1 eok (7)
",3.5 Multi-instance Learning,[0],[0]
The objective of multi-instance learning is to discriminate bags rather than instances.,3.5 Multi-instance Learning,[0],[0]
"To do so, we must define the objective function on the bags.",3.5 Multi-instance Learning,[0],[0]
"Given all (T ) training bags (Mi, yi), we can define the objective function using cross-entropy at the bag level as follows:
J (θ) = T∑
i=1
log p(yi|mji ; θ) (8)
where j is constrained as follows:
j∗ = arg max j p(yi|mji ; θ) 1 ≤ j ≤ qi (9)
",3.5 Multi-instance Learning,[0],[0]
"Using this defined objective function, we maximize J(θ) through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) update rule.",3.5 Multi-instance Learning,[0],[0]
"The entire training procedure is described in Algorithm 1.
",3.5 Multi-instance Learning,[0],[0]
"From the introduction presented above, we know that the traditional backpropagation algorithm modifies a network in accordance with all training instances, whereas backpropagation with multi-instance learning modifies a network based on bags.",3.5 Multi-instance Learning,[0],[0]
"Thus, our method captures the nature of distant supervised relation extraction, in which some training instances will inevitably be incorrectly labeled.",3.5 Multi-instance Learning,[0],[0]
"When a trained PCNN is used for prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is assigned a positive label.",3.5 Multi-instance Learning,[0],[0]
Our experiments are intended to provide evidence that supports the following hypothesis: automatically learning features using PCNNs with multiinstance learning can lead to an increase in performance.,4 Experiments,[0],[0]
"To this end, we first introduce the dataset and evaluation metrics used.",4 Experiments,[0],[0]
"Next, we test several variants via cross-validation to determine the parameters to be used in our experiments.",4 Experiments,[0],[0]
We then compare the performance of our method to those of several traditional methods.,4 Experiments,[0],[0]
"Finally, we evaluate the effects of piecewise max pooling and multiinstance learning3.",4 Experiments,[0],[0]
"We evaluate our method on a widely used dataset4 that was developed by (Riedel et al., 2010) and has also been used by (Hoffmann et al., 2011; Surdeanu et al., 2012).",4.1 Dataset and Evaluation Metrics,[0],[0]
"This dataset was generated by aligning Freebase relations with the NYT corpus, with sentences from the years 2005-2006 used as the training corpus and sentences from 2007 used as the testing corpus.
",4.1 Dataset and Evaluation Metrics,[0],[0]
"Following previous work (Mintz et al., 2009), we evaluate our method in two ways: the held-out evaluation and the manual evaluation.",4.1 Dataset and Evaluation Metrics,[0],[0]
The heldout evaluation only compares the extracted relation instances against Freebase relation data and reports the precision/recall curves of the experiments.,4.1 Dataset and Evaluation Metrics,[0],[0]
"In the manual evaluation, we manually check the newly discovered relation instances that are not in Freebase.",4.1 Dataset and Evaluation Metrics,[0],[0]
"In this paper, we use the Skip-gram model (word2vec)5 to train the word embeddings on the NYT corpus.",4.2.1 Pre-trained Word Embeddings,[0],[0]
Word2vec first constructs a vocabulary from the training text data and then learns vector representations of the words.,4.2.1 Pre-trained Word Embeddings,[0],[0]
"To obtain the embeddings of the entities, we concatenate the tokens of a entity using the ## operator when the entity has multiple word tokens.",4.2.1 Pre-trained Word Embeddings,[0],[0]
"Since a comparison of the word embeddings is beyond the scope
3With regard to the position feature, our experiments yield the same positive results described in Zeng et al. (2014).",4.2.1 Pre-trained Word Embeddings,[0],[0]
"Because the position feature is not the main contribution of this paper, we do not present the results without the position feature.
",4.2.1 Pre-trained Word Embeddings,[0],[0]
"4http://iesl.cs.umass.edu/riedel/ecml/ 5https://code.google.com/p/word2vec/
Window size Feature maps
Word dimension
Position dimension
Batch size
Adadelta parameter Dropout probability
of this paper, our experiments directly utilize 50- dimensional vectors.",4.2.1 Pre-trained Word Embeddings,[0],[0]
"In this section, we experimentally study the effects of two parameters on our models: the window size, w, and the number of feature maps, n. Following (Surdeanu et al., 2012), we tune all of the models using three-fold validation on the training set.",4.2.2 Parameter Settings,[0],[0]
"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces: w ∈ {1, 2, 3, · · · , 7} and n ∈ {50, 60, · · · , 300}.",4.2.2 Parameter Settings,[0],[0]
Table 1 shows all parameters used in the experiments.,4.2.2 Parameter Settings,[0],[0]
"Because the position dimension has little effect on the result, we heuristically choose dp = 5.",4.2.2 Parameter Settings,[0],[0]
The batch size is fixed to 50.,4.2.2 Parameter Settings,[0],[0]
"We use Adadelta (Zeiler, 2012) in the update procedure; it relies on two main parameters, ρ and ε, which do not significantly affect the performance (Zeiler, 2012).",4.2.2 Parameter Settings,[0],[0]
"Following (Zeiler, 2012), we choose 0.95 and 1e−6, respectively, as the values of these parameters.",4.2.2 Parameter Settings,[0],[0]
"In the dropout operation, we randomly set the hidden unit activities to zero with a probability of 0.5 during training.",4.2.2 Parameter Settings,[0],[0]
The held-out evaluation provides an approximate measure of precision without requiring costly human evaluation.,4.3.1 Held-out Evaluation,[0],[0]
Half of the Freebase relations are used for testing.,4.3.1 Held-out Evaluation,[0],[0]
"The relation instances discovered from the test articles are automatically compared with those in Freebase.
",4.3.1 Held-out Evaluation,[0],[0]
"To evaluate the proposed method, we select the following three traditional methods for comparison.",4.3.1 Held-out Evaluation,[0],[0]
"Mintz represents a traditional distantsupervision-based model that was proposed by (Mintz et al., 2009).",4.3.1 Held-out Evaluation,[0],[0]
"MultiR is a multi-instance learning method that was proposed by (Hoffmann et al., 2011).",4.3.1 Held-out Evaluation,[0],[0]
"MIML is a multi-instance multilabel model that was proposed by (Surdeanu et al., 2012).",4.3.1 Held-out Evaluation,[0],[0]
"Figure 4 shows the precision-recall curves for each method, where PCNNs+MIL denotes our method, and demonstrates that PCNNs+MIL achieves higher precision over the entire range of recall.",4.3.1 Held-out Evaluation,[0],[0]
"PCNNs+MIL enhances the recall to ap-
proximately 34% without any loss of precision.",4.3.1 Held-out Evaluation,[0],[0]
"In terms of both precision and recall, PCNNs+MIL outperforms all other evaluated approaches.",4.3.1 Held-out Evaluation,[0],[0]
"Notably, the results of the methods evaluated for comparison were obtained using manually crafted features.",4.3.1 Held-out Evaluation,[0],[0]
"By contrast, our result is obtained by automatically learning features from original words.",4.3.1 Held-out Evaluation,[0],[0]
The results demonstrate that the proposed method is an effective technique for distant supervised relation extraction.,4.3.1 Held-out Evaluation,[0],[0]
Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction.,4.3.1 Held-out Evaluation,[0],[0]
Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem.,4.3.1 Held-out Evaluation,[0],[0]
It is worth emphasizing that there is a sharp decline in the held-out precision-recall curves of PCNNs+MIL at very low recall (Figure 4).,4.3.2 Manual Evaluation,[0],[0]
"A manual check of the misclassified examples that were produced with high confidence reveals that the ma-
jorities of these examples are false negatives and are actually true relation instances that were misclassified due to the incomplete nature of Freebase.
",4.3.2 Manual Evaluation,[0],[0]
"Thus, the held-out evaluation suffers from false negatives in Freebase.",4.3.2 Manual Evaluation,[0],[0]
We perform a manual evaluation to eliminate these problems.,4.3.2 Manual Evaluation,[0],[0]
"For the manual evaluation, we choose the entity pairs for which at least one participating entity is not present in Freebase as a candidate.",4.3.2 Manual Evaluation,[0],[0]
This means that there is no overlap between the held-out and manual candidates.,4.3.2 Manual Evaluation,[0],[0]
"Because the number of relation instances that are expressed in the test data is unknown, we cannot calculate the recall in this case.",4.3.2 Manual Evaluation,[0],[0]
"Instead, we calculate the precision of the top N extracted relation instances.",4.3.2 Manual Evaluation,[0],[0]
"Table 2 presents the manually evaluated precisions for the top 100, top 200, and top 500 extracted instances.",4.3.2 Manual Evaluation,[0],[0]
"The results show that PCNNs+MIL achieves the best performance; moreover, the precision is higher than in the held-out evaluation.",4.3.2 Manual Evaluation,[0],[0]
"This finding indicates that many of the false negatives that we predict are, in fact, true relational facts.",4.3.2 Manual Evaluation,[0],[0]
The sharp decline observed in the held-out precision-recall curves is therefore reasonable.,4.3.2 Manual Evaluation,[0],[0]
"In this paper, we develop a method of piecewise max pooling and incorporate multi-instance learning into convolutional neural networks for distant supervised relation extraction.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"To demonstrate the effects of these two techniques, we empirically study the performance of systems in which these techniques are not implemented through held-out evaluations (Figure 5).",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
CNNs represents convolutional neural networks to which single max pooling is applied.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Figure 5 shows that when piecewise max pooling is used (PCNNs), better results are produced than those achieved using CNNs.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Moreover, compared with CNNs+MIL, PCNNs achieve slightly higher precision when the recall is greater than 0.08.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Since the parameters for all the model are determined by grid search, we can observe that CNNs cannot achieve competitive results compared to PCNNs when increasing the size of the hidden layer of convolutional neural networks.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
It means that we cannot capture more useful information by simply increasing the network parameter.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"These results demonstrate that the proposed piecewise max pooling technique is beneficial and
can effectively capture structural information for relation extraction.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
A similar phenomenon is also observed when multi-instance learning is added to the network.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Both CNNs+MIL and PCNNs+MIL outperform their counterparts CNNs and PCNNs, respectively, thereby demonstrating that incorporation of multi-instance learning into our neural network was successful in solving the wrong label problem.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"As expected, PCNNs+MIL obtains the best results because the advantages of both techniques are achieved simultaneously.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"In this paper, we exploit Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning for distant supervised relation extraction.",5 Conclusion,[0],[0]
"In our method, features are automatically learned without complicated NLP preprocessing.",5 Conclusion,[0],[0]
We also successfully devise a piecewise max pooling layer in the proposed network to capture structural information and incorporate multi-instance learning to address the wrong label problem.,5 Conclusion,[0],[0]
Experimental results show that the proposed approach offers significant improvements over comparable methods.,5 Conclusion,[0],[0]
This work was sponsored by the National Basic Research Program of China (no. 2014CB340503) and the National Natural Science Foundation of China (no. 61272332 and no. 61202329).,Acknowledgments,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
Two problems arise when using distant supervision for relation extraction.,abstractText,[0],[0]
"First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data.",abstractText,[0],[0]
"However, the heuristic alignment can fail, resulting in wrong label problem.",abstractText,[0],[0]
"In addition, in previous approaches, statistical models have typically been applied to ad hoc features.",abstractText,[0],[0]
The noise that originates from the feature extraction process can cause poor performance.,abstractText,[0],[0]
"In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems.",abstractText,[0],[0]
"To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account.",abstractText,[0],[0]
"To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features.",abstractText,[0],[0]
Experiments show that our method is effective and outperforms several competitive baseline methods.,abstractText,[0],[0]
Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 614–620 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
614",text,[0],[0]
Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers.,1 Introduction,[0],[0]
"To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annota-
tions to target languages (Yarowsky et al., 2001; Agić",1 Introduction,[0],[0]
"et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; Täckström",1 Introduction,[0],[0]
"et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013).",1 Introduction,[0],[0]
"However, only one or two compatible sources of distant supervision are typically employed.",1 Introduction,[0],[0]
In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint.,1 Introduction,[0],[0]
"Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers.
",1 Introduction,[0],[0]
We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.,1 Introduction,[0],[0]
"Our
system is a uniform neural model for POS tagging that learns from disparate sources of distant supervision (DSDS).",1 Introduction,[0],[0]
"We use it to combine: i) multi-source annotation projection, ii) instance selection, iii) noisy tag dictionaries, and iv) distributed word and sub-word representations.",1 Introduction,[0],[0]
"We examine how far we can get by exploiting only the wide-coverage resources that are currently readily available for more than 300 languages, which is the breadth of the parallel corpus we employ.
",1 Introduction,[0],[0]
DSDS yields a new state of the art by jointly leveraging disparate sources of distant supervision in an experiment with 25 languages.,1 Introduction,[0],[0]
"We demonstrate: i) substantial gains in carefully selecting high-quality instances in annotation projection, ii) the usefulness of lexicon features for neural tagging, and iii) the importance of word embeddings initialization for faster convergence.",1 Introduction,[0],[0]
DSDS is illustrated in Figure 1.,2 Method,[0],[0]
"The base model is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016; Kiperwasser and Goldberg, 2016).",2 Method,[0],[0]
"Let x1:n
be a given sequence of input vectors.",2 Method,[0],[0]
"In our base model, the input sequence consists of word embeddings ~w and the two output states of a character-level bi-LSTM ~c.",2 Method,[0],[0]
"Given x1:n and a desired index i, the functionBiRNNθ(x1:n, i) (here instantiated as LSTM) reads the input sequence in forward and reverse order, respectively, and uses the concatenated (◦) output states as input for tag prediction at position i.1",2 Method,[0],[0]
"Our model differs from prior work on the type of input vectors x1:n and distant data sources, in particular, we extend the input with lexicon embeddings, all described next.
",2 Method,[0],[0]
Annotation projection.,2 Method,[0],[0]
"Ever since the seminal work of Yarowsky et al. (2001), projecting sequential labels from source to target languages has been one of the most prevalent approaches to crosslingual learning.",2 Method,[0],[0]
"Its only requirement is that parallel texts are available between the languages, and that the source side is annotated for POS.
",2 Method,[0],[0]
"We apply the approach by Agić et al. (2016), where labels are projected from multiple sources and then decoded through weighted majority voting with word alignment probabilities and source POS tagger confidences.",2 Method,[0],[0]
"We exploit their widecoverage Watchtower corpus (WTC), in contrast to the typically used Europarl data.",2 Method,[0],[0]
"Europarl covers 21 languages of the EU with 400k-2M sentence pairs, while WTC spans 300+ widely diverse languages with only 10-100k pairs, in effect sacrificing depth for breadth, and introducing a more radical domain shift.",2 Method,[0],[0]
"However, as our results show little projected data turns out to be the most beneficial, reinforcing breadth for depth.
",2 Method,[0],[0]
"While Agić et al. (2016) selected 20k projected sentences at random to train taggers, we propose a novel alternative: selection by coverage.",2 Method,[0],[0]
"We rank the target sentences by percentage of words covered by word alignment from 21 sources of Agić et al. (2016), and select the top k covered instances for training.",2 Method,[0],[0]
"In specific, we employ the mean coverage ranking of target sentences, whereby each target sentence is coupled with the arithmetic mean of the 21 individual word alignment coverages for each of the 21 source-language sentences.",2 Method,[0],[0]
"We show that this simple approach to instance selection offers substantial improvements: across all languages, we learn better taggers with significantly fewer training instances.
",2 Method,[0],[0]
"1CRF decoding did not consistently improve POS accuracy, as recently also independently found (Yang et al., 2018).
",2 Method,[0],[0]
Dictionaries.,2 Method,[0],[0]
"Dictionaries are a useful source for distant supervision (Li et al., 2012; Täckström",2 Method,[0],[0]
"et al., 2013).",2 Method,[0],[0]
"There are several ways to exploit such information: i) as type constraints during encoding (Täckström et al., 2013), ii) to guide unsupervised learning (Li et al., 2012), or iii) as additional signal at training.",2 Method,[0],[0]
"We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., ~esrc is a lexicon src embedded into an l-dimensional space.",2 Method,[0],[0]
"We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise.",2 Method,[0],[0]
"Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations.
",2 Method,[0],[0]
"We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIKTIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016).",2 Method,[0],[0]
"For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agić",2 Method,[0],[0]
et al. (2017).,2 Method,[0],[0]
"The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph).",2 Method,[0],[0]
"Sizes are provided in Table 1, first columns.",2 Method,[0],[0]
"UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).
",2 Method,[0],[0]
Word embeddings.,2 Method,[0],[0]
Embeddings are available for many languages.,2 Method,[0],[0]
Pre-initialization of ~w offers consistent and considerable performance improvements in our distant supervision setup (Section 4).,2 Method,[0],[0]
"We use off-the-shelf Polyglot embeddings (AlRfou et al., 2013), which performed consistently better than FastText (Bojanowski et al., 2016).",2 Method,[0],[0]
Baselines.,3 Experiments,[0],[0]
"We compare to the following weaklysupervised POS taggers: – AGIC: Multi-source annotation projection with
Bible parallel data by Agić",3 Experiments,[0],[0]
et al. (2015).,3 Experiments,[0],[0]
– DAS:,3 Experiments,[0],[0]
"The label propagation approach by Das
and Petrov (2011) over Europarl data.
– GARRETTE:",3 Experiments,[0],[0]
"The approach by Garrette and Baldridge (2013) that works with projections, dictionaries, and unlabeled target text. – LI: Wiktionary supervision (Li et al., 2012).
",3 Experiments,[0],[0]
Data.,3 Experiments,[0],[0]
Our set of 25 languages is motivated by accessibility to embeddings and dictionaries.,3 Experiments,[0],[0]
"In all experiments we work with the 12 Universal POS tags (Petrov et al., 2012).",3 Experiments,[0],[0]
"For development, we use 21 dev sets of the Universal Dependencies 2.1 (Nivre et al., 2017).",3 Experiments,[0],[0]
We employ UD test sets on additional languages as well as the test sets of Agić,3 Experiments,[0],[0]
et al. (2015) to facilitate comparisons.,3 Experiments,[0],[0]
"Their test sets are a mixture of CoNLL (Buchholz and Marsi, 2006; Nivre et al., 2007) and HamleDT test data (Zeman et al., 2014), and are more distant from the training and development data.
",3 Experiments,[0],[0]
Model and parameters.,3 Experiments,[0],[0]
We extend an off-theshelf state-of-the-art bi-LSTM tagger with lexicon information.,3 Experiments,[0],[0]
The code is available at: https:// github.com/bplank/bilstm-aux.,3 Experiments,[0],[0]
The parameter l=40 was set on dev data across all languages.,3 Experiments,[0],[0]
"Besides using 10 epochs, word dropout rate (p=.25) and 40-dimensional lexicon embeddings, we use the parameters from Plank et al. (2016).",3 Experiments,[0],[0]
"For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.",3 Experiments,[0],[0]
"For the learning curve, we average over 5 random samples with 3 runs each.",3 Experiments,[0],[0]
"Table 1 shows the tagging accuracy for individual languages, while the means over all languages are given in Figure 2.",4 Results,[0],[0]
"There are several take-aways.
",4 Results,[0],[0]
Data selection.,4 Results,[0],[0]
"The first take-away is that coverage-based instance selection yields substan-
tially better training data.",4 Results,[0],[0]
"Most prior work on annotation projection resorts to arbitrary selection; informed selection clearly helps in this noisy data setup, as shown in Figure 2 (a).",4 Results,[0],[0]
"Training on 5k instances results in a sweet spot; more data (10k) starts to decrease performance, at a cost of runtime.",4 Results,[0],[0]
Training on all WTC data (around 120k) is worse for most languages.,4 Results,[0],[0]
"From now on we consider the 5k model trained with Polyglot as our baseline (Table 1, column “5k”), obtaining a mean accuracy of 83.0 over 21 languages.
",4 Results,[0],[0]
Embeddings initialization.,4 Results,[0],[0]
"Polyglot initialization offers a large boost; on average +3.8% absolute improvement in accuracy for our 5k training scheme, as shown in Figure 2 (b).",4 Results,[0],[0]
"The big gap in low-resource setups further shows their effectiveness, with up to 10% absolute increase in accuracy when training on only 500 instances.
",4 Results,[0],[0]
Lexical information.,4 Results,[0],[0]
"The main take-away is that lexical information helps neural tagging, and embedding it proves the most helpful.",4 Results,[0],[0]
"Embedding Wiktionary tags reaches 83.7 accuracy on average, versus 83.4 for n-hot encoding, and 83.2 for type constraints.",4 Results,[0],[0]
Only on 4 out of 21 languages are type constraints better.,4 Results,[0],[0]
This is the case for only one language for n-hot encoding (French).,4 Results,[0],[0]
"The best approach is to embed both Wiktionary and Unimorph, boosting performance further to 84.0, and resulting in our final model.",4 Results,[0],[0]
"It helps the most on morphological rich languages such as Uralic.
",4 Results,[0],[0]
"On the test sets (Table 4, right) DSDS reaches 87.2 over 8 test languages intersecting Li et al. (2012) and Agić",4 Results,[0],[0]
et al. (2016).,4 Results,[0],[0]
"It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011), compared to their 83.4.",4 Results,[0],[0]
"This shows that our novel “soft” inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps.",4 Results,[0],[0]
"We did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences.",4 Results,[0],[0]
Analysis.,5 Discussion,[0],[0]
The inclusion of lexicons results in higher coverage and is part of the explanation for the improvement of DSDS; see correlation in Figure 3 (a).,5 Discussion,[0],[0]
"What is more interesting is that our model benefits from the lexicon beyond its content: OOV accuracy for words not present in the lexicon overall improves, besides the expected improvement on known OOV, see Figure 3 (b).
More languages.",5 Discussion,[0],[0]
All data sources employed in our experiment are very high-coverage.,5 Discussion,[0],[0]
"However, for true low-resource languages, we cannot safely assume the availability of all disparate information sources.",5 Discussion,[0],[0]
Table 2 presents results for four additional languages where some supervision sources are missing.,5 Discussion,[0],[0]
"We observe that adding lexicon information always helps, even in cases where only 1k entries are available, and embedding it is usually the most beneficial way.",5 Discussion,[0],[0]
"For closely-related languages such as Serbian and Croatian, using resources for one aids tagging the other, and modern resources are a better fit.",5 Discussion,[0],[0]
"For example, using the Croatian WTC projections to train a model for Serbian is preferable over in-language Serbian Bible data where the OOV rate is much higher.
",5 Discussion,[0],[0]
How much gold data?,5 Discussion,[0],[0]
We assume not having access to any gold annotated data.,5 Discussion,[0],[0]
It is thus interesting to ask how much gold data is needed to reach our performance.,5 Discussion,[0],[0]
"This is a tricky question, as training within the same corpus naturally favors the same corpus data.",5 Discussion,[0],[0]
"We test both in-corpus (UD)
and out-of-corpus data (our test sets) and notice an important gap: while in-corpus only 50 sentences are sufficient, outside the corpus one would need over 200 sentences.",5 Discussion,[0],[0]
"This experiment was done for a subset of 18 languages with both in- and out-ofcorpus test data.
",5 Discussion,[0],[0]
Further comparison.,5 Discussion,[0],[0]
"In Table 1 we directly report the accuracies from the original contributions by DAS, LI, GARRETTE, and AGIC over the same test data.",5 Discussion,[0],[0]
We additionally attempted to reach the scores of LI by running their tagger over the Table 1 data setup.,5 Discussion,[0],[0]
The results are depicted in Figure 4 as mean accuracies over EM iterations until convergence.,5 Discussion,[0],[0]
"We show: i) LI peaks at 10 iterations for their test languages, and at 35 iterations for all the rest.",5 Discussion,[0],[0]
"This is in slight contrast to 50 iterations that Li et al. (2012) recommend, although selecting 50 does not dramatically hurt the scores; ii)",5 Discussion,[0],[0]
Our replication falls ∼5 points short of their 84.9 accuracy.,5 Discussion,[0],[0]
"There is a large 33-point accuracy gap between the scores of Li et al. (2012), where the dictionaries are large, and the other languages in Figure 4, with smaller dictionaries.
",5 Discussion,[0],[0]
"Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC.",5 Discussion,[0],[0]
"Similar applies to Täckström et al. (2013), as they use 1-5M near-perfect parallel sentences.",5 Discussion,[0],[0]
"Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011), and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.",5 Discussion,[0],[0]
"Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001), tag dictionaries (Li et al., 2012), annotation of seed training data (Garrette and Baldridge, 2013) or even more recently some combination of these, e.g., via multi-task learning (Fang and
Cohn, 2016; Kann et al., 2018).",6 Related Work,[0],[0]
"Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.
",6 Related Work,[0],[0]
Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods.,6 Related Work,[0],[0]
They rely on end-to-end training without resorting to additional linguistic resources.,6 Related Work,[0],[0]
Our study shows that this is not the case.,6 Related Work,[0],[0]
"Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Martı́nez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect.",6 Related Work,[0],[0]
We show that our approach of distant supervision from disparate sources (DSDS) is simple yet surprisingly effective for low-resource POS tagging.,7 Conclusions,[0],[0]
"Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neural tagging performance.",7 Conclusions,[0],[0]
"We introduce DSDS: a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages.",abstractText,[0],[0]
"The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework.",abstractText,[0],[0]
"The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.",abstractText,[0],[0]
Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1393–1402 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1393",text,[0],[0]
"Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.)",1 Introduction,[0],[0]
"as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016).",1 Introduction,[0],[0]
It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time.,1 Introduction,[0],[0]
A stochastic policy that controls the whole search process is usually learned by imitating a reference policy.,1 Introduction,[0],[0]
"The imitation is usually addressed as training a classifier to predict the ref-
∗*",1 Introduction,[0],[0]
"Email corresponding.
erence policy’s search action on the encountered states when performing the reference policy.",1 Introduction,[0],[0]
Such imitation process can sometimes be problematic.,1 Introduction,[0],[0]
"One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance (Goldberg and Nivre, 2012).",1 Introduction,[0],[0]
"Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned (Ross and Bagnell, 2010; Ross et al., 2011).",1 Introduction,[0],[0]
"All these problems harm the generalization ability of search-based structured prediction and lead to poor performance.
",1 Introduction,[0],[0]
Previous works tackle these problems from two directions.,1 Introduction,[0],[0]
"To overcome the ambiguities in data, techniques like ensemble are often adopted (Di-
etterich, 2000).",1 Introduction,[0],[0]
"To mitigate the discrepancy, exploration is encouraged during the training process (Ross and Bagnell, 2010; Ross et al., 2011; Goldberg and Nivre, 2012; Bengio et al., 2015; Goodman et al., 2016).",1 Introduction,[0],[0]
"In this paper, we propose to consider these two problems in an integrated knowledge distillation manner (Hinton et al., 2015).",1 Introduction,[0],[0]
We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble’s output distribution on the reference states.,1 Introduction,[0],[0]
We also let the ensemble randomly explore the search space and learn the single model to mimic ensemble’s distribution on the encountered exploration states.,1 Introduction,[0],[0]
Combing the distillation from reference and exploration further improves our single model’s performance.,1 Introduction,[0],[0]
"The workflow of our method is shown in Figure 1.
",1 Introduction,[0],[0]
We conduct experiments on two typical searchbased structured prediction tasks: transition-based dependency parsing and neural machine translation.,1 Introduction,[0],[0]
The results of both these two experiments show the effectiveness of our knowledge distillation method by outperforming strong baselines.,1 Introduction,[0],[0]
"In the parsing experiments, an improvement of 1.32 in LAS is achieved and in the machine translation experiments, such improvement is 2.65 in BLEU.",1 Introduction,[0],[0]
"Our model also outperforms the greedy models in previous works.
",1 Introduction,[0],[0]
"Major contributions of this paper include:
• We study the knowledge distillation in search-based structured prediction and propose to distill the knowledge of an ensemble into a single model by learning to match its distribution on both the reference states (§3.2) and exploration states encountered when using the ensemble to explore the search space (§3.3).",1 Introduction,[0],[0]
"A further combination of these two methods is also proposed to improve the performance (§3.4).
",1 Introduction,[0],[0]
• We conduct experiments on two search-based structured prediction problems: transitionbased dependency parsing and neural machine translation.,1 Introduction,[0],[0]
"In both these two problems, the distilled model significantly improves over strong baselines and outperforms other greedy structured prediction (§4.2).",1 Introduction,[0],[0]
Comprehensive analysis empirically shows the feasibility of our distillation method (§4.3).,1 Introduction,[0],[0]
Structured prediction maps an input x =,2.1 Search-based Structured Prediction,[0],[0]
"(x1, x2, ..., xn) to its structural output y = (y1, y2, ..., ym), where each component of y has some internal dependencies.",2.1 Search-based Structured Prediction,[0],[0]
"Search-based structured prediction (Collins and Roark, 2004; Daumé III et al., 2005; Daumé III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S,A, T (s, a),S0,ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states.",2.1 Search-based Structured Prediction,[0],[0]
"Starting from an initial state s0 ∈ S0, the structured prediction model repeatably chooses an action at ∈ A by following a policy π(s) and applies at to st and enter a new state st+1 as st+1 ← T (st, at), until a final state sT ∈ ST is achieved.",2.1 Search-based Structured Prediction,[0],[0]
"Several natural language structured prediction problems can be modeled under the search-based framework including dependency parsing (Nivre, 2008) and neural machine translation (Liang et al., 2006; Sutskever et al., 2014).",2.1 Search-based Structured Prediction,[0],[0]
"Table 1 shows the search-based structured prediction view of these two problems.
",2.1 Search-based Structured Prediction,[0],[0]
"In the data-driven settings, π(s) controls the whole search process and is usually parameterized by a classifier p(a | s) which outputs the proba-
Algorithm 1: Generic learning algorithm for search-based structured prediction.
",2.1 Search-based Structured Prediction,[0],[0]
"Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y).
",2.1 Search-based Structured Prediction,[0],[0]
Output: classifier p(a|s).,2.1 Search-based Structured Prediction,[0],[0]
"1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n)); 5 while st /∈ ST do 6 at ← πR(st,y(n)); 7 D ← D ∪ {st}; 8 st+1 ← T (st, at); 9 t← t+ 1;
10 end 11 end 12 optimize LNLL;
bility of choosing an action a on the given state s.",2.1 Search-based Structured Prediction,[0],[0]
The commonly adopted greedy policy can be formalized as choosing the most probable action with π(s) = argmaxa,2.1 Search-based Structured Prediction,[0],[0]
p(a | s) at test stage.,2.1 Search-based Structured Prediction,[0],[0]
"To learn an optimal classifier, search-based structured prediction requires constructing a reference policy πR(s,y), which takes an input state s, gold structure y and outputs its reference action a, and training p(a | s) to imitate the reference policy.",2.1 Search-based Structured Prediction,[0],[0]
"Algorithm 1 shows the common practices in training p(a | s), which involves: first, using πR(s,y) to generate a sequence of reference states and actions on the training data (line 1 to line 11 in Algorithm 1); second, using the states and actions on the reference sequences as examples to train p(a | s) with negative log-likelihood (NLL) loss (line 12 in Algorithm 1),
LNLL = ∑ s∈D ∑ a −1{a = πR} · log p(a | s)
where D is a set of training data.",2.1 Search-based Structured Prediction,[0],[0]
"The reference policy is sometimes sub-optimal and ambiguous which means on one state, there can be more than one action that leads to the optimal prediction.",2.1 Search-based Structured Prediction,[0],[0]
"In transition-based dependency parsing, Goldberg and Nivre (2012) showed that one dependency tree can be reached by several search sequences using Nivre (2008)’s arcstandard algorithm.",2.1 Search-based Structured Prediction,[0],[0]
"In machine translation, the ambiguity problem also exists because one source language sentence usually has multiple semantically correct translations but only one reference
translation is presented.",2.1 Search-based Structured Prediction,[0],[0]
"Similar problems have also been observed in semantic parsing (Goodman et al., 2016).",2.1 Search-based Structured Prediction,[0],[0]
"According to Frénay and Verleysen (2014), the widely used NLL loss is vulnerable to ambiguous data which make it worse for searchbased structured prediction.
",2.1 Search-based Structured Prediction,[0],[0]
"Besides the ambiguity problem, training and testing discrepancy is another problem that lags the search-based structured prediction performance.",2.1 Search-based Structured Prediction,[0],[0]
"Since the training process imitates the reference policy, all the states in the training data are optimal which means they are guaranteed to reach the optimal structure.",2.1 Search-based Structured Prediction,[0],[0]
"But during the test phase, the model can predict non-optimal states whose search action is never learned.",2.1 Search-based Structured Prediction,[0],[0]
The greedy search which is prone to error propagation also worsens this problem.,2.1 Search-based Structured Prediction,[0],[0]
"A cumbersome model, which could be an ensemble of several models or a single model with larger number of parameters, usually provides better generalization ability.",2.2 Knowledge Distillation,[0],[0]
"Knowledge distillation (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) is a class of methods for transferring the generalization ability of the cumbersome teacher model into a small student model.",2.2 Knowledge Distillation,[0],[0]
"Instead of optimizing NLL loss, knowledge distillation uses the distribution q(y | x) outputted by the teacher model as “soft target” and optimizes the knowledge distillation loss,
LKD = ∑ x∈D ∑ y −q(y",2.2 Knowledge Distillation,[0],[0]
"| x) · log p(y | x).
",2.2 Knowledge Distillation,[0],[0]
"In search-based structured prediction scenario, x corresponds to the state s and y corresponds to the action a. Through optimizing the distillation loss, knowledge of the teacher model is learned by the student model p(y | x).",2.2 Knowledge Distillation,[0],[0]
"When correct label is presented, NLL loss can be combined with the distillation loss via simple interpolation as
L = αLKD + (1− α)LNLL (1)",2.2 Knowledge Distillation,[0],[0]
"As Hinton et al. (2015) pointed out, although the real objective of a machine learning algorithm is to generalize well to new data, models are usually trained to optimize the performance on training data, which bias the model to the training data.
",3.1 Ensemble,[0],[0]
"In search-based structured prediction, such biases can result from either the ambiguities in the training data or the discrepancy between training and testing.",3.1 Ensemble,[0],[0]
"It would be more problematic to train p(a | s) using the loss which is in-robust to ambiguities and only considering the optimal states.
",3.1 Ensemble,[0],[0]
The effect of ensemble on ambiguous data has been studied in Dietterich (2000).,3.1 Ensemble,[0],[0]
They empirically showed that ensemble can overcome the ambiguities in the training data.,3.1 Ensemble,[0],[0]
Daumé III et al. (2005) also use weighted ensemble of parameters from different iterations as their final structure prediction model.,3.1 Ensemble,[0],[0]
"In this paper, we consider to use ensemble technique to improve the generalization ability of our search-based structured prediction model following these works.",3.1 Ensemble,[0],[0]
"In practice, we train M search-based structured prediction models with different initialized weights and ensemble them by the average of their output distribution as q(a | s) = 1M ∑ m qm(a | s).",3.1 Ensemble,[0],[0]
"In Section 4.3.1, we empirically show that the ensemble has the ability to choose a good search action in the optimal-yetambiguous states and the non-optimal states.",3.1 Ensemble,[0],[0]
"As we can see in Section 4, ensemble indeed improves the performance of baseline models.",3.2 Distillation from Reference,[0],[0]
"However, real world deployment is usually constrained by computation and memory resources.",3.2 Distillation from Reference,[0],[0]
"Ensemble requires running the structured prediction models for multiple times, and that makes it less applicable in real-world problem.",3.2 Distillation from Reference,[0],[0]
"To take the advantage of the ensemble model while avoid running the models multiple times, we use the knowledge distillation technique to distill a single model from the ensemble.",3.2 Distillation from Reference,[0],[0]
We started from changing the NLL learning objective in Algorithm 1 into the distillation loss (Equation 1) as shown in Algorithm 2.,3.2 Distillation from Reference,[0],[0]
"Since such method learns the model on the states produced by the reference policy, we name it as distillation from reference.",3.2 Distillation from Reference,[0],[0]
Blocks connected by in dashed red lines in Figure 1 show the workflow of our distillation from reference.,3.2 Distillation from Reference,[0],[0]
"In the scenario of search-based structured prediction, transferring the teacher model’s generalization ability into a student model not only includes matching the teacher model’s soft targets on the reference search sequence, but also imitating the search decisions made by the teacher model.",3.3 Distillation from Exploration,[0],[0]
"One way to accomplish the imitation can be sampling
Algorithm 2: Knowledge distillation for search-based structured prediction.
",3.3 Distillation from Exploration,[0],[0]
"Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y); the exploration policy: πE(s) which samples an action from the annealed ensemble q(a | s) 1 T
Output: classifier p(a | s).",3.3 Distillation from Exploration,[0],[0]
1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n));,3.3 Distillation from Exploration,[0],[0]
"5 while st /∈ ST do 6 if distilling from reference then 7 at ← πR(st,y(n)); 8 else 9 at ← πE(st);
10 end 11 D ← D ∪ {st}; 12 st+1 ← T (st, at); 13 t← t+ 1; 14 end 15 end 16 if distilling from reference then 17 optimize αLKD +",3.3 Distillation from Exploration,[0],[0]
"(1− α)LNLL; 18 else 19 optimize LKD; 20 end
search sequence from the ensemble and learn from the soft target on the sampled states.",3.3 Distillation from Exploration,[0],[0]
"More concretely, we change πR(s,y) into a policy πE(s) which samples an action a from q(a | s) 1 T , where T is the temperature that controls the sharpness of the distribution (Hinton et al., 2015).",3.3 Distillation from Exploration,[0],[0]
The algorithm is shown in Algorithm 2.,3.3 Distillation from Exploration,[0],[0]
"Since such distillation generate training instances from exploration, we name it as distillation from exploration.",3.3 Distillation from Exploration,[0],[0]
"Blocks connected by in solid blue lines in Figure 1 show the workflow of our distillation from exploration.
",3.3 Distillation from Exploration,[0],[0]
"On the sampled states, reference decision from πR is usually non-trivial to achieve, which makes learning from NLL loss infeasible.",3.3 Distillation from Exploration,[0],[0]
"In Section 4, we empirically show that fully distilling from the soft target, i.e. setting α = 1 in Equation 1, achieves comparable performance with that both from distillation and NLL.",3.3 Distillation from Exploration,[0],[0]
Distillation from reference can encourage the model to predict the action made by the reference policy and distillation from exploration learns the model on arbitrary states.,3.4 Distillation from Both,[0],[0]
They transfer the generalization ability of the ensemble from different aspects.,3.4 Distillation from Both,[0],[0]
Hopefully combining them can further improve the performance.,3.4 Distillation from Both,[0],[0]
"In this paper, we combine distillation from reference and exploration with the following manner: we use πR and πE to generate a set of training states.",3.4 Distillation from Both,[0],[0]
"Then, we learn p(a | s) on the generated states.",3.4 Distillation from Both,[0],[0]
"If one state was generated by the reference policy, we minimize the interpretation of distillation and NLL loss.",3.4 Distillation from Both,[0],[0]
"Otherwise, we minimize the distillation loss only.",3.4 Distillation from Both,[0],[0]
We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation.,4 Experiments,[0],[0]
"Both these two tasks are converted to search-based structured prediction as Section 2.1.
",4 Experiments,[0],[0]
"For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier.1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following Luong et al. (2015).2 We encourage the reader of this paper to refer corresponding papers for more details.",4 Experiments,[0],[0]
"We perform experiments on Penn Treebank (PTB) dataset with standard data split (Section 2-21 for training, Section 22 for development, and Section 23 for testing).",4.1.1 Transition-based Dependency Parsing,[0],[0]
Stanford dependencies are converted from the original constituent trees using Stanford CoreNLP 3.3.03 by following Dyer et al. (2015).,4.1.1 Transition-based Dependency Parsing,[0],[0]
Automatic part-of-speech tags are assigned by 10-way jackknifing whose accuracy is 97.5%.,4.1.1 Transition-based Dependency Parsing,[0],[0]
Labeled attachment score (LAS) excluding punctuation are used in evaluation.,4.1.1 Transition-based Dependency Parsing,[0],[0]
"For the other hyper-parameters, we use the same settings as Dyer et al. (2015).",4.1.1 Transition-based Dependency Parsing,[0],[0]
"The best iteration and α is determined on the development set.
",4.1.1 Transition-based Dependency Parsing,[0],[0]
"1The code for parsing experiments is available at: https://github.com/Oneplus/twpipe.
",4.1.1 Transition-based Dependency Parsing,[0],[0]
"2We based our NMT experiments on OpenNMT (Klein et al., 2017).",4.1.1 Transition-based Dependency Parsing,[0],[0]
"The code for NMT experiments is available at: https://github.com/Oneplus/OpenNMT-py.
3stanfordnlp.github.io/CoreNLP/ history.html
Reimers and Gurevych (2017) and others have pointed out that neural network training is nondeterministic and depends on the seed for the random number generator.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"To control for this effect, they suggest to report the average of M differentlyseeded runs.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"In all our dependency parsing, we set n = 20.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign.",4.1.2 Neural Machine Translation,[0],[0]
"The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs.",4.1.2 Neural Machine Translation,[0],[0]
"We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries.",4.1.2 Neural Machine Translation,[0],[0]
One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016).,4.1.2 Neural Machine Translation,[0],[0]
"BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score.
",4.1.2 Neural Machine Translation,[0],[0]
Optimizing the distillation loss in Equation 1 requires enumerating over the action space.,4.1.2 Neural Machine Translation,[0],[0]
It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments).,4.1.2 Neural Machine Translation,[0],[0]
"In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the whole probability distribution of q(a | s) as ∑ a q(a |
s) · log p(a | s)",4.1.2 Neural Machine Translation,[0],[0]
"≈ ∑K
k q(âk",4.1.2 Neural Machine Translation,[0],[0]
"| s) · log p(âk | s), where âk is the k-th probable action.",4.1.2 Neural Machine Translation,[0],[0]
"We fix α to
4We use multi-bleu.perl to evaluate our model’s performance
1 and vary K and evaluate the distillation model’s performance.",4.1.2 Neural Machine Translation,[0],[0]
"These results are shown in Figure 2 where there is no significant difference between different Ks and in speed consideration, we set K to 1 in the following experiments.",4.1.2 Neural Machine Translation,[0],[0]
Table 2 shows our PTB experimental results.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS.",4.2.1 Transition-based Dependency Parsing,[0],[0]
"For our distillation from reference, when setting α = 1.0, best performance on development set is achieved and the test LAS is 91.99.
",4.2.1 Transition-based Dependency Parsing,[0],[0]
We tune the temperature T during exploration and the results are shown in Figure 3.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Sharpen the distribution during the sampling process generally performs better on development set.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14.
",4.2.1 Transition-based Dependency Parsing,[0],[0]
We also compare our parser with the other parsers in Table 2.,4.2.1 Transition-based Dependency Parsing,[0],[0]
The second group shows the greedy transition-based parsers in previous literatures.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"(Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle.",4.2.1 Transition-based Dependency Parsing,[0],[0]
Our distillation parser outperforms all these greedy counterparts.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"The third group shows
parsers trained on different techniques including decoding with beam search (Buckman et al., 2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al., 2016), graph-based parsing (Dozat and Manning, 2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017).",4.2.1 Transition-based Dependency Parsing,[0],[0]
Our distillation parser still outperforms its transition-based counterparts but lags the others.,4.2.1 Transition-based Dependency Parsing,[0],[0]
We attribute the gap between our parser with the other parsers to the difference in parsing algorithms.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Table 3 shows the experimental results on IWSLT 2014 dataset.,4.2.2 Neural Machine Translation,[0],[0]
"Similar to the PTB parsing results, the ensemble 10 translators outperforms the baseline translator by 3.47 in BLEU score.",4.2.2 Neural Machine Translation,[0],[0]
"Distilling from the ensemble by following the reference leads to a single translator of 24.76 BLEU score.
",4.2.2 Neural Machine Translation,[0],[0]
"Like in the parsing experiments, sharpen the distribution when exploring the search space is more helpful to the model’s performance but the differences when T ≤ 0.2 is not significant as shown in Figure 3.",4.2.2 Neural Machine Translation,[0],[0]
We set T = 0.1 in our distillation from exploration experiments since it achieves the best development score.,4.2.2 Neural Machine Translation,[0],[0]
Table 3 shows the exploration result of a BLEU score of 24.64 and it slightly lags the best reference model.,4.2.2 Neural Machine Translation,[0],[0]
"Distilling from both the reference and exploration improves the single model’s performance by a large margin and achieves a BLEU score of 25.44.
",4.2.2 Neural Machine Translation,[0],[0]
"We also compare our model with other translation models including the one trained with reinforcement learning (Ranzato et al., 2015) and that using beam search in training (Wiseman and Rush, 2016).",4.2.2 Neural Machine Translation,[0],[0]
"Our distillation translator outperforms these models.
",4.2.2 Neural Machine Translation,[0],[0]
Both the parsing and machine translation experiments confirm that it’s feasible to distill a reasonable search-based structured prediction model by just exploring the search space.,4.2.2 Neural Machine Translation,[0],[0]
Combining the reference and exploration further improves the model’s performance and outperforms its greedy structured prediction counterparts.,4.2.2 Neural Machine Translation,[0],[0]
"In Section 4.2, improvements from distilling the ensemble have been witnessed in both the transition-based dependency parsing and neural machine translation experiments.",4.3 Analysis,[0],[0]
"However, questions like “Why the ensemble works better? Is it feasible to fully learn from the distillation loss without NLL?",4.3 Analysis,[0],[0]
Is learning from distillation loss stable?” are yet to be answered.,4.3 Analysis,[0],[0]
"In this section, we first study the ensemble’s behavior on “problematic” states to show its generalization ability.",4.3 Analysis,[0],[0]
"Then, we empirically study the feasibility of fully learning from the distillation loss by studying the effect of α in the distillation from reference setting.",4.3 Analysis,[0],[0]
"Finally, we show that learning from distillation loss is less sensitive to initialization and achieves a more stable model.",4.3 Analysis,[0],[0]
"As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Ensemble shows to improve the performance in Section 4.2, which indicates it does better on these states.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle.
",4.3.1 Ensemble on “Problematic” States,[0],[0]
"The dynamic oracle (Goldberg and Nivre, 2012; Goldberg et al., 2014) can be used to efficiently determine, given any state s, which transition action leads to the best achievable parse from s; if some errors may have already made, what is the best the parser can do, going forward?",4.3.1 Ensemble on “Problematic” States,[0],[0]
"This allows us to analyze the accuracy of each parser’s individual decisions, in the “problematic” states.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"In this paper, we evaluate the output distributions of the baseline and ensemble parser against the reference actions suggested by the dynamic oracle.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Since dynamic oracle yields more than one reference actions due to ambiguities and previous mistakes and the output distribution can be treated as their scoring, we evaluate them as a ranking problem.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Intuitively, when multiple reference actions exist, a good parser should push probability mass to these actions.",4.3.1 Ensemble on “Problematic” States,[0],[0]
We draw problematic states by sampling from our baseline parser.,4.3.1 Ensemble on “Problematic” States,[0],[0]
The comparison in Table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states.,4.3.1 Ensemble on “Problematic” States,[0],[0]
"This observation indicates the ensemble’s output distribution is more “informative”, thus generalizes well on problematic states and achieves better performance.",4.3.1 Ensemble on “Problematic” States,[0],[0]
We also observe that the distillation model perform better than both the baseline and ensemble.,4.3.1 Ensemble on “Problematic” States,[0],[0]
We attribute this to the fact that the distillation model is learned from exploration.,4.3.1 Ensemble on “Problematic” States,[0],[0]
"Over our distillation from reference model, we study the effect of α in Equation 1.",4.3.2 Effect of α,[0],[0]
We vary α from 0 to 1 by a step of 0.1 in both the transitionbased dependency parsing and neural machine translation experiments and plot the model’s performance on development sets in Figure 4.,4.3.2 Effect of α,[0],[0]
Similar trends are witnessed in both these two experiments that model that’s configured with larger α generally performs better than that with smaller α.,4.3.2 Effect of α,[0],[0]
"For the dependency parsing problem, the best development performance is achieved when we set α = 1, and for the machine translation, the best α is 0.8.",4.3.2 Effect of α,[0],[0]
There is only 0.2 point of difference between the best α model and the one with α equals to 1.,4.3.2 Effect of α,[0],[0]
Such observation indicates that when distilling from the reference policy paying more attention to the distillation loss rather than the NLL is more beneficial.,4.3.2 Effect of α,[0],[0]
It also indicates that fully learning from the distillation loss outputted by the ensemble is reasonable because models configured with α = 1 generally achieves good performance.,4.3.2 Effect of α,[0],[0]
"Besides the improved performance, knowledge distillation also leads to more stable learning.",4.3.3 Learning Stability,[0],[0]
The performance score distributions of differentlyseed runs are depicted as violin plot in Figure 5.,4.3.3 Learning Stability,[0],[0]
Table 5 also reveals the smaller standard derivations are achieved by our distillation methods.,4.3.3 Learning Stability,[0],[0]
"As Keskar et al. (2016) pointed out that the general-
ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers.",4.3.3 Learning Stability,[0],[0]
Several works have been proposed to applying knowledge distillation to NLP problems.,5 Related Work,[0],[0]
Kim and Rush (2016) presented a distillation model which focus on distilling the structured loss from a large model into a small one which works on sequencelevel.,5 Related Work,[0],[0]
"In contrast to their work, we pay more attention to action-level distillation and propose to do better action-level distillation by both from reference and exploration.
",5 Related Work,[0],[0]
Freitag et al. (2017) used an ensemble of 6- translators to generate training reference.,5 Related Work,[0],[0]
Exploration was tried in their work with beam-search.,5 Related Work,[0],[0]
"We differ their work by training the single model
to match the distribution of the ensemble.",5 Related Work,[0],[0]
"Using ensemble in exploration was also studied in reinforcement learning community (Osband et al., 2016).",5 Related Work,[0],[0]
"In addition to distilling the ensemble on the labeled training data, a line of semisupervised learning works show that it’s effective to transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014).",5 Related Work,[0],[0]
"Their extensions to knowledge distillation call for further study.
",5 Related Work,[0],[0]
Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser.,5 Related Work,[0],[0]
"Different from their work, we directly do the distillation on the classifier of the transition-based parser.
",5 Related Work,[0],[0]
"Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by unfolding and then use SVD to shrink its parameters, which can be treated as another kind of knowledge distillation.",5 Related Work,[0],[0]
"In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states.",6 Conclusion,[0],[0]
Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance.,6 Conclusion,[0],[0]
Comparison analysis gives empirically guarantee for our distillation method.,6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgments,[0],[0]
This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772153.,Acknowledgments,[0],[0]
Many natural language processing tasks can be modeled into structured prediction and solved as a search problem.,abstractText,[0],[0]
"In this paper, we distill an ensemble of multiple models trained with different initialization into a single model.",abstractText,[0],[0]
"In addition to learning to match the ensemble’s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration.",abstractText,[0],[0]
Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.,abstractText,[0],[0]
Distilling Knowledge for Search-based Structured Prediction,title,[0],[0]
"Over the last several years, the world has witnessed the emergence of data sets of an unprecedented scale across different scientific disciplines.",1. Introduction,[0],[0]
"This development has created a need for scalable, distributed machine learning algorithms to deal with the increasing amount of data.",1. Introduction,[0],[0]
"In this paper, we consider large-scale clustering or, more specifically, the task of finding provably good seedings for kMeans in a massive data setting.
",1. Introduction,[0],[0]
Seeding — the task of finding initial cluster centers — is critical to finding good clusterings for k-Means.,1. Introduction,[0],[0]
"In fact, the seeding step of the state of the art algorithm k-means++ (Arthur & Vassilvitskii, 2007) provides the
1Department of Computer Science, ETH Zurich.",1. Introduction,[0],[0]
"Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"theoretical guarantee on the solution quality while the subsequent refinement using Lloyd’s algorithm (Lloyd, 1982) only guarantees that the quality does not deteriorate.",1. Introduction,[0],[0]
"While the k-means++ seeding step guarantees a solution that is O(log k) competitive with the optimal solution in expectation, it also requires k inherently sequential passes through the data set.",1. Introduction,[0],[0]
"This makes it unsuitable for the massive data setting where the data set is distributed across machines and computation has to occur in parallel.
",1. Introduction,[0],[0]
"As a remedy, Bahmani et al. (2012) propose the k-means‖ algorithm which produces seedings for kMeans with a reduced number of sequential iterations.",1. Introduction,[0],[0]
"Whereas k-means++ only samples a single cluster center in each of k rounds, k-means‖ samples in expectation ` points in each of t iterations.",1. Introduction,[0],[0]
"Provided t is small enough, this makes k-means‖ suitable for a distributed setting as the number of synchronizations is reduced.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
"We provide a novel analysis of k-means‖ that bounds the expected solution quality for any number of rounds t and any oversampling factor ` ≥ k, the two parameters that need to be chosen in practice.",1. Introduction,[0],[0]
Our bound on the expected quantization error includes both a “traditional” multiplicative error term based on the optimal solution as well as a scale-invariant additive error term based on the variance of the data.,1. Introduction,[0],[0]
The key insight is that this additive error term vanishes at a rate of ( k e` )t if t or ` is increased.,1. Introduction,[0],[0]
"This shows that k-means‖ provides provably good clusterings even for a small, constant number of iterations and explains the commonly observed phenomenon that k-means‖ works very well even for small t.
We further provide a hard instance on which k-means‖ provably incurs an additive error based on the variance of the data and for which an exclusively multiplicative error guarantee cannot be achieved.",1. Introduction,[0],[0]
This implies that an additive error term such as the one in our analysis is in fact necessary if less than k − 1 rounds are employed.,1. Introduction,[0],[0]
k-Means clustering.,2. Background & related work,[0],[0]
Let X denote a set of points in Rd.,2. Background & related work,[0],[0]
"The k-Means clustering problem is to find a set C of k cluster centers in Rd that minimizes the quantization error
φX (C) = ∑ x∈X d(x,C)2 = ∑",2. Background & related work,[0],[0]
x∈X min,2. Background & related work,[0],[0]
"q∈C ‖x− q‖22.
Algorithm 1 k-means++ seeding Require: weighted data set (X , w), number of clusters k
1: C ← sample single x ∈ X with probability wx∑ x′∈X wx′ 2: for i = 2, . . .",2. Background & related work,[0],[0]
", k do 3: Sample x ∈ X with probability wx d(x,C)
2∑ x′∈X wx′ d(x",2. Background & related work,[0],[0]
"′,C)2
4: C ← C ∪ {x} 5: Return C
We denote the optimal quantization error by φOPT(X ) while the variance of the data is defined as Var(X ) = φX ({µ(X )}) where µ(X ) is the mean of X .
k-means++ seeding.",2. Background & related work,[0],[0]
"Given a data set X and any set of cluster centers C ⊂ X , the D2-sampling strategy selects a new center by sampling each point x ∈ X with probability
p(x) = d(x,C)2∑
x′∈X d(x ′, C ′)2
.
",2. Background & related work,[0],[0]
"The seeding step of k-means++ (Arthur & Vassilvitskii, 2007), detailed for potentially weighted data sets in Algorithm 1, selects an initial cluster center uniformly at random and then sequentially adds k − 1 cluster centers using D2 sampling whereby C is always the set of previously sampled centers.",2. Background & related work,[0],[0]
"Arthur & Vassilvitskii (2007) show that the solution quality φk-means++ of k-means++ seeding is bounded in expectation by
E[φk-means++] ≤ 8 (log2 k + 2)φOPT(X ).
",2. Background & related work,[0],[0]
The computational complexity of k-means++ seeding is O(nkd) where n is the number of data points and d the dimensionality.,2. Background & related work,[0],[0]
"Unfortunately, the iterations in k-means++ seeding are inherently sequential and, as a result, the algorithm requires k full passes through the data.",2. Background & related work,[0],[0]
"This makes the algorithm unsuitable for the distributed setting.
",2. Background & related work,[0],[0]
k-means‖ seeding.,2. Background & related work,[0],[0]
"As a remedy, Bahmani et al. (2012) propose the algorithm k-means‖ which aims to reduce the number of sequential iterations.",2. Background & related work,[0],[0]
"The key component of k-means‖ is detailed in Algorithm 2 in what we call k-means‖ overseeding: First, a data point is sampled as the first cluster center uniformly at random.",2. Background & related work,[0],[0]
"Then, in each of t sequential rounds, each data point x ∈ X is independently sampled with probability min ( 1, ` d(x,C) 2
φX (C)
) and
added to the set of sampled centers C at the end of the round.",2. Background & related work,[0],[0]
"The parameter ` ≥ 1 is called the oversampling factor and determines the expected number of sampled points in each iteration.
",2. Background & related work,[0],[0]
"At the end of Algorithm 2, one obtains an oversampled solution with t` cluster centers in expectation.",2. Background & related work,[0],[0]
"The full k-means‖ seeding algorithm as detailed in Algorithm 3 reduces such a solution to k centers as follows: First, each of the centers in the oversampled solution is weighted by the number of data points which are closer to it than the
Algorithm 2 k-means‖ overseeding Require: data set X , # rounds t, oversampling factor `
1: C ← sample a point uniformly at random from X 2: for i = 1, 2, . . .",2. Background & related work,[0],[0]
", t do 3: C ′",2. Background & related work,[0],[0]
← ∅ 4: for x ∈ X,2. Background & related work,[0],[0]
"do 5: Add x to C ′ with probability min ( 1, ` d(x,C) 2
φX (C) ) 6: C ← C ∪ C ′ 7: Return C
Algorithm 3 k-means‖ seeding Require: data set X , # rounds t, oversampling factor `
1: B ← Result of Algorithm 2 applied to (X , t, `) 2: for c ∈ B do 3:",2. Background & related work,[0],[0]
Xc ← points x ∈ X,2. Background & related work,[0],[0]
"whose closest center in B is c (ties broken arbitrarily but consistently) 4: wc ← |Xc| 5: C ← Result of Algorithm 1 applied to (B,w) 6:",2. Background & related work,[0],[0]
"Return C
other centers.",2. Background & related work,[0],[0]
"Then, k-means++ seeding is run on the weighted oversampled solution to produce a set of k final centers.",2. Background & related work,[0],[0]
"The total computational complexity of Algorithm 3 is O(nt`d) in expectation.
",2. Background & related work,[0],[0]
"The key intuition behind k-means‖ is that, if we choose a large oversampling factor `, the number of rounds t can be small — certainly much smaller than k, preferably even constant.",2. Background & related work,[0],[0]
The step in lines 4 and 5 in Algorithm 2 can be distributed over several machines and after each round the set C can be synchronized.,2. Background & related work,[0],[0]
"Due to the low number of synchronizations (i.e., rounds), Algorithm 2 can be efficiently run in a distributed setting.1
Other related work.",2. Background & related work,[0],[0]
Celebi et al. (2013) provide an overview over different seeding methods for k-Means.,2. Background & related work,[0],[0]
D2sampling and k-means++ style algorithms have been independently studied by both Ostrovsky et al. (2006) and Arthur & Vassilvitskii (2007).,2. Background & related work,[0],[0]
"This research direction has led to polynomial time approximation schemes based on D2-sampling (Jaiswal et al., 2014; 2015), constant factor approximations based on sampling more than k centers (Ailon et al., 2009; Aggarwal et al., 2009) and the analysis of hard instances (Arthur & Vassilvitskii, 2007; Brunsch & Röglin, 2011).",2. Background & related work,[0],[0]
"Recently, algorithms to approximate k-means++ seeding based on Markov Chain Monte Carlo have been proposed by Bachem et al. (2016b;a).",2. Background & related work,[0],[0]
"Finally, k-means++ has been used to construct coresets — small data set summaries — for k-Means clustering (Lucic et al., 2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ackermann et al., 2012) and Gaussian mixture models (Lucic et al., 2017).
",2. Background & related work,[0],[0]
"1A popular choice is the MLLib library of Apache Spark (Meng et al., 2016) which uses k-means‖ by default.",2. Background & related work,[0],[0]
"In this section, we provide the intuition and the main results behind our novel analysis of k-means‖ and defer the formal statements and the formal proofs to Section 4.",3. Intuition and key results,[0],[0]
Solution quality of Algorithm 2.,3.1. Solution quality of k-means‖,[0],[0]
We first consider Algorithm 2 as it largely determines the final solution quality.,3.1. Solution quality of k-means‖,[0],[0]
"Algorithm 3 with its use of k-means++ to obtain the final k cluster centers, only adds an additional O(log k) factor as shown in Theorem 1.",3.1. Solution quality of k-means‖,[0],[0]
"Our key result is Lemma 4 (see Section 4) which guarantees that, for ` ≥ k, the expected error of solutions computed by Algorithm 2 is at most
E[φX (C)] ≤ 2 ( k
e`
)t Var(X ) + 26φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
"(1)
The first term may be regarded as a scale-invariant additive error: It is additive as it does not depend on the optimal quantization error φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
It is scale-invariant since both the variance and the quantization error are scaled by λ2 if we scale the data setX by λ > 0.,3.1. Solution quality of k-means‖,[0],[0]
"The second term is a “traditional” multiplicative error term based on the optimal quantization error.
",3.1. Solution quality of k-means‖,[0],[0]
"Given a fixed oversampling factor `, the additive error term decreases exponentially if the number of rounds t is increased.",3.1. Solution quality of k-means‖,[0],[0]
"Similarly, for a fixed number of rounds t, it decreases polynomially at a rate O ( 1 `t )",3.1. Solution quality of k-means‖,[0],[0]
if the over sampling factor ` is increased.,3.1. Solution quality of k-means‖,[0],[0]
This result implies that even for a constant number of rounds one may obtain good clusterings by increasing the oversampling factor `.,3.1. Solution quality of k-means‖,[0],[0]
"This explains the empirical observation that often even a low number of rounds t is sufficient and that increasing ` increases the solution quality (Bahmani et al., 2012).",3.1. Solution quality of k-means‖,[0],[0]
The practical implications of this result are non-trivial: Even for the choice of t = 5 and ` = 5k one retains at most 0.0004% of the variance as an additive error.,3.1. Solution quality of k-means‖,[0],[0]
"Furthermore, state of the art uniform deviation bounds for k-Means include a similar additive error term (Bachem et al., 2017).
",3.1. Solution quality of k-means‖,[0],[0]
Comparison to previous result.,3.1. Solution quality of k-means‖,[0],[0]
Bahmani et al. (2012) show the following result: Let C be the set returned by Algorithm 2 with t rounds.,3.1. Solution quality of k-means‖,[0],[0]
For α = exp ( −(1− e−`/(2k)) ),3.1. Solution quality of k-means‖,[0],[0]
"≈ e− `2k , Corollary 3 of Bahmani et al. (2012) bounds the expected quality of C by
E[φX (C)] ≤",3.1. Solution quality of k-means‖,[0],[0]
"( 1 + α
2
)t ψ + 16
1− α φOPT(X ), (2)
where ψ denotes the quantization error of X based on the first, uniformly sampled center in k-means‖.",3.1. Solution quality of k-means‖,[0],[0]
"The key difference compared to our result is as follows: First, even as we increase `, the factor α is always non-negative.",3.1. Solution quality of k-means‖,[0],[0]
"Hence, regardless of the choice of `, the additive ψ term is reduced
by at most 12 per round.",3.1. Solution quality of k-means‖,[0],[0]
2,3.1. Solution quality of k-means‖,[0],[0]
"This means that, given the analysis in Bahmani et al. (2012), one would always obtain a constant additive error for a constant number of rounds t, even as ` is increased.
",3.1. Solution quality of k-means‖,[0],[0]
Guarantee for Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
Our main result — Theorem 1 — bounds the expected quality of solutions produced by Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
"As in Bahmani et al. (2012), one loses another factor ofO(ln k) compared to (1) due to Algorithm 3.",3.1. Solution quality of k-means‖,[0],[0]
Theorem 1.,3.1. Solution quality of k-means‖,[0],[0]
"Let k ∈ N, t ∈ N",3.1. Solution quality of k-means‖,[0],[0]
and ` ≥ k. Let X be a data set in Rd and C be the set returned by Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
"Then,
E[φX (C)] ≤",3.1. Solution quality of k-means‖,[0],[0]
"O
(( k
e`
)t ln k ) Var(X )+O(ln k)φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
We consider the case t < k−1 which captures the scenario where k-means‖ is useful in practice as for t ≥,3.2. A hard instance for k-means‖,[0],[0]
"k one may simply use k-means++ instead.
",3.2. A hard instance for k-means‖,[0],[0]
Theorem 2.,3.2. A hard instance for k-means‖,[0],[0]
"For any β > 0, k ∈ N, t < k",3.2. A hard instance for k-means‖,[0],[0]
"− 1 and ` ≥ 1, there exists a data set X of size 2(t+ 1) such that
E[φX (C)] ≥ 1
4 (4`t)
−t Var(X ),
where C is the output of Algorithm 2 or Algorithm 3 applied to X with t and `.",3.2. A hard instance for k-means‖,[0],[0]
"Furthermore,
Var(X )",3.2. A hard instance for k-means‖,[0],[0]
"> 0, φOPT(X ) = 0 and n∆2 ≤ β
where ∆ is the largest distance between any points in X .
",3.2. A hard instance for k-means‖,[0],[0]
Theorem 2 shows that there exists a data set on which k-means‖ provably incurs a non-negligible error even if the optimal quantization error is zero.,3.2. A hard instance for k-means‖,[0],[0]
This implies that k-means‖ with t < k,3.2. A hard instance for k-means‖,[0],[0]
− 1 cannot provide a multiplicative guarantee on the expected quantization error for general data sets.,3.2. A hard instance for k-means‖,[0],[0]
We thus argue that an additive error bound such as the one in Theorem 1 is required.,3.2. A hard instance for k-means‖,[0],[0]
"We note that the upper bound in (1) and the lower bound in Theorem 2 exhibit the same 1`t dependence on the oversampling factor ` for a given number of rounds t.
Furthermore, Theorem 2 implies that, for general data sets, k-means‖ cannot achieve the multiplicative error of O(log k) in expectation as claimed by Bahmani et al. (2012).3 In particular, if the optimal quantization error is
2Note that E[ψ] ≤ 2Var(X ) (Arthur & Vassilvitskii, 2007).",3.2. A hard instance for k-means‖,[0],[0]
"3To see this, let ψ = φX (c1) be the quantization error of the first sampled center in Algorithm 2 and choose β small enough such that the choice of t ∈ O(logψ) leads to t < k",3.2. A hard instance for k-means‖,[0],[0]
− 1.,3.2. A hard instance for k-means‖,[0],[0]
"For X in Theorem 2, φOPT(X ) = 0 which implies that the desired multiplicative guarantee would require E[φX (C)] = 0.",3.2. A hard instance for k-means‖,[0],[0]
"However, the non-negligible, additive error in Theorem 2 and Var(X )",3.2. A hard instance for k-means‖,[0],[0]
> 0 implies that E[φX (C)],3.2. A hard instance for k-means‖,[0],[0]
"> 0.
zero, then k-means‖ would need to return a solution with quantization error zero.",3.2. A hard instance for k-means‖,[0],[0]
"While we are guaranteed to remove a constant fraction of the error in each round, the number of required iterations may be unbounded.",3.2. A hard instance for k-means‖,[0],[0]
Proof of Theorem 1.,4. Theoretical analysis,[0],[0]
"The proof is divided into four steps: First, we relate k-means‖-style oversampling to k-means++-style D2-sampling in Lemmas 1 and 2.",4. Theoretical analysis,[0],[0]
"Second, we analyze a single iteration of Algorithm 2 in Lemma 3.",4. Theoretical analysis,[0],[0]
"Third, we bound the expected solution quality of Algorithm 2 in Lemma 4.",4. Theoretical analysis,[0],[0]
"Finally, we use this to bound the expected solution quality of Algorithm 3 in Theorem 1.
Lemma 1.",4. Theoretical analysis,[0],[0]
Let A be a finite set and let,4. Theoretical analysis,[0],[0]
"f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V )",4. Theoretical analysis,[0],[0]
"≥ f(U) ≥ 0, for all V ⊆ U .
",4. Theoretical analysis,[0],[0]
"Let P be a probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa ∈",4. Theoretical analysis,[0],[0]
"[0, 1].",4. Theoretical analysis,[0],[0]
"Let C be the set of elements a ∈ A for which the event Ea occurs.
",4. Theoretical analysis,[0],[0]
Let Q be the probability distribution on A where,4. Theoretical analysis,[0],[0]
"a single a ∈ A is sampled with probability qa/ ∑ a∈A qa.
",4. Theoretical analysis,[0],[0]
"Then, with ∅ denoting the empty set, we have that
EP [f(C)] ≤ EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
"To prove the claim, we first construct a series of sub-events of the events {Ea}a∈A and then use them to recursively bound EP [f(C)].
",4. Theoretical analysis,[0],[0]
Let m ∈ N.,4. Theoretical analysis,[0],[0]
"For each a ∈ A, let ia be an independent random variable drawn uniformly at random from {1, 2, . . .",4. Theoretical analysis,[0],[0]
",m}.",4. Theoretical analysis,[0],[0]
"For each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m, let Fai be an independent event that occurs with probability
P[Fai] = (
1− qa m
)i−1 .
",4. Theoretical analysis,[0],[0]
"For each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m, denote by Eai the event that occurs if i = ia and both Ea and Fai occur.",4. Theoretical analysis,[0],[0]
"By design, all these events are independent and thus
P[Eai] = P[Ea]P[Fai]P[ia = i] =",4. Theoretical analysis,[0],[0]
"qa m
( 1− qa
m
)i−1 (3)
for each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m. Furthermore, for any a, a′ ∈ A with a 6= a′ and any i, i′ ∈ {1, 2, . . .",4. Theoretical analysis,[0],[0]
",m}, the events Eai and Ea′i′ are independent.
",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m let Gi be the event that none of the events {Eai′}a∈A,i′≤i occur, i.e.,
Gi =",4. Theoretical analysis,[0],[0]
⋂ i′≤i ⋂ a∈A,4. Theoretical analysis,[0],[0]
"Eai′
where A denotes the complement of A. For convenience, let G0 be the event that occurs with probability one.
",4. Theoretical analysis,[0],[0]
"Let (a1, a2, . . .",4. Theoretical analysis,[0],[0]
", a|A|) be any enumeration of A.",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m and j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A|+1, define the event
Gi,j = Gi−1 ∩ ⋂
0<j′<j
Eaj′ i.
We note that by definitionGi,1 = Gi−1 andGi,|A|+1 = Gi for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m.
For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m and j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A|, we have E[f(C)|Gi,j ] =",4. Theoretical analysis,[0],[0]
"P [ Eaji | Gi,j ] E",4. Theoretical analysis,[0],[0]
[ f(C),4. Theoretical analysis,[0],[0]
"| Eaji ∩Gij ] + P [ Eaj′ i | Gij ] E[f(C) | Gi,j+1].
(4)
We now bound the individual terms.",4. Theoretical analysis,[0],[0]
"The event Gi,j implies that the events {Eaji′}i′<i did not occur.",4. Theoretical analysis,[0],[0]
"Furthermore, Eaji is independent of the events {Eaj′ i′}i′=1,2,...,m for j′ 6=",4. Theoretical analysis,[0],[0]
"j. Hence, we have
P",4. Theoretical analysis,[0],[0]
"[ Eaji | Gi,j ] = P [ Eaji | G0 ∩ ⋂ i′<i Eaji′ ]
= P",4. Theoretical analysis,[0],[0]
"[ Eaji ] P [ G0 ∩ ⋂ i′<iEaji′
] = P [ Eaji ] 1− P",4. Theoretical analysis,[0],[0]
"[⋃ i′<iEaji′
] = P [ Eaji ] 1− ∑ i′<i P [ Eaji′ ] ,
(5)
where the last equality follows since the events {Eaji′}i′<i are disjoint.",4. Theoretical analysis,[0],[0]
"Using (3), we observe that ∑ i′<i P [ Eaji′ ] is a sum of a finite geometric series and we have∑ i′<i P [ Eaji′ ] = ∑ i′<i qa m ( 1− qa m
)i′−1 =",4. Theoretical analysis,[0],[0]
qa m 1− ( 1− qam ),4. Theoretical analysis,[0],[0]
i−1 1−,4. Theoretical analysis,[0],[0]
"( 1− qam
)",4. Theoretical analysis,[0],[0]
"= 1− ( 1− qa
m
)",4. Theoretical analysis,[0],[0]
"i−1 .
",4. Theoretical analysis,[0],[0]
"Together with (3) and (5), this implies
P",4. Theoretical analysis,[0],[0]
"[ Eaji | Gi,j ] = qa m
( 1− qam )",4. Theoretical analysis,[0],[0]
"i−1( 1− qam
)i−1 = qam.",4. Theoretical analysis,[0],[0]
(6) The event Eaji implies that C contains aj .,4. Theoretical analysis,[0],[0]
"Hence, since f is monotonically decreasing, we have
E [ f(C) | Eaji ∩Gij ] ≤ f({aj}).
",4. Theoretical analysis,[0],[0]
"Using (4) and (6), this implies
E[f(C)|Gi,j ] ≤ qaj m f({aj})+
( 1−
qaj m
) E[f(C)",4. Theoretical analysis,[0],[0]
"| Gi,j+1].
",4. Theoretical analysis,[0],[0]
"Applying this result iteratively for j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A| implies E[f(C)|Gi,1] = |A|∑ j=1 qaj m ∏",4. Theoretical analysis,[0],[0]
j′<j ( 1− qaj′ m ) f({aj}),4. Theoretical analysis,[0],[0]
"+
 |A|∏ j=1 ( 1− qaj m )E[f(C) | Gi,|A|+1].",4. Theoretical analysis,[0],[0]
Note that 0 ≤ 1,4. Theoretical analysis,[0],[0]
− qam ≤ 1 for all a ∈ A and that f is non-negative.,4. Theoretical analysis,[0],[0]
"This implies that for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m
E[f(C)|Gi,1] ≤ ∑ a∈A qa m f({a}) + c · E [ f(C)|Gi,|A|+1 ] where
c = ∏ a∈A",4. Theoretical analysis,[0],[0]
"( 1− qa m ) .
",4. Theoretical analysis,[0],[0]
"Since Gi,1 = Gi−1 and Gi,|A|+1 = Gi, we have for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m
E[f(C)|Gi−1] ≤ ∑ a∈A qa m f({a}) + c ·",4. Theoretical analysis,[0],[0]
"E[f(C)|Gi].
",4. Theoretical analysis,[0],[0]
"Applying this result iteratively, we obtain
E[f(C)] ≤",4. Theoretical analysis,[0],[0]
( m∑ i=1,4. Theoretical analysis,[0],[0]
"ci−1 )∑ a∈A qa m f({a}) + cm · f(∅).
",4. Theoretical analysis,[0],[0]
"Since 0 ≤ c ≤ 1, we have m∑ i=1",4. Theoretical analysis,[0],[0]
ci−1 ≤ ∞∑ i=1,4. Theoretical analysis,[0],[0]
"ci−1 = 1 1− c .
",4. Theoretical analysis,[0],[0]
For x ∈,4. Theoretical analysis,[0],[0]
"[−1, 0]",4. Theoretical analysis,[0],[0]
"it holds that log(1 + x) ≤ x and hence
cm = ∏ a∈A ( 1− qa m )m = exp ( m ∑ a∈A log ( 1− qa m ))
≤ exp ( −m
∑ a∈A qa m
) = e− ∑ a∈A qa .
",4. Theoretical analysis,[0],[0]
"This implies that
E[f(C)]",4. Theoretical analysis,[0],[0]
≤ 1 1− c ∑ a∈A,4. Theoretical analysis,[0],[0]
qa m f({a})+e− ∑ a∈A qa ·f(∅).,4. Theoretical analysis,[0],[0]
"(7)
We show the main claim by contradiction.",4. Theoretical analysis,[0],[0]
Assume,4. Theoretical analysis,[0],[0]
"that
EP [f(C)]",4. Theoretical analysis,[0],[0]
> EQ[f({a})],4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
",4. Theoretical analysis,[0],[0]
"If EQ[f({a})] = 0, the contradiction follows directly from (7).",4. Theoretical analysis,[0],[0]
"Otherwise, EQ[f({a})]",4. Theoretical analysis,[0],[0]
"> 0 implies that there exists an > 0 such that
EP [f(C)]",4. Theoretical analysis,[0],[0]
> (1 + )EQ[f({a})],4. Theoretical analysis,[0],[0]
+ e− ∑ a∈A qaf(∅).,4. Theoretical analysis,[0],[0]
"(8)
By definition, we have
c = ∏ a∈A",4. Theoretical analysis,[0],[0]
( 1− qa m ),4. Theoretical analysis,[0],[0]
= 1− ∑ a∈A,4. Theoretical analysis,[0],[0]
qa m + o,4. Theoretical analysis,[0],[0]
"( 1 m ) .
",4. Theoretical analysis,[0],[0]
"Thus, there exists a m ∈ N sufficiently large such that
c = 1− ∑ a∈A qa m + o",4. Theoretical analysis,[0],[0]
( 1 m ) ≤,4. Theoretical analysis,[0],[0]
"1− 1 1 + ∑ a∈A qa m .
",4. Theoretical analysis,[0],[0]
"Together with (7), this implies
E[f(C)]",4. Theoretical analysis,[0],[0]
≤ 1 + ∑ a∈A qa m ∑ a∈A qa m f({a}),4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qa · f(∅)
= (1 + )",4. Theoretical analysis,[0],[0]
EQ[f({a})],4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
which is a contradiction to (8) and thus proves the claim.
",4. Theoretical analysis,[0],[0]
"Lemma 2 extends Lemma 1 to k-means‖-style sampling probabilities of the form qa = min (1, `pa).
",4. Theoretical analysis,[0],[0]
Lemma 2.,4. Theoretical analysis,[0],[0]
Let ` ≥ 1.,4. Theoretical analysis,[0],[0]
LetA be a finite set and let,4. Theoretical analysis,[0],[0]
"f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V )",4. Theoretical analysis,[0],[0]
"≥ f(U) ≥ 0, for all V ⊆ U .",4. Theoretical analysis,[0],[0]
"For each a ∈ A, let pa ≥ 0",4. Theoretical analysis,[0],[0]
"and ∑ a∈A pa ≤ 1.
",4. Theoretical analysis,[0],[0]
"Let P be the probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa = min (1, `pa).",4. Theoretical analysis,[0],[0]
"Let C be the set of elements a ∈ A for which the event Ea occurs.
",4. Theoretical analysis,[0],[0]
Let Q be the probability distribution on A where,4. Theoretical analysis,[0],[0]
"a single a ∈ A is sampled with probability pa/ ∑ a∈A pa.
",4. Theoretical analysis,[0],[0]
"Then, with ∅ denoting the empty set, we have that
EP [f(C)] ≤ 2EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e−` ∑ a∈A paf(∅).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
LetA1 be the set of elements a ∈,4. Theoretical analysis,[0],[0]
A such that `pa ≤ 1 and A2 the set of elements a ∈,4. Theoretical analysis,[0],[0]
A such that `pa > 1.,4. Theoretical analysis,[0],[0]
"By definition, every element in A2 is sampled almost surely, i.e., A2 ⊆ C.",4. Theoretical analysis,[0],[0]
"This implies that almost surely
f(C) = f (A2 ∪ (C ∩A1)) .",4. Theoretical analysis,[0],[0]
"(9)
If |A1|= 0, the result follows trivially since
EP [f(C)] = f(A2) = EQ[f({a})].
",4. Theoretical analysis,[0],[0]
"Similarly, if |A2|= 0, the result follows directly from Lemma 1 with qa = `pa.",4. Theoretical analysis,[0],[0]
"For the remainder of the proof, we may thus assume that both A1 and A2 are non-empty.
",4. Theoretical analysis,[0],[0]
"For a ∈ A1, let qa = `pa and define the non-negative and monotonically decreasing function
g(C) = f (A2 ∪ C) .
",4. Theoretical analysis,[0],[0]
Let p1 = ∑ a∈A1 pa and p2 = ∑ a∈A2 pa.,4. Theoretical analysis,[0],[0]
"Lemma 1 applied to A1, qa and g implies that
EP [f(C)]",4. Theoretical analysis,[0],[0]
"= E[g(C)] ≤ ∑ a∈A1 pa p1 g({a}) + e−`p1g(∅).
",4. Theoretical analysis,[0],[0]
"(10)
Let d = ( 1− e−`p2 ) e−`p1
and define α =
p2 p1 + p2 − p1 p1 + p2 d.
By design, α ≤ 1.",4. Theoretical analysis,[0],[0]
"Furthermore
`p1 ≥ log `p1.
SinceA2 is nonempty and pa ≥ 1` for all a ∈ A2, it follows that p2 ≥ 1` .",4. Theoretical analysis,[0],[0]
"This implies
e`p1 ≥ `p1 ≥ p1 p2 .
",4. Theoretical analysis,[0],[0]
"Since 0 ≤ ( 1− e−`p2 ) ≤ 1, we have
p2 ≥ p1e−`p1 ≥",4. Theoretical analysis,[0],[0]
"p1 ( 1− e−`p2 ) e−`p1 = p1d.
",4. Theoretical analysis,[0],[0]
"Hence,
α = p2 p1 + p2 − p1",4. Theoretical analysis,[0],[0]
"p1 + p2
( 1− e−`p2 ) e−`p1 ≥ 0.
",4. Theoretical analysis,[0],[0]
Since α ∈,4. Theoretical analysis,[0],[0]
"[0, 1] and g({a}) ≤ g(∅) for any a ∈ A1, we may write (10), i.e., EP [f(C)] ≤ (1− α) ∑ a∈A1 pa p1 g({a})",4. Theoretical analysis,[0],[0]
+ ( α+ e−`p1 ),4. Theoretical analysis,[0],[0]
"g(∅).
(11)
",4. Theoretical analysis,[0],[0]
"By definition, we have
1− α = 1− p2 p1 + p2 + p1 p1 + p2 d = p1 p1 + p2 (1 + d).
",4. Theoretical analysis,[0],[0]
"Since g({a}) ≤ f({a}), we thus have
(1− α) ∑ a∈A1 pa p1 g({a}) ≤ (1 + d) ∑ a∈A1 pa p1 + p2 f({a}).
",4. Theoretical analysis,[0],[0]
"(12)
Similarly, we have
α+ e−`p1 = p2 p1 + p2 − p1 p1 + p2 d+ e−`p1
= p2
p1 + p2 + d p2 p1 + p2 − d+ e−`p1
= (1 + d)",4. Theoretical analysis,[0],[0]
"p2
p1 + p2 + e−`(p1+p2).
",4. Theoretical analysis,[0],[0]
"Since g(∅) ≤ f(∅), it follows that( α+ e−`p1 ) g(∅) ≤ (1+d) p2
p1 + p2 g(∅)+e−`(p1+p2)f(∅).
",4. Theoretical analysis,[0],[0]
(13) Since g(∅) = f (A2) and thus g(∅) ≤,4. Theoretical analysis,[0],[0]
"f({a}) for all a ∈ A2, we have
p2g(∅) = ∑ a∈A2 pag(∅) ≤ ∑",4. Theoretical analysis,[0],[0]
a∈A2 paf({a}).,4. Theoretical analysis,[0],[0]
"(14)
Combining (11), (12), (13), and (14) leads to
EP [f(C)] ≤ (1 + d)EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e−` ∑ a∈A paf(∅).
",4. Theoretical analysis,[0],[0]
"Since p1 ≥ 0, we have 1 + d = 1 + ( 1− e−`p2 ) e−`p1 ≤ 2
which proves the main claim.
",4. Theoretical analysis,[0],[0]
"Lemma 3 bounds the solution quality after each iteration of Algorithm 2 based on the solution before the iteration.
",4. Theoretical analysis,[0],[0]
Lemma 3.,4. Theoretical analysis,[0],[0]
Let k ∈,4. Theoretical analysis,[0],[0]
N,4. Theoretical analysis,[0],[0]
and ` ≥ 1.,4. Theoretical analysis,[0],[0]
Let X be a data set in Rd and denote by φOPT(X ),4. Theoretical analysis,[0],[0]
the optimal k-Means clustering cost.,4. Theoretical analysis,[0],[0]
LetC denote the set of cluster centers at the beginning of an iteration in Algorithm 2 and C ′,4. Theoretical analysis,[0],[0]
the random set added in the iteration.,4. Theoretical analysis,[0],[0]
"Then, it holds that
E[φX (C ∪ C ′)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
) φX (C) + 16φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
The proof relies on applying Lemma 2 to each cluster of the optimal solution.,4. Theoretical analysis,[0],[0]
Let OPT denote any clustering achieving the minimal cost φOPT(X ) on X .,4. Theoretical analysis,[0],[0]
We assign all the points x ∈ X to their closest cluster center in OPT with ties broken arbitrarily but consistently.,4. Theoretical analysis,[0],[0]
"For c ∈ OPT we denote by Xc the subset of X assigned to c. For each c ∈ OPT, let
C ′c = C ′ ∩",4. Theoretical analysis,[0],[0]
"Xc.
",4. Theoretical analysis,[0],[0]
"By definition, a ∈ Xc is included in C ′c with probability
qa = min
( 1,
`d(a,C)2∑ a′∈X d(a ′, C)2
) .
",4. Theoretical analysis,[0],[0]
"For each c ∈ OPT, we define the monotonically decreasing function fc : 2Xc → R≥0 to be
fc(C ′",4. Theoretical analysis,[0],[0]
"c) = φXc(C ∪ C ′c).
",4. Theoretical analysis,[0],[0]
"For each c ∈ OPT, Lemma 2 applied to Xc, C ′c and fc implies
E[fc(C ′c)] ≤2 ∑ a∈Xc d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a})
+ e",4. Theoretical analysis,[0],[0]
"−`
∑ a∈Xc d(a,C)
2∑ a′∈X d(a
′,C)2 fc(∅).
",4. Theoretical analysis,[0],[0]
"(15)
Since fc({a}) = φXc(C ∪ {a}), the first term is equivalent to sampling a single element from Xc using D2 sampling.",4. Theoretical analysis,[0],[0]
"Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we have for all c ∈ OPT∑
a∈Xc
d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a}).",4. Theoretical analysis,[0],[0]
≤ 8φOPT(Xc).,4. Theoretical analysis,[0],[0]
"(16)
For each c ∈ OPT, we further have
e −`
∑ a∈Xc d(a,C)
",4. Theoretical analysis,[0],[0]
"2∑ a′∈X d(a
′,C)2 fc(∅) =",4. Theoretical analysis,[0],[0]
"e−`ucucφX (C).
",4. Theoretical analysis,[0],[0]
"where
uc =
∑ a∈Xc d(a,C)
2∑ a′∈X d(a ′, C)2 = φXc(C) φX (C) .
",4. Theoretical analysis,[0],[0]
"We have that
log `uc ≤",4. Theoretical analysis,[0],[0]
"`uc − 1 ⇐⇒ `uc ≤ e`uc
e
which implies
e−`ucucφX (C) ≤ 1
e` φX (C).",4. Theoretical analysis,[0],[0]
"(17)
Combining (15), (16) and (17), we obtain
E[fc(C ′c)] ≤16φOPT(Xc)",4. Theoretical analysis,[0],[0]
"+ 1
e` φX (C).",4. Theoretical analysis,[0],[0]
"(18)
",4. Theoretical analysis,[0],[0]
"Since E[φX (C ∪ C ′)] ≤ ∑ c∈OPT E[fc(C ′c)]
and φX (OPT) = ∑ c∈OPT φXc(OPT),
we thus have E[φX (C ∪ C ′)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
) φX (C) + 16φOPT(X )
which concludes the proof.
",4. Theoretical analysis,[0],[0]
"An iterated application of Lemma 3 allows us to bound the solution quality of Algorithm 2 in Lemma 4.
",4. Theoretical analysis,[0],[0]
Lemma 4.,4. Theoretical analysis,[0],[0]
"Let k ∈ N, t ∈ N",4. Theoretical analysis,[0],[0]
and ` ≥ k. Let X be a data set in Rd and C be the random set returned by Algorithm 2.,4. Theoretical analysis,[0],[0]
"Then,
E[φX (C)] ≤ 2 ( k
e`
)t Var(X ) + 26φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
The algorithm starts with a uniformly sampled initial cluster center c1.,4. Theoretical analysis,[0],[0]
"We iteratively apply Lemma 3 for each of the t rounds to obtain
E[φX (C)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
)t E[φX ({c1})]",4. Theoretical analysis,[0],[0]
"+ 16stφOPT(X ) (19)
",4. Theoretical analysis,[0],[0]
"where
st = t∑ i=1",4. Theoretical analysis,[0],[0]
"( k e` )i−1 .
",4. Theoretical analysis,[0],[0]
"For ` ≥ k, we have 0 ≤ ke` ≤ 1/e and hence
st ≤",4. Theoretical analysis,[0],[0]
t∑ i=1,4. Theoretical analysis,[0],[0]
1,4. Theoretical analysis,[0],[0]
"ei−1 ≤ ∞∑ i=0 1 ei =
1
1− 1/e .",4. Theoretical analysis,[0],[0]
"(20)
By Lemma 3.2 of Arthur & Vassilvitskii (2007), we have that E[φX ({c1})] ≤ 2 Var(X ).",4. Theoretical analysis,[0],[0]
"Together with (19), (20) and 16/(1− 1/e) ≈ 25.31 < 26, this implies the required result.
",4. Theoretical analysis,[0],[0]
"With Lemma 4, we are further able to bound the solution quality of Algorithm 3 and prove Theorem 1.
",4. Theoretical analysis,[0],[0]
Proof of Theorem 1.,4. Theoretical analysis,[0],[0]
Let B be the set returned by Algorithm 2.,4. Theoretical analysis,[0],[0]
"For any x ∈ X , let bx denote its closest point in B with ties broken arbitrarily.",4. Theoretical analysis,[0],[0]
"By the triangle inequality and since (|a|+|b|)2 ≤ 2a2 + 2b2, for any x ∈ X
d(x,C)2 ≤ 2 d(x, bx)2 + 2 d(bx, C)2
and hence
E[φX (C)]",4. Theoretical analysis,[0],[0]
"= ∑ x∈X d(x,C)2
≤ 2 ∑ x∈X d(x, bx) 2 + 2 ∑ x∈X d(bx, C) 2
= 2φX (B) + 2 ∑ x∈B wx d(x,C) 2.
(21)
",4. Theoretical analysis,[0],[0]
"Let OPTX be the optimal k-Means clustering solution on X and OPT(B,w) the optimal solution on the weighted set (B,w).",4. Theoretical analysis,[0],[0]
"By Theorem 1.1 of Arthur & Vassilvitskii (2007),
k-means++ produces an α = 8(log2 k + 2) approximation to the optimal solution.",4. Theoretical analysis,[0],[0]
"This implies that∑
x∈B wx d(x,C) 2 ≤ α ∑ x∈B wx d",4. Theoretical analysis,[0],[0]
"( x,OPT(B,w) )",4. Theoretical analysis,[0],[0]
"2
≤ α ∑ x∈B wx d(x,OPTX ) 2
= α ∑ x∈X d(bx,OPTX ) 2.
(22)
",4. Theoretical analysis,[0],[0]
"By the triangle inequality and since (|a|+|b|)2 ≤ 2a2+2b2, it holds for any x ∈ X that
d(bx,OPTX ) 2 ≤ 2 d(x, bx)2 + 2 d(x,OPTX )2
and hence∑",4. Theoretical analysis,[0],[0]
x∈X,4. Theoretical analysis,[0],[0]
"d(bx,OPTX ) 2 ≤",4. Theoretical analysis,[0],[0]
2φX (B) + 2φOPT(X ).,4. Theoretical analysis,[0],[0]
"(23)
Combining (21), (22) and (23), we obtain
E[φX (C)] ≤ 2(1 + α)φX (B) + 2αφOPT(X ).
",4. Theoretical analysis,[0],[0]
"Finally, by Lemma 4, we have E[φX (C)]",4. Theoretical analysis,[0],[0]
"≤(32 log2 k + 68) ( k
e`
)t Var(X )
+ (432 log2 k + 916)φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof of Theorem 2.,4. Theoretical analysis,[0],[0]
"For this proof, we explicitly construct a data set: Let β′ > 0 and consider points in onedimensional Euclidean space.",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t, set
xi =
√ β′(4`t) 1−i",4. Theoretical analysis,[0],[0]
"− β′(4`t)−i
as well as xt+1 = √ β′(4`t) −t .
",4. Theoretical analysis,[0],[0]
"Let the data setX consist of the t+1 points {xi}t=1,2,...,t+1 as well as t + 1 points at the origin.",4. Theoretical analysis,[0],[0]
Since t < k,4. Theoretical analysis,[0],[0]
"− 1, the optimal k-Means clustering solution consists of t + 2 points placed at each of the {xi}i=1,2,...t+1 and at 0.",4. Theoretical analysis,[0],[0]
"By design, this solution has a quantization error of zero and the variance is nonzero, i.e., φOPT(X )",4. Theoretical analysis,[0],[0]
= 0 and Var(X ),4. Theoretical analysis,[0],[0]
"> 0 as claimed.
",4. Theoretical analysis,[0],[0]
Choose β′ = β2(t+1) .,4. Theoretical analysis,[0],[0]
"The maximal distance ∆ between any two points in X is bounded by ∆ = d(0, x1)2 ≤ β′.",4. Theoretical analysis,[0],[0]
"Since n = 2(t+ 1), this implies ψ ≤",4. Theoretical analysis,[0],[0]
n∆2 ≤,4. Theoretical analysis,[0],[0]
"β as claimed.
",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t, let Ci consist of 0 and all xj with j < i.",4. Theoretical analysis,[0],[0]
"By design, we have d(0, Ci)2 = 0 as well as d(xj , Ci)2 = 0 for j < i.",4. Theoretical analysis,[0],[0]
For j ≥,4. Theoretical analysis,[0],[0]
"i, we have d(xj , Ci)2 = d(xj , 0)2.",4. Theoretical analysis,[0],[0]
"For any i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t+ 1, we thus have∑
j≥i
d(xj , 0) 2 = β′(4`t) 1−i .
",4. Theoretical analysis,[0],[0]
Consider a single iteration of Algorithm 2 where C = Ci.,4. Theoretical analysis,[0],[0]
"In this case, all points in Xj with j <",4. Theoretical analysis,[0],[0]
i are added to C ′ with probability zero and for j >,4. Theoretical analysis,[0],[0]
i each point xj is added to C ′,4. Theoretical analysis,[0],[0]
"with probability
min ( 1, `d(xj , 0) 2∑
j′≥i d(xj′ , 0) 2
) =",4. Theoretical analysis,[0],[0]
"`d(xj , 0) 2
β′(4`t) 1−i .
",4. Theoretical analysis,[0],[0]
"By the union bound, the probability that any of the points in ⋃ j>i{xj} are sampled is bounded by∑
j>i
`d(xj , 0) 2 β′(4`t) 1−i = 1 4t .
",4. Theoretical analysis,[0],[0]
"The point xi is not sampled with probability at most
1−min ( 1, ` d(xi, 0) 2∑
j′≥i d(xj′ , 0) 2
) = 1−min ( 1, `− 1
4t ) ≤ 1
4t .
",4. Theoretical analysis,[0],[0]
"By the union bound, a single iteration of Algorithm 2 with C = Ci hence samples exactly the set C ′ = {xi} with probability at least ( 1− 12t ) .
",4. Theoretical analysis,[0],[0]
"In Algorithm 2, the first center is sampled uniformly at random from X .",4. Theoretical analysis,[0],[0]
"Since half of the elements in X are placed at 0, with probability at least 12 , the first center is at 0 or equivalently C = C1.",4. Theoretical analysis,[0],[0]
"With probability ( 1− 12t
)t ≥ 12 , we then sample exactly the points x1, x2, . . .",4. Theoretical analysis,[0],[0]
", xt in the t subsequent iterations.",4. Theoretical analysis,[0],[0]
"Hence, with probability at least 14 , the solution produced by Algorithm 2 consists of 0 and all xi except xt+1.",4. Theoretical analysis,[0],[0]
"Since xt+1 is closest to 0, this implies
E[φX (C)]",4. Theoretical analysis,[0],[0]
"≥ 1
4 d(xt+1, 0)
2 = 1
4 β′(4`t) −t .",4. Theoretical analysis,[0],[0]
"(24)
The variance of X is bounded by a single point at 0, i.e., Var(X ) ≤ φX ({0}) =",4. Theoretical analysis,[0],[0]
"∑ j≥1 d(xj , 0) 2 = β′.
Together with (24), we have that
E[φX (C)]",4. Theoretical analysis,[0],[0]
"≥ 1
4 (4`t)
−t Var(X ).
",4. Theoretical analysis,[0],[0]
The same result extends to the output of Algorithm 3 as it always picks a subset of the output of Algorithm 2.,4. Theoretical analysis,[0],[0]
"This research was partially supported by SNSF NRP 75, ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship.",Acknowledgements,[0],[0]
This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.,Acknowledgements,[0],[0]
The k-means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation.,abstractText,[0],[0]
"However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets.",abstractText,[0],[0]
The standard remedy is to use the k-means‖ algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting.,abstractText,[0],[0]
"In this paper, we provide a novel analysis of the k-means‖ algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice.",abstractText,[0],[0]
"In particular, we show that k-means‖ provides provably good clusterings even for a small, constant number of iterations.",abstractText,[0],[0]
This theoretical finding explains the common observation that k-means‖ performs extremely well in practice even if the number of rounds is low.,abstractText,[0],[0]
We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k−1 rounds are employed.,abstractText,[0],[0]
Distributed and Provably Good Seedings for k-Means in Constant Rounds,title,[0],[0]
"Bayesian optimization (BO) has recently gained considerable traction due to its capability of finding the global maximum of a highly complex (e.g., non-convex, no closedform expression nor derivative), noisy black-box objective
1Ludwig-Maximilians-Universität, Munich, Germany.",1. Introduction,[0],[0]
A substantial part of this research was performed during his student exchange program at the National University of Singapore under the supervision of Bryan Kian Hsiang Low and culminated in his Bachelor’s thesis.,1. Introduction,[0],[0]
"2Department of Computer Science, National University of Singapore, Republic of Singapore.",1. Introduction,[0],[0]
"Correspondence to: Bryan Kian Hsiang Low <lowkh@comp.nus.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"function with a limited budget of (often costly) function evaluations, consequently witnessing its use in an increasing diversity of application domains such as robotics, environmental sensing/monitoring, automatic machine learning, among others (Brochu et al., 2010; Shahriari et al., 2016).",1. Introduction,[0],[0]
"A number of acquisition functions (e.g., probability of improvement or expected improvement (EI) over the currently found maximum (Brochu et al., 2010), entropybased (Villemonteix et al., 2009; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), and upper confidence bound (UCB) (Srinivas et al., 2010)) have been devised to perform BO: They repeatedly select an input for evaluating/querying the black-box function (i.e., until the budget is depleted) that intuitively trades off between sampling where the maximum is likely to be given the current, possibly imprecise belief of the function modeled by a Gaussian process (GP) (i.e., exploitation) vs. improving the GP belief of the function over the entire input domain (i.e., exploration) to guarantee finding the global maximum.
",1. Introduction,[0],[0]
"The rapidly growing affordability and availability of hardware resources (e.g., computer clusters, sensor networks, robot teams/swarms) have motivated the recent development of BO algorithms that can repeatedly select a batch of inputs for querying the black-box function in parallel instead.",1. Introduction,[0],[0]
"Such batch/parallel BO algorithms can be classified into two types: On one extreme, batch BO algorithms like multi-points EI (q-EI) (Chevalier & Ginsbourger, 2013), parallel predictive entropy search (PPES)",1. Introduction,[0],[0]
"(Shah & Ghahramani, 2015), and the parallel knowledge gradient method (q-KG) (Wu & Frazier, 2016) jointly optimize the batch of inputs and hence scale poorly in the batch size.",1. Introduction,[0],[0]
"On the other extreme, greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016) boost the scalability by selecting the inputs of the batch one at a time.",1. Introduction,[0],[0]
"We argue that such a highly suboptimal approach to gain scalability is an overkill: In practice, each function evaluation is often much more computationally and/or economically costly (e.g., hyperparameter tuning for deep learning, drug testing on human subjects), which justifies dedicating more time to obtain better BO performance.",1. Introduction,[0],[0]
"In this paper, we show that it is in fact possible to jointly optimize the batch of inputs and still preserve scalability in the batch size by giving practitioners the flexibility to trade off BO performance for time efficiency.
",1. Introduction,[0],[0]
"To achieve this, we first observe that, interestingly, batch BO can be perceived as a cooperative multi-agent decision making problem whereby each agent optimizes a separate input of the batch while coordinating with the other agents doing likewise.",1. Introduction,[0],[0]
"To the best of our knowledge, this has not been considered in the BO literature.",1. Introduction,[0],[0]
"In particular, if batch BO can be framed as some known class of multi-agent decision making problems, then it can be solved efficiently and scalably by the latter’s state-of-the-art solvers.",1. Introduction,[0],[0]
"The key technical challenge would therefore be to investigate how batch BO can be cast as one of such to exploit its advantage of scalability in the number of agents (hence, batch size) while at the same time theoretically guaranteeing the resulting BO performance.
",1. Introduction,[0],[0]
"To tackle the above challenge, this paper presents a novel distributed batch BO algorithm (Section 3) that, in contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016), can jointly optimize a batch of inputs and, unlike the batch BO algorithms (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size.",1. Introduction,[0],[0]
"To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem (DCOP) in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.",1. Introduction,[0],[0]
Our proposed distributed batch GPUCB (DB-GP-UCB) algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,1. Introduction,[0],[0]
We provide a theoretical guarantee for the convergence rate of our DBGP-UCB algorithm via bounds on its cumulative regret.,1. Introduction,[0],[0]
We empirically evaluate the cumulative regret incurred by our DB-GP-UCB algorithm and its scalability in the batch size on synthetic benchmark objective functions and a realworld optimization problem (Section 4).,1. Introduction,[0],[0]
Consider the problem of sequentially optimizing an unknown objective function f : D → R where D ⊂ Rd denotes a domain of d-dimensional input feature vectors.,"2. Problem Statement, Background, and Notations",[0],[0]
"We consider the domain to be discrete as it is known how to generalize results to a continuous, compact domain via suitable discretizations (Srinivas et al., 2010).","2. Problem Statement, Background, and Notations",[0],[0]
"In each iteration t = 1, . . .","2. Problem Statement, Background, and Notations",[0],[0]
", T , a batch Dt ⊂ D of inputs is selected for evaluating/querying f to yield a corresponding column vector yDt , (yx) >","2. Problem Statement, Background, and Notations",[0],[0]
"x∈Dt of noisy observed outputs yx , f(x)+ with i.i.d.","2. Problem Statement, Background, and Notations",[0],[0]
"Gaussian noise ∼ N (0, σ2n) and noise variance σ2n.
Regret.","2. Problem Statement, Background, and Notations",[0],[0]
"Supposing our goal is to get close to the global maximum f(x∗) as rapidly as possible where x∗ , arg maxx∈D f(x), this can be achieved by minimizing a standard batch BO objective such as the batch or full cumulative regret (Contal et al., 2013; Desautels et al., 2014):","2. Problem Statement, Background, and Notations",[0],[0]
The notion of regret intuitively refers to a loss in reward from not knowing x∗ beforehand.,"2. Problem Statement, Background, and Notations",[0],[0]
"Formally, the instantaneous regret incurred by selecting a single input x to evaluate its corresponding f is defined as rx , f(x∗)− f(x).","2. Problem Statement, Background, and Notations",[0],[0]
"Assuming a fixed cost of evaluating f for every possible batch Dt of the same size, the batch and full cumulative regrets are, respectively, defined as sums (over iteration t = 1, . .","2. Problem Statement, Background, and Notations",[0],[0]
.,"2. Problem Statement, Background, and Notations",[0],[0]
", T ) of the smallest instantaneous regret incurred by any input within every batch","2. Problem Statement, Background, and Notations",[0],[0]
"Dt, i.e., RT , ∑T t=1 minx∈Dt rx, and of the instantaneous regrets incurred by all inputs of every batch","2. Problem Statement, Background, and Notations",[0],[0]
"Dt, i.e., R′T ,∑T t=1 ∑ x∈Dt rx.","2. Problem Statement, Background, and Notations",[0],[0]
The convergence rate of a batch BO algorithm can then be assessed based on some upper bound on the average regret RT /T or R′T /T,"2. Problem Statement, Background, and Notations",[0],[0]
(Section 3) since the currently found maximum after T iterations is no further away from f(x∗) than RT /T,"2. Problem Statement, Background, and Notations",[0],[0]
or R′T /T .,"2. Problem Statement, Background, and Notations",[0],[0]
"It is desirable for a batch BO algorithm to asymptotically achieve no regret, i.e., limT→∞RT /T","2. Problem Statement, Background, and Notations",[0],[0]
=,"2. Problem Statement, Background, and Notations",[0],[0]
0 or limT→∞R′T /T,"2. Problem Statement, Background, and Notations",[0],[0]
"= 0, implying that it will eventually converge to the global maximum.
","2. Problem Statement, Background, and Notations",[0],[0]
Gaussian Processes (GPs).,"2. Problem Statement, Background, and Notations",[0],[0]
"To guarantee no regret (Section 3), the unknown objective function f is modeled as a sample of a GP.","2. Problem Statement, Background, and Notations",[0],[0]
"Let {f(x)}x∈D denote a GP, that is, every finite subset of {f(x)}x∈D follows a multivariate Gaussian distribution (Rasmussen & Williams, 2006).","2. Problem Statement, Background, and Notations",[0],[0]
"Then, the GP is fully specified by its prior mean mx , E[f(x)] and covariance kxx′ , cov[f(x), f(x′)] for all x,x′ ∈ D, which, for notational simplicity (and w.l.o.g.), are assumed to be zero, i.e., mx = 0, and bounded, i.e., kxx′ ≤ 1, respectively.","2. Problem Statement, Background, and Notations",[0],[0]
"Given a column vector yD1:t-1 , (yx) >","2. Problem Statement, Background, and Notations",[0],[0]
"x∈D1:t-1 of noisy observed outputs for some set D1:t−1 , D1 ∪ . . .","2. Problem Statement, Background, and Notations",[0],[0]
"∪ Dt−1 of inputs after t− 1 iterations, a GP model can perform probabilistic regression by providing a predictive distribution p(fDt |yD1:t-1)","2. Problem Statement, Background, and Notations",[0],[0]
"= N (µDt ,ΣDtDt) of the latent outputs fDt , (f(x))","2. Problem Statement, Background, and Notations",[0],[0]
>,"2. Problem Statement, Background, and Notations",[0],[0]
"x∈Dt for any set Dt ⊆ D of inputs selected in iteration t with the following posterior mean vector and covariance matrix:
µDt,KDtD1:t-1(KD1:t-1D1:t-1+σ 2 nI) −1yD1:t-1 , ΣDtDt,KDtDt−KDtD1:t-1(KD1:t-1D1:t-1+σ2nI)−1KD1:t-1Dt (1) where KBB′ , (kxx′)x∈B,x′∈B′ for all B,B′ ⊂ D. GP-UCB and its Greedy Batch Variants.","2. Problem Statement, Background, and Notations",[0],[0]
"Inspired by the UCB algorithm for the multi-armed bandit problem, the GP-UCB algorithm (Srinivas et al., 2010) selects, in each iteration, an input x ∈ D for evaluating/querying f that trades off between sampling close to an expected maximum (i.e., with large posterior mean µ{x}) given the current GP belief of f (i.e., exploitation) vs. that of high predictive un-
certainty (i.e., with large posterior variance Σ{x}{x}) to improve the GP belief of f over D (i.e., exploration), that is, maxx∈D µ{x} + β 1/2 t Σ 1/2
{x}{x} where the parameter βt","2. Problem Statement, Background, and Notations",[0],[0]
"> 0 is set to trade off between exploitation vs. exploration for bounding its cumulative regret.
","2. Problem Statement, Background, and Notations",[0],[0]
"Existing generalizations of GP-UCB such as GP batch UCB (GP-BUCB) (Desautels et al., 2014) and GP-UCB with pure exploration (GP-UCB-PE) (Contal et al., 2013) are greedy batch BO algorithms that select the inputs of the batch one at a time (Section 1).","2. Problem Statement, Background, and Notations",[0],[0]
"Specifically, to avoid selecting the same input multiple times within a batch (hence reducing to GP-UCB), they update the posterior variance (but not the posterior mean) after adding each input to the batch, which can be performed prior to evaluating its corresponding f since the posterior variance is independent of the observed outputs (1).","2. Problem Statement, Background, and Notations",[0],[0]
"They differ in that GPBUCB greedily adds each input to the batch using GP-UCB (without updating the posterior mean) while GP-UCB-PE selects the first input using GP-UCB and each remaining input of the batch by maximizing only the posterior variance (i.e., pure exploration).","2. Problem Statement, Background, and Notations",[0],[0]
"Similarly, a recently proposed UCB-DPP-SAMPLE algorithm (Kathuria et al., 2016) selects the first input using GP-UCB and the remaining inputs by sampling from a determinantal point process (DPP).","2. Problem Statement, Background, and Notations",[0],[0]
"Like GP-BUCB, GP-UCB-PE, and UCB-DPP-SAMPLE, we can theoretically guarantee the convergence rate of our DB-GP-UCB algorithm, which, from a theoretical point of view, signifies an advantage of GP-UCB-based batch BO algorithms over those (e.g., q-EI and PPES) inspired by other acquisition functions such as EI and PES.","2. Problem Statement, Background, and Notations",[0],[0]
"Unlike these greedy batch BO algorithms (Contal et al., 2013; Desautels et al., 2014), our DB-GP-UCB algorithm can jointly optimize the batch of inputs while still preserving scalability in batch size by casting as a DCOP to be described next.
","2. Problem Statement, Background, and Notations",[0],[0]
Distributed Constraint Optimization Problem (DCOP).,"2. Problem Statement, Background, and Notations",[0],[0]
"A DCOP can be defined as a tuple (X ,V,A, h,W) that comprises a set X of input random vectors, a set V of |X | corresponding finite domains (i.e., a separate domain for each random vector), a set A of agents, a function h : X → A assigning each input random vector to an agent responsible for optimizing it, and a setW , {wn}n=1,...,N of non-negative payoff functions such that each function wn defines a constraint over only a subset Xn ⊆ X of input random vectors and represents the joint payoff that the corresponding agents An , {h(x)|x ∈ Xn} ⊆ A achieve.","2. Problem Statement, Background, and Notations",[0],[0]
"Solving a DCOP involves finding the input values ofX that maximize the sum of all functions w1, . . .","2. Problem Statement, Background, and Notations",[0],[0]
", wn (i.e., social welfare maximization), that is, maxX ∑N n=1 wn(Xn).","2. Problem Statement, Background, and Notations",[0],[0]
"To achieve a truly decentralized solution, each agent can only optimize its local input random vector(s) based on the assignment function h but communicate with its neighboring agents: Two agents are considered neighbors if there is a function/constraint involving input random vectors that
the agents have been assigned to optimize.","2. Problem Statement, Background, and Notations",[0],[0]
"Complete and approximation algorithms exist for solving a DCOP; see (Chapman et al., 2011; Leite et al., 2014) for reviews of such algorithms.","2. Problem Statement, Background, and Notations",[0],[0]
"A straightforward generalization of GP-UCB (Srinivas et al., 2010) to jointly optimize a batch of inputs is to simply consider summing the GP-UCB acquisition function over all inputs of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This, however, results in selecting the same input |Dt| times within a batch, hence reducing to GP-UCB, as explained earlier in Section 2.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"To resolve this issue but not suffer from the suboptimal behavior of greedy batch BO algorithms such as GP-BUCB (Desautels et al., 2014) and GP-UCB-PE (Contal et al., 2013), we propose a batch variant of GP-UCB that jointly optimizes a batch of inputs in each iteration t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T according to
maxDt⊂D 1 >µDt + α 1/2 t I[fD;yDt |yD1:t-1 ]1/2 (2)
where the parameter αt > 0, which performs a similar role to that of βt in GP-UCB, is set to trade off between exploitation vs. exploration for bounding its cumulative regret (Theorem 1) and the conditional mutual information1 I[fD;yDt |yD1:t-1 ] can be interpreted as the information gain on f over D (i.e., equivalent to fD , (f(x))>x∈D) by selecting the batch Dt of inputs for evaluating/querying f given the noisy observed outputs yD1:t-1 from the previous t − 1 iterations.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"So, in each iteration t, our proposed batch GP-UCB algorithm (2) selects a batch Dt ⊂ D of inputs for evaluating/querying f that trades off between sampling close to expected maxima (i.e., with a large sum of posterior means 1>µDt = ∑ x∈Dt µ{x}) given the current GP belief of f (i.e., exploitation) vs. that yielding a large information gain I[fD;yDt |yD1:t-1 ] on f over D to improve its GP belief (i.e., exploration).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"It can be derived that I[fD;yDt |yD1:t-1 ] = 0.5 log |I+σ−2n ΣDtDt | (Appendix A), which implies that the exploration term in (2) can be maximized by spreading the batch Dt of inputs far apart to achieve large posterior variance individually and small magnitude of posterior covariance between them to encourage diversity.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Unfortunately, our proposed batch variant of GP-UCB (2) involves evaluating prohibitively many batches of inputs (i.e., exponential in the batch size), hence scaling poorly in the batch size.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"However, we will show in this section that our batch variant of GP-UCB is, interestingly, amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the
1In contrast to the BO algorithm of Contal et al. (2014) that also uses mutual information, our work here considers batch BO by exploiting the correlation information between inputs of a batch in our acquisition function in (2) to encourage diversity.
efficiency of its state-of-the-art solvers for achieving linear time in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Markov Approximation.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"The key idea is to design the structure of a matrix ΨDtDt whose log-determinant can closely approximate that of ΨDtDt , I + σ −2 n ΣDtDt residing in the I[fD;yDt |yD1:t-1 ] term in (2) and at the same time be decomposed into a sum of log-determinant terms, each of which is defined by submatrices of ΨDtDt that all depend on only a subset of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Such a decomposition enables our resulting approximation of (2) to be formulated as a DCOP (Section 2).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"At first glance, our proposed idea may be naively implemented by constructing a sparse block-diagonal matrix ΨDtDt using, say, the N > 1 diagonal blocks of ΨDtDt .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, log |ΨDtDt | can be decomposed into a sum of logdeterminants of its diagonal blocks2, each of which depends on only a disjoint subset of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This, however, entails an issue similar to that discussed at the beginning of this section of selecting the same |Dt|/N inputs N times within a batch due to the assumption of independence of outputs between different diagonal blocks of ΨDtDt .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"To address this issue, we significantly relax this assumption and show that it is in fact possible to construct a more refined, dense matrix approximation ΨDtDt by exploiting a Markov assumption, which consequently correlates the outputs between all its constituent blocks and is, perhaps surprisingly, still amenable to the decomposition to achieve scalability in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Specifically, evenly partition the batch Dt of inputs into N ∈ {1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", |Dt|} disjoint subsets Dt1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
",DtN and ΨDtDt (ΨDtDt ) into N ×N square blocks, i.e., ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N (ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N ).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our first result below derives a decomposition of the logdeterminant of any symmetric positive definite block matrix ΨDtDt into a sum of log-determinant terms, each of which is defined by a separate diagonal block of the Cholesky factor of Ψ
−1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"DtDt :
Proposition 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Consider the Cholesky factorization of a symmetric positive definite Ψ
−1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"DtDt , U >U where Cholesky factor U , [Unn′ ]n,n′=1,...,N (i.e., partitioned into N × N square blocks) is an upper triangular block matrix (i.e., Unn′ = 0 for n > n′).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, log |ΨDtDt | =∑N n=1 log |(U>nnUnn)−1|.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof (Appendix B) utilizes properties of the determinant and that the determinant of an upper triangular block matrix is a product of determinants of its diagonal blocks (i.e., |U | = ∏Nn=1 |Unn|).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 1 reveals a subtle possibility of imposing some structure on the inverse of
2The determinant of a block-diagonal matrix is a product of determinants of its diagonal blocks.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
ΨDtDt such that each diagonal block Unn of its Cholesky factor (and hence each log |(U>nnUnn)−1| term) will depend on only a subset of the batch.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
The following result presents one such possibility: Proposition 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"LetB ∈ {1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N−1} be given.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"If Ψ−1DtDt is B-block-banded3, then
(U>nnUnn) −1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"= ΨDtnDtn −ΨDtnDBtnΨ −1 DBtnDBtnΨDBtnDtn
(3) for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N where η , min(n + B,N), DBtn , ⋃η n′=n+1Dtn′ , ΨDtnDBtn , [ΨDtnDtn′ ]n′=n+1,...,η , ΨDBtnDBtn , [ΨDtn′Dtn′′ ]n′,n′′=n+1,...,η, and ΨDBtnDtn , Ψ > DtnDBtn .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 1).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 2 indicates that if Ψ
−1 DtDt is B-block-banded (Fig. 1b), then
each log |(U>nnUnn)−1| term depends on only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Dt (Fig. 1c).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our next result defines a structure of ΨDtDt in terms of the blocks within the B-block band of ΨDtDt to induce a B-block-banded inverse of ΨDtDt :
Proposition 3.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let
ΨDtnDtn′,  ΨDtnDtn′ if |∆| ≤ B, ΨDtnDBtnΨ −1 DBtnDBtn
ΨDBtnDtn′ if ∆ < −B, ΨDtnDBtn′ Ψ−1DB tn′D B tn′ ΨDB tn′Dtn′ if ∆ > B;
(4) where ∆ , n−n′ for n, n′ = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N (see Fig. 1a).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, Ψ −1 DtDt is B-block-banded (see Fig. 1b).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 3).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"It can be observed from (4) and Fig. 1 that (a) though Ψ
−1 DtDt is a sparse
B-block-banded matrix, ΨDtDt is a dense matrix approximation for B = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"− 1; (b) when B = N − 1 or N = 1, ΨDtDt = ΨDtDt ; and (c) the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) coincide with that of ΨDtDt while each block outside the Bblock band of ΨDtDt (i.e., |n − n′| > B) is fully specified by the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) due to its recursive series of |n − n′| − B reduced-rank approximations (Fig. 1a).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Note, however, that the log |(U>nnUnn)−1| terms (3) for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N depend on only the blocks within (and not outside) the B-block band of ΨDtDt (Fig. 1c).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 1.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 3 provides an attractive principled interpretation: Let εx , σ−1n (yx − µ{x}) denote a scaled
3A block matrix P , [Pnn′ ]n,n′=1,...,N (i.e., partitioned into N ×N square blocks) is B-block-banded if any block Pnn′ outside its B-block band (i.e., |n− n′| > B) is 0.
residual incurred by the GP predictive mean (1).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its covariance is then cov[εx, εx′ ] = Ψ{x}{x′}.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In the same spirit as a Gaussian Markov random process, imposing a B-th order Markov property on the residual process {εx}x∈Dt is equivalent to approximating ΨDtDt with ΨDtDt (4) whose inverse isB-block-banded.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In other words, if |n−n′| > B, then {εx}x∈Dtn and {εx}x∈Dtn′ are conditionally independent given {εx}x∈Dt\(Dtn∪Dtn′ ).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
This conditional independence assumption therefore becomes more relaxed with a larger batch Dt.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 2 demonstrates the importance of such a B-th order Markov assumption (or, equivalently, the sparsity ofB-block-banded Ψ
−1 DtDt ) to achieving
scalability in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Regarding the approximation quality of ΨDtDt (4), the following result (see Appendix C for its proof) shows that the Kullback-Leibler (KL) distance of ΨDtDt from ΨDtDt measures an intuitive notion of the approximation error of ΨDtDt being the difference in information gain when relying on our Markov approximation, which can be bounded by some quantity νt:
Proposition 4.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let the KL distance DKL(Ψ, Ψ̃) , 0.5(tr(ΨΨ̃−1)",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"− log |ΨΨ̃−1| − |Dt|) between two symmetric positive definite |Dt| × |Dt| matrices Ψ and Ψ̃ measure the error of approximating Ψ with Ψ̃. Also, let Ĩ[fD;yDt |yD1:t-1 ] , 0.5 log |ΨDtDt | denote the approximated information gain, and C ≥ I[f{x};yDt |yD1:t-1 ] for all x ∈ D and t ∈",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
N.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, for all t ∈ N,
DKL(ΨDtDt ,ΨDtDt) = Ĩ[fD;yDt |yD1:t-1 ]− I[fD;yDt |yD1:t-1 ] ≤ (exp(2C)− 1) I[fD;yDt |yD1:t-1 ] , νt .
Proposition 4 implies that the approximated information gain Ĩ[fD;yDt |yD1:t-1 ] is never smaller than the exact information gain I[fD;yDt |yD1:t-1 ] since DKL(ΨDtDt ,ΨDtDt) ≥ 0 with equality when N = 1, in which case ΨDtDt = ΨDtDt (4).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Thus, intuitively, our proposed Markov approximation hallucinates information into ΨDtDt to yield an optimistic estimate of the information gain (by selecting a particular batch), ultimately making our resulting algorithm overconfident in selecting a batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This overconfidence is information-theoretically quantified by the approximation error DKL(ΨDtDt ,ΨDtDt) ≤ νt.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 3.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"The KL distanceDKL(ΨDtDt ,ΨDtDt) of ΨDtDt from ΨDtDt is also the least among all |Dt|×|Dt|matrices with a B-block-banded inverse, as proven in Appendix D.
DCOP Formulation.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"By exploiting the approximated information gain Ĩ[fD;yDt |yD1:t-1 ] (Proposition 4), Proposition 1, (3), and (4), our batch variant of GP-UCB (2) can be reformulated in an approximate sense4 to a distributed batch GP-UCB (DB-GP-UCB) algorithm5 that jointly optimizes a batch of inputs in each iteration t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T according to
Dt , arg max Dt⊂D N∑ n=1 wn(Dtn ∪ DBtn)
wn(Dtn ∪ DBtn) , 1>µDtn+(0.5αt log |ΨDtnDtn|DBtn |) 1/2
(5) with ΨDtnDtn|DBtn ,ΨDtnDtn−ΨDtnDBtnΨ −1 DBtnDBtn ΨDBtnDtn .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"4Note that our acquisition function (5) uses ∑N
n=1(log | · |) 1/2 instead of ( ∑N
n=1 log | · |) 1/2 to enable the decomposition.
5Pseudocode for DB-GP-UCB is provided in Appendix E.
Note that (5) is equivalent to our batch variant of GP-UCB (2) when N = 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
It can also be observed that (5) is naturally formulated as a multi-agent DCOP (Section 2) whereby every agent an ∈,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"A is responsible for optimizing a disjoint subset Dtn of the batch Dt for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N and each function wn defines a constraint over only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch Dt and represents the joint payoff that the corresponding agents An , {an′}ηn′=n ⊆ A achieve.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"As a result, (5) can be efficiently and scalably solved by the state-of-the-art DCOP algorithms (Chapman et al., 2011; Leite et al., 2014).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"For example, the time complexity of an iterative message-passing algorithm called max-sum (Farinelli et al., 2008) scales exponentially in only the largest arity maxn∈{1,...,N} |Dtn ∪ DBtn| =",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"(B+1)|Dt|/N of the functionsw1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", wN .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Given a limited time budget, a practitioner can set a maximum arity of ω for any function wn, after which the number N of functions is adjusted to d(B + 1)|Dt|/ωe so that the time incurred by max-sum to solve the DCOP in (5) is O(|D|ωω3B|Dt|)6 per iteration (i.e., linear in the batch size |Dt| by assuming ω and the Markov orderB to be constants).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In contrast, our batch variant of GP-UCB (2) incurs exponential time in the batch size |Dt|.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
The max-sum algorithm is also amenable to a distributed implementation on a cluster of parallel machines to boost scalability further.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"If a solution quality guarantee is desired, then a variant of maxsum called bounded max-sum (Rogers et al., 2011) can be used7.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Finally, the Markov order B can be varied to trade off between the approximation quality of ΨDtDt (4) and the time efficiency of max-sum in solving the DCOP in (5).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Regret Bounds.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our main result to follow derives probabilistic bounds on the cumulative regret of DB-GP-UCB:
Theorem 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let δ ∈ (0, 1) be given, C1 , 4/ log(1 + σ−2n ), γT , maxD1:T⊂D I[fD;yD1:T ], αt , C1|Dt| exp(2C) log(|D|t2π2/(6δ)), and ν̄T , ∑T t=1 νt.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, for the batch and full cumulative regrets incurred by our DB-GP-UCB algorithm (5),
RT ≤ 2 ( T |DT |−2αTN(γT + ν̄T ) )1/2 and R′T ≤ 2 (TαTN(γT + ν̄T )) 1/2
hold with probability of at least 1− δ.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"6We assume the use of online sparse GP models (Csató &
Opper, 2002; Hensman et al., 2013; Hoang et al., 2015; 2017; Low et al., 2014b; Xu et al., 2014) that can update the GP predictive/posterior distribution (1) in constant time in each iteration.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"7Bounded max-sum is previously used in (Rogers et al., 2011) to solve a related maximum entropy sampling problem (Shewry & Wynn, 1987) formulated as a DCOP.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"But, the largest arity of any function wn in this DCOP is still the batch size |Dt| and, unlike the focus of our work here, no attempt is made in (Rogers et al., 2011) to reduce it, thus causing max-sum and bounded max-sum to incur exponential time in |Dt|.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In fact, our proposed Markov approximation can be applied to this problem to reduce the largest arity of any function wn in this DCOP to again (B + 1)|Dt|/N .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof (Appendix F), when compared to that of GP-UCB (Srinivas et al., 2010) and its greedy batch variants (Contal et al., 2013; Desautels et al., 2014), requires tackling the additional technical challenges associated with jointly optimizing a batch Dt of inputs in each iteration t. Note that the uncertainty sampling based initialization strategy proposed by Desautels et al. (2014) can be employed to replace the √ exp(2C) term (i.e., growing linearly in |Dt|) appearing in our regret bounds by a kernel-dependent constant factor of C ′",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"that is independent of |Dt|; values of C ′ for the most commonly-used kernels are replicated in Table 2 in Appendix G (see section 4.5 in (Desautels et al., 2014) for a more detailed discussion on this issue).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Table 1 in Appendix G compares the bounds on RT of DB-GP-UCB (5), GP-UCB-PE, GP-BUCB, GP-UCB, and UCB-DPP-SAMPLE.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Compared to the bounds on RT of GP-UCB-PE and UCB-DPP-SAMPLE, our bound includes the additional kernel-dependent factor of C ′, which is similar to GP-BUCB.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In fact, our regret bound is of the same form as that of GP-BUCB except that our bound incorporates a parameter N of our Markov approximation and an upper bound ν̄T on the cumulative approximation error, both of which vanish for our batch variant of GP-UCB (2):
Corollary 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"For our batch variant of GP-UCB (2), the cumulative regrets reduce to RT ≤ 2 ( T |DT |−2αT γT
)1/2 and R′T ≤ 2 (TαT γT ) 1/2.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Corollary 1 follows directly from Theorem 1 and by noting that for our batch variant (2), N = 1 (since ΨDtDt then trivially reduces to ΨDtDt ) and νt",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"= 0 for t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Finally, the convergence rate of our DB-GP-UCB algorithm is dominated by the growth behavior of γT + ν̄T .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"While it is well-known that the bounds on the maximum mutual information γT established for the commonly-used linear, squared exponential, and Matérn kernels in (Srinivas et al., 2010; Kathuria et al., 2016) (i.e., replicated in Table 2 in Appendix G) only grow sublinearly in T , it is not immediately clear how the upper bound ν̄T on the cumulative approximation error behaves.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Our next result reveals that ν̄T in fact only grows sublinearly in T as well: Corollary 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"ν̄T ≤ (exp(2C)− 1)γT .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Corollary 2 follows directly from the definitions of νt in Proposition 4 and ν̄T and γT in Theorem 1 and applying the chain rule for mutual information.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Since γT grows sublinearly in T for the above-mentioned kernels (Srinivas et al., 2010) and C can be chosen to be independent of T (e.g., C , γ|Dt|−1) (Desautels et al., 2014), it follows from Corollary 2 that ν̄T grows sublinearly in T .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"As a result, Theorem 1 guarantees sublinear cumulative regrets for the above-mentioned kernels, which implies that our DBGP-UCB algorithm (5) asymptotically achieves no regret, regardless of the degree of our proposed Markov approxi-
mation (i.e., configuration of [N,B]).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Thus, our batch variant of GP-UCB (2) achieves no regret as well.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This section evaluates the cumulative regret incurred by our DB-GP-UCB algorithm (5) and its scalability in the batch size empirically on two synthetic benchmark objective functions such as Branin-Hoo (Lizotte, 2008) and gSobol (González et al., 2016)",4. Experiments and Discussion,[0],[0]
"(Table 3 in Appendix H) and a real-world pH field of Broom’s Barn farm (Webster & Oliver, 2007) (Fig. 3 in Appendix H) spatially distributed over a 1200 m by 680 m region discretized into a 31 × 18 grid of sampling locations.",4. Experiments and Discussion,[0],[0]
"These objective functions and the real-world pH field are each modeled as a sample of a GP whose prior covariance is defined by the widelyused squared exponential kernel kxx′ , σ2s exp(−0.5(x− x′)>Λ−2(x− x′)) where Λ , diag[`1, . . .",4. Experiments and Discussion,[0],[0]
", `d] and σ2s are its length-scale and signal variance hyperparameters, respectively.",4. Experiments and Discussion,[0],[0]
"These hyperparameters together with the noise variance σ2n are learned using maximum likelihood estimation (Rasmussen & Williams, 2006).
",4. Experiments and Discussion,[0],[0]
"The performance of our DB-GP-UCB algorithm (5) is compared with the state-of-the-art batch BO algorithms such as GP-BUCB (Desautels et al., 2014), GP-UCB-PE (Contal et al., 2013), SM-UCB (Azimi et al., 2010), q-EI (Chevalier & Ginsbourger, 2013), and BBO-LP by plugging in GP-UCB (González et al., 2016), whose implementations8 are publicly available.",4. Experiments and Discussion,[0],[0]
"These batch BO algorithms are evaluated using a performance metric that measures the cumulative regret incurred by a tested algorithm:∑T t=1 f(x
∗)",4. Experiments and Discussion,[0],[0]
"− f(x̃t) where x̃t , arg maxxt∈D µ{xt} (1) is the recommendation of the tested algorithm after t batch evaluations.",4. Experiments and Discussion,[0],[0]
"For each experiment, 5 noisy observations are randomly selected and used for initialization.",4. Experiments and Discussion,[0],[0]
This is independently repeated 64 times and we report the resulting mean cumulative regret incurred by a tested algorithm.,4. Experiments and Discussion,[0],[0]
"All experiments are run on a Linux system with Intelr Xeonr E5-2670 at 2.6GHz with 96 GB memory.
",4. Experiments and Discussion,[0],[0]
"For our experiments, we use a fixed budget of T |DT | = 64 function evaluations and analyze the trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) on the performance of the tested algorithms.",4. Experiments and Discussion,[0],[0]
"This experimental setup represents a practical scenario of costly function evaluations: On one hand, when a function evaluation is computationally costly (i.e., time-consuming), it is more desirable to evaluate f for a larger batch (e.g., |DT | = 16) of inputs in parallel in each iteration t (i.e., if hardware resources permit) to reduce the total time needed (hence smaller T ).",4. Experiments and Discussion,[0],[0]
"On the other hand,
8Details on the used implementations are given in Table 4 in Appendix I. We implemented DB-GP-UCB in MATLAB to exploit the GPML toolbox (Rasmussen & Williams, 2006).
",4. Experiments and Discussion,[0],[0]
"when a function evaluation is economically costly, one may be willing to instead invest more time (hence larger T ) to evaluate f for a smaller batch (e.g., |DT | = 2) of inputs in each iteration t in return for a higher frequency of information and consequently a more adaptive BO to achieve potentially better performance.",4. Experiments and Discussion,[0],[0]
"In some settings, both factors may be equally important, that is, moderate values of |DT | and T are desired.",4. Experiments and Discussion,[0],[0]
"To the best of our knowledge, such a form of empirical analysis does not seem to be available in the batch BO literature.
",4. Experiments and Discussion,[0],[0]
"Fig. 2 shows results9 of the cumulative regret incurred by the tested algorithms to analyze their trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) using a fixed budget of T |DT | = 64 function evaluations for the Branin-Hoo function (left column), gSobol function (middle column), and real-world pH field (right column).",4. Experiments and Discussion,[0],[0]
"Our DB-GP-UCB algorithm uses the configurations of [N,B] =",4. Experiments and Discussion,[0],[0]
"[4, 2], [8, 5], [16, 10] in the experiments with batch size |DT | = 4, 8, 16, respectively; in the case of |DT | = 2, we use our batch variant of GPUCB (2) which is equivalent to DB-GP-UCB whenN = 1.",4. Experiments and Discussion,[0],[0]
"It can be observed that DB-GP-UCB achieves lower cumulative regret than GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP in all experiments (with the only exception being the gSobol function for the smallest batch size of |DT | = 2 where BBO-LP performs slightly better) since DB-GPUCB can jointly optimize a batch of inputs while GPBUCB, GP-UCB-PE, SM-UCB, and BBO-LP are greedy batch algorithms that select the inputs of a batch one at time.",4. Experiments and Discussion,[0],[0]
"Note that as the real-world pH field is not as wellbehaved as the synthetic benchmark functions (see Fig. 3 in Appendix H), the estimate of the Lipschitz constant by BBO-LP is potentially worse, hence likely degrading its performance.",4. Experiments and Discussion,[0],[0]
"Furthermore, DB-GP-UCB can scale to a much larger batch size of 16 than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES (Shah & Ghahramani, 2015) and q-KG (Wu & Frazier, 2016): Results of q-EI are not available for |DT | ≥ 4 as they require a prohibitively huge computational effort to be obtained10 while PPES can only operate with a small batch size of up to 3 for the Branin-Hoo function and up to 4 for other functions, as reported in (Shah & Ghahramani, 2015), and q-KG can only operate with a small batch size of 4 for all tested functions (including the Branin-Hoo function and four others), as reported in (Wu & Frazier, 2016).",4. Experiments and Discussion,[0],[0]
"The scalability of DB-GP-UCB is attributed to our proposed Markov approximation of our
9Error bars are omitted in Fig. 2 to preserve the readability of the graphs.",4. Experiments and Discussion,[0],[0]
"A replication of the graphs in Fig. 2 including standard error bars is provided in Appendix K.
10In the experiments of González et al. (2016), q-EI can reach a batch size of up to 10 but performs much worse than GP-BUCB, which is likely due to a considerable downsampling of possible batches available for selection in each iteration.
batch variant of GP-UCB (2) (Section 3), which can then be naturally formulated as a multi-agent DCOP (5) in order to fully exploit the efficiency of one of its state-of-the-art solvers called max-sum (Farinelli et al., 2008).",4. Experiments and Discussion,[0],[0]
"In the experiments with the largest batch size of |DT | = 16, we have reduced the number of iterations in max-sum to less than 5 without waiting for convergence to preserve the efficiency of DB-GP-UCB, thus sacrificing its BO performance.",4. Experiments and Discussion,[0],[0]
"Nevertheless, DB-GP-UCB can still outperform the other tested
batch BO algorithms.
",4. Experiments and Discussion,[0],[0]
We have also investigated and analyzed the trade-off between approximation quality and time efficiency of our DPGP-UCB algorithm and reported the results in Appendix J due to lack of space.,4. Experiments and Discussion,[0],[0]
"To summarize, it can be observed from our results that the approximation quality improves near-linearly with an increasing Markov order B at the expense of higher computational cost (i.e., exponential in B).",4. Experiments and Discussion,[0],[0]
"This paper develops a novel distributed batch GP-UCB (DB-GP-UCB) algorithm for performing batch BO of highly complex, costly-to-evaluate, noisy black-box objective functions.",5. Conclusion,[0],[0]
"In contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016), our DB-GP-UCB algorithm can jointly optimize a batch of inputs and, unlike (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size.",5. Conclusion,[0],[0]
"To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the efficiency of its state-of-the-art solvers such as max-sum (Farinelli et al., 2008; Rogers et al., 2011) for achieving linear time in the batch size.",5. Conclusion,[0],[0]
Our proposed DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,5. Conclusion,[0],[0]
We provide a theoretical guarantee for the convergence rate of our DB-GP-UCB algorithm via bounds on its cumulative regret.,5. Conclusion,[0],[0]
"Empirical evaluation on synthetic benchmark objective functions and a real-world pH field shows that our DB-GP-UCB algorithm can achieve lower cumulative regret than the greedy batch BO algorithms such as GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP, and scale to larger batch sizes than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES, and q-KG.",5. Conclusion,[0],[0]
"For future work, we plan to generalize DB-GP-UCB (a) to the nonmyopic context by appealing to existing literature on nonmyopic BO (Ling et al., 2016) and active learning (Cao et al., 2013; Hoang et al., 2014a;b; Low et al., 2008; 2009; 2011; 2014a) as well as (b) to be performed by a multi-robot team to find hotspots in environmental sensing/monitoring by seeking inspiration from existing literature on multi-robot active sensing/learning (Chen et al., 2012; 2013b; 2015; Low et al., 2012; Ouyang et al., 2014).",5. Conclusion,[0],[0]
"For applications with a huge budget of function evaluations, we like to couple DB-GP-UCB with the use of parallel/distributed sparse GP models (Chen et al., 2013a; Hoang et al., 2016; Low et al., 2015) to represent the belief of the unknown objective function efficiently.",5. Conclusion,[0],[0]
"This research is supported by Singapore Ministry of Education Academic Research Fund Tier 2, MOE2016-T2-2-156.",Acknowledgements,[0],[0]
Erik A. Daxberger would like to thank Volker Tresp for his advice throughout this research project.,Acknowledgements,[0],[0]
"This paper presents a novel distributed batch Gaussian process upper confidence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions.",abstractText,[0],[0]
"In contrast to existing batch BO algorithms, DBGP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still preserving scalability in the batch size.",abstractText,[0],[0]
"To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.",abstractText,[0],[0]
Our DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,abstractText,[0],[0]
We provide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret.,abstractText,[0],[0]
Empirical evaluation on synthetic benchmark objective functions and a real-world optimization problem shows that DB-GP-UCB outperforms the stateof-the-art batch BO algorithms.,abstractText,[0],[0]
Distributed Batch Gaussian Process Optimization,title,[0],[0]
"Clustering is a fundamental problem in the analysis and understanding of data, and is used widely in different areas of science.",1. Introduction,[0],[0]
The broad goal of clustering is to divide a (typically large) dataset into groups that such that data points within a group are “similar” to one another.,1. Introduction,[0],[0]
"In most applications, there is a measure of similarity between any two objects, which typically forms a metric.",1. Introduction,[0],[0]
"The problem can be formalized in many different ways, depending on the properties desired of the obtained clustering.",1. Introduction,[0],[0]
"While a “perfect” formulation may not exist (see (Kleinberg, 2002)),
*Equal contribution 1School of Computing, University of Utah.",1. Introduction,[0],[0]
Correspondence to: Aditya Bhaskara,1. Introduction,[0],[0]
<bhaskaraaditya@gmail.com,1. Introduction,[0],[0]
">, Maheshakya Wijewardena <pmaheshakya4@gmail.com",1. Introduction,[0],[0]
">.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"many formulations have been very successful in applications, including k-means, k-median, k-center, and various notions of hierarchical clustering (see (Hastie et al., 2009; Dasgupta, 2016) and references there-in).
",1. Introduction,[0],[0]
"In this paper, we focus on k-means clustering, in which the input is a set of n points in Euclidean space.",1. Introduction,[0],[0]
"Here the goal is to partition the points into k clusters, so as to minimize the sum of squared distances from the points to the respective cluster centers (see Section 2 for a formal definition).",1. Introduction,[0],[0]
k-means is one of the most well-studied clustering variants.,1. Introduction,[0],[0]
"Lloyd’s algorithm (Lloyd, 1982), developed over 35 years ago, has been extremely successful in practice (the success has been ‘explained’ in many recent works; see (Arthur et al., 2011; Awasthi & Sheffet, 2012; Kumar & Kannan, 2010) and references there-in).",1. Introduction,[0],[0]
"Despite the success, Lloyd’s algorithm can have an arbitrarily bad approximation ratio in the worst case.",1. Introduction,[0],[0]
"To address this, constant factor approximation algorithms have been developed, which are more involved but have worst case guarantees (see (Kanungo et al., 2004) and (Ahmadian et al., 2017)).",1. Introduction,[0],[0]
"In another direction, works by (Ostrovsky et al., 2006; Jaiswal et al., 2012; Arthur & Vassilvitskii, 2007) have shown how to obtain simple bicriteria approximation algorithms for k-means.",1. Introduction,[0],[0]
"(Arthur & Vassilvitskii, 2007) also proposed a variant of Lloyd’s algorithm, termed “k-means++”, which also comes with a theoretical approximation factor guarantee of O(log k) approximation.
",1. Introduction,[0],[0]
All the algorithms above assume that data fits in a single machine.,1. Introduction,[0],[0]
"However, with the ubiquity of large data sets, there has been a lot of interest in distributed algorithms where data is spread across several machines.",1. Introduction,[0],[0]
"The goal is to use available distributed models of computation to design algorithms that can (a) work with machines having access only to their local data set, (b) use small amount of memory and only a few “rounds” of communication, and (c) have approximation guarantees for the solution they output.
",1. Introduction,[0],[0]
"For k-means and related objectives, the paradigm of iterative ‘data reduction’ has been remarkably successful.",1. Introduction,[0],[0]
"The main idea is that in each round, a machine chooses a small subset of its input, and only this subset is carried to the next round.",1. Introduction,[0],[0]
"Thus the total number of points reduces by a significant factor in every round, and this results in a small number of rounds overall.",1. Introduction,[0],[0]
"Such an algorithm can be implemented
efficiently in the MapReduce framework, introduced by (Dean & Ghemawat, 2004), and formalized by (Karloff et al., 2010)).",1. Introduction,[0],[0]
"(Ene et al., 2011) gave one of the first such implementations (for the k-median problem), and showed theoretical guarantees.",1. Introduction,[0],[0]
"This line of work has subsequently been developed in (Kumar et al., 2013; Balcan et al., 2013a; Awasthi et al., 2017).",1. Introduction,[0],[0]
"The last work also gives a summary of the known results in this space.
",1. Introduction,[0],[0]
The high level ideas used in these works are similar to those used in streaming algorithms for clustering.,1. Introduction,[0],[0]
"The literature here is very rich; one of the earliest works is that of (Charikar et al., 1997), for the k-center problem.",1. Introduction,[0],[0]
"The work of (Guha et al., 2001) introduced many ideas crucial to the distributed algorithms mentioned above.",1. Introduction,[0],[0]
"Indeed, all of these algorithms can be viewed as implicitly constructing coresets (or summaries) for the underlying clustering problem.",1. Introduction,[0],[0]
"We refer to the works of (Agarwal et al., 2004; 2012; Balcan et al., 2013b; Indyk et al., 2014) for more on this connection.
",1. Introduction,[0],[0]
Motivation for our work.,1. Introduction,[0],[0]
"While iterative data reduction is powerful, it has a key bottleneck: in order to have approximation guarantees, machines always need to store > k data points.",1. Introduction,[0],[0]
"Indeed, all the algorithms we are aware of require a memory of kn if they are to useO(1/ ) rounds of MAPREDUCE computation.1 The high level reason for this is that if a machine sees k points that are all very far from one another, it needs to keep all of them, or else we might lose all the information about one of the clusters, and this could lead to a large objective value.",1. Introduction,[0],[0]
"This is also the reason each machine needs to communicate ≥ k points to the others (such a lower bound was proved formally in (Chen et al., 2016), as we will discuss later).",1. Introduction,[0],[0]
"The natural question is thus to ask: can we partition the data across machines so that different machines work in different “regions of space”, and thus focus on finding different clusters?",1. Introduction,[0],[0]
"This would result in a smaller space requirement per machine, and lesser communication between machines.",1. Introduction,[0],[0]
"Our main result is to show that this is possible, as long as we have a rough estimate of the optimum objective value (up to an arbitrary polynomial factor).",1. Introduction,[0],[0]
"We give an algorithm based on a variant of locality sensitive hashing, and prove that this yields a bi-criteria approximation guarantee.
",1. Introduction,[0],[0]
"Locality sensitive hashing was introduced in the seminal work of (Indyk & Motwani, 1998), which gave an efficient algorithm for nearest neighbor search in high dimensional space.",1. Introduction,[0],[0]
"The idea has found several applications in machine learning and data science, ranging from the early applications of similarity search to the speeding up of neural networks (Spring & Shrivastava, 2017).",1. Introduction,[0],[0]
"(Datar et al., 2004)
1All the algorithms mentioned above can be naturally implemented in the MAPREDUCE framework.
generalized the original result of (Indyk & Motwani, 1998) to the case of `p norms, and (Andoni & Indyk, 2006) gave an improved analysis.",1. Introduction,[0],[0]
"Extensions of LSH are still an active area of research, but a discussion is beyond the scope of this paper.",1. Introduction,[0],[0]
Our contribution here is to understand the behavior of clusters of points under LSH and its variants.,1. Introduction,[0],[0]
Our focus in the paper will be on the k-means objective (defined formally in Section 2).,1.1. Our results,[0],[0]
"The data set is assumed to be a collection of points in a Euclidean space Rd for some d, and distance refers to the `2 distance.
",1.1. Our results,[0],[0]
"Our first contribution is an analysis of “product LSH” (PLSH), a hash obtained by concatenating independent copies of an LSH.",1.1. Our results,[0],[0]
"For each LSH, we consider the implementation of (Andoni & Indyk, 2006).
",1.1. Our results,[0],[0]
Informal theorem 1 (See Lemmas 1 and 2).,1.1. Our results,[0],[0]
Let C be any cluster of points with diameter σ.,1.1. Our results,[0],[0]
"Then PLSH with appropriate parameters yields the same hash for all the points in C, with probability ≥ 3/4.",1.1. Our results,[0],[0]
"Furthermore, for any two points u, v such that ‖u",1.1. Our results,[0],[0]
"− v‖ ≥ α · σ, where α ≈ log n log log n, the probability that u and v have the same hash is < 1/n2.
",1.1. Our results,[0],[0]
"Thus, PLSH has a “cluster preserving” property.",1.1. Our results,[0],[0]
"We show the above by extending the analyses of (Indyk & Motwani, 1998) and (Andoni & Indyk, 2006).",1.1. Our results,[0],[0]
"Then, we use this observation to give a simple bi-criteria approximation algorithm for k-means clustering.",1.1. Our results,[0],[0]
(A bi-criteria algorithm is one that is allowed to output a slightly larger number of centers; see Section 2.),1.1. Our results,[0],[0]
"We assume knowledge of k, as well as a very rough estimate of the objective value.",1.1. Our results,[0],[0]
"The algorithm returns a polylogarithmically larger number of clusters, while obtaining a polylogarithmic factor approximation.",1.1. Our results,[0],[0]
We refer to Theorem 2 for the statement.,1.1. Our results,[0],[0]
"As we note below, if s > k polylog(()n), then we can avoid violating the bound on the number of clusters (and obtain a “true” guarantee as opposed to a bi-criteria one).
",1.1. Our results,[0],[0]
"The algorithm can be implemented in a distributed manner, specifically in the MAPREDUCE model, with dlogs ne+ 2 rounds, using machines of memory s (when we say memory s, we mean that each machine can store at most s of the points.",1.1. Our results,[0],[0]
"This will be roughly the same as measuring s in bytes, as we see in Section 2.",1.1. Our results,[0],[0]
The formal result is stated in Theorem 3.,1.1. Our results,[0],[0]
"We highlight that the distributed algorithm works for any s ≥ ω(log n), even s k (in which case the standard reduce-and-merge framework has no non-trivial guarantees).
",1.1. Our results,[0],[0]
"Finally, we prove that for any MapReduce algorithm that uses poly(n) machines of space s, the number of rounds necessary to obtain any non-trivial approximation to k-means is at least dlogs ne.",1.1. Our results,[0],[0]
"Thus the ‘round/memory tradeoff’ we
obtain is nearly optimal.",1.1. Our results,[0],[0]
"This is based on ideas from the recent remarkable result of (Roughgarden et al., 2016).",1.1. Our results,[0],[0]
(See Theorem 4.),1.1. Our results,[0],[0]
Going beyond communication lower bounds.,"1.2. Discussion, extensions and limitations",[0],[0]
"The recent result of (Chen et al., 2016) shows lower bounds on the total amount of communication necessary for distributed clustering.","1.2. Discussion, extensions and limitations",[0],[0]
"They show that for a worst-case partition of points across machines, Ω(Mk) bits are necessary, where M is the number of machines.","1.2. Discussion, extensions and limitations",[0],[0]
"Our result in Section 4.2 implies that if points have been partitioned across machines according to PLSH hashes, we can bypass this lower bound.
","1.2. Discussion, extensions and limitations",[0],[0]
Round lower bound.,"1.2. Discussion, extensions and limitations",[0],[0]
"In light of Theorem 4, one way to interpret our algorithmic result is as saying that as far as obtaining polylogarithmic bi-criteria approximations go, clustering is essentially as easy as “aggregation” (i.e., summing a collection of n numbers – which also has the same logs n upper and lower bounds).
","1.2. Discussion, extensions and limitations",[0],[0]
Precisely k clusters.,"1.2. Discussion, extensions and limitations",[0],[0]
"Theorem 3 gives only a bi-criteria guarantee, so it is natural to ask if we can obtain any guarantee when we desire precisely k centers.","1.2. Discussion, extensions and limitations",[0],[0]
"In the case when s ≥ k log2 n, we can apply known results to obtain this.","1.2. Discussion, extensions and limitations",[0],[0]
"(Guha et al., 2001) showed (in our notation) that:
Theorem 1.","1.2. Discussion, extensions and limitations",[0],[0]
"Let U be a set of points, and let S be a set of centers with the property that ∑ u∈U d(u, S)
2 ≤ γ · OPT, where OPT is the optimum k-means objective value on U .","1.2. Discussion, extensions and limitations",[0],[0]
"Let g : U 7→ S map every u ∈ U to its closest point in S, breaking ties arbitrarily.","1.2. Discussion, extensions and limitations",[0],[0]
"Now, consider a new weighted instance I of k-means where we have points in S, with weight of v ∈ S being |g−1(v)|.","1.2. Discussion, extensions and limitations",[0],[0]
"Then, any set of centers that ρ-approximate the optimum objective for I give a (4γ+ 2ρ) approximation to the original instance (given by U ).
","1.2. Discussion, extensions and limitations",[0],[0]
"Thus, if s ≥ k log2 n, we can aggregate the output of our bi-criteria algorithm onto one machine, and solve k-means approximately.","1.2. Discussion, extensions and limitations",[0],[0]
"In essence, we are using the output of our algorithm as a coreset for k-means.","1.2. Discussion, extensions and limitations",[0],[0]
"We demonstrate this in our experimental results.
","1.2. Discussion, extensions and limitations",[0],[0]
Balanced clustering.,"1.2. Discussion, extensions and limitations",[0],[0]
A common constraint for clustering algorithms is that of the clusters being balanced.,"1.2. Discussion, extensions and limitations",[0],[0]
This is often captured by requiring an upper bound on the size of a cluster.,"1.2. Discussion, extensions and limitations",[0],[0]
"(Bateni et al., 2014) showed that balanced clustering can also be solved in a distributed setting.","1.2. Discussion, extensions and limitations",[0],[0]
"Specifically, they showed that any bi-criteria algorithm for k-means can be used to solve the balanced clustering problem, via a result analogous to 1.","1.2. Discussion, extensions and limitations",[0],[0]
"In our context, this implies that if s > k log2 n, our method also gives a distributed algorithm for balanced clustering with a k-means objective.
Limitations and lower bounds.","1.2. Discussion, extensions and limitations",[0],[0]
"There are two key limita-
tions to our result.","1.2. Discussion, extensions and limitations",[0],[0]
"First, the polylogarithmic approximation factor in the approximation ratio seems difficult to avoid (although our experiments show that the guarantees are very pessimistic).","1.2. Discussion, extensions and limitations",[0],[0]
"In our argument, it arises as a by-product of being able to detect very small clusters.","1.2. Discussion, extensions and limitations",[0],[0]
"This is in contrast with single machine algorithms (e.g., (Kanungo et al., 2004; Ahmadian et al., 2017)) and the prior work in MapReduce algorithms, (Ene et al., 2011), which give constant factor approximations.","1.2. Discussion, extensions and limitations",[0],[0]
Another restriction is that our algorithms assume a Euclidean setting for the points.,"1.2. Discussion, extensions and limitations",[0],[0]
"The algorithms of (Ene et al., 2011) and related works can handle the case of arbitrary metric spaces.","1.2. Discussion, extensions and limitations",[0],[0]
The bottleneck here is the lack of locality sensitive hashing for such spaces.,"1.2. Discussion, extensions and limitations",[0],[0]
"A very interesting open problem is to develop new methods in this case, or prove stronger lower bounds.","1.2. Discussion, extensions and limitations",[0],[0]
We now introduce some notation and definitions that will be used for the rest of the paper.,2. Notation and Preliminaries,[0],[0]
We will denote by U the set of points in the input.,2. Notation and Preliminaries,[0],[0]
We denote n = |U |.,2. Notation and Preliminaries,[0],[0]
"All of our algorithms are for the Euclidean setting, where the points in U are in Rd, and the distance between x, y ∈ U is the `2 norm ‖x− y‖2 = √∑ i(xi",2. Notation and Preliminaries,[0],[0]
"− yi)2.
",2. Notation and Preliminaries,[0],[0]
"A k-clustering of the points U is a partition C of U into subsets C1, C2, . . .",2. Notation and Preliminaries,[0],[0]
", Ck.",2. Notation and Preliminaries,[0],[0]
The centroid of a cluster Ci is the point µi := 1|Ci| ∑ u∈Ci u.,2. Notation and Preliminaries,[0],[0]
The k-means objective for the clustering C is now defined as∑ i∈[k],2. Notation and Preliminaries,[0],[0]
∑ u∈Ci ‖u− µi‖22.,2. Notation and Preliminaries,[0],[0]
"(1)
The problem of k-means clustering is to find C that minimizes the objective defined above.",2. Notation and Preliminaries,[0],[0]
The minimum objective value will be denoted by OPT(U).,2. Notation and Preliminaries,[0],[0]
"(When the U is clear from context, we simply write OPT.)",2. Notation and Preliminaries,[0],[0]
A ρ-approximation algorithm for k-means clustering is a polynomial time algorithm that outputs a clustering C′ whose objective value is at most ρ ·OPT(U).,2. Notation and Preliminaries,[0],[0]
We will be interested in ρ being a constant or polylog(n).,2. Notation and Preliminaries,[0],[0]
"A (ρ, β) bi-criteria approximation (where β ≥ 1) is an efficient algorithm that outputs a clustering C′ that has at most βk clusters and has an objective value at most ρ ·OPT(U).",2. Notation and Preliminaries,[0],[0]
"Note that the optimum still has k clusters.
",2. Notation and Preliminaries,[0],[0]
Note on the dimension d.,2. Notation and Preliminaries,[0],[0]
We assume that d = O(log n).,2. Notation and Preliminaries,[0],[0]
"This is without loss of generality, because we may assume that we have pre-processed the data by applying JohnsonLindenstrauss transform.",2. Notation and Preliminaries,[0],[0]
"As the JL transform preserves all pairwise `2 distances (Johnson & Lindenstrauss, 1984; Indyk & Motwani, 1998), clustering in the transformed space gives a (1 + ) approximation to clustering in the original one.",2. Notation and Preliminaries,[0],[0]
"Furthermore, the transform can be applied in parallel, individually to each point.",2. Notation and Preliminaries,[0],[0]
"Thus we henceforth assume that the space required to store a point is O(log n).
MapReduce model.",2. Notation and Preliminaries,[0],[0]
"To illustrate our ideas, we use the well-studied MapReduce model (Dean & Ghemawat, 2004; Karloff et al., 2010).",2. Notation and Preliminaries,[0],[0]
The details of the map and reduce operations are not important for our purpose.,2. Notation and Preliminaries,[0],[0]
We will view it as a model of computation that proceeds in levels.,2. Notation and Preliminaries,[0],[0]
"At each level, we have M machines that can perform computation on their local data (the input is distributed arbitrarily among machines in the first layer).",2. Notation and Preliminaries,[0],[0]
"Once all the machines are done with computation, they send information to machines in the next layer.",2. Notation and Preliminaries,[0],[0]
The information received by a machine acts as its local data for the next round of computation.,2. Notation and Preliminaries,[0],[0]
"We assume that each machine has a memory of s.
Constants.",2. Notation and Preliminaries,[0],[0]
"For the sake of easier exposition, we do not attempt to optimize the constants in our bounds.",2. Notation and Preliminaries,[0],[0]
The key to our distributed algorithm is a two step hashing scheme.,3. Two Step Hashing,[0],[0]
"The first step is a ‘product’ of locality sensitive hashes (LSH), and the second is a random hash that maps the tuples obtained from the product-LSH to a bin with a label in the range 1, . . .",3. Two Step Hashing,[0],[0]
", Lk, for an appropriate constant L.",3. Two Step Hashing,[0],[0]
We begin with a short discussion of LSH.,3.1. Product LSH,[0],[0]
"We follow the presentation of (Andoni & Indyk, 2006).2
Suppose we have a collection of points U in Rd.
",3.1. Product LSH,[0],[0]
Locality sensitive hashing (LSH).,3.1. Product LSH,[0],[0]
"Let t, w be parameters.",3.1. Product LSH,[0],[0]
"LSHt,w is a procedure that takes a u ∈ U , and produces a (t + 1)-tuple of integers.",3.1. Product LSH,[0],[0]
"The hash uses as parameters a matrix A of dimensions t × d, whose entries are i.i.d. N",3.1. Product LSH,[0],[0]
"(0, 1) Gaussians, and a collection of shift vectors S = {s1, . . .",3.1. Product LSH,[0],[0]
", sU}, where si is picked uniformly from [0, 4w]t.",3.1. Product LSH,[0],[0]
"The shifts are used to generate a collection of shifted grids Gti := G
t + si, where Gt is the integer grid Zt, scaled by 4w.",3.1. Product LSH,[0],[0]
"Now to compute the hash of a point u, first its projection to Rt is computed by u′ = Au.",3.1. Product LSH,[0],[0]
"Next, one searches for the smallest index",3.1. Product LSH,[0],[0]
i ∈,3.1. Product LSH,[0],[0]
"[U ] for which the ball B(u′, w) contains a point of the shifted grid Gti.",3.1. Product LSH,[0],[0]
"(Alternately one could imagine radius-w balls around the points in the shifted grids, and we look for the smallest i for which the point u′ is contained in one such ball.)",3.1. Product LSH,[0],[0]
"The hash of the point is then (i, x1, . . .",3.1. Product LSH,[0],[0]
", xt), where i is the index as above, and (x1, . . .",3.1. Product LSH,[0],[0]
", xt) are the integer coordinates corresponding to the grid point in Gti that is at distance ≤ w from u′.
(Andoni & Indyk, 2006) show that to cover all of Rt (and thus to have a well-defined hash for every point), the number of shifts that suffice is 2O(t log t).",3.1. Product LSH,[0],[0]
"Consequently, this is also
2The earlier LSH schemes of (Indyk & Motwani, 1998) and (Datar et al., 2004) can also be used; however, they give a weaker approximation factor.
",3.1. Product LSH,[0],[0]
"the time required to compute hash for a point, as we may need to go through all the shifts.",3.1. Product LSH,[0],[0]
"In our setting, we will choose t = o(log n/ log log n), and thus the time needed to hash is no(1).
",3.1. Product LSH,[0],[0]
Product LSH (PLSH).,3.1. Product LSH,[0],[0]
"Given an integer `, the product LSH PLSHt,w,` is a hashing scheme that maps a point u to a concatenation of ` independent copies of LSHt,w; it thus outputs an `(t+ 1)-tuple of integers.
",3.1. Product LSH,[0],[0]
We show the following properties of PLSH.,3.1. Product LSH,[0],[0]
"In what follows, let σ be a parameter.",3.1. Product LSH,[0],[0]
"Let
w = 8σ(log n)3/2; t = log n
(log log n)2 ; ` = 32(log log n)2.
",3.1. Product LSH,[0],[0]
"(2)
Lemma 1. Suppose C ⊆ U has diameter ≤ σ.",3.1. Product LSH,[0],[0]
"Then with probability at least 3/4, PLSHt,w,` maps all the points in C to the same hash value.",3.1. Product LSH,[0],[0]
Lemma 2.,3.1. Product LSH,[0],[0]
"Let u, v be points that are at distance≥ 4w/",3.1. Product LSH,[0],[0]
√ t (= O(log n log log n) · σ).,3.1. Product LSH,[0],[0]
"Then the probability that they have the same hash value is < 1n4 .
",3.1. Product LSH,[0],[0]
Proof of Lemma 1.,3.1. Product LSH,[0],[0]
Let C ′ = {,3.1. Product LSH,[0],[0]
Ax : x ∈ C}.,3.1. Product LSH,[0],[0]
"First, we claim that the diameter ofC ′ is at most 4σ √ t+ log n, w.h.p.",3.1. Product LSH,[0],[0]
over the choice of A.,3.1. Product LSH,[0],[0]
"This is because for any x, y ∈ C, the quantity ‖A(x− y)‖22/‖x− y‖22 is distributed as a χ2 with t degrees of freedom.",3.1. Product LSH,[0],[0]
"It is known (e.g., (Laurent & Massart, 2000), Lemma 1) that the tail of a chi-square statistic Y with t degrees of freedom satisfies: for any z > 0, Pr[Y ≥ t + 2 √ tz + 2z] ≤ e−z .",3.1. Product LSH,[0],[0]
"Setting z = 4 log n, and using 2 √ tz ≤ t+ z, we get that Pr[Y ≥ 16(t+ log n)]",3.1. Product LSH,[0],[0]
< 1/n4.,3.1. Product LSH,[0],[0]
"Thus by taking union bound over all pairs of points x, y, we have that with probability ≥ 1− 1n2 , the diameter of C
′ is ≤ 4σ √ t+ log n.
Conditioned on this, let us calculate the probability that the points in C all have the same hash value.",3.1. Product LSH,[0],[0]
"(The conditioning does not introduce any dependencies, as the above argument depended only on the choice of A, while the next step will depend on the choice of the shifts.)",3.1. Product LSH,[0],[0]
"Now, consider a ball B∗ ⊆ Rt of radius r′ := 4σ √ t+ log n that contains C ′",3.1. Product LSH,[0],[0]
"(as C ′ is of small diameter, such a ball exists).
",3.1. Product LSH,[0],[0]
"Before analyzing the probability of interest, let us understand when a shifted grid Gti contains a point that has distance ≤",3.1. Product LSH,[0],[0]
w to a given point x.,3.1. Product LSH,[0],[0]
This is equivalent to saying that (x−si) is w-close to a lattice point inGt.,3.1. Product LSH,[0],[0]
"This happens iff si is in the ball B(x,w), where the ball has been reduced modulo",3.1. Product LSH,[0],[0]
"[0, 4w]t (see Figure 1).
",3.1. Product LSH,[0],[0]
"Now, we can see how it could happen that some x ∈ B∗ is w-close to a lattice point in Gti but the entirety of B
∗ does not have this property.",3.1. Product LSH,[0],[0]
"Geometrically, the bad choices of si are shown in Figure 1 (before reducing moulo",3.1. Product LSH,[0],[0]
"[0, 4w]t).",3.1. Product LSH,[0],[0]
"Thus, we have that the probability that all points in B∗ are w-close to a lattice point in Gti conditioned on there existing
a point x ∈ B∗ that is close to a lattice point is at least
p1 := (w − r′)t
(w + r′)t =
( 1− 2r ′
w + r′
)t .
",3.1. Product LSH,[0],[0]
"Thus p1 is a lower bound on a single LSH giving the same hash value for all the points inC. Repeating this ` times, and plugging in our choice of values for r′, w, t, `, the desired claim follows.
",3.1. Product LSH,[0],[0]
"Next, we get to the proof of Lemma 2.
",3.1. Product LSH,[0],[0]
Proof.,3.1. Product LSH,[0],[0]
"Let u, v be points as in the statement of the lemma.",3.1. Product LSH,[0],[0]
We show that the probability that ‖A(u− v)‖ ≤ 2w in all the ` hashes is < 1/n4.,3.1. Product LSH,[0],[0]
"This clearly implies what we need, because if in even one LSH we haveAu",3.1. Product LSH,[0],[0]
"andAv being> 2w away, they cannot have the same PLSH.3
Now, for a randomA, the quantity ‖A(u−v)‖22/‖u−v‖22 is distributed as a χ2 distribution with t degrees of freedom (as we saw earlier), and thus using the lower tail from (Laurent & Massart, 2000), we get that for any z > 0, for such a random variable Y , we have Pr[Y ≤ t",3.1. Product LSH,[0],[0]
− 2 √ tz] < e−z .,3.1. Product LSH,[0],[0]
Thus Pr[Y ≤ (1− 1√ 2 )t] ≤ e−t/8.,3.1. Product LSH,[0],[0]
"Now, for our choice of parameters, we have 4w2/‖u",3.1. Product LSH,[0],[0]
− v‖22 ≤ t/4 <,3.1. Product LSH,[0],[0]
"(1 − 1√2 )t, and thus the probability that u and v have the same PLSH is upper bounded by e−t`/8 = 1/n4, as desired.",3.1. Product LSH,[0],[0]
We have shown that the probability that a cluster of diameter ≤ σ hashes to precisely one tuple (for appropriate parameters) is ≥ 3/4.,3.2. Number of tuples for a cluster,[0],[0]
"We now show something slightly stronger (as we will need it later).
",3.2. Number of tuples for a cluster,[0],[0]
Lemma 3.,3.2. Number of tuples for a cluster,[0],[0]
"Let C be a cluster of diameter σ, and let t, w, ` be set as in Eq. (2).",3.2. Number of tuples for a cluster,[0],[0]
"The expected number of distinct tuples
3This reasoning allows us to get a bound slightly better than (Andoni & Indyk, 2006).
for points in C (produced by PLSHt,w,`) is O(1).",3.2. Number of tuples for a cluster,[0],[0]
The PLSH maps each point u ∈ U to an `(t + 1) tuple of integers.,3.3. Second step of hashing,[0],[0]
"The second step of hashing is very simple – we simply hash each tuple independently and uniformly to an integer in [Lk], for a prescribed parameter L.",3.3. Second step of hashing,[0],[0]
We start by describing our algorithm in a single machine setting.,4. Approximation Algorithm,[0],[0]
"Then in Section 4.2, we describe how it can be implemented in parallel, with a small number of machines, and a small memory per machine.",4. Approximation Algorithm,[0],[0]
"The high level idea in our algorithm is to perform the twolevel hashing above, and choose a random subset of points from each bin.
",4.1. Main algorithm,[0],[0]
"Now, in order to choose the w parameter in the hash, we need a rough scale of the optimum.",4.1. Main algorithm,[0],[0]
"To this end, we will assume that we know a D such that the optimum objective value is in the range (D/f,D), for some f = poly(n).",4.1. Main algorithm,[0],[0]
"Note that f can be something like n2, so this is a very mild assumption.",4.1. Main algorithm,[0],[0]
"With this assumption, we have that the average contribution of a point to the objective (i.e., its squared distance to its center) is≥ D/(n · f).",4.1. Main algorithm,[0],[0]
Let us denote r0 := √ D/(n · f).,4.1. Main algorithm,[0],[0]
"Also, observe that no point can have a contribution more than D to the objective (as it is an upper bound on the sum of the contributions).",4.1. Main algorithm,[0],[0]
"Thus, inuitively, all the clusters have a radius (formally defined below) ≤",4.1. Main algorithm,[0],[0]
"√ D. Let κ = dlog(nf)e, and let ri := 2i/2r0, for 1 ≤",4.1. Main algorithm,[0],[0]
i ≤,4.1. Main algorithm,[0],[0]
κ.,4.1. Main algorithm,[0],[0]
These will be the different radius “scales” for our clusters of interest.,4.1. Main algorithm,[0],[0]
"Note that κ = O(log n), as f = poly(n).
",4.1. Main algorithm,[0],[0]
The algorithm can now be described (see Algorithm 4.1).,4.1. Main algorithm,[0],[0]
"Algorithm 1 Find-Cover
Input: set of points U , rough estimate of optimum D. Output: a subset of points S. for i = 1 . . .",4.1. Main algorithm,[0],[0]
"κ do
- Hash every point in U to a bin (range [Lk]) using the two layer hash with params t, wi, `, Lk, where wi := 8ri(log n)
3/2.",4.1. Main algorithm,[0],[0]
"Let Uj be the points hashed to bin j. - Let Gj be the group of machines assigned for bin j. For each j, assign points in Uj uniformly at random to a machine in Gj . -",4.1. Main algorithm,[0],[0]
"For each j, select a uniformly random subset of Uj of size O(1) from Gj and add them to S. (If the number of points in the group is O(1), add all of them to S.)
end for
In the remainder of this section, we analyze this algorithm.",4.1. Main algorithm,[0],[0]
"We start with a definition.
",4.1. Main algorithm,[0],[0]
Definition 1 (Cluster radius).,4.1. Main algorithm,[0],[0]
"For a cluster C with centroid µ, we define the radius to be the quantity ρ :=√
1 |C| ∑ p∈C‖p− µ‖22, i.e., the “`22 average” radius.
",4.1. Main algorithm,[0],[0]
Observation 1.,4.1. Main algorithm,[0],[0]
"In any clusterC with centroid µ and radius ρ, the number of points p in C such that ‖p− µ‖2 > 2ρ is at most |C|/4.
",4.1. Main algorithm,[0],[0]
The proof follows by an averaging argument.,4.1. Main algorithm,[0],[0]
"Now, a candidate goal is to prove that for every optimum cluster Ci (center µi, radius ρi), the algorithm chooses at least one point at a distance ≤ αρi from the center µi with high probability, for some small α.
",4.1. Main algorithm,[0],[0]
Unfortunately this statement is false in general.,4.1. Main algorithm,[0],[0]
Suppose the instance has an optimal cluster with small ρi and a small |Ci| that is really far from the rest of the points (thus it is essential to “find” that cluster).,4.1. Main algorithm,[0],[0]
"In this case, for rj that is roughly ρi (which is the scale at which we hope to find a point close to this cluster), the bin containing Ci may contain many other points that are far away; thus a random sample is unlikely to choose any point close to Ci.
",4.1. Main algorithm,[0],[0]
The fix for this problem comes from the observation that small clusters (i.e. small |Ci|) can afford to pay more per point to the objective.,4.1. Main algorithm,[0],[0]
We thus define the notion of “adjusted radius” of a cluster.,4.1. Main algorithm,[0],[0]
"First, we define θ to be the real number satisfying OPT = nθ2, i.e., the typical distance of a point to its cluster center in the optimal",4.1. Main algorithm,[0],[0]
"clustering.4 Now, we have:
Definition 2 (Adjusted radius).",4.1. Main algorithm,[0],[0]
"For a cluster C with radius ρ, we define the adjusted radius to be the quantity ρ satisfying ρ2 = ρ2 + θ2 + nθ 2
k|C| .
",4.1. Main algorithm,[0],[0]
"Our main lemma about the algorithm is the following.
",4.1. Main algorithm,[0],[0]
Lemma 4.,4.1. Main algorithm,[0],[0]
Let C be a cluster in the optimal clustering with adjusted radius ρ.,4.1. Main algorithm,[0],[0]
"With probability ≥ 1/4, Algorithm 4.1 outputs a point that is at a distance ≤ α · ρ from the center of the cluster C, where α = O(log n log log n).
",4.1. Main algorithm,[0],[0]
"This is used to show the main result of this section.
",4.1. Main algorithm,[0],[0]
Theorem 2.,4.1. Main algorithm,[0],[0]
Let S′ be the union of the sets output by O(log k) independent runs of Algorithm 4.1.,4.1. Main algorithm,[0],[0]
"For α = O(log n log log n), S′ gives an (α2, O(log n log k))",4.1. Main algorithm,[0],[0]
"bicriteria approximation for k-means, w.p. at least 9/10.
",4.1. Main algorithm,[0],[0]
Proof of Theorem 2 assuming Lemma 4.,4.1. Main algorithm,[0],[0]
"First, let us analyze the number of points output.",4.1. Main algorithm,[0],[0]
"Note that in each run of the algorithm, we output O(Lk) = O(k) points for each radius range.",4.1. Main algorithm,[0],[0]
There areO(log n) radius ranges andO(log k) independent runs.,4.1. Main algorithm,[0],[0]
"Thus we have the desired bound.
",4.1. Main algorithm,[0],[0]
"Next, consider the approximation factor.",4.1. Main algorithm,[0],[0]
"As we take S′ to be the union of O(log k) independent runs of Algorithm 4.1,
4We note that θ is used solely for analysis purposes – the algorithm is not assumed to know it.
",4.1. Main algorithm,[0],[0]
"the success probability in Lemma 4 can be boosted to 1− 1 10k , and by a union bound, we have that the conclusion of the lemma holds for all clusters, with probability > 1/10.",4.1. Main algorithm,[0],[0]
"Thus for every optimal cluster Ci of adjusted radius ρi, Algorithm 4.1 outputs at least one point at a distance≤ α·ρi, for α as desired.",4.1. Main algorithm,[0],[0]
"Thus, assigning all the points in Ci to one such point would imply that the points of Ci contribute at most |Ci|ρ2i +α2|Ci|ρ2i to the objective.5 Thus the objective value is at most∑
i
|Ci|ρ2i + α2|Ci| ( ρ2i + θ 2 + nθ2
k|Ci| ) =",4.1. Main algorithm,[0],[0]
"(1 + α2)OPT + α2 · 2nθ2 ≤ 4α2OPT.
",4.1. Main algorithm,[0],[0]
This completes the proof.,4.1. Main algorithm,[0],[0]
"We now see how to implement algorithm from Theorem 2 in a distributed setting with a small number of rounds and machines, while also using memory ≤ s per machine.",4.2. Distributed implementation,[0],[0]
"Our final result is the following.
",4.2. Distributed implementation,[0],[0]
Theorem 3.,4.2. Distributed implementation,[0],[0]
"There is a distributed algorithm that performs dlogs ne + 2 rounds of MAPREDUCE computation, and outputs a bi-criteria approximation to k-means, with the same guarantee as Theorem 2.",4.2. Distributed implementation,[0],[0]
"The number of machines needed is Õ ( n
min{k,s} · k s
) , and the space per machine is
s.
Note.",4.2. Distributed implementation,[0],[0]
"Whenever s ≥ k, the bound on the number of machines is Õ(n/s), which is essentially optimal, because we need n/s machines to hold n points, if each machine has a memory of s.
While most parts of the algorithm from Theorem 2 can be immediately parallelized, sampling from Uj (which may need to be split across machines) is the tricky part and requires some work.",4.2. Distributed implementation,[0],[0]
The proof is deferred to Section 3 of the supplement.,4.2. Distributed implementation,[0],[0]
"We now show that even in the very simple setting of points on a line, we have a tradeoff between the number of rounds and memory.",5. Lower Bound,[0],[0]
"This matches the behavior of our algorithm, up to an additive constant.
Theorem 4.",5. Lower Bound,[0],[0]
Let α be any parameter that is poly(n).,5. Lower Bound,[0],[0]
"Then, any α factor approximation algorithm for k-means with k ≥ 2 that uses poly(n) machines of memory ≤ s requires at least logs n rounds of MAPREDUCE.",5. Lower Bound,[0],[0]
"The proof is by a simple reduction from Boolean-OR,6 a
5This follows from a “center-of-mass” theorem that is standard: for a set T of points with centroid µ, and any other point µ′,∑
u∈T ‖u− µ ′‖2 = ∑ u∈T ‖u− µ‖
2 + |T |‖µ− µ′‖2.",5. Lower Bound,[0],[0]
"6The input is the set of bits x1, . . .",5. Lower Bound,[0],[0]
", xn, and the desired output
problem for which a round-memory trade-off was established in (Roughgarden et al., 2016).
",5. Lower Bound,[0],[0]
Proof.,5. Lower Bound,[0],[0]
"Suppose we have inputs x1, . . .",5. Lower Bound,[0],[0]
", xn, the inputs for Boolean OR.",5. Lower Bound,[0],[0]
"We produce an instance of clustering with k + n points, all on the line.
",5. Lower Bound,[0],[0]
"First, we place points at 1, 2, . . .",5. Lower Bound,[0],[0]
", k. Additionally, for 1 ≤ i ≤ n, if xi = 1, we add a point at k+α+1.",5. Lower Bound,[0],[0]
"If xi = 0, add a point at 1.",5. Lower Bound,[0],[0]
"Now if the OR of the xi’s is TRUE, then the optimum solution places centers at 1, 2, . . .",5. Lower Bound,[0],[0]
", k−1, k+α+1.",5. Lower Bound,[0],[0]
This results in an objective value of 1.,5. Lower Bound,[0],[0]
"If the OR is FALSE, the optimum solution is to place centers at 1, 2, . . .",5. Lower Bound,[0],[0]
", k (0 cost).",5. Lower Bound,[0],[0]
"Thus an α factor approximation should be able to distinguish between the two cases (because in the NO case, it needs to have error 0, and in the YES case, this solution will be a factor",5. Lower Bound,[0],[0]
"> α off.
",5. Lower Bound,[0],[0]
Note.,5. Lower Bound,[0],[0]
The parameter α implicitly comes up in the reduction.,5. Lower Bound,[0],[0]
The number of bits necessary to write down the points xi is n logα.,5. Lower Bound,[0],[0]
"This is why we insist on α = poly(n).
",5. Lower Bound,[0],[0]
"The lower bound above can be extended in order to rule out both (a) the case in which we have a rough estimate of the optimum (as in our algorithm), and (b) bi-criteria approximations.",5. Lower Bound,[0],[0]
"To handle (a), we can perturb the NO case so as to make the objective 1/p(nα) for a large polynomial p(·).",5. Lower Bound,[0],[0]
"In order to handle (b), i.e., to rule out an (α, β) bicriteria approximation, we need to add a multiplicity of βk for each of the points in the proof above.",5. Lower Bound,[0],[0]
This leads to a slightly weaker lower bound of logs n kβ rounds.,5. Lower Bound,[0],[0]
"The details of these steps are straightforward, so we omit them.",5. Lower Bound,[0],[0]
"We evaluate our algorithmic ideas on two synthetic and two real datasets, of varying sizes.",6. Empirical Study,[0],[0]
"In the former case, we know the ground truth clustering, the “right k”, and the optimum objective value.",6. Empirical Study,[0],[0]
We use it to demonstrate how the quality of clustering depends on the parameter ` – the number of independent hashes we concatenate.,6. Empirical Study,[0],[0]
"In all the datasets, we compare the objective value obtained by our algorithm with the one obtained by k-means++ (part of scikit-learn (Pedregosa et al., 2011)).",6. Empirical Study,[0],[0]
"This will only be possible for small enough datasets, as k-means++ is a single machine algorithm that uses Ω(nk) memory.",6. Empirical Study,[0],[0]
Both the datasets we consider are mixtures of Gaussians.,6.1. Synthetic datasets,[0],[0]
"The first has n = 105 points in R50 and k = 100, while the second has n = 106 point in R50 and k = 1000.",6.1. Synthetic datasets,[0],[0]
For i ∈,6.1. Synthetic datasets,[0],[0]
"[k], the centers are chosen uniformly from a box of length 400 in each dimension, maintaining a distance of 20 from one
is simply the OR of the bits.
another.",6.1. Synthetic datasets,[0],[0]
"To form cluster Ci, a random number of points are chosen from the Gaussian N (µi, 1).
",6.1. Synthetic datasets,[0],[0]
"For the first dataset, we produce PLSH tuples using w = 15, t = 2, and vary `.We call the set of points that hash to a given tuple a bucket of points.",6.1. Synthetic datasets,[0],[0]
"We measure the following quantities: (a) the total number of non-empty buckets, (b) the “purity” of the buckets, i.e., the number of distinct clusters at intersect a non-empty bucket (on average), and (c) the “spread” of a cluster, i.e., the number of buckets that points of a cluster go to.",6.1. Synthetic datasets,[0],[0]
The plots for these quantities as ` varies are shown in Figure 2.,6.1. Synthetic datasets,[0],[0]
"Note that it makes sense for the spread to increase as ` increases, as even a difference in one of the ` independent hashes results in unequal hashes.
",6.1. Synthetic datasets,[0],[0]
"Next, we study the objective value.",6.1. Synthetic datasets,[0],[0]
For this we choose ` = 3.,6.1. Synthetic datasets,[0],[0]
This results in 257 non-empty buckets.,6.1. Synthetic datasets,[0],[0]
"Now, from each bucket, we output j points uniformly at random to form a set S (and do this for different j).",6.1. Synthetic datasets,[0],[0]
"Even for j = 1, the objective value is 41079, which is less than a factor 2 away from the optimum, 26820.",6.1. Synthetic datasets,[0],[0]
This is significantly better than the guarantee we get from Theorem 2.,6.1. Synthetic datasets,[0],[0]
"It is also significantly better than a random subset of 257 points, for which it turns out that the objective is 5897317.
",6.1. Synthetic datasets,[0],[0]
"Intuitively, a random sample will be bad for this instance, because there are many clusters of size n/k, and no points from such clusters will be chosen.",6.1. Synthetic datasets,[0],[0]
This motivates us to measure the cluster recall of our procedure – how many clusters contribute to the 257 size set we output?,6.1. Synthetic datasets,[0],[0]
"Interestingly, all 100 of the clusters do, for the above values of the parameters.",6.1. Synthetic datasets,[0],[0]
"These results are consistent with the theoretical observations that PLSH finds small-cardinality clusters while a random sample does not.
",6.1. Synthetic datasets,[0],[0]
"Next, consider the larger synthetic dataset.",6.1. Synthetic datasets,[0],[0]
"Modulo n, k, the data is generated as before.",6.1. Synthetic datasets,[0],[0]
"Here, we produce PLSH tuples using w = 15, t = 3, and ` = 4.",6.1. Synthetic datasets,[0],[0]
"For these choices of n and k, the single-machine k-means++ runs out of memory.",6.1. Synthetic datasets,[0],[0]
"However, as we know the µi, we can estimate the optimum
objective value, which is 251208.
",6.1. Synthetic datasets,[0],[0]
"In this dataset, we illustrate the use of our algorithm to generate a coreset, as discussed in Section 1.2.",6.1. Synthetic datasets,[0],[0]
"We obtain 5722 buckets, from each of which we sample j points to obtain a coreset S. We then run k-means on S with k = 1000, thus obtaining 1000 centers.",6.1. Synthetic datasets,[0],[0]
We evaluate the k-means objective with these centers.,6.1. Synthetic datasets,[0],[0]
Results for different j are shown in Figure 3.,6.1. Synthetic datasets,[0],[0]
"Note that even with j = 10, the objective is within a 1.1 factor of the optimum.",6.1. Synthetic datasets,[0],[0]
"We show our results on two datasets, both available via the UC Irvine dataset repository.
SUSY.",6.2. Real datasets,[0],[0]
"The first dataset is SUSY (see (P. Baldi, 2014)), which contains 5M points with 18 features.",6.2. Real datasets,[0],[0]
"In order to efficiently compare with k-means++, we use a sub-sample of 100000 points.",6.2. Real datasets,[0],[0]
"In this case we use the values of t, ` as in our theoretical results.",6.2. Real datasets,[0],[0]
"We also try different values for w. We start with a guess of w = σ(log n)3/2, where σ was obtained from k-means++ with k = 10 (which is very fast).",6.2. Real datasets,[0],[0]
We then scale σ from 2−4 to 22 in order to perform the hashing in different radius ranges.,6.2. Real datasets,[0],[0]
"After hashing and finding S, we use it as a coreset and compute k-means.",6.2. Real datasets,[0],[0]
"Figure 4 shows the results, and also compares against a fully random subset of points.",6.2. Real datasets,[0],[0]
"Unlike the synthetic examples, here a random set of points is not orders of magnitude worse, but is still considerably worse than the output of our algorithm.",6.2. Real datasets,[0],[0]
We also note that our values are within a factor 1.2 of k-means++ (which is sequential and significantly slower).,6.2. Real datasets,[0],[0]
"The number of buckets per cluster when k = 600 for ` = 1, . . .",6.2. Real datasets,[0],[0]
", 6 are 0.03, 0.31, 1.21, 3.97, 7, 10.55.
",6.2. Real datasets,[0],[0]
"FMA: A Dataset For Music Analysis This dataset (see (Defferrard et al., 2017)) contains 518 features extracted from audio files available in the free music archive (FMA).",6.2. Real datasets,[0],[0]
It has 106574 points.,6.2. Real datasets,[0],[0]
We perform the same experiment we did for the SUSY dataset.,6.2. Real datasets,[0],[0]
"Figure 5 shows
the results, comparing the outputs with the output of kmeans++, as well as a random subset.",6.2. Real datasets,[0],[0]
"The number of buckets per cluster when k = 512 for ` = 1, . . .",6.2. Real datasets,[0],[0]
", 6 are 0.08, 0.68, 2.29, 5.27, 9.43, 14.09 respectively.",6.2. Real datasets,[0],[0]
"Given the importance of clustering in the analysis of large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied.",abstractText,[0],[0]
"A successful approach here has been the “reduce and merge” paradigm, in which each machine reduces its input size to Õ(k), and this data reduction continues (possibly iteratively) until all the data fits on one machine, at which point the problem is solved locally.",abstractText,[0],[0]
"This approach has the intrinsic bottleneck that each machine must solve a problem of size ≥ k, and needs to communicate at least Ω(k) points to the other machines.",abstractText,[0],[0]
"We propose a novel data partitioning idea to overcome this bottleneck, and in effect, have different machines focus on “finding different clusters”.",abstractText,[0],[0]
"Under the assumption that we know the optimum value of the objective up to a poly(n) factor (arbitrary polynomial), we establish worst-case approximation guarantees for our method.",abstractText,[0],[0]
We see that our algorithm results in lower communication as well as a near-optimal number of ‘rounds’ of computation (in the popular MapReduce framework).,abstractText,[0],[0]
Distributed Clustering via LSH Based Data Partitioning,title,[0],[0]
Given n vectors Xn def=,1.1. Background,[0],[0]
"X 1 , X 2 . .",1.1. Background,[0],[0]
.,1.1. Background,[0],[0]
", Xn 2 Rd that reside on n clients, the goal of distributed mean estimation is to estimate the mean of the vectors:
¯X def =
1
n
nX
i=1
Xi.",1.1. Background,[0],[0]
"(1)
This basic estimation problem is used as a subroutine in several learning and optimization tasks where data is distributed across several clients.",1.1. Background,[0],[0]
"For example, in Lloyd’s algorithm (Lloyd, 1982) for k-means clustering, if data is distributed across several clients, the server needs to compute
1Google Research, New York, NY, USA 2Google Research, Seattle, WA, USA.",1.1. Background,[0],[0]
"Correspondence to: Ananda Theertha Suresh <theertha@google.com>.
",1.1. Background,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1.1. Background,[0],[0]
"Copyright 2017 by the author(s).
",1.1. Background,[0],[0]
the means of all clusters in each update step.,1.1. Background,[0],[0]
"Similarly, for PCA, if data samples are distributed across several clients, then for the power-iteration method, the server needs to average the output of all clients in each step.
",1.1. Background,[0],[0]
"Recently, algorithms involving distributed mean estimation have been used extensively in training large-scale neural networks and other statistical models (McDonald et al., 2010; Povey et al., 2014; Dean et al., 2012; McMahan et al., 2016; Alistarh et al., 2016).",1.1. Background,[0],[0]
"In a typical scenario of synchronized distributed learning, each client obtains a copy of a global model.",1.1. Background,[0],[0]
The clients then update the model independently based on their local data.,1.1. Background,[0],[0]
"The updates (usually in the form of gradients) are then sent to a server, where they are averaged and used to update the global model.",1.1. Background,[0],[0]
A critical step in all of the above algorithms is to estimate the mean of a set of vectors as in Eq.,1.1. Background,[0],[0]
"(1).
",1.1. Background,[0],[0]
One of the main bottlenecks in distributed algorithms is the communication cost.,1.1. Background,[0],[0]
"This has spurred a line of work focusing on communication cost in learning (Tsitsiklis & Luo, 1987; Balcan et al., 2012; Zhang et al., 2013; Arjevani & Shamir, 2015; Chen et al., 2016).",1.1. Background,[0],[0]
"The communication cost can be prohibitive for modern applications, where each client can be a low-power and low-bandwidth device such as a mobile phone (Konečnỳ et al., 2016).",1.1. Background,[0],[0]
"Given such a wide set of applications, we study the basic problem of achieving the optimal minimax rate in distributed mean estimation with limited communication.
",1.1. Background,[0],[0]
"We note that our model and results differ from previous works on mean estimation (Zhang et al., 2013; Garg et al., 2014; Braverman et al., 2016) in two ways: previous works assume that the data is generated i.i.d.",1.1. Background,[0],[0]
according to some distribution; we do not make any distribution assumptions on data.,1.1. Background,[0],[0]
"Secondly, the objective in prior works is to estimate the mean of the underlying statistical model; our goal is to estimate the empirical mean of the data.",1.1. Background,[0],[0]
"Our proposed communication algorithms are simultaneous and independent, i.e., the clients independently send data to the server and they can transmit at the same time.",1.2. Model,[0],[0]
"In any independent communication protocol, each client transmits a function of Xi (say f(Xi)), and a central server estimates the mean by some function of f(X
1 ), f(X 2 ), . . .",1.2. Model,[0],[0]
", f(Xn).
",1.2. Model,[0],[0]
"Let ⇡ be any such protocol and let Ci(⇡, Xi) be the expected number of transmitted bits by the i-th client during protocol ⇡, where throughout the paper, expectation is over the randomness in protocol ⇡.
",1.2. Model,[0],[0]
"The total number of bits transmitted by all clients with the protocol ⇡ is
C(⇡, Xn) def= nX
i=1
Ci(⇡, Xi).
",1.2. Model,[0],[0]
Let the estimated mean be ˆ¯X .,1.2. Model,[0],[0]
"For a protocol ⇡, the MSE of the estimate is
E(⇡, Xn) = E  ˆ¯X ¯X 2
2
.
",1.2. Model,[0],[0]
We allow the use of both private and public randomness.,1.2. Model,[0],[0]
"Private randomness refers to random values that are generated by each machine separately, and public randomness refers to a sequence of random values that are shared among all parties1.
",1.2. Model,[0],[0]
The proposed algorithms work for any Xn.,1.2. Model,[0],[0]
"To measure the minimax performance, without loss of generality, we restrict ourselves to the scenario where each Xi 2 Sd, the ball of radius 1 in Rd, i.e., X 2 Sd iff
||X|| 2  1, where ||X||
2
denotes the ` 2 norm of the vector X .",1.2. Model,[0],[0]
"For a protocol ⇡, the worst case error for all Xn 2 Sd is
E(⇡, Sd) def=",1.2. Model,[0],[0]
"max Xn:Xi2Sd 8i E(⇡, Xn).
",1.2. Model,[0],[0]
Let ⇧(c) denote the set of all protocols with communication cost at most c.,1.2. Model,[0],[0]
"The minimax MSE is
E(⇧(c), Sd) def=",1.2. Model,[0],[0]
"min ⇡2⇧(c) E(⇡, Sd).",1.2. Model,[0],[0]
"We first analyze the MSE E(⇡, Xn) for three algorithms, when C(⇡, Xn) = ⇥(nd), i.e., each client sends a constant number of bits per dimension.
",1.3.1. ALGORITHMS,[0],[0]
Stochastic uniform quantization.,1.3.1. ALGORITHMS,[0],[0]
"In Section 2.1, as a warm-up we first show that a naive stochastic binary quantization algorithm (denoted by ⇡sb) achieves an MSE of
E(⇡sb, Xn) = ⇥",1.3.1. ALGORITHMS,[0],[0]
"d
n · 1 n
nX
i=1
||Xi||2 2
!",1.3.1. ALGORITHMS,[0],[0]
",
1In the absence of public randomness, the server can communicate a random seed that can be used by clients to emulate public randomness.
and C(⇡sb, Xn) = n · (d + ˜O(1))2, i.e., each client sends one bit per dimension.",1.3.1. ALGORITHMS,[0],[0]
We further show that this bound is tight.,1.3.1. ALGORITHMS,[0],[0]
"In many practical scenarios, d is much larger than n and the above error is prohibitive (Konečnỳ et al., 2016).
",1.3.1. ALGORITHMS,[0],[0]
A natural way to decease the error is to increase the number of levels of quantization.,1.3.1. ALGORITHMS,[0],[0]
"If we use k levels of quantization, in Theorem 2, we show that the error deceases as
E(⇡sk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O
d
n(k 1)2 · 1 n
nX
i=1
||Xi||2 2
! .",1.3.1. ALGORITHMS,[0],[0]
"(2)
However, the communication cost would increase to C(⇡sk, Xn) = n ·",1.3.1. ALGORITHMS,[0],[0]
"(ddlog
2 ke + ˜O(1)) bits, which can be expensive, if we would like the MSE to be o(d/n).
",1.3.1. ALGORITHMS,[0],[0]
"In order to reduce the communication cost, we propose two approaches.
",1.3.1. ALGORITHMS,[0],[0]
Stochastic rotated quantization: We show that preprocessing the data by a random rotation reduces the mean squared error.,1.3.1. ALGORITHMS,[0],[0]
"Specifically, in Theorem 3, we show that this new scheme (denoted by ⇡srk) achieves an MSE of
E(⇡srk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O
log d
n(k",1.3.1. ALGORITHMS,[0],[0]
"1)2 · 1 n
nX
i=1
||Xi||2 2
! , 3
and has a communication cost of C(⇡srk, Xn) = n ·",1.3.1. ALGORITHMS,[0],[0]
"(ddlog
2 ke + ˜O(1)).",1.3.1. ALGORITHMS,[0],[0]
"Note that the new scheme achieves much smaller MSE than naive stochastic quantization for the same communication cost.
",1.3.1. ALGORITHMS,[0],[0]
Variable length coding: Our second approach uses the same quantization as ⇡sk but encodes levels via variable length coding.,1.3.1. ALGORITHMS,[0],[0]
"Instead of using dlog
2 ke bits per dimension, we show that using variable length encoding such as arithmetic coding to compress the data reduces the communication cost significantly.",1.3.1. ALGORITHMS,[0],[0]
"In particular, in Theorem 4 we show that there is a scheme (denoted by ⇡svk) such that
C(⇡svk, Xn)",1.3.1. ALGORITHMS,[0],[0]
= O(nd(1 + log(k2/d+ 1)),1.3.1. ALGORITHMS,[0],[0]
"+ ˜O(n)), (3) and E(⇡svk, Xn) = E(⇡sk, Xn).",1.3.1. ALGORITHMS,[0],[0]
"Hence, setting k = p d in Eqs.",1.3.1. ALGORITHMS,[0],[0]
"2 and 3 yields
E(⇡svk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O 1 n · 1 n
nX
i=1
||Xi||2 2
! ,
and with ⇥(nd) bits of communication i.e., constant number of bits per dimension per client.",1.3.1. ALGORITHMS,[0],[0]
"Of the three protocols, ⇡svk has the best MSE for a given communication cost.",1.3.1. ALGORITHMS,[0],[0]
Note that ⇡svk uses k quantization levels but still uses O(1) bits per dimension per client for all k  pd.,1.3.1. ALGORITHMS,[0],[0]
"Theoretically, while variable length coding has better guarantees, stochastic rotated quantization has several practical
2We use ˜O(1) to denote O(log(dn)).",1.3.1. ALGORITHMS,[0],[0]
"3All logarithms are to base e, unless stated.
advantages: it uses fixed length coding and hence can be combined with encryption schemes for privacy preserving secure aggregation (Bonawitz et al., 2016).",1.3.1. ALGORITHMS,[0],[0]
"It can also provide lower quantization error in some scenarios due to better constants (see Section 7 for details).
",1.3.1. ALGORITHMS,[0],[0]
"Concurrent to this work, Alistarh et al. (2016) showed that stochastic quantization and Elias coding can be used to obtain communication-optimal SGD.",1.3.1. ALGORITHMS,[0],[0]
"Recently, Konečnỳ & Richtárik (2016) showed that ⇡sb can be improved further by optimizing the choice of stochastic quantization boundaries.",1.3.1. ALGORITHMS,[0],[0]
"However, their results depend on the number of bits necessary to represent a float, whereas ours do not.",1.3.1. ALGORITHMS,[0],[0]
"In the above protocols, all of the clients transmit the data.",1.3.2. MINIMAX MSE,[0],[0]
"We augment these protocols with a sampling procedure, where only a random fraction of clients transmit data.",1.3.2. MINIMAX MSE,[0],[0]
"We show that a combination of k-level quantization, variable length coding, and sampling can be used to achieve information theoretically optimal MSE for a given communication cost.",1.3.2. MINIMAX MSE,[0],[0]
"In particular, combining Corollary 1 and Theorem 5 yields our minimax result: Theorem 1.",1.3.2. MINIMAX MSE,[0],[0]
"There exists a universal constant t < 1 such that for communication cost c  ndt and n 1/t,
E(⇧(c), Sd) = ⇥",1.3.2. MINIMAX MSE,[0],[0]
"✓ min ✓ 1, d
c
◆◆ .
",1.3.2. MINIMAX MSE,[0],[0]
"This result shows that the product of communication cost and MSE scales linearly in the number of dimensions.
",1.3.2. MINIMAX MSE,[0],[0]
The rest of the paper is organized as follows.,1.3.2. MINIMAX MSE,[0],[0]
We first analyze the stochastic uniform quantization technique in Section 2.,1.3.2. MINIMAX MSE,[0],[0]
"In Section 3, we propose the stochastic rotated quantization technique, and in Section 4 we analyze arithmetic coding.",1.3.2. MINIMAX MSE,[0],[0]
"In Section 5, we combine the above algorithm with a sampling technique and state the upper bound on the minimax risk, and in Section 6 we state the matching minimax lower bounds.",1.3.2. MINIMAX MSE,[0],[0]
"Finally, in Section 7 we discuss some practical considerations and apply these algorithms on distributed power iteration and Lloyd’s algorithm.",1.3.2. MINIMAX MSE,[0],[0]
"For a vector Xi, let Xmaxi = max1jd Xi(j) and similarly let Xmini = min1jd Xi(j).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In the stochastic binary quantization protocol ⇡sb, for each client i, the quantized value for each coordinate j is generated independently with private randomness as
Yi(j) =
( Xmaxi w.p.
Xi(j) Xmini Xmaxi Xmini ,
Xmini otherwise.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Observe EYi(j) = Xi(j).,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡sb = 1
n
nX
i=1
Yi.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We first bound the communication cost of the this protocol.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 1.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists an implementation of stochastic binary quantization that uses d + ˜O(1) bits per client and hence C(⇡sb, Xn)  n · ⇣ d+ ˜O(1) ⌘ .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Proof.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Instead of sending vectors Yi, clients transmit two real values Xmaxi and Xmini (to a desired error) and a bit vector Y 0i such that Y 0i",2.1. Warm-up: Stochastic binary quantization,[0],[0]
(j) = 1 if Yi = Xmaxi and 0 otherwise.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Hence each client transmits d + 2r bits, where r is the number of bits to transmit the real value to a desired error.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Let B be the maximum norm of the underlying vectors.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"To bound r, observe that using r bits, one can represent a number between B and B to an error of B/2r 1.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Thus using 3 log
2 (dn) + 1 bits one can represent the minimum and maximum to an additive error of B/(nd)3.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
This error in transmitting minimum and maximum of the vector does not affect our calculations and we ignore it for simplicity.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We note that in practice, each dimension of Xi is often stored as a 32 bit or 64 bit float, and r should be set as either 32 or 64.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In this case, using an even larger r does not further reduce the error.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We now compute the estimation error of this protocol.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For any set of vectors Xn,
E(⇡sb, Xn) = 1 n2
nX
i=1
dX
j=1
(Xmaxi Xi(j))(Xi(j) Xmini ).
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"E(⇡sb, Xn) = E ˆ¯X ¯X
2
2
=
1 n2 E
nX
i=1
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"(Yi Xi)
2
2
=
1
n2
nX
i=1
E ||Yi Xi||2 2 ,
where the last equality follows by observing that Yi Xi, 8i, are independent zero mean random variables.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof follows by observing that for every i,
E ||Yi Xi||2 2 =
dX
j=1
E[(Yi(j) Xi(j))2]
=
dX
j=1
(Xmaxi Xi(j))(Xi(j) Xmini ).
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 2 implies the following upper bound.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 3.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For any set of vectors Xn,
E(⇡sb, Xn)  d 2n · 1 n
nX
i=1
||Xi||2 2 .
Proof.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof follows by Lemma 2 observing that 8j
(Xmaxi Xi(j))(Xi(j) Xmini )  (Xmaxi Xmini )2
4
,
and (Xmaxi Xmini )2  2 ||Xi||2
2
.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"(4)
We also show that the above bound is tight: Lemma 4.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists a set of vectors Xn such that
E(⇡sb, Xn) d 2 2n · 1 n
nX
i=1
||Xi||2 2 .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Proof.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every i, let Xi be defined as follows.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Xi(1),2.1. Warm-up: Stochastic binary quantization,[0],[0]
"= 1/ p 2, Xi(2)",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"= 1/ p 2, and for all j > 2, Xi(j) = 0.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every i, Xmaxi = 1p 2 and Xmini = 1p 2
.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Substituting these bounds in the conclusion of Lemma 2 (which is an equality) yields the theorem.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Therefore, the simple algorithm proposed in this section gives MSE ⇥(d/n).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Such an error is too large for realworld use.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For example, in the application of neural networks (Konečnỳ et al., 2016), d can be on the order of millions, yet n can be much smaller than that.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In such cases, the MSE is even larger than the norm of the vector.
2.2.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Stochastic k-level quantization
A natural generalization of binary quantization is k-level quantization.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Let k be a positive integer larger than 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
We propose a k-level stochastic quantization scheme ⇡sk to quantize each coordinate.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Recall that for a vector Xi, Xmaxi = max1jd Xi(j) and Xmini = min1jd Xi(j).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every integer r in the range [0, k), let
Bi(r) def = Xmini + rsi",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"k 1 ,
where si satisfies Xmini + si Xmaxi .",2.1. Warm-up: Stochastic binary quantization,[0],[0]
A natural choice for si would be Xmaxi Xmini .4,2.1. Warm-up: Stochastic binary quantization,[0],[0]
The algorithm quantizes each coordinate into one of Bi(r)s stochastically.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In ⇡sk, for the i-th client and j-th coordinate, if Xi(j) 2 [Bi(r), Bi(r + 1)),
Yi(j) =
( Bi(r + 1) w.p.
Xi(j) Bi(r) Bi(r+1) Bi(r)
Bi(r) otherwise.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"4We will show in Section 4, however, a higher value of si and variable length coding has better guarantees.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡sk = 1
n
nX
i=1
Yi.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"As before, the communication complexity of this protocol is bounded.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof is similar to that of Lemma 1 and hence omitted.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 5.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists an implementation of stochastic klevel quantization that uses ddlog(k)e+ ˜O(1) bits per client and hence C(⇡sk, Xn)  n ·",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"⇣ ddlog
2
ke+ ˜O(1) ⌘ .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
The mean squared loss can be bounded as follows.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
Theorem 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"If Xmaxi Xmini  si  p 2 ||Xi||
2 8i, then for any Xn, the ⇡sk protocol satisfies,
E(⇡sk, Xn)  ",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"d 2n(k 1)2 · 1 n
nX
i=1
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"||Xi||2 2 .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"E(⇡sk, Xn) = E ˆ¯X ¯X
2
2
=
1 n2 E
nX
i=1
(Yi Xi)
2
2
=
1
n2
nX
i=1
E ||Yi Xi||2 2  1 n2
nX
i=1
d s2i
4(k 1)2 , (5)
where the last equality follows by observing Yi(j) Xi(j) is an independent zero mean random variable with E(Yi(j) Xi(j))2  s 2 i",2.1. Warm-up: Stochastic binary quantization,[0],[0]
4(k 1)2 .,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"si  p 2 ||Xi|| 2
completes the proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
We conclude this section by noting that si = Xmaxi Xmini satisfies the conditions for the above theorem by Eq. (4).,2.1. Warm-up: Stochastic binary quantization,[0],[0]
We show that the algorithm of the previous section can be significantly improved by a new protocol.,3. Stochastic rotated quantization,[0],[0]
"The motivation comes from the fact that the MSE of stochastic binary quantization and stochastic k-level quantization is O( dn (X max
i Xmini )2) (the proof of Lemma 3 and Theorem 2 with si = Xmaxi Xmini ).",3. Stochastic rotated quantization,[0],[0]
Therefore the MSE is smaller when Xmaxi and Xmaxi are close.,3. Stochastic rotated quantization,[0],[0]
"For example, when Xi is generated uniformly on the unit sphere, with
high probability, Xmaxi Xmini is O ✓q log d d ◆ (Dasgupta
& Gupta, 2003).",3. Stochastic rotated quantization,[0],[0]
"In such case, E(⇡sk, Xn) is O( log dn ) instead of O( dn ).",3. Stochastic rotated quantization,[0],[0]
"In this section, we show that even without any assumptions on the distribution of the data, we can “reduce” Xmaxi Xmini with a structured random rotation, yielding
an O( log dn ) error.",3. Stochastic rotated quantization,[0],[0]
"We call the method stochastic rotated quantization and denote it by ⇡srk.
",3. Stochastic rotated quantization,[0],[0]
"Using public randomness, all clients and the central server generate a random rotation matrix (random orthogonal matrix) R 2 Rd⇥d according to some known distribution.",3. Stochastic rotated quantization,[0],[0]
Let Zi = RXi and ¯Z = R ¯X .,3. Stochastic rotated quantization,[0],[0]
"In the stochastic rotated quantization protocol ⇡srk(R), clients quantize the vectors Zi instead of Xi and transmit them similar to ⇡srk.",3. Stochastic rotated quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡srk = R 1 ˆ
¯Z, ˆ¯Z = 1
n
nX
i=1
Yi.
",3. Stochastic rotated quantization,[0],[0]
The communication cost is same as ⇡sk and is given by Lemma 5.,3. Stochastic rotated quantization,[0],[0]
"We now bound the MSE.
Lemma 6.",3. Stochastic rotated quantization,[0],[0]
"For any Xn, E(⇡srk(R), Xn) is at most
d 2n2(k 1)2 nX
i=1
ER h (Zmaxi ) 2 + Zmini 2 i ,
where Zi = RXi and for every i, let si = Zmaxi Zmini .
",3. Stochastic rotated quantization,[0],[0]
"Proof.
",3. Stochastic rotated quantization,[0],[0]
"E(⇡srk, Xn) = E⇡ ˆ¯X ¯X
2
= E⇡ R 1",3. Stochastic rotated quantization,[0],[0]
"ˆ¯Z R 1 ¯Z
2 (a) = E⇡ ˆ¯Z ¯Z 2
(b) = ERE⇡  ˆ¯Z ¯Z 2 |Zn 1
 d 4n2(k",3. Stochastic rotated quantization,[0],[0]
"1)2
nX
i=1
ER[(Zmaxi Zmini )2],
where the last inequality follows Eq. (5) and the value of si.",3. Stochastic rotated quantization,[0],[0]
"(a) follows from the fact that rotation does not change the norm of the vector, and (b) follows from the tower law of expectation.",3. Stochastic rotated quantization,[0],[0]
"The lemma follows from observing that
(Zmaxi Zmini )2  2(Zmaxi )2 + 2(Zmini )2.
To obtain strong bounds, we need to find an orthogonal matrix R that achieves low (Zmaxi )2 and (Zmini )2.",3. Stochastic rotated quantization,[0],[0]
"In addition, due to the fact that d can be huge in practice, we need a type of orthogonal matrix that permits fast matrix-vector products.",3. Stochastic rotated quantization,[0],[0]
Naive orthogonal matrices that support fast multiplication such as block-diagonal matrices often result in high values of (Zmaxi )2 and (Zmini )2.,3. Stochastic rotated quantization,[0],[0]
"Motivated by recent works of structured matrices (Ailon & Chazelle, 2006; Yu et al., 2016), we propose to use a special type of orthogonal matrix R = HD, where D is a random diagonal matrix with i.i.d.",3. Stochastic rotated quantization,[0],[0]
Rademacher entries (±1 with probability 0.5).,3. Stochastic rotated quantization,[0],[0]
"H is a Walsh-Hadamard matrix (Horadam, 2012).",3. Stochastic rotated quantization,[0],[0]
"The WalshHadamard matrix of dimension 2m for m 2 N is given by
the recursive formula,
H(21) =
 1 1
1 1 , H(2m) =  H(2m 1) H(2m 1) H(2m 1) H(2m 1) .
",3. Stochastic rotated quantization,[0],[0]
Both applying the rotation and inverse rotation take O(d log d) time and O(1) additional space (with an inplace algorithm).,3. Stochastic rotated quantization,[0],[0]
"The next lemma bounds E (Zmaxi )
2 and E Zmini 2 for this choice of R. The lemma is similar to that of Ailon & Chazelle (2006), and we give the proof in Appendix A for completeness.",3. Stochastic rotated quantization,[0],[0]
Lemma 7.,3. Stochastic rotated quantization,[0],[0]
"Let R = HD, where D is a diagonal matrix with independent Radamacher random variables.",3. Stochastic rotated quantization,[0],[0]
"For every i and every sequence Xn,
E ⇥",3. Stochastic rotated quantization,[0],[0]
(Zmini ) 2 ⇤ = E ⇥,3. Stochastic rotated quantization,[0],[0]
"(Zmaxi ) 2 ⇤  ||Xi|| 2 2 (2 log d+ 2)
d .
",3. Stochastic rotated quantization,[0],[0]
Combining the above two lemmas yields the main result.,3. Stochastic rotated quantization,[0],[0]
Theorem 3.,3. Stochastic rotated quantization,[0],[0]
"For any Xn, ⇡srk(HD) protocol satisfies,
E(⇡srk(HD), Xn)  2 log d+ 2 n(k 1)2 · 1 n
nX
i=1
||Xi||2 2 .",3. Stochastic rotated quantization,[0],[0]
"Instead of preprocessing the data via a rotation matrix as in ⇡srk, in this section we propose to use a variable length coding strategy to minimize the number of bits.
Consider the stochastic k-level quantization technique.",4. Variable length coding,[0],[0]
"A natural way of transmitting Yi is sending the bin number for each coordinate, thus the total number of bits the algorithm sends per transmitted coordinate would be ddlog
2 ke.",4. Variable length coding,[0],[0]
This naive implementation is sub-optimal.,4. Variable length coding,[0],[0]
"Instead, we propose to further encode the transmitted values using universal compression schemes (Krichevsky & Trofimov, 1981; Falahatgar et al., 2015).",4. Variable length coding,[0],[0]
"We first encode hr, the number of times each quantized value r has appeared, and then use arithmetic or Huffman coding corresponding to the distribution pr = hrd .",4. Variable length coding,[0],[0]
We denote this scheme by ⇡svk.,4. Variable length coding,[0],[0]
"Since we quantize vectors the same way in ⇡sk and ⇡svk, the MSE of ⇡svk is also given by Theorem 2.",4. Variable length coding,[0],[0]
We now bound the communication cost.,4. Variable length coding,[0],[0]
Theorem 4.,4. Variable length coding,[0],[0]
Let si = p 2 ||Xi||.,4. Variable length coding,[0],[0]
"There exists an implementation of ⇡svk such that C(⇡svk, Xn) is at most
n ✓ d ✓ 2 + log2 ✓ (k 1)2
2d +
5
4
◆◆ + k log2
(d+ k)e",4. Variable length coding,[0],[0]
"k +
˜O(1) ◆ .
",4. Variable length coding,[0],[0]
Proof.,4. Variable length coding,[0],[0]
"As in Lemma 1, ˜O(1) bits are used to transmit the si’s and Xmini .",4. Variable length coding,[0],[0]
"Recall that hr is the number of coordinates that are quantized into bin r, and r takes k possible values.",4. Variable length coding,[0],[0]
"Furthermore, P r hr = d.",4. Variable length coding,[0],[0]
"Thus the number of bits
necessary to represent the hr’s is ⇠ log
2 ✓ d+",4. Variable length coding,[0],[0]
"k 1 k 1 ◆⇡  k log 2 (d+ k)e k .
",4. Variable length coding,[0],[0]
"Once we have compressed the hr’s, we use arithmetic coding corresponding to the distribution pr = hr/d to compress and transmit bin values for each coordinate.",4. Variable length coding,[0],[0]
"The total number of bits arithmetic coding uses is (MacKay, 2003)
d k 1X
r=0
hr d log 2 d hr + 2.
",4. Variable length coding,[0],[0]
"Let pr = hr/d, a = (k 1)Xmini , b = si, and =Pk 1 r=0 1/((a+ br) 2 + ).",4. Variable length coding,[0],[0]
"Note that
X
r
pr log 2
1
pr =
X
r
pr log 2
1/(((a+ br)2 + ) )
",4. Variable length coding,[0],[0]
"pr
+
X
r
pr log 2 (((a+ br)2 + ) )
 ",4. Variable length coding,[0],[0]
"X
r
pr log 2 (((a+ br)2 + ) )
 log 2 (
X
r
pr(a+ br) 2
+ ) + log 2
,
where the first inequality follows from the positivity of KLdivergence.",4. Variable length coding,[0],[0]
"Choosing = s2i , yields  4/s2i and hence, X
r
pr log 2
1 pr  log 2 (
X
r
pr(a+br) 2 +s2i )+",4. Variable length coding,[0],[0]
"log2(4/s 2 i ).
",4. Variable length coding,[0],[0]
"Note that if Yi(j) belongs to bin r, (a + br)2 = (k 1)
2Y 2i (j).",4. Variable length coding,[0],[0]
Recall that hr is the number of coordinates quantized into bin r.,4. Variable length coding,[0],[0]
"Hence P r hr(a + br)
2 is the scaled norm-square of Yi, i.e.,
X
r
hr(a+ br) 2
=",4. Variable length coding,[0],[0]
"(k 1)2 dX
j=1
Y 2i (j)
=
dX
j=1
((Xi(j) + ↵(j))(k 1))2 ,
where the ↵(j) = Yi(j) Xi(j).",4. Variable length coding,[0],[0]
"Taking expectations on both sides and using the fact that the ↵(j) are independent zero mean random variables over a range of si/(k 1), we get
E X
r
hr(a+ br) 2 =
dX
j=1
E(Xi(j)2 + ↵(j)2)(k 1)2
 ||Xi||2 2
✓ (k 1)2 + d
2
◆ .
",4. Variable length coding,[0],[0]
"Using Jensen’s inequality yields the result.
",4. Variable length coding,[0],[0]
"Thus if k = p d + 1, the communication complexity is O(nd) and the MSE is O(1/n).",4. Variable length coding,[0],[0]
"In the above protocols, all the clients transmit and hence the communication cost scales linearly with n. Instead, we show that any of the above protocols can be combined by client sampling to obtain trade-offs between the MSE and the communication cost.",5. Communication MSE trade-off,[0],[0]
"Note that similar analysis also holds for sampling the coordinates.
",5. Communication MSE trade-off,[0],[0]
"Let ⇡ be a protocol where the mean estimate is of the form:
ˆ ¯X = R 1 1
n
nX
i=1
Yi.",5. Communication MSE trade-off,[0],[0]
"(6)
All three protocols we have discussed are of this form.",5. Communication MSE trade-off,[0],[0]
Let ⇡p be the protocol where each client participates independently with probability p.,5. Communication MSE trade-off,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡p = R 1 · 1
np
X i2S Yi,
where Yis are defined in the previous section and S is the set of clients that transmitted.",5. Communication MSE trade-off,[0],[0]
Lemma 8.,5. Communication MSE trade-off,[0],[0]
"For any set of vectors Xn and protocol ⇡ of the form Equation (6), its sampled version ⇡p satisfies
E(⇡p, Xn) = 1 p · E(⇡, Xn) + 1 p np
nX
i=1
||Xi||2 2 .
and C(⇡p, Xn) = p · C(⇡, Xn).
",5. Communication MSE trade-off,[0],[0]
Proof.,5. Communication MSE trade-off,[0],[0]
"The proof of communication cost follows from Lemma 5 and the fact that in expectation, np clients transmit.",5. Communication MSE trade-off,[0],[0]
We now bound the MSE.,5. Communication MSE trade-off,[0],[0]
Let S be the set of clients that transmit.,5. Communication MSE trade-off,[0],[0]
"The error E(⇡p, Xn) is
E  ˆ¯X ¯X 2
2
= E
2
4 1
np
X i2S R 1Yi ¯X 2
2
3
5
=E
2
4 1
np
X i2S Xi ¯X 2
2
+
1
n2p2
X i2S (R 1Yi Xi) 2
2
3
5 ,
where the last equality follows by observing that R 1Yi Xi are independent zero mean random variables and hence for any i, E[(R",5. Communication MSE trade-off,[0],[0]
1Yi Xi)T ( P i2S Xi ¯X)] = 0.,5. Communication MSE trade-off,[0],[0]
"The first term can be bounded as
E 1
np
X i2S Xi ¯X 2
2
=
1
n2
nX
i=1
E 1
p Xi i2S Xi
2
2
=
1
n2
nX
i=1
✓ p (1 p)2
p2 ||Xi||2 2 + (1 p) ||Xi||2 2
◆
= 1 p np · 1 n
nX
i=1
||Xi||2 2 .
",5. Communication MSE trade-off,[0],[0]
"Furthermore, the second term can be bounded as
E
2
4 1 n2p2
X i2S (R 1Yi Xi) 2
2
3
5
(a) = 1
n2p2
X i2S E h (R 1Yi Xi) 2 2 i
=
1
n2p2
nX
i=1
E h
(R 1Yi Xi) 2 2 i2S
i
=
1
n2p
nX
i=1
E h",5. Communication MSE trade-off,[0],[0]
"R 1Yi Xi
2 2
i
=
1 n2p E
2
4
nX
i=1
(R 1Yi Xi)
2
2
3
5 = 1 p E(⇡, Xn)
where the last equality follows from the assumption that ⇡’s mean estimate is of the form (6).",5. Communication MSE trade-off,[0],[0]
"(a) follows from the fact that R 1Yi Xi are independent zero mean random variables.
",5. Communication MSE trade-off,[0],[0]
"Combining the above lemma with Theorem 4, and choosing k = p d+ 1 results in the following.",5. Communication MSE trade-off,[0],[0]
Corollary 1.,5. Communication MSE trade-off,[0],[0]
"For every c  nd(2+log 2
(7/4)), there exists a protocol ⇡",5. Communication MSE trade-off,[0],[0]
"such that C(⇡, Sd)  c and
E(⇡, Sd) =",5. Communication MSE trade-off,[0],[0]
"O ✓ min ✓ 1, d
c
◆◆ .",5. Communication MSE trade-off,[0],[0]
The lower bound relies on the lower bounds on distributed statistical estimation due to Zhang et al. (2013).,6. Lower bounds,[0],[0]
"Lemma 9 ((Zhang et al., 2013) Proposition 2).",6. Lower bounds,[0],[0]
"There exists a set of distributions Pd supported on h 1p
d ,",6. Lower bounds,[0],[0]
"1p d
id such
that if any centralized server wishes to estimate the mean of the underlying unknown distribution, then for any independent protocol ⇡
max pd2Pd E  ✓(pd) ˆ✓⇡ 2 2
tmin ✓ 1, d C(⇡) ◆ ,
where C(⇡) is the communication cost of the protocol, ✓(pd) is the mean of pd, and t is a positive constant.",6. Lower bounds,[0],[0]
Theorem 5.,6. Lower bounds,[0],[0]
Let t be the constant in Lemma 9.,6. Lower bounds,[0],[0]
"For every c  ndt/4 and n 4/t,
E(⇧(c), Sd) t 4 min
✓ 1, d
c
◆ .
",6. Lower bounds,[0],[0]
Proof.,6. Lower bounds,[0],[0]
"Given n samples from the underlying distribution where each sample belongs to Sd, it is easy to see that
E ✓(pd) ˆ✓(pd)
2
2
 1 n ,
0 1 2 3 4 5 6 −25
−20
−15
−10
−5
Bits per dimension
lo g
(M S
E )
uniform rotation variable
Figure 1.",6. Lower bounds,[0],[0]
"Distributed mean estimation on data generated from a Gaussian distribution.
",6. Lower bounds,[0],[0]
where ˆ✓(pd) is the empirical mean of the observed samples.,6. Lower bounds,[0],[0]
Let Pd be the set of distributions in Lemma 9.,6. Lower bounds,[0],[0]
"Hence for any protocol ⇡ there exists a distribution pd such that
E ˆ✓(pd) ˆ✓⇡
2
2
(a) 1
2
E ✓(pd) ˆ✓⇡
2
2
E ✓(pd) ˆ✓(pd)
2
2
(b) t
2
min ✓ 1, d C(⇡) ◆ 1 n (c) t 4 min ✓ 1, d C(⇡) ◆ ,
(a) follows from the fact that 2(a b)2 + 2(b c)2 (a c)2.",6. Lower bounds,[0],[0]
"(b) follows from Lemma 9 and (c) follows from the fact that C(⇡, Sd)  ndt/4 and n 4/t. Corollary 1 and Theorem 5 yield Theorem 1.",6. Lower bounds,[0],[0]
We note that the above lower bound holds only for communication cost c < O(nd).,6. Lower bounds,[0],[0]
"Extending the results for larger values of c remains an open problem.
",6. Lower bounds,[0],[0]
"At a first glance it may appear that combining structured random matrix and variable length encoding may improve the result asymptotically, and therefore violates the lower bound.",6. Lower bounds,[0],[0]
"However, this is not true.
",6. Lower bounds,[0],[0]
Observe that variable length coding ⇡svk and stochastic rotated quantization ⇡srk use different aspects of the data: the variable length coding uses the fact that bins with large values of index r are less frequent.,6. Lower bounds,[0],[0]
"Hence, we can use fewer bits to encode frequent bins and thus improve communication.",6. Lower bounds,[0],[0]
In this scheme bin-width (si/(k 1)) is p 2||Xi||2/(k 1).,6. Lower bounds,[0],[0]
Rotated quantization uses the fact that rotation makes the min and max closer to each other and hence we can make bins with smaller width.,6. Lower bounds,[0],[0]
"In such a case, all the bins become more or less equally likely and hence variable length coding does not help.",6. Lower bounds,[0],[0]
"In this scheme bin-width (si/(k 1)) is (Zmaxi Zmini )/(k 1) ⇡ ||Xi||2(log d)/(kd), which is much smaller than bin-width for variable length coding.",6. Lower bounds,[0],[0]
Hence variable length coding and random rotation cannot be used simultaneously.,6. Lower bounds,[0],[0]
"Based on the theoretical analysis, the variable-length coding method provides the lowest quantization error asymptotically when using a constant number of bits.",7. Practical considerations and applications,[0],[0]
"However in practice, stochastic rotated quantization may be preferred due to (hidden) constant factors and the fact that it uses a fixed amount of bits per dimension.",7. Practical considerations and applications,[0],[0]
"For example, considering quantizing the vector [ 1, 1, 0, 0], stochastic rotated
quantization can use 1 bit per dimension and gives zero error, whereas the other two protocols do not.",7. Practical considerations and applications,[0],[0]
"To see this, observe that the naive quantization will quantize 0 to either 1 or 1 and variable length coding cannot achieve 0 error with 1 bit per dimension due to its constant factors.
",7. Practical considerations and applications,[0],[0]
"We further note that the rotated quantization is preferred when applied on “unbalanced” data, due to the fact that the rotation can correct the unbalancedness.",7. Practical considerations and applications,[0],[0]
We demonstrate this by generating a dataset where the value of the last feature dimension entry is much larger than others.,7. Practical considerations and applications,[0],[0]
We generate 1000 datapoints each with 256 dimensions.,7. Practical considerations and applications,[0],[0]
The first 255 dimensions are generated i.i.d.,7. Practical considerations and applications,[0],[0]
"from N(0, 1),",7. Practical considerations and applications,[0],[0]
"and the last dimension is generated from N(100, 1).",7. Practical considerations and applications,[0],[0]
"As shown in Figure 1, the rotated stochastic quantization has the best performance.",7. Practical considerations and applications,[0],[0]
"The improvement is especially significant for low bit rate cases.
",7. Practical considerations and applications,[0],[0]
We demonstrate two applications in the rest of this section.,7. Practical considerations and applications,[0],[0]
"The experiments are performed on the MNIST (d = 1024) and CIFAR (d = 512) datasets.
",7. Practical considerations and applications,[0],[0]
Distributed Lloyd’s algorithm.,7. Practical considerations and applications,[0],[0]
"In the distributed Lloyd’s (k-means) algorithm, each client has access to a subset of data points.",7. Practical considerations and applications,[0],[0]
"In each iteration, the server broadcasts the cluster centers to all the clients.",7. Practical considerations and applications,[0],[0]
"Each client updates the centers based on its local data, and sends the centers back to the server.",7. Practical considerations and applications,[0],[0]
The server then updates the centers by computing the weighted average of the centers sent from all clients.,7. Practical considerations and applications,[0],[0]
"In
the quantized setting, the client compresses the new centers before sending to the server.",7. Practical considerations and applications,[0],[0]
"This saves the uplink communication cost, which is often the bottleneck of distributed learning5.",7. Practical considerations and applications,[0],[0]
We set both the number of centers and number of clients to 10.,7. Practical considerations and applications,[0],[0]
"Figure 2 shows the result.
",7. Practical considerations and applications,[0],[0]
Distributed power iteration.,7. Practical considerations and applications,[0],[0]
Power iteration is a widely used method to compute the top eigenvector of a matrix.,7. Practical considerations and applications,[0],[0]
"In the distributed setting, each client has access to a subset of data.",7. Practical considerations and applications,[0],[0]
"In each iteration, the server broadcasts the current estimate of the eigenvector to all clients.",7. Practical considerations and applications,[0],[0]
"Each client then updates the eigenvector based on one power iteration on its local data, and sends the updated eigenvector back to the server.",7. Practical considerations and applications,[0],[0]
The server updates the eigenvector by computing the weighted average of the eigenvectors sent by all clients.,7. Practical considerations and applications,[0],[0]
"Similar to the above distributed Lloyd’s algorithm, in the quantized setting, the client compresses the estimated eigenvector before sending to the server.",7. Practical considerations and applications,[0],[0]
Figure 3 shows the result.,7. Practical considerations and applications,[0],[0]
"The dataset is distributed over 100 clients.
",7. Practical considerations and applications,[0],[0]
"For both of these applications, variable-length coding achieves the lowest quantization error in most of the settings.",7. Practical considerations and applications,[0],[0]
"Furthermore, for low-bit rate, stochastic rotated quantization is competitive with variable-length coding.
",7. Practical considerations and applications,[0],[0]
"5In this setting, the downlink is a broadcast, and therefore its cost can be reduced by a factor of O(n/ log n) without quantization, where n is the number of clients.",7. Practical considerations and applications,[0],[0]
"We thank Jayadev Acharya, Keith Bonawitz, Dan Holtmann-Rice, Jakub Konecny, Tengyu Ma, and Xiang Wu for helpful comments and discussions.",Acknowledgments,[0],[0]
"Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation.",abstractText,[0],[0]
"Unlike previous works, we make no probabilistic assumptions on the data.",abstractText,[0],[0]
"We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of ⇥(d/n) and uses a constant number of bits per dimension per client.",abstractText,[0],[0]
We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((log d)/n) and a better coding strategy further reduces the error to O(1/n).,abstractText,[0],[0]
"We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost.",abstractText,[0],[0]
We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd’s algorithm for kmeans and power iteration for PCA.,abstractText,[0],[0]
Distributed Mean Estimation with Limited Communication,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 12–21, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",text,[0],[0]
"Distributional models induce vector-based semantic representations of words from their contextual distributions in corpora, exploiting the observation that words with related meanings tend to occur in similar linguistic contexts (Turney and Pantel, 2010; Erk, 2012).",1 Introduction,[0],[0]
"Since the approach only requires raw text as input, it can be used to harvest word representations on a very large scale.",1 Introduction,[0],[0]
"By encoding the rich knowledge that is present in text, these representations are able to capture many aspects of word meaning.",1 Introduction,[0],[0]
"Moreover, approximating semantic similarity by graded geometric distance in a vector space is an effective strategy to address the
many linguistic phenomena that are better characterized in gradient rather than discrete terms, such as synonymy, selectional preferences, and semantic priming (Baroni and Lenci, 2010; Erk et al., 2010; Padó and Lapata, 2007, among others).
",1 Introduction,[0],[0]
"However, not all aspects of human semantic knowledge are satisfactorily captured in terms of fuzzy relations and graded similarity.",1 Introduction,[0],[0]
"In particular, our knowledge of the meaning of words denoting specific entities involves a number of “hard facts” about the referents they denote that are best formalized as attribute-value pairs, of the sort that are stored in manually-curated knowledge bases, such as FreeBase or Wikidata.1 While distributional vectors can capture the useful fact that, say, Italy is in many ways more similar to Spain than to Germany, as humans we also know (or we can easily look up) a set of objective facts about Italy, such as what is its capital, its area, its official language and GDP, that are difficult to express in the language of vector algebra and geometry.
",1 Introduction,[0],[0]
"In this paper, we explore the hypothesis that distributional vectors implicitly encode such attributes of referential entities, which we will call referential attributes.",1 Introduction,[0],[0]
We show that a simple supervised algorithm applied to vectors can retrieve them so that they can be expressed in the explicit language of structured knowledge bases.,1 Introduction,[0],[0]
"Concretely, we train a logistic regression model to predict the values of both numeric and categorical FreeBase attributes of countries and cities from their distributional vectors.",1 Introduction,[0],[0]
"This model makes predictions that are significantly better than an informed baseline, in-between the latter and an upper-bound method.",1 Introduction,[0],[0]
"Qualitative analysis of the results points both to the inherent difficulty of correctly retrieving certain classes of attributes, and to some intriguing properties of the conceptual nature of the knowledge encoded in distributional data, that bias their predictions about certain objective attributes of geographic entities.
",1 Introduction,[0],[0]
"1www.freebase.com, www.wikidata.org.
12
",1 Introduction,[0],[0]
"We see our experiment as a first step towards integrating conceptual and referential aspects of meaning in distributional semantics, as we further discuss in the conclusion.",1 Introduction,[0],[0]
Mikolov et al.’s (2013) skip-gram model is a state-of-the-art “predictive” distributional semantic model which represents each word in a space of latent dimensions optimized to predict the contexts of the word’s occurrences.,2.1 Distributional Representations,[0],[0]
"For our study, we adopt the pre-trained 1,000-dimensional skipgram model for Named Entities that is available at https://code.google.com/p/word2vec/ and was produced from a 100-billion token news corpus.",2.1 Distributional Representations,[0],[0]
We refer to this model as WORD2VEC.,2.1 Distributional Representations,[0],[0]
"As our source of referential attributes, we use FreeBase (see footnote 1), a knowledge base of structured information on a wide range of entities of different semantic types (people, geographical entities, etc.).",2.2 Referential Representations,[0],[0]
"The information in FreeBase comes from various sources, including Wikipedia and domainspecific databases, plus user content generation and correction.",2.2 Referential Representations,[0],[0]
"FreeBase currently records at least 2 attributes for over 47 million entities, and it has been used fairly extensively in NLP before (Mintz et al., 2009; Socher et al., 2013a, among others).
",2.2 Referential Representations,[0],[0]
"For each entity, FreeBase contains a list of attribute-value tuples (where values can in turn be entities, allowing a graph view of the data that we do not exploit here).",2.2 Referential Representations,[0],[0]
Table 1 shows a sample of the attributes that FreeBase records for countries.,2.2 Referential Representations,[0],[0]
"Note that some attributes are simple (e.g., date founded), while other can be called complex, in the sense that they are attributes of attributes (e.g., geolocation::latitude).",2.2 Referential Representations,[0],[0]
We use a double-colon notation to refer to complex attributes.,2.2 Referential Representations,[0],[0]
The values of all attributes can be either numeric or categorical.,2.2 Referential Representations,[0],[0]
"The numeric attributes in particular are often strongly correlated, both within attributes types across years (e.g., fertility rate in different years) and across attributes within years (e.g., absolute GDP and GDP per capita in a given year).
",2.2 Referential Representations,[0],[0]
"We built two datasets for our experiments, one for countries and one for cities, with data automatically extracted from FreeBase.2 We consider two
2Both datasets are publicly available at http:
datasets in order to check that the mapping we seek can be established not just for one, possible handpicked, type of entities; we leave it to future work to study very different kinds of entities, such as people or institutions.
",2.2 Referential Representations,[0],[0]
The Countries dataset consists of the 260 countries for which we have a distributional vector.,2.2 Referential Representations,[0],[0]
"Some countries do not exist anymore, like Yugoslavia, but, since this does not impact our method, we keep them in the dataset.",2.2 Referential Representations,[0],[0]
"The dataset records all simple attributes as well as complex attributes of at most two hops in the FreeBase graph, without manual inspection.",2.2 Referential Representations,[0],[0]
We linearly rescale all numeric attributes to [0..1] and translate all categorical attributes into a binary representation by suffixing the original value to the original attribute name.,2.2 Referential Representations,[0],[0]
"For example, the attribute member-of::organization with the value world bank results in a binary attribute member-of::organization::world bank having value 1 for all and only those countries that are members of the World Bank, 0 for the others.3 Attributes that occur less than 15 times are discarded, since they are either not consistently recorded or rare.",2.2 Referential Representations,[0],[0]
This results in a total of 707 numeric and 247 binary attributes.,2.2 Referential Representations,[0],[0]
"Finally, we partition the data into training, validation, and test set, using a 60-20-20 percent split.
",2.2 Referential Representations,[0],[0]
"We apply the same process to the Cities dataset, which consists of 1645 cities from the intersection of the distributional and FreeBase city lists.",2.2 Referential Representations,[0],[0]
"In
//www.ims.uni-stuttgart.de/forschung/ ressourcen/korpora/CityCountry.html.
3We considered treating some categorical attributes as multi-valued, but decided against it since the cases in which alternative values are mutually exclusive are rare (e.g., the same country can be containedBy multiple entities, cf. Table 1).
",2.2 Referential Representations,[0],[0]
"this case, we have 211 numeric and 106 binary attributes – the numbers are smaller because countries have a richer representation in FreeBase than cities.",2.2 Referential Representations,[0],[0]
"We do zero-shot learning of full FreeBase attributebased country/city representations, based on distributional (WORD2VEC) representations.",2.3 Attribute Prediction,[0],[0]
It is zeroshot learning in the sense of Palatucci et al. (2009):,2.3 Attribute Prediction,[0],[0]
"We split the datasets at the entity, rather than attribute level, such that at test time our system must predict the full attribute set of countries and cities that were not seen during training at all.
",2.3 Attribute Prediction,[0],[0]
We use logistic regression.,2.3 Attribute Prediction,[0],[0]
"In effect, we predict each output variable (FreeBase attribute) with an independent logistic regression model based on a constant set of input features (WORD2VEC distributional dimensions).",2.3 Attribute Prediction,[0],[0]
We call this model DIST2REF.,2.3 Attribute Prediction,[0],[0]
"DIST2REF does not take advantage of the correlations between the output attributes mentioned in Section 2.2.
",2.3 Attribute Prediction,[0],[0]
"The dependent variables are binary as well as numeric FreeBase attributes, and our model does not distinguish between them.",2.3 Attribute Prediction,[0],[0]
"For binary attributes, we interpret the value returned by the model as the probability of “success” of a binary Bernoulli trial.",2.3 Attribute Prediction,[0],[0]
"In the numeric case, we view the probability returned by the model as directly representing normalized attribute values.",2.3 Attribute Prediction,[0],[0]
"We design the model using the Countries dataset, and apply it to Cities without further tuning to test its robustness.",2.4 Experimental Setup,[0],[0]
"We optimize the parameters with gradient descent, using the Cross Entropy error function.",2.4 Experimental Setup,[0],[0]
"We considered L2 regularization to address possible overfitting, but experiments on validation set showed that the model performs best without any regularization.
",2.4 Experimental Setup,[0],[0]
"As for baselines, for binary features we predict the majority class (0 or 1), and for numeric features we predict the mean value of the feature in the training set.",2.4 Experimental Setup,[0],[0]
"These are of course strong baselines to beat.
",2.4 Experimental Setup,[0],[0]
"As an upper bound, we train a model that uses the same architecture as described above but uses as input not distributional vectors but the FreeBase attributes themselves.",2.4 Experimental Setup,[0],[0]
"In other words, this model has to learn “only” an identity mapping.",2.4 Experimental Setup,[0],[0]
"This is not trivial, though, for example due to the presence of strong correlations among attributes, in particular
the time series attributes (cf. Section 2.2).",2.4 Experimental Setup,[0],[0]
We call this model REF2REF.,2.4 Experimental Setup,[0],[0]
"Since there is no appropriate unified evaluation measure that covers both numeric and binary attributes, we evaluate them separately.",2.5 Evaluation,[0],[0]
"For binary attributes, we report the attributes’ mean accuracy.
",2.5 Evaluation,[0],[0]
"For numeric attributes, we consider attribute prediction a ranking task.",2.5 Evaluation,[0],[0]
"As an example, take the population::2011::number attribute, and imagine that we only have three countries (Germany: 80M; Spain: 36M; and Netherlands: 17M).",2.5 Evaluation,[0],[0]
"If we predict 56M for Spain’s population, it is still (correctly) predicted as the second most populous country (rank difference of 0); a prediction of 16M, however, would push Spain to third place (rank difference of 1).
",2.5 Evaluation,[0],[0]
This suggests the use of rank correlation coefficients like Spearman’s ρ.,2.5 Evaluation,[0],[0]
"However, we want to measure not only how well the model can rank the countries in the test set, but also whether these predictions are consistent with the training set (which makes evaluation both more challenging and more realistic).",2.5 Evaluation,[0],[0]
"One way of achieving this goal would be to use ρ on the union of training and test instances, but this could lead to misleadingly high correlation coefficients since this method would include the labels of the training instances in the evaluation.
",2.5 Evaluation,[0],[0]
"Consequently, we define our own evaluation measure, following a rationale similar to Frome et al.’s (2013) evaluation of a zero-shot learning scenario.",2.5 Evaluation,[0],[0]
"What we evaluate, for each attribute, is the rank of the test countries in the whole country list.",2.5 Evaluation,[0],[0]
"Note that this makes our task harder, as there are more confounders: If we only evaluated on the test set, there would be shorter lists and therefore less chances of getting bad rankings.",2.5 Evaluation,[0],[0]
"So, concretely, we first define the prediction quality of each attribute, Q(a), as the median of the rank difference between the prediction and the gold standard in a list that includes both training and test countries (we use the median to give less weight to outlier countries).",2.5 Evaluation,[0],[0]
We also normalize the rank difference to obtain a number between zero and one.,2.5 Evaluation,[0],[0]
"In a second step, we define the quality of the complete model, the normalized rank score (NRS), as the mean of all attribute quality scores, in parallel to our evaluation on binary attributes.
",2.5 Evaluation,[0],[0]
Let the set of instances I be partitioned into training instances Tr and test instances Ts.,2.5 Evaluation,[0],[0]
Let a ∈,2.5 Evaluation,[0],[0]
"A
denote an attribute.",2.5 Evaluation,[0],[0]
We write pa(i) for the predicted value of attribute a for instance i and ga(i) for the gold standard value.,2.5 Evaluation,[0],[0]
"Finally, let r(v, S) denote the rank of value v in the list resulting when ordering the set S.",2.5 Evaluation,[0],[0]
"Now we can define:
Q(a) = 1 ||I||med{|r(pa(i), I)",2.5 Evaluation,[0],[0]
"− (1)
r(ga(i), I)| − 1 | i ∈ Ts} NRS =
1 ||A|| ∑ a∈A Q(a) (2)
",2.5 Evaluation,[0],[0]
"This measure can be interpreted similarly to Mean Reciprocal Rank (Manning et al., 2008):",2.5 Evaluation,[0],[0]
"It has range [0..1], with smaller numbers indicating better ranking: 0.1, for example, means that, on average, the prediction is 10% of the ranks off (e.g., by four countries in a forty-country list).4
Note that, when evaluating each instance i, we use gold-standard values for all other instances, so that there the baseline is not hampered by ties.",2.5 Evaluation,[0],[0]
Table 2 shows the results of our experiments on the two test sets.,3 Results,[0],[0]
"For accuracy 1 is best, but for NRS 0 is best.",3 Results,[0],[0]
"Recall from Section 2.2 that we perform model selection on the Countries dataset only.
",3 Results,[0],[0]
"The baseline is relatively high, in particular for the binary attributes, many of which are positive for a small subset of entities only.",3 Results,[0],[0]
"The amount of skew differs considerably between the two datasets, though.",3 Results,[0],[0]
"For Countries, the baseline yields an accuracy of 0.86, but it achieves 0.97 on Cities.",3 Results,[0],[0]
"The increase stems from very sparse categorical City features such as containedBy, which includes all
4Subtracting 1 in Equation (1) ensures that, when the predicted and gold value of an attribute are adjacent in the ranking, their rank difference is 0, capturing the intuition of rank difference as counting the number of falsely intervening items.
levels of administrative divisions – that is, for the US, all counties appear as values and are transformed into sparse binary features (cf. Section 2.2).",3 Results,[0],[0]
"Of course, the predictions of the baseline are useless, since it always predicts the absence of any features.",3 Results,[0],[0]
"On numeric features, where the baseline predicts the mean, its performance is 0.35 NRS on both datasets.",3 Results,[0],[0]
"In other words, its average prediction is off by about one third the length of the ranked list for each attribute.
",3 Results,[0],[0]
"Recall that the upper bound model, REF2REF, uses FreeBase attributes to predict FreeBase attributes.",3 Results,[0],[0]
All it has to learn is that there is one feature in the input that corresponds ideally to the output.,3 Results,[0],[0]
"This works almost perfectly for binary attributes, with accuracy values of 0.96 (Countries) and 1.00 (Cities).",3 Results,[0],[0]
"However, its performance on numeric features (with NRS at 0.14 and 0.21, respectively) is not quite perfect.",3 Results,[0],[0]
"We attribute this to the presence of correlations (cf. Section 2.2).
",3 Results,[0],[0]
"The model whose performance we are actually interested in, DIST2REF, in which we map from distributional information to FreeBase features, performs with remarkable consistency between these two extremes.",3 Results,[0],[0]
"In fact, we see a consistent error reduction of around 30% over the baseline, with a similar distance to the upper bound.",3 Results,[0],[0]
"A significance test with bootstrap resampling (Efron and Tibshirani, 1994) showed that all pairwise comparisons (Baseline vs. DIST2REF, DIST2REF vs. REF2REF) are statistically significant at p<0.001.
",3 Results,[0],[0]
"To rule out that we misinterpret our accuracybased evaluation for the binary features in the face of a highly skewed class distribution, we also computed precision, recall, and F-Score values.",3 Results,[0],[0]
"The relative patterns match those of the accuracybased evaluation well (Countries: baseline F=0.13, DIST2REF",3 Results,[0],[0]
"F=0.51, REF2REF F=0.77) and indicate that generally precision is higher than recall.
",3 Results,[0],[0]
"We think that these are overall promising results, given that the FreeBase attributes we predict are fairly fine-grained, and we only use generic distributional information as input.",3 Results,[0],[0]
We take the overall results just presented to suggest that we are able to learn referential attributes from distributional information to a large extent.,4 Analysis,[0],[0]
"In this section we take a closer look at what kind of information we are able to learn, what is beyond the scope of our model, and what are the differences between the entity representations in WORD2VEC and the ones our model produces.",4 Analysis,[0],[0]
All the data concerns the test sets only.,4 Analysis,[0],[0]
We start with a qualitative analysis of the Countries dataset.,4.1 Attribute Groups,[0],[0]
"Due to the large number of attributes, we sort all individual attributes into attribute groups by their base name (i.e. the leftmost component of their name, cf. Section 2.2), which offers an accessible level of granularity for inspection.",4.1 Attribute Groups,[0],[0]
"We obtain 34 numeric and 40 binary attribute groups with median sizes of 8.5 and 2 attributes per group, respectively.
",4.1 Attribute Groups,[0],[0]
Table 3 shows the attribute groups for both types sorted by quality.,4.1 Attribute Groups,[0],[0]
"For each group, we report average normalized rank score (NRS) and accuracy, respectively, for both DIST2REF and the baseline.
",4.1 Attribute Groups,[0],[0]
"The analysis suggests that there are two main factors that account for the results: (1) The degree to which an attribute is contextually supported, that is, to what extent its values can be identified on the basis of the contextual information that is captured in a distributional model, and (2) general properties of the data that affect Machine Learning, most notably data sparseness, possibly also feature value distributions.
",4.1 Attribute Groups,[0],[0]
"Attributes that are contextually supported include for instance those related to socioeconomic development (see below for details); people talk (and so write) about countries being more or less developed, rich, having one or another kind of laws, and this is captured in the abstractions over textual context that distributional models perform.",4.1 Attribute Groups,[0],[0]
"As an extreme example of an attribute that is not contextually supported, consider the numeric ISO code of a country (iso numeric), whose values are arbitrary: They do not correspond to facts about the world that are reflected in the way people use lan-
guage, and so can’t be picked up by the distributional model.",4.1 Attribute Groups,[0],[0]
"For this reason, DIST2REF does worse than the baseline.
",4.1 Attribute Groups,[0],[0]
"Note that, in a sufficiently large corpus, we might indeed encounter statements like The numeric ISO code for Spain is 724.",4.1 Attribute Groups,[0],[0]
"However, since distributional models represent words as aggregated distributions of their contexts, and compute semantic similarity from these context distributions, the contexts that they use need to be generic enough to yield meaningful overlap between concepts (e.g., words).",4.1 Attribute Groups,[0],[0]
"As a result, distributional models cannot easily represent knowledge of the form “the value for property Y of word/concept X is Z”.
",4.1 Attribute Groups,[0],[0]
"Fortunately, we find that many FreeBase attributes are contextually supported to a substantial degree, even some seemingly arbitrary ones.",4.1 Attribute Groups,[0],[0]
"An example is calling codes, which we predict very well.",4.1 Attribute Groups,[0],[0]
"They turn out to be correlated with geolocations: 2X calling codes are located in Africa, 3X calling codes in Southern and Eastern Europe and 4X calling codes in Western and Northern Europe (for comparison, ISO codes are assigned in a roughly alphabetical order).
",4.1 Attribute Groups,[0],[0]
Numeric Attributes.,4.1 Attribute Groups,[0],[0]
Our best numeric attributes belong to the geolocation group (latitude and longitude).,4.1 Attribute Groups,[0],[0]
We provide a more detailed analysis of these attributes below (Section 4.2).,4.1 Attribute Groups,[0],[0]
"As mentioned above, we also excel at many attributes related to a country’s economic and social development (broadly construed), such as GNI, GDP, CO2 emissions, internet usage (each per capita), or fertility rate.",4.1 Attribute Groups,[0],[0]
"These attributes can be expected to be contextually grounded – e.g., Luxembourg will occur with contexts like “broadband” or “rich” more than India.
",4.1 Attribute Groups,[0],[0]
"Note, however, that the information contained in the vectors is surprisingly subtle: For instance, the fertility rate is a function of both general development status (lower rates in more developed countries) and of specific social factors (higher rates in countries with more support for families, such as France and Finland compared countries with less support, such as Germany or Italy).
",4.1 Attribute Groups,[0],[0]
"Around the middle of the table, we find the absolute versions of the developmental cluster above (GNI in $, real and nominal GDP).",4.1 Attribute Groups,[0],[0]
"Evidently, the absolute versions of these attributes are substantially less contextually supported than the relative versions.",4.1 Attribute Groups,[0],[0]
"This is not surprising: While India and China have high absolute GDPs because they are
large countries, and for instance Luxembourg has a much smaller one, these numbers are not indicative of the actual conditions in these countries, and therefore also not so clearly correlated with what people write about them.",4.1 Attribute Groups,[0],[0]
This provides another interesting angle on the difference between distributional and formal knowledge representation.,4.1 Attribute Groups,[0],[0]
"In a formal system, absolute GDP, relative GDP, and population stand in a fixed linear relationship and knowing any two of the three uniquely determines the third – thus, all three attributes have equal status.",4.1 Attribute Groups,[0],[0]
"In our distributional space, their status is clearly
different, determined by the conceptual relevance of the different attributes.
",4.1 Attribute Groups,[0],[0]
"Towards the end of the table, we find more attributes related to socioeconomic development, such as government percent debt and minimum wage.",4.1 Attribute Groups,[0],[0]
"While these should be contextually supported, too, the problem here is factor (2) mentioned above, namely severe data sparsity (see column f(A) in Table 3, which lists the median number of datapoints that exhibit each attribute group).",4.1 Attribute Groups,[0],[0]
"The same goes for the remaining attribute groups, for instance casualties (describing the
total number of military casualties incurred in history), date founded and date dissolved,5 or climate avg rainfall.
",4.1 Attribute Groups,[0],[0]
Binary Attributes.,4.1 Attribute Groups,[0],[0]
"The binary attributes show a similar picture, albeit somewhat less sharp.",4.1 Attribute Groups,[0],[0]
"We again find contextually unsupported groups, many of them arising from our fully automatic attribute mining from FreeBase (cf. Section 2.2).",4.1 Attribute Groups,[0],[0]
"There are many categorical attributes that store metadata about numeric attributes (such as the currency in the gdp and gni groups) as well as meta-information of FreeBase: exceptions is a specific marker of potentially inconsistent entries about Ghana, and equivalent instances is a flag concerning links between FreeBase and OpenCyc.",4.1 Attribute Groups,[0],[0]
"Fortunately, almost all contextually unsupported groups are small, with only one or two attributes, and do not have a large impact on the overall performance.",4.1 Attribute Groups,[0],[0]
"We decided not to exclude them from evaluation for robustness’ sake, since there is no automatic way to identify contextually unsupported attributes in a new dataset.
",4.1 Attribute Groups,[0],[0]
"We obtain good results on meaningful attributes that are arguably strongly contextually grounded, such as geographical and geopolitical attributes (member of: membership in international organizations; location on a continent, etc.).",4.1 Attribute Groups,[0],[0]
"However, we fare relatively badly on government-related attributes (form of government, governing officials).",4.1 Attribute Groups,[0],[0]
"While this seems surprising at first glance, the form of government attribute in FreeBase makes very fine-grained distinctions: Its values include “unitary state”, “presidential system”, “parliamentary system” and “republic”, which are not mutually exclusive, and misses obvious alternatives like “authoritarian system”.",4.1 Attribute Groups,[0],[0]
It is not surprising that distributional models cannot make such subtle distinction between presidential and parliamentary systems.,4.1 Attribute Groups,[0],[0]
The attribute governing official presents a similar case.,4.1 Attribute Groups,[0],[0]
"Other bad attributes are very domain-specific, including athletes, encoding the athletic disciplines that countries participate in (such as swimming, judo, running, etc.), and the data sparsity issue is certainly worse for the binary attributes.
",4.1 Attribute Groups,[0],[0]
"5Note that date-based attributes can be contextually supported: We do better on national anthem since, for which we have more datapoints, 97.",4.1 Attribute Groups,[0],[0]
"To analyze the difference between the distributional representations and the output of our model, we focus on geolocation, our best attribute group.
",4.2 Geolocation,[0],[0]
"It has already been shown that geometric distance in distributional space captures, to a certain extent, physical distance between locations in the real world (Louwerse and Zwaan, 2009).",4.2 Geolocation,[0],[0]
Table 4 shows that DIST2REF extracts even more precise distance information from distributional vectors.,4.2 Geolocation,[0],[0]
The table reports the correlation between real and model-predicted distances for countries and cities.,4.2 Geolocation,[0],[0]
"Ground-truth great circle distances (Kern and Bland, 1948) between items are computed using the FreeBase longitude and latitude values; for DIST2REF we use its predicted latitude and longitude values; for WORD2VEC, the cosines between the corresponding distributional vectors.
",4.2 Geolocation,[0],[0]
"We obtain highly significant correlations in all cases (p<10−14), but much higher for DIST2REF.",4.2 Geolocation,[0],[0]
"For countries, as shown in Table 4, the correlation is -0.36 for WORD2VEC (negative, because cosine is a similarity measure), 0.49 for DIST2REF.",4.2 Geolocation,[0],[0]
"For cities, WORD2VEC reaches -0.45 correlation, and DIST2REF distances are at 0.88, showing that the method can estimate city positions to a perhaps unexpectedly high degree of accuracy.6
This result suggests that we manage to objectify the information in the distributional model, anchoring the entities more firmly in the external world.",4.2 Geolocation,[0],[0]
"Indeed, distributional models are known to be subject to conceptual or cultural effects in their distance estimations.",4.2 Geolocation,[0],[0]
"For instance, in WORD2VEC German and Spanish cities are much farther away than in the physical world, while cities within Spain and within Germany are predicted to be a bit closer than they actually are.",4.2 Geolocation,[0],[0]
"Note that these effects have
6The results are confirmed when the analysis is repeated using the Spearman correlation measure: The DIST2REF coefficients are stable, whereas those of WORD2VEC go down to 0.22 (countries) and 0.40 (cities), respectively.",4.2 Geolocation,[0],[0]
"The good results for Spearman, as a rank-based measure, indicate that our success is not dominated by outliers.
",4.2 Geolocation,[0],[0]
"an actual cognitive basis: Human intuitions about objective physical distance between countries and cities are biased by cognitive, cultural and socioeconomic factors, as explored for example in Friedman et al. (2002), who report that Texans locate Canadian cities closer to the US border relative to Mexican cities, despite their proximity to the latter, and that they place Southern US cities further south than they really are.
",4.2 Geolocation,[0],[0]
"Interestingly, DIST2REF does also show some cultural effects in its geolocation errors: For example, some Pacific island states with lesser-known identities (e.g., Nauru and French Polynesia) are placed in the Indian Ocean, where we find the perhaps prototypes of beautiful islands, like Seychelles and Mauritius; also, Central American countries (such as Panama, El Salvador, and Nicaragua) move towards their “cultural center of gravity”, South America.
",4.2 Geolocation,[0],[0]
"However, this kind of cultural bias is much more prominent in the original WORD2VEC distributional representation.",4.2 Geolocation,[0],[0]
The Spain/Germany effect discussed above is not found in the DIST2REF model at all.,4.2 Geolocation,[0],[0]
"And while both DIST2REF and WORD2VEC place Mexican and Spanish cities in our test set closer to each other than they actually are, WORD2VEC does so to a much larger extent.",4.2 Geolocation,[0],[0]
"In line with our goal to extract referential attributes, thus, we are satisfied to see that DIST2REF manages to minimize this bias and distill the referential part from the distributional representations.",4.2 Geolocation,[0],[0]
"There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., Buitelaar and Cimiano (2008) and references therein).",5 Related Work,[0],[0]
"This line of work, however, does not attempt to connect entity representations extracted from corpora and from KBs, as we do.",5 Related Work,[0],[0]
"Moreover, it focuses on harvesting relations between entities or between entities and a limited number of discrete attributes, rather than predicting full-fledged KB representations of specific entities, like we do.",5 Related Work,[0],[0]
Freitas and Curry (2014) and Freitas et al. (2014) embed relational graphs from KBs in a distributional semantic space to support various forms of search and reasoning about the KB.,5 Related Work,[0],[0]
"The focus is again on relations between discrete entities, and on exploiting distributional semantics to navigate among them.
",5 Related Work,[0],[0]
Socher et al. (2013a) represent WordNet and FreeBase entities with corpus-based distributional vectors.,5 Related Work,[0],[0]
They train a tensor for each relation of interest to return high scores when combined with the vectors of two entities that hold the intended relation.,5 Related Work,[0],[0]
"At test time, the system is used to classify relational tuples as true or false, as well as to predict new entities that hold a certain relationship with a target entity.",5 Related Work,[0],[0]
"This is quite close in spirit to what we do, except that, given an entity1-relation-entity2 tuple, we treat relation-entity2 as a binary attribute of entity1, and we try to induce such attributes on a larger scale (Socher et al. consider seven relations in total).",5 Related Work,[0],[0]
"Moreover, we rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities.
",5 Related Work,[0],[0]
"Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein).",5 Related Work,[0],[0]
"This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes.
",5 Related Work,[0],[0]
The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015).,5 Related Work,[0],[0]
"However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities.",5 Related Work,[0],[0]
"Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; Făgărăşan",5 Related Work,[0],[0]
"et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012).",5 Related Work,[0],[0]
"In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes.",5 Related Work,[0],[0]
"Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes.",5 Related Work,[0],[0]
Our target features are conceptually very different from those of all these studies.,5 Related Work,[0],[0]
"We have shown that a simple model can learn to predict, to a reasonable degree of accuracy, ref-
erential attributes of an entity that are typically seen in a knowledge base from the corresponding corpus-based distributional representation.",6 Discussion and Conclusion,[0],[0]
"The results suggest that, while distributional semantic vectors can be used “as-is” to capture generic word similarity, with some supervision it is also possible to extract other kinds of information from them, including structured factual statements of the sort encoded in manually-curated knowledge bases.",6 Discussion and Conclusion,[0],[0]
"This makes distributional vectors very attractive as general-purpose word meaning representations.
",6 Discussion and Conclusion,[0],[0]
"We have also shown that some of the errors in the predictions can be explained on cultural grounds, but that these effects are more pronounced in the input of our model, a standard distributional semantic model, than in its output.",6 Discussion and Conclusion,[0],[0]
"In this sense, our model manages to objectify the information that it is provided with.",6 Discussion and Conclusion,[0],[0]
"Our analyses also suggest that the main limiting factor in learning referential attributes, apart from good old data sparseness, is the degree to which they are contextually supported, that is, to what extent they are expressed with consistent and specific linguistic means in the context of their target words.",6 Discussion and Conclusion,[0],[0]
"This determines whether they are actually represented in the distributional model in the first place.
",6 Discussion and Conclusion,[0],[0]
"More generally, we see our work as a small step towards the more general goal of bridging the concept-referent gap in distributional semantics.",6 Discussion and Conclusion,[0],[0]
"A common noun such as dog denotes a concept, based on a prototype with fuzzy boundaries, susceptible of metaphorical extensions, and bearing all the other hallmarks of generic conceptual knowledge (Carlson, 2009; Murphy, 2002).",6 Discussion and Conclusion,[0],[0]
These might be adequately captured by the properties of the dog vector in distributional semantic space.,6 Discussion and Conclusion,[0],[0]
"However, when used in a specific discourse, words and more complex linguistic expressions often denote specific referents with fixed, “hard” properties, such as this dog, or Amur, when used for my neighbor’s dog at 3.31pm on May 29th 2015 in Novosibirsk, a 61cm-tall black-and-tan foxhound.",6 Discussion and Conclusion,[0],[0]
Amur is more easily characterized by a set of precise attributevalue pairs than by a vector in a generic conceptual space.,6 Discussion and Conclusion,[0],[0]
Our experiment suggests that distributional vectors encode both generic conceptual knowledge and more precise attributes of specific referents.,6 Discussion and Conclusion,[0],[0]
"Of course, while we can use FreeBase and other knowledge bases to gather training data about public-domain entities, such as countries or cities, it is still not clear where we could gather
appropriate training data to learn about the specific properties of “private-discourse” referents such as Amur.",6 Discussion and Conclusion,[0],[0]
"Moreover, it remains to be seen whether the properties of common named entities, such as countries and cities, that are in a sense “hybrid” between the conceptual and referential domains, also transfer to entities of a more specific and private kind.",6 Discussion and Conclusion,[0],[0]
"Finally, it is still not clear how to extend the current approach beyond words and phrases directly denoting an entity (Amur) to other kinds of definite descriptions (this dog).
",6 Discussion and Conclusion,[0],[0]
"Acknowledgments: This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES); DFG (SFB 732, Project D10); and Spanish MINECO (grant FFI2013-41301-P).",6 Discussion and Conclusion,[0],[0]
"This paper reflects the authors’ view only, and the EU is not responsible for any use that may be made of the information it contains.",6 Discussion and Conclusion,[0],[0]
"Special thanks to Christian Scheible for help with the Machine Learning part, to the anonymous reviewers for insightful and constructive feedback, and to the FLOSS reading group for helping us shape our ideas on the topic of this paper.",6 Discussion and Conclusion,[0],[0]
"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany).",abstractText,[0],[0]
"In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants).",abstractText,[0],[0]
"In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes.",abstractText,[0],[0]
"We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound.",abstractText,[0],[0]
"Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",abstractText,[0],[0]
Distributional vectors encode referential attributes,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1063–1072 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1098
Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.",text,[0],[0]
"Over the past few years neural models based on the encode-attend-decode (Bahdanau et al.,
2014) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation (Bahdanau et al., 2014), abstractive summarization ((Rush et al., 2015),(Nallapati et al., 2016)) dialog (Li et al., 2016), etc.",1 Introduction,[0],[0]
One such NLG problem which has not received enough attention in the past is query based abstractive text summarization where the aim is to generate the summary of a document in the context of a query.,1 Introduction,[0],[0]
"In general, abstractive summarization, aims to cover all the salient points of a document in a compact and coherent fashion.",1 Introduction,[0],[0]
"On the other hand, query focused summarization highlights those points that are relevant in the context of the query.",1 Introduction,[0],[0]
"Thus given a document on “the super bowl”, the query “How was the half-time show?”, would result in a summary that would not cover the actual game itself.
",1 Introduction,[0],[0]
Note that there has been some work on query based extractive summarization in the past where the aim is to simply extract the most salient sentence(s) from a document and treat these as a summary.,1 Introduction,[0],[0]
There is no natural language generation involved.,1 Introduction,[0],[0]
"Since, we were interested in abstractive (as opposed to extractive) summarization we created a new dataset based on debatepedia.",1 Introduction,[0],[0]
"This dataset contains triplets of the form (query, document, summary).",1 Introduction,[0],[0]
"Further, each summary is abstractive and not extractive in the sense that the summary does not necessarily comprise of a sentence which is simply copied from the original document.
",1 Introduction,[0],[0]
"Using this dataset as a testbed, we focus on a recurring problem in models based on the encode-attend-decode paradigm.",1 Introduction,[0],[0]
"Specifically, it is observed that the summaries produced by such models contain repeated phrases.",1 Introduction,[0],[0]
"Table 1 shows a few such examples of summaries gener-
1063
ated by such a model when trained on this new dataset.",1 Introduction,[0],[0]
"This problem has also been reported by (Chen et al., 2016) in the context of summarization and by (Sankaran et al., 2016) in the context of machine translation.
",1 Introduction,[0],[0]
We first provide an intuitive explanation for this problem and then propose a solution for alleviating it.,1 Introduction,[0],[0]
A typical encode-attend-decode model first computes a vectorial representation for the document and the query and then produces a contextual summary one word at a time.,1 Introduction,[0],[0]
Each word is produced by feeding a new context vector to the decoder at each time step by attending to different parts of the document and query.,1 Introduction,[0],[0]
"If the decoder produces the same word or phrase repeatedly then it could mean that the context vectors fed to the decoder at these time steps are very similar.
",1 Introduction,[0],[0]
We propose a model which explicitly prevents this by ensuring that successive context vectors are orthogonal to each other.,1 Introduction,[0],[0]
"Specifically, we subtract out any component that the
current context vector has in the direction of the previous context vector.",1 Introduction,[0],[0]
"Notice that, we do not require the current context vector to be orthogonal to all previous context vectors but just its immediate predecessor.",1 Introduction,[0],[0]
This enables the model to attend to words repeatedly if required later in the process.,1 Introduction,[0],[0]
"To account for the complete history (or all previous context vectors) we also propose an extension of this idea where we pass the sequence of context vectors through a LSTM (Hochreiter and Schmidhuber, 1997) and ensure that the current state produced by the LSTM is orthogonal to the history.",1 Introduction,[0],[0]
"At each time step, the state of the LSTM is then fed to the decoder to produce one word in the summary.
",1 Introduction,[0],[0]
Our contributions can be summarized as follows: (i) We propose a new dataset for query based abstractive summarization and evaluate encode-attend-decode models on this dataset (ii) We study the problem of repeating phrases in NLG in the context of this dataset and propose two solutions for countering this problem.,1 Introduction,[0],[0]
We show that our method outperforms a vanilla encoder-decoder model with a gain of 28% (absolute) in ROUGE-L score (iii) We also demonstrate that our method clearly outperforms a recent state of the art method proposed for handling the problem of repeating phrases with a gain of 7% (absolute) in ROUGE-L scores (iv) We do a qualitative analysis of the results and show that our model indeed produces outputs with fewer repetitions.,1 Introduction,[0],[0]
"Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al., 2009)).",2 Related Work,[0],[0]
"A vast majority of this work has focused on extractive summarization where the idea is to construct a summary by selecting the most relevant sentences from the document ((Neto et al., 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al., 2015), (Riedhammer et al., 2010), (Ribeiro et al., 2013)).",2 Related Work,[0],[0]
There has been some work on abstractive summarization in the context of DUC-2003 and DUC-2004 contests (Zajic et al.).,2 Related Work,[0],[0]
"We refer the reader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of
the field.",2 Related Work,[0],[0]
"Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014).",2 Related Work,[0],[0]
"For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model.",2 Related Work,[0],[0]
"Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories.",2 Related Work,[0],[0]
Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets.,2 Related Work,[0],[0]
"Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it.
",2 Related Work,[0],[0]
One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency).,2 Related Work,[0],[0]
Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other.,2 Related Work,[0],[0]
"Similarly, and more relevant to this work, Chen et al. (2016) propose a distraction based attention model which maintains a history of attention vectors and context vectors.",2 Related Work,[0],[0]
It then subtracts this history from the current attention and context vector.,2 Related Work,[0],[0]
When evaluated on our dataset their method performs poorly.,2 Related Work,[0],[0]
This could be because their method is very aggressive in dealing with the history (as explained later in the Experiments section).,2 Related Work,[0],[0]
"On the other hand, our method has a better way of handling history (by passing context vectors through an LSTM recurrent network) which gives us the flexibility to forget/retain some portions of the history and at the same time produce diverse context vectors at successive time steps.
",2 Related Work,[0],[0]
We evaluate our method in the context of query based abstractive summarization - a problem which has received almost no attention in the past due to unavailability of datasets.,2 Related Work,[0],[0]
We create a new dataset for this task and show that our method indeed produces better output by reducing the number of repeated phrases produced by encoder decoder models.,2 Related Work,[0],[0]
"As mentioned earlier, there are no existing datasets for query based abstractive summarization.",3 Dataset,[0],[0]
We create such a dataset from Debatepedia an encyclopedia of pro and con arguments and quotes on critical debate topics.,3 Dataset,[0],[0]
There are 663 debates in the corpus (we have considered only those debates which have at least one query with one document).,3 Dataset,[0],[0]
"These 663 debates belong to 53 overlapping categories such as Politics, Law, Crime, Environment, Health, Morality, Religion, etc.",3 Dataset,[0],[0]
A given topic can belong to more than one category.,3 Dataset,[0],[0]
"For example, the topic “Eye for an Eye philosophy” belongs to both “Law” as well as “Morality”.",3 Dataset,[0],[0]
The average number of queries per debate is 5 and the average number of documents per query is 4.,3 Dataset,[0],[0]
"Please refer to the dataset url1 for more details about number of debates per category.
",3 Dataset,[0],[0]
"For example, Figure 1 shows the queries associated with the topic “Algae Biofuel”.",3 Dataset,[0],[0]
It also lists the set of documents and an abstractive summary associated with each query.,3 Dataset,[0],[0]
"As is obvious from the example, the summary is an abstractive summary and not extracted directly from the document.",3 Dataset,[0],[0]
"We crawled 12695 such {query, document, summary} triples from debatepedia (these were all the triples that were available).",3 Dataset,[0],[0]
"Table 2 reports the average length of the query, summary and documents in this dataset.
",3 Dataset,[0],[0]
We used 10 fold cross validation for all our experiments.,3 Dataset,[0],[0]
"Each fold uses 80% of the documents for training, 10% for validation and 10% for testing.",3 Dataset,[0],[0]
"Given a query q = q1, q2, ..., qk containing k words, a document d = d1, d2, ..., dn containing n words, the task is to generate a contextual summary y = y1, y2, ..., ym containing
1http://www.cse.iitm.ac.in/˜miteshk/ datasets/qbas.html
Figure 1: Queries associated with the topic “algae biofuel”
Figure 2: Documents and summaries for a given query
m words.",4 Proposed model,[0],[0]
"This can be modeled as the problem of finding a y∗ that maximizes the probability p(y|q,d) which can be further decomposed as:
y∗ = argmax y
m∏
t=1
p(yt|y1, ..., yt−1,q,d) (1)
We now describe a way of modeling p(yt|y1, ..., yt−1,q,d) using the neural encoderattention-decoder paradigm.",4 Proposed model,[0],[0]
The proposed model contains the following components: (i) an encoder RNN for the query (ii) an encoder RNN for the document (iii) attention mechanism for the query (iv) attention mechanism for the document and (v) a decoder RNN.,4 Proposed model,[0],[0]
All the RNNs use a GRU cell.,4 Proposed model,[0],[0]
Encoder for the query: We use a recurrent neural network with Gated Recurrent Units (GRU) for encoding the query.,4 Proposed model,[0],[0]
"It reads the query q = q1, q2, ..., qk from left to right and computes a hidden representation for each time-step as:
hqi = GRUq(h q i−1, e(qi))",4 Proposed model,[0],[0]
"(2)
where e(qi) ∈ Rd is the d-dimensional embedding of the query word qi.",4 Proposed model,[0],[0]
Encoder for the document:,4 Proposed model,[0],[0]
"This is similar to the query encoder and reads the document d = d1, d2, ..., dn from left to right and computes a hidden representation for each time-step as:
hdi = GRUd(h d i−1, e(di)) (3)
where e(di) ∈ Rd is the d-dimensional embedding of the document word di.",4 Proposed model,[0],[0]
"Attention mechanism for the query : At each time step, the decoder produces an output word
by focusing on different portions of the query (document) with the help of a query (document) attention model.",4 Proposed model,[0],[0]
"We first describe the query attention model which assigns weights αqt,i to each word in the query at each decoder timestep using the following equations.
",4 Proposed model,[0],[0]
"aqt,i = v T q tanh(Wqst +",4 Proposed model,[0],[0]
"Uqh q i ) (4) αqt,i = exp(aqt,i)∑k j=1 exp(a q t,j) (5)
where st is the current state of the decoder at time step t (we will see an exact formula for this soon).",4 Proposed model,[0],[0]
"Wq ∈ Rl2×l1 , Uq ∈ Rl2×l2 , vq ∈ Rl2 , l1 is the size of the decoder’s hidden state, l2 is both the size of hqi and also the size of the final query representation at time step t, which is computed as:
qt = k∑
i=1
αqt,ih q i (6)
Attention mechanism for the document : We now describe the document attention model which assigns weights to each word in the document using the following equations.
",4 Proposed model,[0],[0]
"adt,i = v T d",4 Proposed model,[0],[0]
tanh(Wdst,4 Proposed model,[0],[0]
+,4 Proposed model,[0],[0]
"Udh d i + Zqt) (7) αdt,i = exp(adt,i)∑n j=1 exp(a d t,j)
where st is the current state of the decoder at time step t (we will see an exact formula for this
soon).",4 Proposed model,[0],[0]
"Wd ∈ Rl4×l1 , Ud ∈ Rl4×l4 , Z ∈",4 Proposed model,[0],[0]
"Rl4×l2 , vd ∈ Rl2 , l4 is the size of hdi and also the size of the final document representation dt which is passed to the decoder at time step t as:
dt = n∑
i=1
αdt,ih d i (8)
Note that dt now encodes the relevant information from the document as well as the query (see Equation (7)) at time step t. We refer to this as the context vector for the decoder.",4 Proposed model,[0],[0]
"Decoder: The hidden state of the decoder st at each time t is again computed using a GRU as follows:
st = GRUdec(st−1, [e(yt−1), dt−1]) (9)
where, yt−1 gives a distribution over the vocabulary words at timestep t − 1 and is computed as:
yt = softmax(Wof(Wdecst + Vdecdt)) (10)
where Wo ∈ RN×l1 , Wdec ∈ Rl1×l1 , Vdec ∈ Rl1×l4 , N is the vocabulary size, yt is the final output of the model which defines a probability distribution over the output vocabulary.",4 Proposed model,[0],[0]
"This is exactly the quantity defined in Equation (1) that we wanted to model (p(yt|y1, ..., yt−1,q,d)).",4 Proposed model,[0],[0]
"Further, note that, e(yt−1) is the d-dimensional embedding of the word which has the highest probability under the distribution yt−1.",4 Proposed model,[0],[0]
"Also [e(yt−1), dt−1] means a concatenation of the vectors e(yt−1), dt−1.",4 Proposed model,[0],[0]
"We chose f to be the identity function.
",4 Proposed model,[0],[0]
The model as described above is an instantiation of the encoder-attention-decoder idea applied to query based abstractive summarization.,4 Proposed model,[0],[0]
"As mentioned earlier (and demonstrated later through experiments), this model suffers from the problem of repeating the same phrase/word in the output.",4 Proposed model,[0],[0]
We now propose a new attention model which we refer to as diversity based attention model to address this problem.,4 Proposed model,[0],[0]
"As hypothesized earlier, if the decoder produces the same phrase/word multiple times then it is possible that the context vectors being fed to the decoder at consecutive time steps are
very similar.",4.1 Diversity based attention model,[0],[0]
"We propose four models (D1, D2, SD1, SD2) to directly address this problem.",4.1 Diversity based attention model,[0],[0]
D1:,4.1 Diversity based attention model,[0],[0]
"In this model, after computing dt as described in Equation (8), we make it orthogonal to the context vector at time t− 1:
d ′",4.1 Diversity based attention model,[0],[0]
"t = dt −
dTt",4.1 Diversity based attention model,[0],[0]
d ′,4.1 Diversity based attention model,[0],[0]
"t−1
d ′T t−1d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′ t−1 (11)
SD1: The above model imposes a hard orthogonality constraint on the context vector(d ′ t).",4.1 Diversity based attention model,[0],[0]
We also propose a relaxed version of the above model which uses a gating parameter.,4.1 Diversity based attention model,[0],[0]
"This gating parameter decides what fraction of the previous context vector should be subtracted from the current context vector using the following equations:
γt = Wgdt−1 + bg
d ′",4.1 Diversity based attention model,[0],[0]
t = dt,4.1 Diversity based attention model,[0],[0]
"− γt
dTt d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′T t−1d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′",4.1 Diversity based attention model,[0],[0]
"t−1
where Wg ∈ Rl4×l4 , bg ∈ Rl4 , l4 is the dimension of dt as defined in equation (8).",4.1 Diversity based attention model,[0],[0]
D2:,4.1 Diversity based attention model,[0],[0]
The above model only ensures that the current context vector is diverse w.r.t the previous context vector.,4.1 Diversity based attention model,[0],[0]
It ignores all history before time step t − 1.,4.1 Diversity based attention model,[0],[0]
"To account for the history, we treat successive context vectors as a sequence and use
a modified LSTM cell to compute the new state at each time step.",4.1 Diversity based attention model,[0],[0]
"Specifically, we use the following set of equations to compute a diverse context at time t:
it = σ(Widt + Uiht−1 + bi)
ft = σ(Wfdt + Ufht−1 + bf )
ot = σ(Wodt + Uoht−1 + bo)
ĉt = tanh(Wcdt + Ucht−1 + bc)
ct = it ĉt + ft ct−1
cdiverset",4.1 Diversity based attention model,[0],[0]
=,4.1 Diversity based attention model,[0],[0]
ct,4.1 Diversity based attention model,[0],[0]
"− ct T ct−1 cTt−1ct−1 ct−1 (12)
ht = ot tanh(cdiverset ) d ′",4.1 Diversity based attention model,[0],[0]
t = ht,4.1 Diversity based attention model,[0],[0]
"(13)
where Wi,Wf ,Wo,Wc ∈ Rl5×l4 , Ui, Uf , Uo, Uc ∈ Rl5×l4 , dt is the l4dimensional output of Equation (8); l5 is number of hidden units in the LSTM cell.",4.1 Diversity based attention model,[0],[0]
This final d ′ t from Equation (13) is then used in Equation (9).,4.1 Diversity based attention model,[0],[0]
Note that Equation (12) ensures that state of the LSTM at time step t is orthogonal to the previous history.,4.1 Diversity based attention model,[0],[0]
Figure 3 shows a pictorial representation of the model with a diversity LSTM cell.,4.1 Diversity based attention model,[0],[0]
SD2: This model again uses a relaxed version of the orthogonality constraint used in D2.,4.1 Diversity based attention model,[0],[0]
"Specifically, we define a gating parameter gt and replace (12) above by (14) as define below:
gt = σ(Wgdt + Ught−1",4.1 Diversity based attention model,[0],[0]
"+ bo)
cdiverset = ct",4.1 Diversity based attention model,[0],[0]
"− gt ct T ct−1 cTt−1ct−1 ct−1 (14)
where Wg ∈ Rl5×l4 , Ug ∈ Rl5×l4",4.1 Diversity based attention model,[0],[0]
"We compare with two recently proposed baseline diversity methods (Chen et al., 2016) as described below.",5 Baseline Methods,[0],[0]
Note that these methods were proposed in the context of abstractive summarization (not query based abstractive summarization) and we adapt them for the task of query based abstractive summarization.,5 Baseline Methods,[0],[0]
Below we just highlight the key differences from our model in computing the context vector d ′,5 Baseline Methods,[0],[0]
t passed to the decoder.,5 Baseline Methods,[0],[0]
M1:,5 Baseline Methods,[0],[0]
"This model accumulates all the previous context vectors as ∑t−1 j=1 d ′ j and incorporates
this history while computing a diverse context vector:
d ′",5 Baseline Methods,[0],[0]
"t = tanh(Wcdt − Uc
t−1∑
j=1
d ′ j) (15)
where Wc, Uc ∈ Rl4×l4 are diagonal matrices.",5 Baseline Methods,[0],[0]
We then use this diversity driven context d ′,5 Baseline Methods,[0],[0]
t in Equation (9) and (10).,5 Baseline Methods,[0],[0]
M2:,5 Baseline Methods,[0],[0]
"In this model, in addition to computing a diverse context as described in Equation (15), the attention weights at each time step are also forced to be diverse from the attention weights at the previous time step.
",5 Baseline Methods,[0],[0]
α ′,5 Baseline Methods,[0],[0]
"t,i = v T a tanh(Was ′",5 Baseline Methods,[0],[0]
t +,5 Baseline Methods,[0],[0]
"Uadt − ba
t−1∑
j=1
α ′",5 Baseline Methods,[0],[0]
"j,i)
where Wa ∈ Rl1×l1 , Ua ∈ Rl1×l4 , ba, va ∈ Rl1 , l1 is the number of hidden units in the decoder GRU.",5 Baseline Methods,[0],[0]
"Once again, they maintain a history of attention weights and compute a diverse attention vector by subtracting the history from the current attention vector.",5 Baseline Methods,[0],[0]
We evaluate our models on the dataset described in section 3.,6 Experimental Setup,[0],[0]
Note that there are no prior baselines on query based abstractive summarization so we could only compare with different variations of the encoder decoder models as described above.,6 Experimental Setup,[0],[0]
"Further, we compare our diversity based attention models with existing models for diversity by suitably adapting them to this problem as described earlier.",6 Experimental Setup,[0],[0]
"Specifically, we compare the performance of the following models:
• Vanilla e-a-d:",6 Experimental Setup,[0],[0]
This is the vanilla encoderattention-decoder model adapted to the problem of abstractive summarization.,6 Experimental Setup,[0],[0]
It contains the following components (i) document encoder (ii) document attention model (iii) decoder.,6 Experimental Setup,[0],[0]
It does not contain an encoder or attention model for the query.,6 Experimental Setup,[0],[0]
"This helps us understand the importance of the query.
",6 Experimental Setup,[0],[0]
• Queryenc:,6 Experimental Setup,[0],[0]
This model contains the query encoder in addition to the three components used in the vanilla model above.,6 Experimental Setup,[0],[0]
"It does not contain any attention model for the query.
",6 Experimental Setup,[0],[0]
• Queryatt:,6 Experimental Setup,[0],[0]
"This model contains the query attention model in addition to all the components in Queryenc.
",6 Experimental Setup,[0],[0]
"• D1: The diversity attention model as described in Section 4.1.
",6 Experimental Setup,[0],[0]
"• D2: The LSTM based diversity attention model as described in Section 4.1.
",6 Experimental Setup,[0],[0]
"• SD1: The soft diversity attention model as described in Section 4.1
•",6 Experimental Setup,[0],[0]
"SD2: The soft LSTM based diversity attention model as described in Section 4.1
• B1: Diversity cell in Figure3 is replaced by the basic LSTM cell (i.e. cdiverset = ct instead of using Equation (12).",6 Experimental Setup,[0],[0]
"This helps us understand whether simply using an LSTM to track the history of context vectors (without imposing a diversity constraint) is sufficient.
",6 Experimental Setup,[0],[0]
"• M1: The baseline model which operates on the context vector as described in Section 5.
",6 Experimental Setup,[0],[0]
"• M2: The baseline model which operates on the attention weights in addition to the context vector as described in Section 5.
",6 Experimental Setup,[0],[0]
"We used 80% of the data for training, 10% for validation and 10% for testing.",6 Experimental Setup,[0],[0]
"We create 10 such folds and report the average Rouge-1, Rouge-2, Rouge-L scores across the 10 folds.",6 Experimental Setup,[0],[0]
The hyperparameters (batch size and GRU cell sizes) of all the models are tuned on the validation set.,6 Experimental Setup,[0],[0]
"We tried the following batch sizes : 32, 64 and the following GRU cell sizes 200, 300, 400.",6 Experimental Setup,[0],[0]
"We used Adam (Kingma and Ba, 2014) as the optimization algorithm with the initial learning rate set to 0.0004, β1 = 0.9, β2 = 0.999.",6 Experimental Setup,[0],[0]
We used pre-trained publicly available Glove word embeddings2 and fine-tuned them during training.,6 Experimental Setup,[0],[0]
"The same word embeddings are used for the query words and the document words.
",6 Experimental Setup,[0],[0]
"Table 3 summarizes the results of our experiments.
",6 Experimental Setup,[0],[0]
2http://nlp.stanford.edu/projects/glove/,6 Experimental Setup,[0],[0]
"In this section, we discuss the results of the experiments reported in Table 3. 1.",7 Discussions,[0],[0]
Effect of Query:,7 Discussions,[0],[0]
Comparing rows 1 and 2 we observe that adding an encoder for the query and allowing it to influence the outputs of the decoder indeed improves the performance.,7 Discussions,[0],[0]
This is expected as the query contains some keywords which could help in sharpening the focus of the summary.,7 Discussions,[0],[0]
2.,7 Discussions,[0],[0]
Effect of Query attention model:,7 Discussions,[0],[0]
Comparing rows 2 and 3 we observe that using an attention model to dynamically compute the query representation at each time step improves the results.,7 Discussions,[0],[0]
This suggests that the attention model indeed learns to focus on relevant portions of the query at different time steps.,7 Discussions,[0],[0]
3.,7 Discussions,[0],[0]
"Effect of Diversity models: All the diversity models introduced in the paper (rows 7, 8, 9, 10) give significant improvement over the nondiversity models.",7 Discussions,[0],[0]
"In particular, the modified LSTM based diversity model gives the best results.",7 Discussions,[0],[0]
This is indeed very encouraging and Table 4 shows some sample summaries comparing the performance of different models.,7 Discussions,[0],[0]
4.,7 Discussions,[0],[0]
Comparison with baseline diversity models: The baseline diversity model M1 performs at par with our models D1 and SD1 but not as good as D2 and SD2.,7 Discussions,[0],[0]
"However, the model M2 performs very poorly.",7 Discussions,[0],[0]
We believe that simultaneously adding a constraint on the context vectors as well as attention weights (as is indeed the case with M2) is a bit too aggressive and leads to poor performance (although this needs further investigation).,7 Discussions,[0],[0]
5.,7 Discussions,[0],[0]
Quantitative Analysis:,7 Discussions,[0],[0]
"In addition to the qualitative analysis reported in Table 4 we also did a quantitative analysis by counting the num-
ber of sentences containing repeated words generated by different models.",7 Discussions,[0],[0]
Specifically for the 1268 test instances we counted the number of sentences containing repeated words as generated by different modes.,7 Discussions,[0],[0]
Table 5 summarizes this analysis.,7 Discussions,[0],[0]
In this work we proposed a query-based summarization method.,8 Conclusion,[0],[0]
"The unique feature of
the model is a novel diversification mechanism based on successive orthogonalization.",8 Conclusion,[0],[0]
This gives us the flexibility to: (i) provide diverse context vectors at successive time steps and (ii) pay attention to words repeatedly if need be later in the summary (as opposed to existing models which aggressively delete the history).,8 Conclusion,[0],[0]
We also introduced a new data set and empirically verified we perform significantly better (gain of 28% (absolute) in ROUGE-L score) than applying a plain encode-attend-decode mechanism to this problem.,8 Conclusion,[0],[0]
We observe that adding an attention mechanism on the query string gives significant improvements.,8 Conclusion,[0],[0]
We also compare with a state of the art diversity model and outperform it by a good margin (gain of 7% (absolute) in ROUGE-L score).,8 Conclusion,[0],[0]
The diversification model proposed is general enough to apply to other NLG tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.,8 Conclusion,[0],[0]
Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion.,abstractText,[0],[0]
"On the other hand, query-based summarization highlights those points that are relevant in the context of a given query.",abstractText,[0],[0]
"The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc.",abstractText,[0],[0]
But it suffers from the drawback of generation of repeated phrases.,abstractText,[0],[0]
In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary.,abstractText,[0],[0]
In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia.,abstractText,[0],[0]
Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.ive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion.,abstractText,[0],[0]
"On the other hand, query-based summarization highlights those points that are relevant in the context of a given query.",abstractText,[0],[0]
"The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc.",abstractText,[0],[0]
But it suffers from the drawback of generation of repeated phrases.,abstractText,[0],[0]
In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary.,abstractText,[0],[0]
In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia.,abstractText,[0],[0]
Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.,abstractText,[0],[0]
Diversity driven attention model for query-based abstractive summarization,title,[0],[0]
"ar X
iv :1
70 9.
01 12
1v 2
[ cs
.C L
] 2
6 Fe
learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to groundtruth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",text,[0],[0]
"Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016).",1 Introduction,[0],[0]
"Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce
∗Now at eBay, Inc.
parse trees that they then consume.",1 Introduction,[0],[0]
"Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.",1 Introduction,[0],[0]
"These models are designed to learn grammars—strategies for assigning trees to sentences—that are suited to help solve the sentence understanding task at hand, rather than ones that approximate expert-designed grammars like that of the Penn Treebank (PTB; Marcus et al., 1999).
",1 Introduction,[0],[0]
"Latent tree learning models have shown striking success at sentence understanding, reliably performing better on sentiment analysis and textual entailment than do comparable TreeRNN models which use parses assigned by conventional parsers, and setting the state of the art among sentence-encoding models for textual entailment.",1 Introduction,[0],[0]
"However, none of the
work in latent tree learning to date has included any substantial evaluation of the quality of the trees induced by these models, leaving open an important question which this paper aims to answer: Do these models owe their success to consistent, principled latent grammars?",1 Introduction,[0],[0]
"If they do, these grammars may be worthy objects of study for researchers in syntax and semantics.",1 Introduction,[0],[0]
"If they do not, understanding why the models succeed without such syntax could lead to new insights into the use of TreeRNNs and into sentence understanding more broadly.
",1 Introduction,[0],[0]
"While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has long been clear that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units, or constituents (Chomsky, 1965; Frege, 1892; Heim and Kratzer, 1998).",1 Introduction,[0],[0]
"This is well illustrated by structurally ambiguous sentences like the one below, repeated from Sag (1991)",1 Introduction,[0],[0]
"a.o.:
1. (a) I saw the [ man [ with the telescope ] ]
→֒I saw the man who had a telescope.",1 Introduction,[0],[0]
(b) I [ saw the man ],1 Introduction,[0],[0]
"[ with the telescope ]
→֒ I used the telescope to view the man.
",1 Introduction,[0],[0]
"Under the partial constituency parse shown in 1a, with a telescope forms a constituent with man, thereby providing additional information about the individual man describes.",1 Introduction,[0],[0]
"On the other hand, in 1b, with a telescope does not form a constituent with man, but instead provides additional information about the action described by saw a man.",1 Introduction,[0],[0]
"In this way, the same string of words can be assigned two different, yet equally valid constituency structures reflecting the different interpretations for the string.",1 Introduction,[0],[0]
"Constituency can be straightforwardly expressed using unlabeled parse trees like the ones used in TreeRNNs, and expressing constituency information is generally thought to be the primary motivation for using trees in TreeRNNs.
",1 Introduction,[0],[0]
"In this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al. (2018) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four
issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.
",1 Introduction,[0],[0]
"We confirm that both types of model succeed at producing usable sentence representations, but find that only the stronger of the two models—that of Choi et al. (2018)—outperforms either a comparable TreeRNN baseline or a simple LSTM RNN.",1 Introduction,[0],[0]
"We find that the grammar of the Choi et al. model varies dramatically across random restarts, and tends to agree with PTB grammar with roughly chance accuracy.",1 Introduction,[0],[0]
"We do find, though, that the resulting grammar has some regularities, including a preference for shallow trees, a somewhat systematic treatment of negation, and a preference to treat pairs of adjacent words at the edges of a sentence as constituents.",1 Introduction,[0],[0]
"The work discussed in this paper is closely related to work on grammar induction, in which statistical learners attempt to solve the difficult problem of reconstructing the grammar that generated a corpus of text using only that corpus and, optionally, some heuristics about the nature of the expected grammar.",2 Background,[0],[0]
"Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977).",2 Background,[0],[0]
"One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.",2 Background,[0],[0]
"In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model.",2 Background,[0],[0]
"They find that the resulting SRL model is more effective than one built on a purely unsupervised grammar induction system, but that using a conventionally trained parser instead yields better SRL performance.
",2 Background,[0],[0]
"There is also a long history of work on artificial neural network models that build latent hierarchical structures without direct supervision when solving
algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently, Joulin and Mikolov (2015) and Grefenstette et al. (2015).
",2 Background,[0],[0]
We are only aware of four prior works on latent tree learning for sentence understanding with neural networks.,2 Background,[0],[0]
"All four jointly train two model components—a parser based on distributed representations of words and phrases, and a TreeRNN of some kind that uses those parses—but differ in the parsing strategies, TreeRNN parameterizations, and training objective used.
",2 Background,[0],[0]
Socher et al. (2011) present the first neural network model that we are aware of that use the same learned representations to both parse a sentence and—using the resulting parse in a TreeRNN— perform sentence-level classification.,2 Background,[0],[0]
They use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair.,2 Background,[0],[0]
"They train their model on a sentiment objective, but rather than training the parsing component on that objective as well, they use a combination of an auxiliary autoencoding objective and a nonparametric scoring function to parse.",2 Background,[0],[0]
"While this work shows good results on sentiment, it does not feature any evaluation of the induced trees, either through direct analysis nor through comparison with any sentiment baseline that uses trees from a conventionally-trained parser.",2 Background,[0],[0]
"Bowman et al. (2016) introduce an efficient, batchable architecture for doing this—the Shift-reduce ParserInterpreter Neural Network (SPINN; Figure 2)— which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work.
",2 Background,[0],[0]
The remaining three models all use TreeLSTMs,2 Background,[0],[0]
"(Tai et al., 2015), and all train and evaluate both components on a shared semantic objective.",2 Background,[0],[0]
"All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective.",2 Background,[0],[0]
"The models differ from one another primarily in the ways in which they use this task objective to train their parsing components, and in the structures of those components.
",2 Background,[0],[0]
"Yogatama et al. (2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise
receive gradients through backpropagation.",2 Background,[0],[0]
"Surprisingly, and in contrast to Gormley et al., they find that a small 100D instance of this RL-SPINN model performs somewhat better on several text classification tasks than an otherwise-identical model which is explicitly trained to parse.
",2 Background,[0],[0]
"In unpublished work, Maillard et al. (2017) present a model which explicitly computes O(N2) possible tree nodes for N words, and uses a soft gating strategy to approximately select valid combinations of these nodes that form a tree.",2 Background,[0],[0]
"This model is trainable entirely using backpropagation, and a 100D instance of the model performs slightly better than RL-SPINN on SNLI.
",2 Background,[0],[0]
"Choi et al. (2018) present a model (which we call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al., but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016).",2 Background,[0],[0]
"This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al.. They report substantial gains in both speed and accuracy over Maillard et al. and Yogatama et al. on SNLI.
",2 Background,[0],[0]
"Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models.",2 Background,[0],[0]
"Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth.
",2 Background,[0],[0]
Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information.,2 Background,[0],[0]
Recent highlights from this work include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation.,2 Background,[0],[0]
"Also on translation, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering.",2 Background,[0],[0]
"These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion
of syntactic constituency.",2 Background,[0],[0]
This paper investigates the behavior of two models: RL-SPINN and ST-Gumbel.,3 Models and Methods,[0],[0]
"Both have been shown to outperform similar models based on supervised parsing, and the two represent substantially different approaches to latent tree learning.
",3 Models and Methods,[0],[0]
SPINN Variants Three of our of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016).,3 Models and Methods,[0],[0]
"Figure 2 shows and describes the architecture.
",3 Models and Methods,[0],[0]
"In the base SPINN model, all model components are used, and the transition classifier is trained on binarized Penn Treebank-style parses from the Stanford PCFG Parser (Klein and Manning, 2003), which are included with SNLI and MultiNLI.",3 Models and Methods,[0],[0]
"These binary-branching parse trees are converted to SHIFT/REDUCE sequences for use in the model through a simple reversible transformation.
",3 Models and Methods,[0],[0]
"RL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss.",3 Models and Methods,[0],[0]
"Because this component produces discrete decisions, the REINFORCE algorithm (with the standard exponential moving average baseline) is used to supply gradients for it.",3 Models and Methods,[0],[0]
We explored several alternative baseline strategies—including parametric value networks and strategies based on greedy decoding— as well as additional strategies for increasing exploration.,3 Models and Methods,[0],[0]
We also thoroughly tuned the relevant hyperparameter values for each alternative.,3 Models and Methods,[0],[0]
"In all of these experiments, we found that a standard implementation with the exponential moving average baseline produces accuracy no worse than any readily available alternative.
",3 Models and Methods,[0],[0]
We also evaluate two other variants of SPINN as baselines.,3 Models and Methods,[0],[0]
"In SPINN-NC (for No Connection from tracking to composition), the connection from the tracking LSTM to the composition function is severed.",3 Models and Methods,[0],[0]
"This weakens the model, but makes it exactly equivalent to a plain TreeLSTM—it will produce the exact same vector that a TreeLSTM with the same composition function would have produced for the tree that the transition classifier implicitly pro-
duces.",3 Models and Methods,[0],[0]
"This model serves as a maximally comparable baseline for the ST-Gumbel model, which also performs composition using a standard TreeLSTM in forward-propagation.
",3 Models and Methods,[0],[0]
"SPINN-PI-NT (for Parsed Input, No Tracking) removes the tracking LSTM, as well as the two components that depend on it: the tracking-composition connection and the transition decision function.",3 Models and Methods,[0],[0]
"As such, it cannot produce its own parse trees and must rely on trees from the input data.",3 Models and Methods,[0],[0]
"We include this in our comparison to understand the degree to which training a parser, rather than using a higher-quality off-the-shelf parser, impacts performance on our semantic task.
",3 Models and Methods,[0],[0]
ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2018) and is shown in Figure 3.,3 Models and Methods,[0],[0]
The model takes a sequence of N − 1 steps to build a tree over N words.,3 Models and Methods,[0],[0]
"At every step, every possible pair of adjacent words or phrase vectors in the partial tree is given to a TreeLSTM composition function to produce a new candidate phrase vector.",3 Models and Methods,[0],[0]
"A simple learned scoring function then selects the best of these candidates, which forms a constituent node in the tree and replaces its two children in the list of nodes that are available to compose.",3 Models and Methods,[0],[0]
"This repeats until only two nodes remain, at which point they are composed and the tree is complete.",3 Models and Methods,[0],[0]
"This exhaustive search increases the computational complexity of the model over (RL-)SPINN, but also allows the model to perform a form of easy-first parsing, making it easier for the model to explore the space of possible parsing strategies.
",3 Models and Methods,[0],[0]
"Though the scoring function yields discrete decisions, the Jang et al. Straight-Through GumbelSoftmax estimator makes it possible to nonetheless efficiently compute an approximate gradient for the full model without the need for relatively brittle policy gradient techniques like REINFORCE.
",3 Models and Methods,[0],[0]
Other Baselines We also train three baselines that do not depend on a parser.,3 Models and Methods,[0],[0]
The first is a unidirectional LSTM RNN over the embedded tokens.,3 Models and Methods,[0],[0]
"The second is a version of SPINN-PI-NT that is supplied sequences of randomly sampled legal transitions (corresponding to random parse trees), rather than the output of a parser.",3 Models and Methods,[0],[0]
"The third is also a version of SPINN-PI-NT, and receives transition sequences corresponding to maximally-
shallow, approximately-balanced parse trees based on the “full binary” trees used in Munkhdalai and Yu (2017b).
",3 Models and Methods,[0],[0]
"Data To ensure that we are able to roughly reproduce the results reported by Yogatama et al. and Choi et al., we conduct an initial set of experiments on the Stanford NLI corpus of Bowman et al. (2015).",3 Models and Methods,[0],[0]
"Our primary experiments use the newer Multi-Genre Natural Language Inference Corpus (MultiNLI; Williams et al., 2017).",3 Models and Methods,[0],[0]
"MultiNLI is a 433k-example textual entailment dataset created in the style of SNLI, but with a more diverse range
of source texts and longer, more complex sentences, which we expect will encourage the models to produce more consistent and interpretable trees than they otherwise might.",3 Models and Methods,[0],[0]
"Following Williams et al., we train on the combination of MultiNLI and SNLI in these experiments (yielding just under 1M training examples) and evaluate on MultiNLI (using the matched development and test sets).
",3 Models and Methods,[0],[0]
"We also evaluate trained models on the full Wall Street Journal section of the Penn Treebank, a seminal corpus of manually-constructed constituency parses, which introduced the parsing standard used in this work.",3 Models and Methods,[0],[0]
"Because the models under study produce and consume binary-branching constituency trees without labels (and because such trees are already included with SNLI and MultiNLI), we use the Stanford Parser’s CollapseUnaryTransformer and TreeBinarizer tools to convert these Penn Treebank Trees to this form.
",3 Models and Methods,[0],[0]
"Sentence Pair Classification Because our textual entailment task requires a model to classify pairs of sentences, but the models under study produce vectors for single sentences, we concatenate the two sentence vectors, their difference, and their elementwise product (following Mou et al., 2016), and feed the result into a 1024D ReLU layer to produce a representation for the sentence pair.",3 Models and Methods,[0],[0]
"This representation is fed into a three-way softmax classifier that selects
one of the labels entailment, neutral, and contradiction for the pair.
",3 Models and Methods,[0],[0]
Additional Details We implement all models in PyTorch 0.2.,3 Models and Methods,[0],[0]
"We closely follow the original Theano code for SPINN in our implementation, and we incorporate source code provided by Choi et al. for the core parsing data structures and sampling mechanism of the ST-Gumbel model.",3 Models and Methods,[0],[0]
"Our code, saved models, and model output are available on GitHub.1
We use GloVe vectors to represent words (standard 300D, 840B word package, without fine tuning; Pennington et al., 2014), and feed them into a singlelayer 2",3 Models and Methods,[0],[0]
× 300D bi-directional GRU RNN (based on the leaf LSTM of Choi et al.),3 Models and Methods,[0],[0]
to give the models access to local context information when making parsing decisions.,3 Models and Methods,[0],[0]
"To understand the impact of this component, we follow Choi et al. in also training each model with the leaf GRU replaced with a simpler context-insensitive input encoder that simply multiplies each GloVe vector by a matrix.",3 Models and Methods,[0],[0]
"We find that these models perform best when the temperature of the ST-Gumbel distribution is a trained parameter, rather than fixed at 1.0 as in Choi et al..
We use L2 regularization and apply dropout (Srivastava et al., 2014) to the input of the 1024D sentence pair combination layer.",3 Models and Methods,[0],[0]
"We train all models using the Adam optimizer (Kingma and Ba, 2015).",3 Models and Methods,[0],[0]
"For hyperparameters for which no obvious default value exists—the L2 and dropout parameters, the relative weighting of the gradients from REINFORCE in RL-SPINN, the starting learning rate, and the size of the tracking LSTM state in SPINN—we heuristically select ranges in which usable values can be found (focusing on MultiNLI development set performance), and then randomly sample values from those ranges.",3 Models and Methods,[0],[0]
We train each model five times using different samples from those ranges and different random initializations for model parameters.,3 Models and Methods,[0],[0]
We use early stopping based on development set performance with all models.,3 Models and Methods,[0],[0]
"Table 1 shows the accuracy of all models on two test sets: SNLI (training on SNLI only, for comparison
1 https://github.com/nyu-mll/spinn/tree/is-it-syntax-release
with prior work), and MultiNLI (training on both datasets).",4 Does latent tree learning help sentence understanding?,[0],[0]
"Each figure represents the accuracy of the best run, selected using the development set, of five runs with different random initializations and hyperparameter values.",4 Does latent tree learning help sentence understanding?,[0],[0]
"Our LSTM baseline is strikingly effective, and matches or exceeds the performance of all of our PTB grammar-based tree-structured models on both SNLI and MultiNLI.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"This contradicts the primary result of Bowman et al. (2016), and suggests that there is little value in using the correct syntactic structure for a sentence to guide neural network composition, at least in the context of the TreeLSTM composition function and the NLI task.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"We do, however, reproduce the key result of Choi et al. on both datasets.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Their ST-Gumbel model, which receives no syntactic information at training time, outperforms SPINN-NC, which performs composition in an identical manner but is explicitly trained to parse, and also outperforms the LSTM baseline.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"This suggests that the learned latent trees are helpful in the construction of semantic representations for sentences, whether or not they resemble conventional parse trees.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
Our results with RL-SPINN are more equivocal.,300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"That model matches, but does not beat, the performance of the full SPINN model, which is equivalent except that it is trained to parse.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"However, our implementation of RL-SPINN outperforms Yogatama et al.’s (lower-dimensional) implementation by a substantial margin.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"The impact of the leaf GRU is sometimes substantial, but the direction of its effect is not consistent.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Our results with SPINN-PI-NT are not substantially better than those with any other model, suggesting the relatively simple greedy parsing strategies used by the other models are not a major limiting factor in their performance.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Balanced trees consistently outperform randomly sampled transitions (albeit by a small margin), yet perform worse than ST-Gumbel even though ST-Gumbel uses very shallow trees as well.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Similarly, RL-SPINN depends on mostly left-branching binary parse trees, but is outperformed by a forward LSTM.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Structure is important, but there are differences between the architectures of compositional models worth investigating in future work.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"None of our latent tree learning models reach the state of the art on either task, but all are comparable in both absolute and relative performance to other published results, suggesting that we have trained reasonable examples of latent tree learning models and can draw informative conclusions by studying the behaviors of these models.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"If it were the case that a latent tree learning model outperforms its baselines by identifying some specific grammar for English that is better than the one used in PTB and the Stanford Parser, then we would expect these models to identify roughly the same grammar across random restarts and minor configuration changes, and to use that grammar to produce consistent task performance.",5 Are these models consistent?,[0],[0]
"Table 2 shows two measures of consistency for the four models that produce parses, a simple random baseline that produces parses by randomly merging pairs of adjacent words and phrases, and (trivially) the deterministic strategy used in the Balanced Trees runs.
",5 Are these models consistent?,[0],[0]
We first show the variation in accuracy on the MultiNLI development set across runs.,5 Are these models consistent?,[0],[0]
"While one outlier distorts these numbers for ST-Gumbel without the leaf GRU, these figures are roughly equivalent between the latent tree learning models and the baselines, suggesting that these models are not substantially more brittle or more hyperparameter sensitive in their task performance.",5 Are these models consistent?,[0],[0]
"The second metric shows the self F1 for each model: the unlabeled binary F1 between the parses produced by two runs of the same model for the MultiNLI development
set, averaged out over all possible pairings of different runs.",5 Are these models consistent?,[0],[0]
"This measures the degree to which the models reliably converge on the same parses, and sheds some light on the behavior of the models.",5 Are these models consistent?,[0],[0]
"The baseline models show relatively high consistency, with self F1 above 65%.",5 Are these models consistent?,[0],[0]
"ST-Gumbel is substantially less consistent, with scores below 50% but above the 32.6% random baseline.",5 Are these models consistent?,[0],[0]
"RL-SPINN appears to be highly consistent, with the runs without the leaf GRU reaching 98.5% self F1, suggesting that it reliably converges to a specific grammar.",5 Are these models consistent?,[0],[0]
"However, as we will discuss in later sections, this grammar appears to be trivial.",5 Are these models consistent?,[0],[0]
"Given that our latent tree learning models are at least somewhat consistent in what they learn, it is reasonable to ask what it is that they learn.",6 Do these models learn PTB grammar?,[0],[0]
"We investigate this first quantitatively, then, in the next section, more qualitatively.
",6 Do these models learn PTB grammar?,[0],[0]
Table 3 shows parsing performance on the Wall Street Journal sections of PTB for models trained on SNLI and MultiNLI.,6 Do these models learn PTB grammar?,[0],[0]
"The baseline models perform fairly poorly in absolute terms, as they are neither well tuned for parse quality nor trained on news text, but the latent tree learning models perform dramatically worse.",6 Do these models learn PTB grammar?,[0],[0]
"The ST-Gumbel models perform at or slightly above chance (represented by the random trees results), while the RL-SPINN models per-
form consistently below chance.",6 Do these models learn PTB grammar?,[0],[0]
"These results suggest that these models do not learn grammars that in any way resemble PTB grammar.
",6 Do these models learn PTB grammar?,[0],[0]
"To confirm this, we also show results for individual Penn Treebank nonterminal node types.",6 Do these models learn PTB grammar?,[0],[0]
"On common intermediate node types such as ADJP, NP, and PP, the latent tree learning models do not perform substantially better than chance.",6 Do these models learn PTB grammar?,[0],[0]
"It is only on a two rare types that any latent tree learning model, or the balanced tree baseline, outperforms random trees by a significant margin: INTJ (interjection, as in Oh no, he ’s...) and the even rarer LST (list marker, as in 1 .",6 Do these models learn PTB grammar?,[0],[0]
"Determine if...), both of which are generally short and sentence-initial (discussed in more detail in Section 7).
",6 Do these models learn PTB grammar?,[0],[0]
"Next, we turn to the MultiNLI development set for further investigation.",6 Do these models learn PTB grammar?,[0],[0]
Table 4 shows results on MultiNLI for a wider range of measures.,6 Do these models learn PTB grammar?,[0],[0]
"The table shows F1 measured with respect to three different references: automatically generated trivial trees for the corpus that are either strictly left-branching or strictly right-branching, and the PTB-style trees produced by the Stanford Parser for the corpus.",6 Do these models learn PTB grammar?,[0],[0]
"We see that the baseline models perform about as well on MultiNLI as on PTB, with scores above 65%, and that these models produce trees that tend toward right branching rather than left branching.
",6 Do these models learn PTB grammar?,[0],[0]
"The ST-Gumbel models perform only at or slightly above chance on the parsed sentences, and
show a similar use of both right- and left-branching structures, with only a slight preference for the more linguistically common right-branching structures.",6 Do these models learn PTB grammar?,[0],[0]
"This suggests that they learn grammars that differ quite substantially from PTB grammars, but may share some minor properties.",6 Do these models learn PTB grammar?,[0],[0]
"Our implementation of an ST-Gumbel model has F1 scores with respect to left-branching and Stanford Parser trees that are much closer to the ones Yogatama et al. report than to the ones we find for our RL-SPINN implementation.
",6 Do these models learn PTB grammar?,[0],[0]
Our RL-SPINN results are unequivocally negative.,6 Do these models learn PTB grammar?,[0],[0]
"We find that our models could be tuned to produce trees that are qualitatively similar to those in Yogatama et al.. However, in our primary experiments, we tune model hyperparameters with the sole criterion of downstream task performance, and find that the trees from these experiments yield relactively trivial trees, with F1 scores that are much lower than theirs with respect to the Stanford Parser, and much higher with respect to left branching trees (see Table 4).",6 Do these models learn PTB grammar?,[0],[0]
"All runs perform below chance on the parsed sentences, and all have F1 scores over 92% with respect to the left-branching structures, suggesting that they primarily learn to produce strictly left-branching trees.",6 Do these models learn PTB grammar?,[0],[0]
"This trivial strategy, which
makes the model roughly equivalent to a sequential RNN, is very easy to learn.",6 Do these models learn PTB grammar?,[0],[0]
"In a shift–reduce model like SPINN, the model can simply learn to perform the REDUCE operation whenever it is possible to do so, regardless of the specific words and phrases being parsed.",6 Do these models learn PTB grammar?,[0],[0]
"This can be done by setting a high bias value for this choice in the transition classifier.
",6 Do these models learn PTB grammar?,[0],[0]
The rightmost column shows another measure of what is learned: the average depth—the length of the path from the root to any given word—of the induced trees.,6 Do these models learn PTB grammar?,[0],[0]
"For the baseline models, this value is slightly above the 5.7 value for the Stanford Parser trees.",6 Do these models learn PTB grammar?,[0],[0]
"For the RL-SPINN models, this number is predictably much higher, reflecting the very deep and narrow left-branching trees that those models tend to produce.",6 Do these models learn PTB grammar?,[0],[0]
"For the ST-Gumbel model, though, this metric is informative: the models consistently produce shallow trees with depth under 5—closer to the balanced trees baseline than to SPINN.",6 Do these models learn PTB grammar?,[0],[0]
"This hints at a possible interpretation: While shallower trees may be less informative about the structure of the sentence than real PTB trees, they reduce the number of layers that a word needs to pass through to reach the final classifier, potentially making it easier to learn an effective composition function that faithfully encodes the contents of a sentence.",6 Do these models learn PTB grammar?,[0],[0]
"This interpretation
is supported by the results of Munkhdalai and Yu (2017b), who show that it is possible to do well on SNLI using a TreeLSTM (with a leaf LSTM) over arbitrarily chosen balanced trees with low depths, and our balanced trees baseline, which approximates this result.
",6 Do these models learn PTB grammar?,[0],[0]
The ST-Gumbel models tend to implement their shallow parsing strategy with a good deal of randomness.,6 Do these models learn PTB grammar?,[0],[0]
"They tend to assign near-zero probability (< 10−10) to many possible compositions, generally those that would result in unnecessarily deep trees, and relatively smooth probabilities (generally > 0.01) to the remaining options.",6 Do these models learn PTB grammar?,[0],[0]
"The trainable temperature parameter for these models generally converged slowly to a value between 1 and 20, and did not fluctuate substantially during training.",6 Do these models learn PTB grammar?,[0],[0]
"In the previous three sections, we have shown that latent tree learning models are able to perform as well or better than models that have access to linguistically principled parse trees at training or test time, but that the grammars that they learn are neither consistent across runs, nor meaningfully similar to PTB grammar.",7 Analyzing the Learned Trees,[0],[0]
"In this section, we investigate the trees produced by these learned grammars directly to identify whether they capture any recognizable syntactic or semantic phenomena.
",7 Analyzing the Learned Trees,[0],[0]
The RL-SPINN models create overwhelmingly left-branching trees.,7 Analyzing the Learned Trees,[0],[0]
"We observe few deviations from this pattern, which occur almost exclusively on sentences with fewer than seven words.",7 Analyzing the Learned Trees,[0],[0]
"Given that the self-F1 scores for these models (92.7 and 98.5, Table 2) are similar to their F1 scores with respect to strictly left-branching trees (95.0 and 99.1, Table 4), there is little room for these models to have learned any consistent behavior beside left branching.
",7 Analyzing the Learned Trees,[0],[0]
"In some preliminary tuning runs not shown above, we saw models that deviated from this pattern more often, and one that fixated on right-branching structures instead, but we find no grammatically interesting patterns in any of these deviant structures.
",7 Analyzing the Learned Trees,[0],[0]
"The ST-Gumbel models learned substantially more complex grammars, and we focus on these for the remainder of the section.",7 Analyzing the Learned Trees,[0],[0]
We discuss three model behaviors which yield linguistically implausible constituents.,7 Analyzing the Learned Trees,[0],[0]
"The first two highlight settings
where the ST-Gumbel model is consistent where it shouldn’t be, and the third highlights a setting in which it is worryingly inconsistent.",7 Analyzing the Learned Trees,[0],[0]
"The models’ treatment of these three phenomena and our observation of these models’ behavior more broadly suggest that the models do not produce trees that follow any recognizable semantic or syntactic principles.
",7 Analyzing the Learned Trees,[0],[0]
Initial and Final Two-Word Constituents The shallow trees produced by the ST-Gumbel models generally contain more constituents comprised of two words (rather than a single word combined with a phrase) than appear in the reference parses.,7 Analyzing the Learned Trees,[0],[0]
"This behavior is especially pronounced at the edges of sentences, where the models frequently treat the first two words and the last two words as constituents.",7 Analyzing the Learned Trees,[0],[0]
"Since this behavior does not correspond to any grammatical phenomenon known to these authors, it likely stems from some unknown bias within the model design.
",7 Analyzing the Learned Trees,[0],[0]
These models parse the first two words of a sentence into a constituent at rates well above both the 50% rate seen in random and balanced parses2 and the 27.7% rate seen with the SPINN models.,7 Analyzing the Learned Trees,[0],[0]
This strategy appeares in 77.4% of the model’s parses with the leaf GRU and 64.1% without.,7 Analyzing the Learned Trees,[0],[0]
"While it was consistently discovered across all of our runs of STGumbel models with the leaf GRU, it was discovered less frequently across restarts for runs without, which do not have direct access to linear position information.",7 Analyzing the Learned Trees,[0],[0]
"We observe that the models combine the final two words in each sentence at similar rates.
",7 Analyzing the Learned Trees,[0],[0]
"While merging the final two words of a sentence nearly always results in a meaningless constituent containing a period or punctuation mark, merging
2The balanced parses are right-aligned, following Munkhdalai and Yu; they parse the first two words as a constituent in about 50% of cases, but the final two words in all cases.
",7 Analyzing the Learned Trees,[0],[0]
the first two words can produce reasonable parses.,7 Analyzing the Learned Trees,[0],[0]
"This strategy is reasonable, for example, in sentences that begin with a determiner and a noun (Figure 4, top left).",7 Analyzing the Learned Trees,[0],[0]
"However, combining the first two words in sentences that start with adverbials, proper names, bare plurals, or noun phrases with multiple modifiers will generally result in meaningless constituents like Kings frequently (Figure 4, bottom).
",7 Analyzing the Learned Trees,[0],[0]
Combining the first two words of a sentence also often results in more subtly unorthodox trees—like the one in the top right of Figure 4—that combine a verb with its subject rather than its object.,7 Analyzing the Learned Trees,[0],[0]
"This contrasts with some mainstream syntactic theories (Adger 2003; Sportiche et al. 2013), which generally take the object and the verb of a sentence to form a constituent for three reasons: Taking the top right sentence in Figure 4 as an example, (i) we can replace it with a new constituent of the same type without changing the surrounding sentence structure, as in he did so, (ii) it can stand alone as an answer to a question like what did he do?, and (iii) it can be omitted in otherwise-repetitive sentences like He shot his gun, but Bill didn’t .
",7 Analyzing the Learned Trees,[0],[0]
"Negation The ST-Gumbel models also tend to learn a systematic and superficially reasonable strategy for negation: they pair any negation word (e.g., not, n’t, no, none) with the word that immediately follows it.",7 Analyzing the Learned Trees,[0],[0]
"Random parses only form these constituents in 34% of the sentences, and balanced parses only do so in 50%, but the ST-Gumbel models with the leaf GRU do so about 67% of the time and consistently across runs, while those without the leaf GRU do so less consistently, but over 90% of the time in some runs.
",7 Analyzing the Learned Trees,[0],[0]
"This strategy is effective when the negation word is meant to modify a single other word to its right, as in Figure 5, top left sub-figure, but this is frequently not the case.",7 Analyzing the Learned Trees,[0],[0]
"In Figure 5, bottom left, although the model creates the potentially reasonable constituent, not at all, it also combines not with the preposition at to form a constituent with no clear interpretation (or, at best, an incredibly bizarre one).",7 Analyzing the Learned Trees,[0],[0]
"Further, combining not with at goes contra the syntactic observation that prepositional phrases can generally move along with the following noun phrases as a constituent (as in semantically comparably sentences like, He is not sure at all.).
",7 Analyzing the Learned Trees,[0],[0]
Function Words and Modifiers,7 Analyzing the Learned Trees,[0],[0]
"Finally, the STGumbel models are not consistent in their treatment of function words, like determiners or prepositions, or in their treatment of modifiers like adverbs and adjectives.",7 Analyzing the Learned Trees,[0],[0]
This reflects quantitative results in Table 3 showing that ST-Gumbel parse trees correspond to PTB for PP and ADJP constituents at much lower rates than do SPINN-based models or models supplied with random trees.,7 Analyzing the Learned Trees,[0],[0]
"For example, the top left tree of Figure 6 (ST-Gumbel) associated the determiner the with the verb, when it should form a constituent with the noun phrase Nazi angle as in the top right tree (PTB).",7 Analyzing the Learned Trees,[0],[0]
"The resulting phrase, the Nazi angle, has a clear meaning—unlike discussed the—and it passes syntactic tests for constituency; for example, one can replace the noun phrase with the pronoun it without otherwise modifying the meaning of the sentence.
",7 Analyzing the Learned Trees,[0],[0]
"Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013), as in the the bottom right tree (PTB) of Figure 6.",7 Analyzing the Learned Trees,[0],[0]
"One syntactic test that with horror forms a P-NP constituent comes from the fact that it can be a stand-alone answer to a question; for example, the question how did the students react?",7 Analyzing the Learned Trees,[0],[0]
can be answered simply with with horror.,7 Analyzing the Learned Trees,[0],[0]
"ST-Gumbel models often instead pair prepositions with the verb phrases that precede them, as in Figure 6, lower left, where this results in the constituent the students acted with, which cannot be a stand-alone answer to a question.",7 Analyzing the Learned Trees,[0],[0]
"From this perspective, constituents like discussed the and we briefly (Figure 6, top left) are also syntactically anomalous, and cannot be given coherent meanings.
",7 Analyzing the Learned Trees,[0],[0]
"The ST-Gumbel models outperform syntax-based models on MultiNLI and SNLI, and the trees that they assign to sentences do not generally resemble
those of PTB grammar.",7 Analyzing the Learned Trees,[0],[0]
"If we attempt to interpret these trees under the standard assumption that all the constituents in a sentence must be interpretable and must contribute to the meaning of the sentence, we force ourselves to interpret implausible constituents like we briefly, and reach implausible sentence-level interpretations, such as taking the sentence in Figure 6, top left, to mean that those of us who are brief discussed the Nazi angle.",7 Analyzing the Learned Trees,[0],[0]
"It is clear that these models do not use constituency in the same way as the widely accepted syntactic or semantic frameworks we cite do.
",7 Analyzing the Learned Trees,[0],[0]
"In sum, we find that RL-SPINN adopts a trivial, largely left-branching parse strategy, which is consistent across runs.",7 Analyzing the Learned Trees,[0],[0]
"ST-Gumbel, on the other hand, adopts the unexpected strategy to merge initial and final constituents at higher than average rates, and is also very inconsistent with its behavior on function words and modifiers.",7 Analyzing the Learned Trees,[0],[0]
"We weren’t able to qualitatively identify structure that matches PTB-style syntax in ST-Gumbel parses, but we do find that it utilizes a strategy for negation—merging it with the immediately following constituent—that can lead to unexpected constituents, but nevertheless, is somewhat promising.",7 Analyzing the Learned Trees,[0],[0]
The experiments and analysis presented in this paper show that the best available models for latent tree learning learn grammars that do not correspond to the structures of formal syntax and semantics in any recognizable way.,8 Conclusion,[0],[0]
"In spite of this, these models perform as well or better on sentence understanding— as measured by MultiNLI performance—as models with access to Penn Treebank-style parses.
",8 Conclusion,[0],[0]
"This result leaves us with an immediate puzzle: What do these models—especially those based on the ST-Gumbel technique—learn that allows them
to do so well?",8 Conclusion,[0],[0]
"We present some observations, but we are left without a fully satisfying explanation.",8 Conclusion,[0],[0]
"A thorough investigation of this problem will likely require a search of new architectures for sentence encoding that borrow various behaviors from the models trained in this work.
",8 Conclusion,[0],[0]
This result also opens farther-reaching questions about grammar and sentence understanding: Will the optimal grammars for sentence understanding problems like NLI—were we to explore the full space of grammars to find them—share any recognizable similarities with the structures seen in formal work on syntax and semantics?,8 Conclusion,[0],[0]
"A priori, we should expect that they should.",8 Conclusion,[0],[0]
"While it is unlikely that PTB grammar is strictly optimal for any task, the empirical motivations for many of its core constituent types—the noun phrase, the prepositional phrase, and so forth—are straightforward and compelling.",8 Conclusion,[0],[0]
"However, our best latent tree learning models do not seem able to discover these structures.
",8 Conclusion,[0],[0]
"If we accept that some form of principled constituent structure is necessary or desirable, then we are left with an engineering problem: How do we identify this structure?",8 Conclusion,[0],[0]
"Making progress in this direction will likely involve both improvements to the TreeRNN models at the heart of latent tree learning systems, to make sure that these models are able to perform composition effectively enough to be able to make full use of learned structures, and also improvements to the structure search methods that are used to explore possible grammars.",8 Conclusion,[0],[0]
"This project has benefited from financial support to SB by Google, Tencent Holdings, and Samsung Research and from a Titan X Pascal GPU donated by the NVIDIA Corporation to AD.",Acknowledgments,[0],[0]
"Jon Gauthier contributed to early discussions that motivated this work, and he, Nikita Nangia, Kelly Zhang, and Cipta Herwana provided help and advice.",Acknowledgments,[0],[0]
"Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to groundtruth parse trees at training time.",abstractText,[0],[0]
"Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers.",abstractText,[0],[0]
This paper aims to investigate what these latent tree learning models learn.,abstractText,[0],[0]
"We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",abstractText,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1722–1732, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness, controlling for embedding dimensionality. We find that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications.",text,[0],[0]
Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks.,1 Introduction,[0],[0]
"Most traditional embedding models associate each word
type with a single embedding (e.g., Bengio et al. (2006)).",1 Introduction,[0],[0]
Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings.,1 Introduction,[0],[0]
"More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding.
",1 Introduction,[0],[0]
"Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).",1 Introduction,[0],[0]
"Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).
",1 Introduction,[0],[0]
"Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps:
1.",1 Introduction,[0],[0]
"Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet.
2.",1 Introduction,[0],[0]
"Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses for its tokens and associate them with corresponding sense-specific embeddings.
3.",1 Introduction,[0],[0]
"Representation acquisition for phrases or sentences: learn representations for text units given sense-specific embeddings and pass them to machine learning classifiers.
",1 Introduction,[0],[0]
"Most existing work on multi-sense embeddings emphasizes the first step by learning sense spe-
1722
cific embeddings, but does not explore the next two steps.",1 Introduction,[0],[0]
"These are important steps, however, since it isn’t clear how existing multi-sense embeddings can be incorporated into and benefit realworld NLU tasks.
",1 Introduction,[0],[0]
"We propose a pipelined architecture to address all three steps and apply it to a variety of NLP tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness.",1 Introduction,[0],[0]
"We find:
• Multi-sense embeddings give improved performance in some tasks (e.g., semantic similarity for words and sentences, semantic relation identification part-of-speech tagging), but not others (e.g., sentiment analysis, named entity extraction).",1 Introduction,[0],[0]
"In our analysis we offer some suggested explanations for these differences.
",1 Introduction,[0],[0]
"• Some of the improvements for multi-sense embeddings are no longer visible when using more sophisticated neural models like LSTMs which have more flexibility in filtering away the informational chaff from the wheat.
",1 Introduction,[0],[0]
"• It is important to carefully compare against embeddings of the same dimensionality.
",1 Introduction,[0],[0]
"• When doing so, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.
",1 Introduction,[0],[0]
"After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks.",1 Introduction,[0],[0]
"Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).",2 Related Work,[0],[0]
"Standard neural models represent each word with a single unique vector representation.
",2 Related Work,[0],[0]
"Recent work has begun to augment the neural paradigm to address the multi-sense problem
by associating each word with a series of sense specific embeddings.",2 Related Work,[0],[0]
"The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”.
",2 Related Work,[0],[0]
"For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings.",2 Related Work,[0],[0]
Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models.,2 Related Work,[0],[0]
Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents.,2 Related Work,[0],[0]
"Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary.",2 Related Work,[0],[0]
"Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word.
",2 Related Work,[0],[0]
Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments.,2 Related Work,[0],[0]
"We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense.",3 Learning Sense-Specific Embeddings,[0],[0]
"Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses.",3 Learning Sense-Specific Embeddings,[0],[0]
"Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Blei et al., 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.",3 Learning Sense-Specific Embeddings,[0],[0]
"In the analogy of
CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new sense).",3 Learning Sense-Specific Embeddings,[0],[0]
The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses).,3 Learning Sense-Specific Embeddings,[0],[0]
We propose such a model and show that it improves over the state of the art on a standard word similarity task.,3 Learning Sense-Specific Embeddings,[0],[0]
"We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Blei et al., 2004; Teh et al., 2006; Pitman, 1995).",3.1 Chinese Restaurant Processes,[0],[0]
"CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for nonparametric clustering.",3.1 Chinese Restaurant Processes,[0],[0]
"In the analogy, each data point is compared to a customer in a restaurant.",3.1 Chinese Restaurant Processes,[0],[0]
"The restaurant has a series of tables t, each of which serves a dish dt.",3.1 Chinese Restaurant Processes,[0],[0]
This dish can be viewed as the index of a cluster or a topic.,3.1 Chinese Restaurant Processes,[0],[0]
"The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution:
Pr(tw = t) ∝",3.1 Chinese Restaurant Processes,[0],[0]
"{ NtP (w|dt) if t already exists γP (w|dnew) if t is new
(1) where Nt denotes the number of customers already sitting at table t and P (w|dt) denotes the probability of assigning the current data point to cluster dt.",3.1 Chinese Restaurant Processes,[0],[0]
"γ is the hyper parameter controlling the preference for sitting at a new table.
",3.1 Chinese Restaurant Processes,[0],[0]
CRPs exhibit a useful “rich get richer” property because they take into account the popularity of different word senses.,3.1 Chinese Restaurant Processes,[0],[0]
"They are also more flexible than a simple threshold strategy for setting up new clusters, due to the robustness introduced by adopting the relative ratio of P (w|dt) and P (w|dnew).",3.1 Chinese Restaurant Processes,[0],[0]
"We describe how we incorporate CRP into a standard distributed language model1.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"1We omit details about training standard distributed models; see Collobert and Weston (2008) and Mikolov et al. (2013).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"As in the standard vector-space model, each token w is associated with a K dimensional global embedding ew.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Additionally, it is associated with a set of senses Zw = {z1w, z2w, ..., z|Zw|w } where |Zw| denotes the number of senses discovered for word w.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Each sense z is associated with a distinct sense-specific embedding ezw.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using the global vector ew:
p(ew|eneigh) = F (ew, eneigh) (2)
F() can take different forms in different learning paradigms, e.g., F = ∏ w′∈neigh p(ew, ew′) for skip-gram or F = p(ew, g(ew)) for SENNA (Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Unlike traditional one-word-one-vector frameworks, eneigh includes sense information in addition to the global vectors for neighbors.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"eneigh can therefore be written as2.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"eneigh = {en−k, , ..., en−1, en+1, ..., en−k} (3)
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Next we would use CRP to decide which sense the current occurrence corresponds to, or construct a new sense if it is a new meaning that we have not encountered before.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Based on CRP, the probability that assigns the current occurrence to each of the discovered senses or a new sense is given by:
Pr(zw = z) ∝  ",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Nwz P (e z w|context) if z already exists
γP (w|znew) if z is new (4)
where Nwz denotes the number of times already assigned to sense z for token w. P (ezw|context) denotes the probability that current occurrence belonging to (or generated by) sense",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"z.
The algorithm for parameter update for the one token predicting procedure is illustrated in Figure
2For models that predict succeeding words, sense labels for preceding words have already been decided.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"For models that predict words using both left and right contexts, the labels for right-context words have not been decided yet.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"In such cases we just use its global word vector to fill up the position.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"01: Input : Token sequence {wn, wneigh}.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
02: Update parameters involved in Equ (3)(4) based on current word prediction.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
03:,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Sample sense label z from CRP.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
04: If a new sense label z is sampled: 05: - add z to Zwn 06: - ezwn = argmax p(wn|zm),3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"07: else: update parameters involved based on sampled sense label z.
Figure 1: Incorporating CRP into Neural Language Models.
1: Line 2 shows parameter updating through predicting the occurrence of current token.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Lines 4-6 illustrate the situation when a new word sense is detected, in which case we would add the newly detected sense z into Zwn .",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The vector representation ezw for the newly detected sense would be obtained by maximizing the function p(ezw|context).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"As we can see, the model performs word-sense clustering and embedding learning jointly, each one affecting the other.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The prediction of the global vector of the current token (line2) is based on both the global and sense-specific embeddings of its neighbors, as will be updated through predicting the current token.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Similarly, once the sense label is decided (line7), the model will adjust the embeddings for neighboring words, both global word vectors and sense-specific vectors.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Training We train embeddings using Gigaword5 + Wikipedia2014.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The training approach is implemented using skip-grams (SG) (Mikolov et al., 2013).",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"We induced senses for the top 200,000 most frequent words (and used a unified “unknown” token for other less-frequent tokens).",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
The window size is set to 11.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
We iterate three times over the corpus.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Next we describe how we decide sense labels for tokens in context.,4 Obtaining Word Representations for NLU tasks,[0],[0]
"The scenario is treated as a inference procedure for sense labels where all global word embeddings and sense-specific embeddings are kept fixed.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
"Given a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.2 over each containing token.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
"Computing the global optimum sense labeling— in which every word gets an optimal sense label— requires searching over the space of all senses for all words, which can be expensive.",4 Obtaining Word Representations for NLU tasks,[0],[0]
"We therefore chose two simplified heuristic approaches:
• Greedy Search: Assign each token the locally optimum sense label and represent the current token with the embedding associated with that sense.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
• Expectation:,4 Obtaining Word Representations for NLU tasks,[0],[0]
"Compute the probability of each possible sense for the current word, and represent the word with the expectation vector:
~ew = ∑ z∈Zw p(w|z, context) ·",4 Obtaining Word Representations for NLU tasks,[0],[0]
ezw,4 Obtaining Word Representations for NLU tasks,[0],[0]
"We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments.
",5 Word Similarity Evaluation,[0],[0]
"Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation.",5 Word Similarity Evaluation,[0],[0]
"We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context.",5 Word Similarity Evaluation,[0],[0]
"Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”.
",5 Word Similarity Evaluation,[0],[0]
We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context.,5 Word Similarity Evaluation,[0],[0]
"These vectors are then used as input to get the value of cosine similarity between two words.
",5 Word Similarity Evaluation,[0],[0]
Performances are reported in Table 1.,5 Word Similarity Evaluation,[0],[0]
"Consistent with earlier work (e.g.., Neelakantan et al. (2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG).",5 Word Similarity Evaluation,[0],[0]
"As expected, performance is not as high when global level information is ignored when choosing word senses (SG+Greedy) as when it is included (SG+Expect), as neighboring words don’t provide sufficient information for word sense disambiguation.
",5 Word Similarity Evaluation,[0],[0]
"To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for
this class of multi-sense models and serves as a promise for being extended to NLU tasks.
",5 Word Similarity Evaluation,[0],[0]
Visualization Table 2 shows examples of semantically related words given the local context.,5 Word Similarity Evaluation,[0],[0]
Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity.,5 Word Similarity Evaluation,[0],[0]
"Like earlier models (e.g., Neelakantan et al. (2014))., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy—senses that are less distinct.",5 Word Similarity Evaluation,[0],[0]
"Having shown that multi-sense embeddings improve word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification and sentence-level semantic relatedness.",6 Experiments on NLP Tasks,[0],[0]
"For each task, we experimented on the following sets of embeddings, which are trained using the word2vec package on the same corpus:
• Standard one-word-one-vector embeddings from skip-gram (50d).
",6 Experiments on NLP Tasks,[0],[0]
"• Sense disambiguated embeddings from Section 3 and 4 using Greedy Search and Expectation (50d)
",6 Experiments on NLP Tasks,[0],[0]
"• The concatenation of global word embeddings and sense-specific embeddings (100d).
",6 Experiments on NLP Tasks,[0],[0]
"• Standard one-word-one-vector skip-gram embeddings with dimensionality doubled (100d) (100d is the correct corresponding
baseline since the concatenation above doubles the dimensionality of word vectors)
",6 Experiments on NLP Tasks,[0],[0]
"• Embeddings with very high dimensionality (300d).
",6 Experiments on NLP Tasks,[0],[0]
"As far as possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one—to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances—rather than an attempt to create state-of-the-art results.",6 Experiments on NLP Tasks,[0],[0]
"Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014).",6 Experiments on NLP Tasks,[0],[0]
"Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)).
",6 Experiments on NLP Tasks,[0],[0]
"Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994).",6 Experiments on NLP Tasks,[0],[0]
"Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d).",6 Experiments on NLP Tasks,[0],[0]
"Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data.",6.1 The Tasks,[0],[0]
"We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model.",6.1 The Tasks,[0],[0]
"We employ a five-layer neural architecture, comprised of an input layer, three convolutional layers with rectifier linear activation function and a softmax output layer.",6.1 The Tasks,[0],[0]
Training is done by gradient descent with minibatches where each sentence is treated as one batch.,6.1 The Tasks,[0],[0]
"Learning rate, window size, number of hidden units of hidden layers, L2 regularizations and number of iterations are tuned on the development set.
",6.1 The Tasks,[0],[0]
"Part-of-Speech Tagging We use Sections 0–18 of the Wall Street Journal (WSJ) data for train-
ing, sections 19–21 for validation and sections 22–24 for testing.",6.1 The Tasks,[0],[0]
"Similar to NER, we trained 5- layer neural models which take the concatenation of neighboring embeddings as inputs.",6.1 The Tasks,[0],[0]
"We adopt a similar training and parameter tuning strategy as for POS tagging.
",6.1 The Tasks,[0],[0]
Sentence-level Sentiment Classification (Pang),6.1 The Tasks,[0],[0]
The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence.,6.1 The Tasks,[0],[0]
We divide the original dataset into training(8101)/dev(500)/testing(2000).,6.1 The Tasks,[0],[0]
Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure.,6.1 The Tasks,[0],[0]
"Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section).",6.1 The Tasks,[0],[0]
"The ob-
tained embedding is then fed into a sigmoid classifier.",6.1 The Tasks,[0],[0]
"Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models.",6.1 The Tasks,[0],[0]
"For training, we adopt AdaGrad with mini-batch.",6.1 The Tasks,[0],[0]
"Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set.",6.1 The Tasks,[0],[0]
"Due to space limitations, we omit details of recurrent models and training.
",6.1 The Tasks,[0],[0]
Sentiment Analysis–Stanford Treebank,6.1 The Tasks,[0],[0]
"The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.
",6.1 The Tasks,[0],[0]
"Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children.",6.1 The Tasks,[0],[0]
"The embeddings for each parse tree constituent are output to a softmax layer; see Socher et al. (2013).
",6.1 The Tasks,[0],[0]
We focus on the standard version of recursive neural models.,6.1 The Tasks,[0],[0]
Again we fixed word embeddings to each of the different embedding settings described above3.,6.1 The Tasks,[0],[0]
"Similarly, we adopted AdaGrad
3Note that this is different from the settings used in
with mini-batch.",6.1 The Tasks,[0],[0]
"Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set.",6.1 The Tasks,[0],[0]
"The number of iterations is treated as a variable to tune and parameters are harvested based on the best performance on the development set.
",6.1 The Tasks,[0],[0]
"Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2” classifying the relation between [apartment] and",6.1 The Tasks,[0],[0]
[kitchen] as component-whole.,6.1 The Tasks,[0],[0]
"The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details.
",6.1 The Tasks,[0],[0]
We follow the recursive implementations defined in Socher et al. (2012).,6.1 The Tasks,[0],[0]
"The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier.",6.1 The Tasks,[0],[0]
"For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features.",6.1 The Tasks,[0],[0]
"We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc).
",6.1 The Tasks,[0],[0]
"(Socher et al., 2013) where word vectors were treated as parameters to optimize.
",6.1 The Tasks,[0],[0]
Sentence Semantic Relatedness,6.1 The Tasks,[0],[0]
"We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927).",6.1 The Tasks,[0],[0]
"Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related).
",6.1 The Tasks,[0],[0]
"In our setting, the similarity between two sentences is measured based on sentence-level embeddings.",6.1 The Tasks,[0],[0]
Let s1 and s2 denote two sentences and es1 and es2 denote corresponding embeddings.,6.1 The Tasks,[0],[0]
es1 and es2 are achieved through recurrent or recursive models (as illustrated in Appendix section).,6.1 The Tasks,[0],[0]
"Again, word embeddings are obtained by simple table look up in one-word-one-vector settings and inferred using the Greedy or Expectation strategy in multi-sense settings.",6.1 The Tasks,[0],[0]
"We adopt two different recurrent models for acquiring sentencelevel embeddings, a standard recurrent model and an LSTM model (Hochreiter and Schmidhuber, 1997).
",6.1 The Tasks,[0],[0]
"The similarity score is predicted using a regression model built on the structure of a three layer convolutional model, with concatenation of es1 and es2 as input, and a regression score from 1- 5 as output.",6.1 The Tasks,[0],[0]
We adopted the same training strategy as described earlier.,6.1 The Tasks,[0],[0]
The trained model is then used to predict the relatedness score between two new sentences.,6.1 The Tasks,[0],[0]
Performance is measured using Pearson’s r between the predicted score and goldstandard labels.,6.1 The Tasks,[0],[0]
"Results for different tasks are represented in Tables 3-9.
",6.2 Discussions,[0],[0]
"At first glance it seems that multi-sense embeddings do indeed offer superior performance, since combining global vectors with sense-specific vectors introduces a consistent performance boost
for every task, when compared with the standard (50d) setting.",6.2 Discussions,[0],[0]
"But of course this is an unfair comparison; combining global vector with sensespecific vector doubles the dimensionality of vector to 100, making comparison with standard dimensionality (50d) unfair.",6.2 Discussions,[0],[0]
"When comparing with standard (100), the conclusions become more nuanced.
",6.2 Discussions,[0],[0]
"For every task, the +Expectation method has performances that often seem to be higher than the simple baseline (both for the 50d case or the 100d case).",6.2 Discussions,[0],[0]
"However, only some of these differences are significant.
",6.2 Discussions,[0],[0]
(1) Using multi-sense embeddings is significantly helpful for tasks like semantic relatedness (Tables 7-8).,6.2 Discussions,[0],[0]
"This is sensible since sentence meaning here is sensitive to the semantics of one particular word, which could vary with word sense and which would directly be reflected on the relatedness score.
",6.2 Discussions,[0],[0]
"(2) By contrast, for sentiment analysis (Tables 5-6), much of the task depends on correctly identifying a few sentiment words like “good” or “bad”, whose senses tend to have similar sentiment values, and hence for which multi-sense embeddings offer little help.",6.2 Discussions,[0],[0]
"Multi-sense embeddings might promise to help sentiment analysis for some cases, like disambiguating the word “sound” in “safe and sound” versus “movie sound”.",6.2 Discussions,[0],[0]
"But we suspect that such cases are not common, explaining the nonsignificance of the improvement.",6.2 Discussions,[0],[0]
"Furthermore, the advantages of neural models in sentiment analysis tasks presumably lie in their capability to capture local composition like negation, and it’s not clear how helpful multi-sense embeddings are for that aspect.
",6.2 Discussions,[0],[0]
"(3) Similarly, multi-sense embeddings help for POS tagging, but not for NER tagging (Table 3-4).",6.2 Discussions,[0],[0]
Word senses have long been known to be related to POS tags.,6.2 Discussions,[0],[0]
"But the largest proportion of NER tags consists of the negative not-a-NER (“O”) tag, each of which is likely correctly labelable regard-
less of whether senses are disambiguated or not (since presumably if a word is not a named entity, most of its senses are not named entities either).
(4) As we apply more sophisticated models like LSTM to semantic relatedness tasks (in Table 9), the advantages caused by multi-sense embeddings disappears.
",6.2 Discussions,[0],[0]
(5) Doubling the number of dimensions is sufficient to increase performance as much as using the complex multi-sense algorithm.,6.2 Discussions,[0],[0]
"(Of course increasing vector dimensionality (to 300) boosts performance even more, although at the significant cost of exponentially increasing time complexity.)",6.2 Discussions,[0],[0]
We do larger one-word-one-vector embeddings do so well?,6.2 Discussions,[0],[0]
"We suggest some hypotheses:
• though information about distinct senses is encoded in one-word-one-vector embeddings in a mixed and less structured way, we suspect that the compositional nature of neural models is able to separate the informational chaff from the wheat and choose what information to take up, bridging the gap between single vector and multi-sense paradigms.",6.2 Discussions,[0],[0]
"For models like LSTMs which are better at doing such a job by using gates to control information flow, the difference between two paradigms should thus be further narrowed, as indeed we found.
",6.2 Discussions,[0],[0]
•,6.2 Discussions,[0],[0]
"The pipeline model proposed in the work requires sense-label inference (i.e., step 2).",6.2 Discussions,[0],[0]
"We proposed two strategies: GREEDY and EXPECTATION, and found that GREEDY models perform worse than EXPECTATION, as we might expect4.",6.2 Discussions,[0],[0]
"But even EXPECTATION can be viewed as another form of one-wordone-vector models, just one where different senses are entangled but weighted to emphasize the important ones.",6.2 Discussions,[0],[0]
"Again, this suggests another cause for the strong relative performance of larger-dimensioned one-word-onevector models.",6.2 Discussions,[0],[0]
"In this paper, we expand ongoing research into multi-sense embeddings by first proposing a new version based on Chinese restaurant processes that achieves state of the art performance on simple
4GREEDY models work in a more aggressive way and likely make mistakes due to the non-global-optimum nature and limited context information
word similarity matching tasks.",7 Conclusion,[0],[0]
"We then introduce a pipeline system for incorporating multisense embeddings into NLP applications, and examine multiple NLP tasks to see whether and when multi-sense embeddings can introduce performance boosts.",7 Conclusion,[0],[0]
Our results suggest that simply increasing the dimensionality of baseline skip-gram embeddings is sometimes sufficient to achieve the same performance wins that come from using multi-sense embeddings.,7 Conclusion,[0],[0]
"That is, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.
",7 Conclusion,[0],[0]
Our results come with some caveats.,7 Conclusion,[0],[0]
"In particular, our conclusions are based on the pipelined system that we introduce, and other multi-sense embedding systems (e.g., a more advanced sense learning model or a better sense label model or a completely different pipeline system) may find stronger effects of multi-sense models.",7 Conclusion,[0],[0]
"Nonetheless we do consistently find improvements for multi-sense embeddings in some tasks (part-ofspeech tagging and semantic relation identification), suggesting the benefits of our multi-sense models and those of others.",7 Conclusion,[0],[0]
"Perhaps the most important implication of our results may be the evidence they provide for the importance of going beyond simple human-matching tasks, and testing embedding models by using them as components in real NLP applications.",7 Conclusion,[0],[0]
"In sentiment classification and sentence semantic relatedness tasks, classification models require embeddings that represent the input at a sentence or phrase level.",8 Appendix,[0],[0]
"We adopt recurrent networks (standard ones or LSTMs) and recursive networks in order to map a sequence of tokens with various length to a vector representation.
",8 Appendix,[0],[0]
"Recurrent Networks A recurrent network successively takes wordwt at step t, combines its vector representation et with the previously built hidden vector ht−1 from time t− 1, calculates the resulting current embedding ht, and passes it to the next step.",8 Appendix,[0],[0]
"The embedding ht for the current time t is thus:
ht = tanh(W · ht−1 + V · et) (5) whereW and V denote compositional matrices.",8 Appendix,[0],[0]
"If Ns denote the length of the sequence, hNs represents the whole sequence S.
Recursive Networks Standard recursive models work in a similar way by working on neighboring words by parse tree order rather than sequence order.",8 Appendix,[0],[0]
They compute the representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree.,8 Appendix,[0],[0]
"For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη:
eη = tanh(W · eηleft + V · eηright) (6)
",8 Appendix,[0],[0]
"Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot.",8 Appendix,[0],[0]
"We notationally disambiguate e and h, where et denote the vector for an individual text unit (e.g., word or sentence) at time step t while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1.",8 Appendix,[0],[0]
σ denotes the sigmoid function.,8 Appendix,[0],[0]
W ∈ R4K×2K .,8 Appendix,[0],[0]
"The vector representation ht for each time-step t is given by:
[ it ft ot lt ] =",8 Appendix,[0],[0]
"[ σ σ σ tanh ] W · [ ht−1 et ] (7)
ct = ft · ct−1 + it · lt (8) hst = ot · ct (9)",8 Appendix,[0],[0]
"We would like to thank Sam Bowman, Ignacio Cases, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as anonymous reviewers for their helpful advice on various aspects of this work.",9 Acknowledgments,[0],[0]
"We gratefully acknowledge the support of the NSF via award IIS-1514268, the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.",9 Acknowledgments,[0],[0]
FA8750-13-2-0040.,9 Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, DARPA, AFRL, or the US government.",9 Acknowledgments,[0],[0]
Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations.,abstractText,[0],[0]
"Yet while ‘multi-sense’ methods have been proposed and tested on artificial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks.",abstractText,[0],[0]
"In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language un-",abstractText,[0],[0]
Do Multi-Sense Embeddings Improve Natural Language Understanding?,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 462–468 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
462",text,[0],[0]
"Neural network mappings are widely used to bridge modalities or spaces in cross-modal retrieval (Qiao et al., 2017; Wang et al., 2016; Zhang et al., 2016), zero-shot learning (Lazaridou et al., 2015b, 2014; Socher et al., 2013) in building multimodal representations (Collell et al., 2017) or in word translation (Lazaridou et al., 2015a), to name a few.",1 Introduction,[0],[0]
"Typically, a neural network is firstly trained
to predict the distributed vectors of one modality (or space) from the other.",1 Introduction,[0],[0]
"At test time, some operation such as retrieval or labeling is performed based on the nearest neighbors of the predicted (mapped) vectors.",1 Introduction,[0],[0]
"For instance, in zero-shot image classification, image features are mapped to the text space and the label of the nearest neighbor word is assigned.",1 Introduction,[0],[0]
"Thus, the success of such systems relies entirely on the ability of the map to make the predicted vectors similar to the target vectors in terms of semantic or neighborhood structure.1",1 Introduction,[0],[0]
"However, whether neural nets achieve this goal in general has not been investigated yet.",1 Introduction,[0],[0]
"In fact, recent work evidences that considerable information about the input modality propagates into the predicted modality (Collell et al., 2017; Lazaridou et al., 2015b; Frome et al., 2013).
",1 Introduction,[0],[0]
"To shed light on these questions, we first introduce the (to the best of our knowledge) first existing measure to quantify similarity between the neighborhood structures of two sets of vectors.",1 Introduction,[0],[0]
"Second, we perform extensive experiments in three benchmarks where we learn image-to-text and text-to-image neural net mappings using a rich variety of state-of-the-art text and image features and loss functions.",1 Introduction,[0],[0]
"Our results reveal that, contrary to expectation, the semantic structure of the mapped vectors consistently resembles more that of the input vectors than that of the target vectors of interest.",1 Introduction,[0],[0]
"In a second experiment, by using six concept similarity tasks we show that the semantic structure of the input vectors is preserved after mapping them with an untrained network, further evidencing that feed-forward nets naturally preserve semantic information about the input.",1 Introduction,[0],[0]
"Overall, we uncover and rise awareness of a largely
1We indistinctly use the terms semantic structure, neighborhood structure and similarity structure.",1 Introduction,[0],[0]
"They refer to all pairwise similarities of a set of N vectors, for some similarity measure (e.g., Euclidean or cosine).
",1 Introduction,[0],[0]
"ignored phenomenon relevant to a wide range of cross-modal / cross-space applications such as retrieval, zero-shot learning or image annotation.
",1 Introduction,[0],[0]
"Ultimately, this paper aims at: (1) Encouraging the development of better architectures to bridge modalities / spaces; (2) Advocating for the use of semantic-based criteria to evaluate the quality of predicted vectors such as the neighborhood-based measure proposed here, instead of purely geometric measures such as mean squared error (MSE).",1 Introduction,[0],[0]
Neural network and linear mappings are popular tools to bridge modalities in cross-modal retrieval systems.,2 Related Work and Motivation,[0],[0]
Lazaridou et al. (2015b) leverage a text-to-image linear mapping to retrieve images given text queries.,2 Related Work and Motivation,[0],[0]
Weston et al. (2011) map label and image features into a shared space with a linear mapping to perform image annotation.,2 Related Work and Motivation,[0],[0]
"Alternatively, Frome et al. (2013), Lazaridou et al. (2014) and Socher et al. (2013) perform zero-shot image classification with an image-to-text neural network mapping.",2 Related Work and Motivation,[0],[0]
"Instead of mapping to latent features, Collell et al. (2018) use a 2-layer feedforward network to map word embeddings directly to image pixels in order to visualize spatial arrangements of objects.",2 Related Work and Motivation,[0],[0]
Neural networks are also popular in other cross-space applications such as cross-lingual tasks.,2 Related Work and Motivation,[0],[0]
"Lazaridou et al. (2015a) learn a linear map from language A to language B and then translate new words by returning the nearest neighbor of the mapped vector in the B space.
",2 Related Work and Motivation,[0],[0]
"In the context of zero-shot learning, shortcomings of cross-space neural mappings have also been identified.",2 Related Work and Motivation,[0],[0]
"For instance, “hubness” (Radovanović et al., 2010) and “pollu-
tion” (Lazaridou et al., 2015a) relate to the highdimensionality of the feature spaces and to overfitting respectively.",2 Related Work and Motivation,[0],[0]
"Crucially, we do not assume that our cross-modal problem has any class labels, and we study the similarity between input and mapped vectors and between output and mapped vectors.
",2 Related Work and Motivation,[0],[0]
Recent work evidences that the predicted vectors of cross-modal neural net mappings are still largely informative about the input vectors.,2 Related Work and Motivation,[0],[0]
Lazaridou et al. (2015b) qualitatively observe that abstract textual concepts are grounded with the visual input modality.,2 Related Work and Motivation,[0],[0]
"Counterintuitively, Collell et al. (2017) find that the vectors “imagined” from a language-to-vision neural map, outperform the original visual vectors in concept similarity tasks.",2 Related Work and Motivation,[0],[0]
The paper argued that the reconstructed visual vectors become grounded with language because the map preserves topological properties of the input.,2 Related Work and Motivation,[0],[0]
"Here, we go one step further and show that the mapped vectors often resemble the input vectors more than the target vectors in semantic terms, which goes against the goal of a cross-modal map.
",2 Related Work and Motivation,[0],[0]
"Well-known theoretical work shows that networks with as few as one hidden layer are able to approximate any function (Hornik et al., 1989).",2 Related Work and Motivation,[0],[0]
"However, this result does not reveal much neither about test performance nor about the semantic structure of the mapped vectors.",2 Related Work and Motivation,[0],[0]
"Instead, the phenomenon described is more closely tied to other properties of neural networks.",2 Related Work and Motivation,[0],[0]
"In particular, continuity guarantees that topological properties of the input, such as connectedness, are preserved (Armstrong, 2013).",2 Related Work and Motivation,[0],[0]
"Furthermore, continuity in a topology induced by a metric also ensures that points that are close together are mapped close together.",2 Related Work and Motivation,[0],[0]
"As a toy example, Fig. 1 illustrates the distortion of a manifold after being mapped by a neural net.2
In a noiseless world with fully statistically dependent modalities, the vectors of one modality could be perfectly predicted from those of the other.",2 Related Work and Motivation,[0],[0]
"However, in real-world problems this is unrealistic given the noise of the features and the fact that modalities encode complementary information (Collell and Moens, 2016).",2 Related Work and Motivation,[0],[0]
"Such unpredictability combined with continuity and topology-preserving properties of neural nets propel the phenomenon identified, namely mapped vectors resembling more the input than the target vectors, in nearest neighbors terms.
2Parameters of these mappings were generated at random.",2 Related Work and Motivation,[0],[0]
"To bridge modalities X and Y , we consider two popular cross-modal mappings f :",3 Proposed Approach,[0],[0]
"X → Y .
",3 Proposed Approach,[0],[0]
"(i) Linear mapping (lin):
f(x) =W0x+ b0
with W0 ∈ Rdy×dx , b0 ∈",3 Proposed Approach,[0],[0]
"Rdy , where dx and dy are the input and output dimensions respectively.
(ii) Feed-forward neural network (nn):
f(x) =W1σ(W0x+ b0) + b1
with W1 ∈ Rdy×dh , W0 ∈",3 Proposed Approach,[0],[0]
"Rdh×dx , b0 ∈ Rdh ,",3 Proposed Approach,[0],[0]
b1 ∈,3 Proposed Approach,[0],[0]
"Rdy where dh is the number of hidden units and σ() the non-linearity (e.g., tanh or sigmoid).",3 Proposed Approach,[0],[0]
"Although single hidden layer networks are already universal approximators (Hornik et al., 1989), we explored whether deeper nets with 3 and 5 hidden layers could improve the fit (see Supplement).
",3 Proposed Approach,[0],[0]
Loss:,3 Proposed Approach,[0],[0]
Our primary choice is the MSE: 1 2‖f(x),3 Proposed Approach,[0],[0]
"− y‖
2, where y is the target vector.",3 Proposed Approach,[0],[0]
"We also tested other losses such as the cosine: 1 − cos(f(x), y) and the max-margin: max{0, γ + ‖f(x)",3 Proposed Approach,[0],[0]
"− y‖ − ‖f(x̃) − y‖}, where x̃ belongs to a different class than (x, y), and γ is the margin.",3 Proposed Approach,[0],[0]
"As in Lazaridou et al. (2015a) and Weston et al. (2011), we choose the first x̃ that violates the constraint.",3 Proposed Approach,[0],[0]
"Notice that losses that do not require class labels such as MSE are suitable for a wider, more general set of tasks than discriminative losses (e.g., cross-entropy).",3 Proposed Approach,[0],[0]
"In fact, cross-modal retrieval tasks often do not exhibit any class labels.",3 Proposed Approach,[0],[0]
"Additionally, our research question concerns the cross-space mapping problem in isolation (independently of class labels).
",3 Proposed Approach,[0],[0]
Let us denote a set of N input and output vectors by X ∈ RN×dx and Y ∈ RN×dy respectively.,3 Proposed Approach,[0],[0]
"Each input vector xi is paired to the output vector yi of the same index (i = 1, · · · , N ).",3 Proposed Approach,[0],[0]
Let us henceforth denote the mapped input vectors by f(X) ∈ RN×dy .,3 Proposed Approach,[0],[0]
"In order to explore the similarity between f(X) and X , and between f(X) and Y , we propose two ad hoc settings below.",3 Proposed Approach,[0],[0]
"To measure the similarity between the neighborhood structure of two sets of paired vectors V and
Z, we propose the mean nearest neighbor overlap measure (mNNOK(V,Z)).",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"We define the nearest neighbor overlap NNOK(vi, zi) as the number of K nearest neighbors that two paired vectors vi, zi share in their respective spaces.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"E.g., if the 3 (= K) nearest neighbors of vcat in V are {vdog, vtiger, vlion} and those of zcat in Z are {zmouse, ztiger, zlion}, the NNO3(vcat, zcat) is 2.
",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
Definition 1 Let V = {vi}Ni=1 and Z = {zi}Ni=1 be two sets of N paired vectors.,3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"We define:
mNNOK(V,Z) = 1
KN N∑ i=1",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"NNOK(vi, zi) (1)
with NNOK(vi, zi) = |NNK(vi) ∩ NNK(zi)|, where NNK(vi) and NNK(zi) are the indexes of the K nearest neighbors of vi and zi, respectively.
",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"The normalizing constant K simply scales mNNOK(V,Z) between 0 and 1, making it independent of the choice of K. Thus, a mNNOK(V,Z) = 0.7 means that the vectors in V and Z share, on average, 70% of their nearest neighbors.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"Notice that mNNO implicitly performs retrieval for some similarity measure (e.g., Euclidean or cosine), and quantifies how semantically similar two sets of paired vectors are.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"To complement the setting above (Sect. 3.1), it is instructive to consider the limit case of an untrained network.",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"Concept similarity tasks provide a suitable setting to study the semantic structure of distributed representations (Pennington et al., 2014).",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"That is, semantically similar concepts should ideally be close together.",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"In particular, our interest is in comparing X with its projection f(X) through a mapping with random parameters, to understand the extent to which the mapping may disrupt or preserve the semantic structure of X .",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"To test the generality of our claims, we select a rich diversity of cross-modal tasks involving texts at three levels: word level (ImageNet), sentence level (IAPR TC-12), and document level (Wiki).",4.1.1 Datasets,[0],[0]
"ImageNet (Russakovsky et al., 2015).",4.1.1 Datasets,[0],[0]
"Consists of ∼14M images, covering ∼22K WordNet synsets
(or meanings).",4.1.1 Datasets,[0],[0]
"Following Collell et al. (2017), we take the most relevant word for each synset and keep only synsets with more than 50 images.",4.1.1 Datasets,[0],[0]
"This yields 9,251 different words (or instances).",4.1.1 Datasets,[0],[0]
"IAPR TC-12 (Grubinger et al., 2006).",4.1.1 Datasets,[0],[0]
Contains 20K images (18K train / 2K test) annotated with 255 labels.,4.1.1 Datasets,[0],[0]
Each image is accompanied with a short description of one to three sentences.,4.1.1 Datasets,[0],[0]
"Wikipedia (Pereira et al., 2014).",4.1.1 Datasets,[0],[0]
"Has 2,866 samples (2,173 train / 693 test).",4.1.1 Datasets,[0],[0]
Each sample is a section of a Wikipedia article paired with one image.,4.1.1 Datasets,[0],[0]
See the Supplement (Sect. 1) for details.,4.1.2 Hyperparameters and Implementation,[0],[0]
"To ensure that results are independent of the choice of image and text features, we use 5 (2 image + 3 text) features of varied dimensionality (64- d, 128-d, 300-d, 2,048-d) and two directions, textto-image (T → I) and image-to-text (I → T ).",4.1.3 Image and Text Features,[0],[0]
We make our extracted features publicly available.3 Text.,4.1.3 Image and Text Features,[0],[0]
"In ImageNet we use 300-dimensional GloVe4 (Pennington et al., 2014) and 300-d word2vec (Mikolov et al., 2013) word embeddings.",4.1.3 Image and Text Features,[0],[0]
"In IAPR TC-12 and Wiki, we employ stateof-the-art bidirectional gated recurrent unit (biGRU) features (Cho et al., 2014) that we learn with a classification task (see Sect.",4.1.3 Image and Text Features,[0],[0]
2 of Supplement).,4.1.3 Image and Text Features,[0],[0]
Image.,4.1.3 Image and Text Features,[0],[0]
"For ImageNet, we use the publicly available5 VGG-128 (Chatfield et al., 2014) and ResNet (He et al., 2015) visual features from Collell et al. (2017), where we obtained 128- dimensional VGG-128 and 2,048-d ResNet features from the last layer (before the softmax) of the forward pass of each image.",4.1.3 Image and Text Features,[0],[0]
The final representation for a word is the average feature vector (centroid) of all available images for this word.,4.1.3 Image and Text Features,[0],[0]
"In IAPR TC-12 and Wiki, features for individual images are obtained similarly from the last layer of a ResNet and a VGG-128 model.",4.1.3 Image and Text Features,[0],[0]
"We include six benchmarks, comprising three types of concept similarity: (i) Semantic similarity: SemSim (Silberer and Lapata, 2014), Simlex999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016); (ii) Relatedness: MEN (Bruni et al.,
3http://liir.cs.kuleuven.be/software.html 4http://nlp.stanford.edu/projects/glove 5http://liir.cs.kuleuven.be/software.html
2014) and WordSim-353 (Finkelstein et al., 2001); (iii) Visual similarity: VisSim (Silberer and Lapata, 2014) which includes the same word pairs as SemSim, rated for visual similarity instead of semantic.",4.2.1 Datasets,[0],[0]
"All six test sets contain human ratings of similarity for word pairs, e.g., (‘cat’,‘dog’).",4.2.1 Datasets,[0],[0]
"The parameters in W0,W1 are drawn from a random uniform distribution [−1, 1] and b0, b1 are set to zero.",4.2.2 Hyperparameters and Implementation,[0],[0]
"We use a tanh activation σ().6 The output dimension dy is set to 2,048 for all embeddings.",4.2.2 Hyperparameters and Implementation,[0],[0]
Textual and visual features are the same as described in Sect.,4.2.3 Image and Text Features,[0],[0]
4.1.3 for the ImageNet dataset.,4.2.3 Image and Text Features,[0],[0]
"We compute the prediction of similarity between two vectors z1, z2 with both the cosine z1z2‖z1‖‖z2‖ and the Euclidean similarity 11+‖z1−z2‖ .",4.2.4 Similarity Predictions,[0],[0]
7,4.2.4 Similarity Predictions,[0],[0]
"As is common practice, we evaluate the predictions of similarity of the embeddings (Sect. 4.2.4) against the human similarity ratings with the Spearman correlation ρ.",4.2.5 Performance Metrics,[0],[0]
We report the average of 10 sets of randomly generated parameters.,4.2.5 Performance Metrics,[0],[0]
We test statistical significance with a two-sided Wilcoxon rank sum test adjusted with Bonferroni.,5 Results and Discussion,[0],[0]
The null hypothesis is that a compared pair is equal.,5 Results and Discussion,[0],[0]
"In Tab. 1, ∗ indicates that mNNO(X, f(X)) differs from mNNO(Y, f(X))",5 Results and Discussion,[0],[0]
"(p < 0.001) on the same mapping, embedding and direction.",5 Results and Discussion,[0],[0]
"In Tab. 2, ∗ indicates that performance of mapped and input vectors differs (p < 0.05) in the 10 runs.",5 Results and Discussion,[0],[0]
Results below are with cosine neighbors and K = 10.,5.1 Experiment 1,[0],[0]
Euclidean neighbors yield similar results and are thus left to the Supplement.,5.1 Experiment 1,[0],[0]
"Similarly, results in ImageNet with GloVe embeddings are shown below and word2vec results in the Supplement.",5.1 Experiment 1,[0],[0]
"The choice ofK = {5, 10, 30} had no visible effect on results.",5.1 Experiment 1,[0],[0]
Results with 3- and 5-layer nets did not show big differences with the results below (see Supplement).,5.1 Experiment 1,[0],[0]
"The cosine and max-margin losses
6We find that sigmoid and ReLu yield similar results.",5.1 Experiment 1,[0],[0]
"7Notice that papers generally use only cosine similarity
(Lazaridou et al., 2015b; Pennington et al., 2014).
performed slightly worse than MSE (see Supplement).",5.1 Experiment 1,[0],[0]
"Although Lazaridou et al. (2015a) and Weston et al. (2011) find that max-margin performs the best in their tasks, we do not find our result entirely surprising given that max-margin focuses on inter-class differences while we look also at intraclass neighbors (in fact, we do not require classes).
",5.1 Experiment 1,[0],[0]
"Tab. 1 shows our core finding, namely that the semantic structure of f(X) resembles more that of X than that of Y , for both lin and nn maps.
",5.1 Experiment 1,[0],[0]
Fig. 2 is particularly revealing.,5.1 Experiment 1,[0],[0]
"If we would only look at train performance (and allow train MSE to reach 0) then f(X) = Y and clearly train mNNO(f(X), Y )",5.1 Experiment 1,[0],[0]
"= 1 while mNNO(f(X), X) can only be smaller than 1.",5.1 Experiment 1,[0],[0]
"However, the interest is always on test samples, and (near-)perfect test prediction is unrealistic.",5.1 Experiment 1,[0],[0]
"Notice in fact in Fig. 2 that even if we look at train fit, MSE needs to be close to 0 for mNNO(f(X), Y ) to be
reasonably large.",5.1 Experiment 1,[0],[0]
"In all the combinations from Tab. 1, the test mNNO(f(X), Y ) never surpasses test mNNO(f(X), X) for any number of epochs, even with an oracle (not shown).",5.1 Experiment 1,[0],[0]
"Tab. 2 shows that untrained linear (flin) and neural net (fnn) mappings preserve the semantic structure of the input X , complementing thus the findings of Experiment 1.",5.2 Experiment 2,[0],[0]
"Experiment 1 concerns learning, while, by “ablating” the learning part and randomizing weights, Experiment 2 is revealing about the natural tendency of neural nets to preserve semantic information about the input, regardless of the choice of the target vectors and loss function.",5.2 Experiment 2,[0],[0]
"Overall, we uncovered a phenomenon neglected so far, namely that neural net cross-modal mappings can produce mapped vectors more akin to the input vectors than the target vectors, in terms of semantic structure.",6 Conclusions,[0],[0]
Such finding has been possible thanks to the proposed measure that explicitly quantifies similarity between the neighborhood structure of two sets of vectors.,6 Conclusions,[0],[0]
"While other measures such as mean squared error can be misleading, our measure provides a more realistic estimate of the semantic similarity between predicted and target vectors.",6 Conclusions,[0],[0]
"In fact, it is the semantic structure (or pairwise similarities) what ultimately matters in cross-modal applications.",6 Conclusions,[0],[0]
This work has been supported by the CHIST-ERA EU project MUSTER8 and by the KU Leuven grant RUN/15/005.,Acknowledgments,[0],[0]
"Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space.",abstractText,[0],[0]
"The predicted vectors are then used to perform e.g., retrieval or labeling.",abstractText,[0],[0]
"Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors.",abstractText,[0],[0]
"However, whether this is achieved has not been investigated yet.",abstractText,[0],[0]
"Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue.",abstractText,[0],[0]
In three cross-modal benchmarks we learn a large number of language-to-vision and visionto-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions.,abstractText,[0],[0]
"Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors.",abstractText,[0],[0]
"In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.",abstractText,[0],[0]
Do Neural Network Cross-Modal Mappings Really Bridge Modalities?,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1175–1185, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Grapheme-to-phoneme (G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols.,1 Introduction,[0],[0]
"Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007),",1 Introduction,[0],[0]
"lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000).",1 Introduction,[0],[0]
"The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x,y)} and then to evaluate model performance on test data.",1 Introduction,[0],[0]
"While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x,y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data.
",1 Introduction,[0],[0]
"State-of-the-art alignments in G2P are characterized by the following properties:
(i) Alignments are monotone in that the ordering of characters in input and output sequences is preserved by the alignments.",1 Introduction,[0],[0]
"Furthermore, they are many-to-many in the sense that several x sequence characters may be matched up with several y sequence characters as illustrated in Table 1.
(ii)",1 Introduction,[0],[0]
"The alignment is a latent variable and learnt in an unsupervised manner from pairs of strings in the training data.
",1 Introduction,[0],[0]
"(iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence).
",1 Introduction,[0],[0]
"To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in German, such as absagt (‘rejects’) with abzusagen (‘to reject’).",1 Introduction,[0],[0]
"Critically, the insertion -zu- appears in infixal position and a plausible alignment might be as in Table 2.",1 Introduction,[0],[0]
"Then, correctly aligning certain
analogous forms such as zusagt (‘accepts’) with
1175
their corresponding extended infinitive zuzusagen (‘to accept’) is beyond the scope of a unigram alignment model since this cannot distinguish the linguistically correct alignment from the following linguistically incorrect alignment
z u s a g t zu z u s a g en
precisely because it has no notion of context.",1 Introduction,[0],[0]
"In this work, we firstly address bigram alignment models in G2P.",1 Introduction,[0],[0]
"We investigate whether there are phenomena in G2P that require bigram alignment models and, more generally, whether bigram alignment models produce better alignments — with respect to a human gold standard — than unigram alignment models within the G2P setting.",1 Introduction,[0],[0]
"We do so, secondly, in a supervised setting where the model learns from gold-standard alignments.",1 Introduction,[0],[0]
"While this may seem an odd scenario at first sight, modern alignment toolkits in the related field of machine translation typically include the possibility to learn both in a supervised and unsupervised manner (Liu et al., 2010; Liu and Sun, 2015).",1 Introduction,[0],[0]
"The rationale behind supervised learning models may be that they perform better than unsupervised models, and if alignment quality has a large impact upon subsequent string translation performance, then a supervised model may be a suitable alternative.",1 Introduction,[0],[0]
"Thirdly, we investigate how alignment quality affects overall G2P performance.",1 Introduction,[0],[0]
"This allows us to address whether it is worthwhile to work on better alignment models, which bigram and supervised alignment models promise to be.",1 Introduction,[0],[0]
"To our knowledge, all three outlined aspects of alignments — bigram models, supervised learning, and systematically estimating the relationship between alignment quality and overall string transduction performance — are novel in the G2P setting and its related fields as outlined; however, see also the related work section.
",1 Introduction,[0],[0]
This work is structured as follows.,1 Introduction,[0],[0]
Section 2 presents definitions and algorithms for uni- and bigram alignment models.,1 Introduction,[0],[0]
Section 3 surveys related work.,1 Introduction,[0],[0]
Section 4 presents our data and Section 5 our experiments.,1 Introduction,[0],[0]
We conclude in Section 6.,1 Introduction,[0],[0]
We first formally define the problem of aligning two strings x and y over arbitrary alphabets in a monotone and many-to-many manner.,2 Uni- and bigram alignment models,[0],[0]
Let `x = |x|,2 Uni- and bigram alignment models,[0],[0]
"and `y = |y| denote the lengths of x and y, respectively.",2 Uni- and bigram alignment models,[0],[0]
"Let N = {0, 1, 2, . . .}, and let S ⊆
N2\{(0, 0)} be a set defining the valid match-up operations between x characters and y characters.",2 Uni- and bigram alignment models,[0],[0]
"In other words, when (s, t) ∈ S, then this means we allow matches of subsequences of x of length s and subsequences of y of length t.1
It is convenient to define a monotone many-tomany alignment of x and y as a 2×k (for k ≥ 1 arbitrary) nonnegative integer matrix Ax,y ∈ N2×k
",2 Uni- and bigram alignment models,[0],[0]
"satisfying Ax,y1k = ( `x `y ) , i.e., the two rows of Ax,y sum up to the lengths of the respective strings,2 and where each column of Ax,y lies in S. For any such alignment, we let (x1, . . .",2 Uni- and bigram alignment models,[0],[0]
",xk) be the corresponding induced segmentation of x and (y1, . . .",2 Uni- and bigram alignment models,[0],[0]
",yk) be the corresponding induced segmentation of y.
Example.",2 Uni- and bigram alignment models,[0],[0]
"For any S ⊇ {(1, 1), (1, 2), (2, 1)}, the alignment of x = phoenix and y = finIks shown in Table 1 may be represented by the ma-
trix Ax,y = (
2 2 1 1 1 1 1 1 1 2
) .",2 Uni- and bigram alignment models,[0],[0]
"The correspond-
ing induced segmentations are (ph,oe,n,i,x) and (f,i,n,I,ks).
",2 Uni- and bigram alignment models,[0],[0]
"Let AS(x,y) denote the class of all alignments of x and y.",2 Uni- and bigram alignment models,[0],[0]
"We call a function f : AS(x,y)→ R an alignment model.",2 Uni- and bigram alignment models,[0],[0]
"We call an alignment model f a unigram alignment model if f takes the form, for any Ax,y ∈ AS(x,y),
f(Ax,y) = k∑ i=1",2 Uni- and bigram alignment models,[0],[0]
"sim1(xi,yi) (1)
where sim1 is an arbitrary (real-valued) similarity function measuring similarity of two subsequences.",2 Uni- and bigram alignment models,[0],[0]
"We call an alignment model f a bigram alignment model if f takes the form
f(Ax,y) = k∑ i=1",2 Uni- and bigram alignment models,[0],[0]
"sim2 ( (xi,yi), (xi−1,yi−1) )",2 Uni- and bigram alignment models,[0],[0]
"(2)
where sim2 is an arbitrary (real-valued) similarity function measuring similarity of successive pairs of subsequences.
Example.",2 Uni- and bigram alignment models,[0],[0]
"Let sim1(u,v) be equal to |u| · |v| and let funi(Ax,y) be as in Eq. (1).",2 Uni- and bigram alignment models,[0],[0]
"Then, funi is a unigram alignment model that assigns the score
1This is sometimes denoted in the manner M -N (e.g., 3- 2, 1-0), indicating that M characters of one string may be matched up with N characters of the other string.",2 Uni- and bigram alignment models,[0],[0]
"Analogously, we could write here s-t rather than (s, t).
2Here, 1k denotes the unit vector of dimension k.",2 Uni- and bigram alignment models,[0],[0]
"given in Table 2.
Example.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Let sim2 ( (u,v), (u′,v′) )",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
= (|u| · |v|)|v′| if |u| = |u′| − 1 or u = v and −2 otherwise.,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Let fbi(Ax,y) be as in Eq.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
(2).,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Then, fbi is a bigram alignment model assigning the score (1 · 1)0 + (1 · 1)1 + (0 · 2)1 + (1 · 1)2 + (1 · 1)1 + (1 · 1)1",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"− 2 = 3 to the alignment in Table 2.
",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"In statistical alignment modeling, the task is to find an optimal alignment (i.e., one with maximal score) given strings x and y and given the alignment model f .",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"When f is a unigram model, this can be solved efficiently via dynamic programming (DP).",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"When f is a bigram alignment model, then finding the optimal alignment can still be solved via DP, by introducing a variable Mijqw denoting the score of the best alignment of x(1 : i) and y(1 : j) that ends in the matchup of x(q : i) with y(w : j).3 The variable Mijqw satisfies a recurrence leading to a DP algorithm, shown in Algorithm 1.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
The actual alignment can be found by storing pointers to the maximizing steps taken.,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
Running time of the algorithm is O(`2x`2y|S|).,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Note also that the sketched algorithm is supervised insofar as it assumes that the similarity values sim2(·, ·) are known.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Typically, such alignment algorithms can be converted into unsupervised algorithms in which similarity measures sim are learnt iteratively, e.g., in an EM-like fashion (cf., e.g., Eger (2012), Eger (2013)); however, in this paper, we only investigate the supervised base version as indicated.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
Monotone alignments have a long tradition in NLP.,3 Related work,[0],[0]
"The classical Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed.",3 Related work,[0],[0]
"It is a special case of the unigram model (1) for which S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0,−1}, depending on whether compared subsequences match or not.",3 Related work,[0],[0]
"As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string
3We denote by x(a : b) the substring xaxa+1 · · ·xb of the string x1x2 · · ·xt.
into another.",3 Related work,[0],[0]
"Substring-to-substring edit operations — or equivalently, (monotone) many-tomany alignments — have appeared in the NLP context, e.g., in Deligne et al. (1995), Brill and Moore (2000), Jiampojamarn et al. (2007), Bisani and Ney (2008), Jiampojamarn et al. (2010), or, significantly earlier, in Ukkonen (1985), Véronis (1988).",3 Related work,[0],[0]
"Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned.",3 Related work,[0],[0]
"All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)})",3 Related work,[0],[0]
"and sim1.4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments.
",3 Related work,[0],[0]
Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010).,3 Related work,[0],[0]
"There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey.",3 Related work,[0],[0]
"This work does not consider supervised alignments or bigram alignments, as we do.",3 Related work,[0],[0]
"Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a “strong correlation” between the two.",3 Related work,[0],[0]
"We more thorougly investigate this question, using, arguably, more heterogeneous aligners, and many more experiments.",3 Related work,[0],[0]
"We also quantitatively estimate how alignment quality influences G2P system accuracy on two different languages via linear regression.
",3 Related work,[0],[0]
"Goldwater et al. (2006) study the effect of context in (unsupervised) word/sequence segmentation, which may be considered the onedimensional specialization of sequence alignment, using a Bayesian method.",3 Related work,[0],[0]
"They find that bigram models greatly outperform unigram models for their task.
",3 Related work,[0],[0]
"Of course, our study is also related to the field of machine translation and its studies on the rela-
4In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (1) (but does not allow for many-to-many match-ups).",3 Related work,[0],[0]
"The contextual dependencies in this model are set up differently from the bigram dependencies in our paper.
",3 Related work,[0],[0]
Algorithm 1 1: procedure BIGRAM-ALIGN(x = x1 . . .,3 Related work,[0],[0]
"xn,y = y1 . . .",3 Related work,[0],[0]
"ym; S, sim2) 2:",3 Related work,[0],[0]
"Mijqw ← −∞ for all (i, j, q, w) ∈ Z4 3: M0000 ← 0 4: for i = 0 . . .",3 Related work,[0],[0]
n,3 Related work,[0],[0]
do 5: for j = 0 . .,3 Related work,[0],[0]
.m,3 Related work,[0],[0]
do 6: for q = 0 . . .,3 Related work,[0],[0]
i+ 1 do 7: for w = 0 . . .,3 Related work,[0],[0]
"j + 1 do 8: if (i, j, q, w) 6=",3 Related work,[0],[0]
"(0, 0, 0, 0) then 9: if (i− q + 1, j − w + 1) ∈ S then
10: Mijqw= max (a,b)∈S Mq−1,w−1,q−a,w−b+sim2
(( x(q:i),y(w:j) ) , ( x(q−a:q−1),y(w−b:w−1) ))
",3 Related work,[0],[0]
"tionship between alignment quality and translation performance (Ganchev et al., 2008).",3 Related work,[0],[0]
"In machine translation, the monotonicity assumption of string transduction does typically not hold, however, rendering alignment and translation techniques different and more heuristic in nature.",3 Related work,[0],[0]
"For English, we conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009).",4.1 Data,[0],[0]
This contains about 128 000 grapheme-phoneme pairs as exemplified in Table 3.,4.1 Data,[0],[0]
"Importantly, Combilex provides goldstandard alignments, which we will make use of for the supervised alignment models as well as for measuring alignment quality.",4.1 Data,[0],[0]
"For German, we ran-
domly extract 3 000 G2P string pairs from CELEX (Baayen et al., 1995).",4.1 Data,[0],[0]
"We had a native speaker manually align them so that gold standard alignments are available here, too.",4.1 Data,[0],[0]
"Both data sets contain quite complex match-ups of character subsequences such as (2,3) as in English s-oi-r-ee-s/swOA-r-P-z or (4,1) as in w-eigh-t/w-P-t but the majority of match-ups are of type (1,1), (2,1), and, to a lesser degree, (1,2) and (3,1).",4.1 Data,[0],[0]
"The M2M aligner (Jiampojamarn et al., 2007), which is based on EM maximum likelihood estimation of alignment parameters, is the classical unsupervised unigram many-to-many aligner in G2P.",4.2 Alignment toolkits/models,[0],[0]
"As has been pointed out (Kubo et al., 2011), M2M greatly overfits the data.5",4.2 Alignment toolkits/models,[0],[0]
"This means that when the M2M aligner is given the freedom to align two sequences without restrictions, it matches them up as a whole.",4.2 Alignment toolkits/models,[0],[0]
"The reason is that a (probabilistic) unigram alignment model adds log-probabilities of matched-up subsequences, which, if not appropriately corrected for, makes alignments with few match-ups a priori more likely than alignments with many matchups, when probabilities of individual match-ups are uniformly or randomly initialized (as is typically the case for EM maximum likelihood estimation in unsupervised models).",4.2 Alignment toolkits/models,[0],[0]
"To address this, M2M must artifically restrain, in our language, the set S to be {(1, 1), (1, 2), (2, 1)}.",4.2 Alignment toolkits/models,[0],[0]
"In contrast, the Mpaligner (Kubo et al., 2011) introduces a prior (or penalty) in the alignment model which favors ‘short’ matches (s, t) over ‘long’ ones.",4.2 Alignment toolkits/models,[0],[0]
"Finally, the Phonetisaurus aligner (Novak et al., 2012) modifies the M2M aligner by adding additional soft constraints.
",4.2 Alignment toolkits/models,[0],[0]
"Our own alignment model is, as indicated, supervised.",4.2 Alignment toolkits/models,[0],[0]
"We implement a unigram alignment model where we specify sim1(u,v) as
α · logp((u,v))",4.2 Alignment toolkits/models,[0],[0]
"+ β · logp((|u|, |v|))",4.2 Alignment toolkits/models,[0],[0]
"+γ · logp(u) + δ · logp(v).
",4.2 Alignment toolkits/models,[0],[0]
"Here, logp(z) denotes the log-probability — estimated from the training data — of observing the
5See also the discussion in (Goldwater et al., 2006) for the related word segmentation problem.
object z, and α, β, γ and δ are parameters.",4.2 Alignment toolkits/models,[0],[0]
"This specification says that the subsequences u and v are similar insofar as (i) u and v have been paired frequently in the training data, (ii) the length of u and the length of v have been paired frequently, (iii)/(iv) u/v by itself is likely.",4.2 Alignment toolkits/models,[0],[0]
"We refer to this unigram alignment model as uniα,β,γ,δ.",4.2 Alignment toolkits/models,[0],[0]
"We also implement a bigram alignment model where we specify sim2 ( (u,v), (u′,v′) )",4.2 Alignment toolkits/models,[0],[0]
"as
α · logp((u,v) | (u′,v′))",4.2 Alignment toolkits/models,[0],[0]
"+β · logp((|u|, |v|)",4.2 Alignment toolkits/models,[0],[0]
"| (|u′|, |v′|))",4.2 Alignment toolkits/models,[0],[0]
"+γ · logp(u|u′)+ δ · logp(v|v′).
",4.2 Alignment toolkits/models,[0],[0]
"Here, logp(z | z′) denotes the logarithm of the conditional probability of observing the object z following the object z′.",4.2 Alignment toolkits/models,[0],[0]
"We refer to this bigram alignment model as biα,β,γ,δ.",4.2 Alignment toolkits/models,[0],[0]
We use two string transduction systems for our experiments.,4.3 Transduction systems,[0],[0]
"The first one is DirecTL+ (Jiampojamarn et al., 2010), a discriminative string-tostring translation system incorporating joint ngram features.",4.3 Transduction systems,[0],[0]
DirecTL+ is an extension of the model presented in Jiampojamarn et al. (2008) which treats string transduction as a source sequence segmentation and subsequent sequence labeling task.,4.3 Transduction systems,[0],[0]
"In addition, we use Phonetisaurus (Novak et al., 2012), a weighted finite state-based joint n-gram model employing recurrent neural network language model N -best rescoring in decoding.",4.3 Transduction systems,[0],[0]
Both systems take aligned pairs of strings as input and from this construct a monotone translation model.6,4.3 Transduction systems,[0],[0]
We employ two measures of alignment quality.,4.4 Measuring alignment quality,[0],[0]
"First, we use word accuracy, defined as the fraction of correctly aligned sequence pairs in a test sample.",4.4 Measuring alignment quality,[0],[0]
This is a very strict measure that penalizes even tiny deviations from the gold standard.,4.4 Measuring alignment quality,[0],[0]
"Additionally, we measure the edit distance between the true alignment Ax,y and the predicted alignment Âx,y. To implement this, we view the two induced segmentations that constitute an alignment — e.g., (ph,oe,n,i,x) and (f,i,n,I,ks) — as strings including splitting signs.",4.4 Measuring alignment quality,[0],[0]
"Thus, we can compute the edit distance between the gold-standard segmented x
6We run both systems with parameters determined by some manual tuning, without trying to systematically optimize their individual performances, however.
string and the predicted segmentation, and analogously for the y sequence.",4.4 Measuring alignment quality,[0],[0]
"Then, we define the edit distance between Ax,y and Âx,y as the sum of these two string edit distances.",4.4 Measuring alignment quality,[0],[0]
"For a test sample, we indicate so-defined average edit distance, averaged over all pairs in the sample.",4.4 Measuring alignment quality,[0],[0]
"To measure alignment quality for the different systems, for English, we run experiments on sets of size x+5 000, where x = 1 000, 2 000, 5 000, 10 000, and 20 000.",5.1 Alignment quality,[0],[0]
"For the supervised models, we consider x as the training data and the 5 000 additional string pairs as test data.7 To quantify effects when training data is very little, we let x also range over 100 and 500 string pairs for the supervised models.",5.1 Alignment quality,[0],[0]
"For the unsupervised models, we simply take all x+5 000 string pairs as data to learn from (but evaluate performance only on the 5 000 string pairs, for comparability).
",5.1 Alignment quality,[0],[0]
"Results are shown in Tables 4, 5, and 6.",5.1 Alignment quality,[0],[0]
"We first note (Table 4) that the unsupervised models perform decently, obtaining accuracy rates of 80% and beyond under appropriate parametrizations.",5.1 Alignment quality,[0],[0]
"We also observe the M2M aligner’s deterioration in performance as we increase its degrees of freedom (allowing it to match subsequences of larger length), confirming our previous remarks.",5.1 Alignment quality,[0],[0]
The Mpaligner does not suffer from this problem as it penalizes large matches.,5.1 Alignment quality,[0],[0]
"Phonetisaurus suffers from the same problems as M2M, but to a lesser degree.",5.1 Alignment quality,[0],[0]
"Overall, we find that, under optimal parametrizations, Phonetisaurus produces best alignments, followed by Mpalign and M2M.",5.1 Alignment quality,[0],[0]
"However, peak performances of all three unsupervised aligners are close.",5.1 Alignment quality,[0],[0]
"Unsurprisingly, the supervised alignment models perform better than the unsupervised ones (Tables 5 and 6).",5.1 Alignment quality,[0],[0]
"Surprisingly, however, they do so with very little training data; fewer than 100 aligned string pairs suffice to outperform the unsupervised models under good calibrations.",5.1 Alignment quality,[0],[0]
"When there is sufficient training data, the supervised models perform splendidly, with a peak accuracy of 99.43% for the bigram alignment model that includes appropriate features (scoring lengths of aligned subsequences,
7For all our below experiments involving the supervised aligners, we set S to a (‘pessimistically’ large) value of {(a, b) | 1 ≤ a ≤ 6, 1 ≤ b ≤ 6}.",5.1 Alignment quality,[0],[0]
"Also, for the bigram models, we add special sequence boundary markers.
etc.).",5.1 Alignment quality,[0],[0]
"We also note that the bigram alignment model is almost consistently better than the unigram alignment model, with a surplus of about 1% point, depending on specific parametrizations.
",5.1 Alignment quality,[0],[0]
We performed an analogous analysis for the German data.,5.1 Alignment quality,[0],[0]
"Results are quite similar except that unigram and bigram alignment model have indistinguishable performance on the German data, indicating (the known fact) that G2P is a more complex task in English, apparently not requiring bigram alignment models.
",5.1 Alignment quality,[0],[0]
"Error analysis Concerning errors that the unigram model commits and the bigram model does not, the majority of errors (roughly 80%) involve match-ups of ed/d and d.",5.1 Alignment quality,[0],[0]
"For example, the unigram model aligns as in
t w",5.1 Alignment quality,[0],[0]
i n k le d t,5.1 Alignment quality,[0],[0]
w I N k @l,5.1 Alignment quality,[0],[0]
"d
while the gold-standard alignment is
t w i n k",5.1 Alignment quality,[0],[0]
l ed t w,5.1 Alignment quality,[0],[0]
I N k @l,5.1 Alignment quality,[0],[0]
"d
While all match-ups in both alignments are plausible, the bigram model assigns here higher probability to the correct ed/d match-up in terminal position (consistently favored in the data set), which
has a particular meaning there, namely, that of a suffix marker for past tense.8,9 In the German data, there is a single instance where the unigram and bigram alignment model disagree, namely, in the alignment of s-t-o-ff-f-l-a-sch-e/S-t-O-f-f-l-&S-@, which the unigram model falsely aligns as s-t-o-f-ff-l-a-sch-e/S-t-O-f-f-l-&-S-@; note that in the correct alignment f must follow ff, not vice versa, which depends on context information, e.g., that o/O signifies a short vowel which is followed by a double consonant, not a single consonant.
",5.1 Alignment quality,[0],[0]
"All remaining errors that the bigram alignment models commits are, for the best considered parametrization and training set size, typically due to match-up types not seen in the training data, and thus mostly concern foreign names or writings (e.g., Bh-u-tt-o/b-u-t-F, falsely aligned as B-hu-tto/b-u-t-F).",5.1 Alignment quality,[0],[0]
"A few other errors might be corrected when the feature coefficients α, β, γ, δ were optimized on a development set rather than set manually.",5.1 Alignment quality,[0],[0]
"We find no indication that our G2P data, either for English or German, would further benefit from n-gram alignment models of order n > 2.",5.1 Alignment quality,[0],[0]
"Next, we estimate the relationship between alignment quality and overall G2P performance (transcription accuracy).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To this end, for the English data, we use the 5 000 aligned string pairs from the previous experiment on alignment quality and feed them in — as training data — to either DirecTL+ or Phonetisaurus as outlined in Section 4.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
We then evaluate G2P performance — in terms of word accuracy (fraction of correctly transcribed strings) — on a distinct test set of size 10 000.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
Figure 1 shows a plot of overall G2P accuracy vs. training set size for the aligner (ranging over the x values in the last section); and a second plot that sketches G2P accuracy as a function of corresponding alignment accuracy.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"We first note that, as the supervised aligner receives more training
8Similar cases are, e.g., alignments of the type f-ee-d-ba-ck/f-i-d-b-a-k, which the unigram model falsely aligns as f-e-ed-b-a-ck/f-i-d-b-a-k. Here, too, the unigram is unable to account for the almost exclusive terminal position of the ed/d match-up in the data.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"9Other errors involve ‘unusual/foreign’ spelling/pronunciation pairs such as Ph-oe-n-i-c-ia/f-@n-i-S-@ (wrongly aligned as Ph-o-en-i-c-ia/f-@-n-i-S-@ by the unigram model) or m-a-d-e-m-oi-s-e-ll-e-’s/m-a-d-@-mw@-z-E-l-0-z (m-a-d-e-m-o-i-s-e-ll-e-’s/m-a-d-@-m-w-@z-E-l-0-z), where the bigram alignment model has apparently gathered the more appropriate statistics.
data from which to align the 5 000 string pairs, the overall G2P accuracy of both DirecTL+ and Phonetisaurus increase substantially (and as a convex function of training set size).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Apparently, the better alignments produced by more training data for the particular supervised aligner considered directly translate into better overall G2P accuracy.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"The other plot in the figure shows that, indeed, there seems to be a linear trend coupling alignment quality with overall G2P performance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Table 7 pairs G2P accuracy with alignment accuracy of selected systems, all run in the x = 20 000 setting.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"While, in the table, better alignments do not necessarily imply better overall G2P performance, the two best alignments also lead to the two best overall G2P performances (although, in this case, the second best alignment is paired with the best overall G2P performance); conversely, the worst alignment quality is coupled with the worst overall G2P performance.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Overall, we ran 249 experiments (including the German data) in which we trained DirecTL+ or Phonetisaurus with alignments of specific quali-
ties obtained from particularly parametrized aligners.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"In each of these cases, we obtained an alignment quality score and a subsequent overall G2P system performance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
The English part of this data is sketched in Figure 2.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This figure seems to corroborate the linear relationship (apparently present in Figure 1) between alignment quality and overall G2P system accuracy, particularly, when alignment quality is measured in the more finegrained metric of edit distance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To formally test
this, we regress overall G2P system performance (measured in word accuracy) on edit distance and other variables.10 This yielded the coefficients as given in Table 8; in each case, the goodness-offit of the linear model was quite large, with R2 values above 90% for the English data and about 84% for the German data.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Also, the coefficients on alignment quality were highly significantly different from zero.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"The table shows that the coefficients are on the order of about −3.80% to −4.70%, meaning that, all else being equal, increasing alignment quality by 1 edit distance to the gold-standard alignment increases overall G2P by about 3.80 to 4.70%.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"So far, we have estimated the effects of alignment quality on overall G2P system performance for a fixed size of training data, namely, 5 000 aligned string pairs.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To see whether this relationship changes when we vary the amount of training data, we run several more experiments.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"In these, we align training sets of sizes 100, 500,
10These include binary dummy variables for the specific systems as well as alignment consistency and its square — measured in conditional entropy H(Y |X)",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"(Pervouchine et al., 2009) — in the regression.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"1 000, 2 000, 10 000, 20 000, 40 000 and 60 000 via our several alignment systems.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
Then we feed the aligned data to the Phonetisaurus system (we omit DirecTL+ here because of its long run times) and compute overall G2P accuracy on a disjoint test set of size 28 000 approximately.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This time, we only use the unsupervised aligners and the gold-standard alignments directly, omitting results for our various supervised aligners.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Note, however, that these aligners could, in principle, imitate the gold-standard alignments with a very high degree of precision, as previously seen.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Table 9
shows that training G2P systems from the human gold standard alignments in each case yields better overall G2P transcriptions than training them from either of the three unsupervised alignments considered here.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"However, we note that the surplus over the unsupervised alignments decreases as training set size increases.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This may be due to the fact that the unsupervised aligners themselves create better alignments once they are boot-
strapped from larger data sets (cf. Table 4).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Additionally, the effect of alignment quality on overall G2P system performance may simply vanish as training set sizes become large enough because the translation modules can better accomodate ‘noisy’ data as long as its size is sufficiently large.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Figure
3 sketches the decreasing influence of alignment system on overall G2P system performance as size of the aligned data increases.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
We have investigated the need for bigram alignment models and the benefit of supervised alignment techniques in G2P. We have also quantitatively estimated the relationship between alignment quality and overall G2P system performance.,6 Conclusion,[0],[0]
"We have found that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task (we find almost no differences between unigram and bigram models for the German sample of G2P data we considered).",6 Conclusion,[0],[0]
"Moreover, we have found that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.",6 Conclusion,[0],[0]
"Finally, we have estimated a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",6 Conclusion,[0],[0]
"At a particular training size, a linear regression model has estimated that improving alignment quality by 1 edit distance toward the
gold standard alignments leads to an 3.80-4.70% increase in G2P transcription accuracy.",6 Conclusion,[0],[0]
"However, we have also found that the importance of good alignments on G2P accuracy appears to dimish as data set size increases, possibly because the translation modules can accomodate more ‘noisy’ data in this scenario.
",6 Conclusion,[0],[0]
"As a ‘policy’ implication, we recommend the use of supervised alignment techniques particularly when the size of the G2P corpus is small or when high quality alignments, as an end in themselves, are required.",6 Conclusion,[0],[0]
"In this case, constructing a few dozen or few hundred alignments in an unsupervised manner and correcting them by hand (to serve as an input for a supervised technique) may be highly beneficial.
",6 Conclusion,[0],[0]
"In future work, it may be worthwhile to study the impact of alignment techniques on overall system performance in other string transduction problems such as transliteration, lemmatization, and spelling error correction.
",6 Conclusion,[0],[0]
Our supervised uni- and bigram aligners are available via https://github.com/ SteffenEger/.,6 Conclusion,[0],[0]
I thank three anonymous reviewers and Tim vor der Brück for valuable suggestions.,Acknowledgments,[0],[0]
We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in graphemeto-phoneme (G2P) conversion.,abstractText,[0],[0]
"Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance.",abstractText,[0],[0]
"We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task.",abstractText,[0],[0]
"Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.",abstractText,[0],[0]
"Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",abstractText,[0],[0]
Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P,title,[0],[0]
"Ambiguity is one of the defining characteristics of human languages, and language understanding crucially relies on the ability to obtain unambiguous representations of linguistic content.",1 Introduction,[0],[0]
"While some ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of many linguistic constructions requires integration of world knowledge and perceptual information obtained from other modalities.
",1 Introduction,[0],[0]
"In this work, we focus on the problem of grounding language in the visual modality, and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed.",1 Introduction,[0],[0]
"This type of inference is frequently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child (Snow, 1972).
",1 Introduction,[0],[0]
"Our task is also fundamental to the problem of grounding vision in language, by focusing on phenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when using language as a medium for expressing understanding of visual content.",1 Introduction,[0],[0]
"Due to such ambiguities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the relevant visual content.",1 Introduction,[0],[0]
"Our task addresses this issue by introducing a deep validation protocol for visual understanding, requiring not only providing a surface description of a visual activity but also demonstrating structural understanding at the levels of syntax, semantics and discourse.
",1 Introduction,[0],[0]
"To enable the systematic study of visually grounded processing of ambiguous language, we create a new corpus, LAVA (Language and Vision Ambiguities).",1 Introduction,[0],[0]
This corpus contains sentences with linguistic ambiguities that can only be resolved using external information.,1 Introduction,[0],[0]
The sentences are paired with short videos that visualize different interpretations of each sentence.,1 Introduction,[0],[0]
"Our sentences encompass a wide range of syntactic, semantic and dis-
ar X
iv :1
60 3.
08 07
9v 1
[ cs
.C",1 Introduction,[0],[0]
"V
] 2
6 M
ar 2
course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis.",1 Introduction,[0],[0]
"Overall, the corpus contains 237 sentences, with 2 to 3 interpretations per sentence, and an average of 3.37 videos that depict visual variations of each sentence interpretation, corresponding to a total of 1679 videos.
",1 Introduction,[0],[0]
"Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentence that matches the content of a given video.",1 Introduction,[0],[0]
"Our approach for tackling this task extends the sentence tracker introduced in (Siddharth et al., 2014).",1 Introduction,[0],[0]
The sentence tracker produces a score which determines if a sentence is depicted by a video.,1 Introduction,[0],[0]
This earlier work had no concept of ambiguities; it assumed that every sentence had a single interpretation.,1 Introduction,[0],[0]
"We extend this approach to represent multiple interpretations of a sentence, enabling us to pick the interpretation that is most compatible with the video.
",1 Introduction,[0],[0]
"To summarize, the contributions of this paper are threefold.",1 Introduction,[0],[0]
"First, we introduce a new task for visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content.",1 Introduction,[0],[0]
"Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts.",1 Introduction,[0],[0]
"Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%.",1 Introduction,[0],[0]
"Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",2 Related Work,[0],[0]
"While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities.
",2 Related Work,[0],[0]
"Previous work relating ambiguity in language to the visual modality addressed the problem of word
sense disambiguation (Barnard et al., 2003).",2 Related Work,[0],[0]
"However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities.",2 Related Work,[0],[0]
"Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014).",2 Related Work,[0],[0]
"Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis.",2 Related Work,[0],[0]
"More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision.
",2 Related Work,[0],[0]
"The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995).",2 Related Work,[0],[0]
"A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans.",2 Related Work,[0],[0]
"Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972).",2 Related Work,[0],[0]
"Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009).",2 Related Work,[0],[0]
Our study leverages such insights to develop a complementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence.,2 Related Work,[0],[0]
In this work we provide a concrete framework for the study of language understanding with visual context by introducing the task of grounded language disambiguation.,3 Task,[0],[0]
This task requires to choose the correct linguistic representation of a sentence given a visual context depicted in a video.,3 Task,[0],[0]
"Specifically, provided with a sentence, n candidate interpretations of that sentence and a video that depicts the content of the sentence, one needs to choose the interpretation that corresponds to the content of the video.
",3 Task,[0],[0]
"To illustrate this task, consider the example in figure 1, where we are given the sentence “Sam approached the chair with a bag” along with two different linguistic interpretations.",3 Task,[0],[0]
"In the first in-
terpretation, which corresponds to parse 1(a), Sam has the bag.",3 Task,[0],[0]
"In the second interpretation associated with parse 1(b), the bag is on the chair rather than with Sam.",3 Task,[0],[0]
"Given the visual context from figure 1(c), the task is to choose which interpretation is most appropriate for the sentence.",3 Task,[0],[0]
"To address the grounded language disambiguation task, we use a compositional approach for determining if a specific interpretation of a sentence is depicted by a video.",4 Approach Overview,[0],[0]
"In this framework, described in detail in section 6, a sentence and an accompanying interpretation encoded in first order logic, give rise to a grounded model that matches a video against the provided sentence interpretation.
",4 Approach Overview,[0],[0]
"The model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words, and trackers which locate objects in video frames.",4 Approach Overview,[0],[0]
"To represent an interpretation of a sentence, word models are combined with trackers through a cross-product which respects the semantic representation of the sentence to create a single model which recognizes that interpretation.
",4 Approach Overview,[0],[0]
"Given a sentence, we construct an HMM based representation for each interpretation of that sentence.",4 Approach Overview,[0],[0]
We then detect candidate locations for objects in every frame of the video.,4 Approach Overview,[0],[0]
"Together the re-
S
NP NNP Bill
VP
VBD held
NP
DT the
NP
JJ
green
NP
NN chair CC and NN bag
(a) First interpretation
S
NP NNP Bill
VP
VBD held
NP
DT the
NP
NP
JJ
green
NN chair
CC and NN bag
(b) Second interpretation
forestation for the sentence and the candidate object locations are combined to form a model which can determine if a given interpretation is depicted by the video.",4 Approach Overview,[0],[0]
We test each interpretation and report the interpretation with highest likelihood.,4 Approach Overview,[0],[0]
"To enable a systematic study of linguistic ambiguities that are grounded in vision, we compiled a corpus with ambiguous sentences describing visual actions.",5 Corpus,[0],[0]
"The sentences are formulated such that the correct linguistic interpretation of each sentence can only be determined using external, non-linguistic, information about the depicted activity.",5 Corpus,[0],[0]
"For example, in the sentence “Bill held the green chair and bag”, the correct scope of “green” can only be determined by integrating additional information about the color of the bag.",5 Corpus,[0],[0]
"This information is provided in the accompanying videos, which visualize the possible interpretations of each sentence.",5 Corpus,[0],[0]
Figure 2 presents the syntactic parses for this example along with frames from the respective videos.,5 Corpus,[0],[0]
"Although our videos contain visual uncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting, and hence a video always corresponds to a single candidate representation of a sentence.
",5 Corpus,[0],[0]
"The corpus covers a wide range of well
known syntactic, semantic and discourse ambiguity classes.",5 Corpus,[0],[0]
"While the ambiguities are associated with various types, different sentence interpretations always represent distinct sentence meanings, and are hence encoded semantically using first order logic.",5 Corpus,[0],[0]
"For syntactic and discourse ambiguities we also provide an additional, ambiguity type specific encoding as described below.
",5 Corpus,[0],[0]
"• Syntax Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase (VP) attachments, and ambiguities in the interpretation of conjunctions.",5 Corpus,[0],[0]
"In addition to logical forms, sentences with syntactic ambiguities are also accompanied with Context Free Grammar (CFG) parses of the candidate interpretations, generated from a deterministic CFG parser.
",5 Corpus,[0],[0]
• Semantics,5 Corpus,[0],[0]
"The corpus addresses several classes of semantic quantification ambiguities, in which a syntactically unambiguous sentence may correspond to different logical forms.",5 Corpus,[0],[0]
"For each such sentence we provide the respective logical forms.
",5 Corpus,[0],[0]
• Discourse,5 Corpus,[0],[0]
"The corpus contains two types of discourse ambiguities, Pronoun Anaphora and Ellipsis, offering examples comprising two sentences.",5 Corpus,[0],[0]
"In anaphora ambiguity cases, an ambiguous pronoun in the second sentence is given its candidate antecedents in the first sentence, as well as a corresponding logical form for the meaning of the second sentence.",5 Corpus,[0],[0]
"In ellipsis cases, a part of the second sentence, which can constitute either the subject and the verb, or the verb and the object, is omitted.",5 Corpus,[0],[0]
"We provide both interpretations of the omission in the form of a single unambiguous sentence, and its logical form, which combines the meanings of the first and the second sentences.
",5 Corpus,[0],[0]
"Table 2 lists examples of the different ambiguity classes, along with the candidate interpretations of each example.
",5 Corpus,[0],[0]
The corpus is generated using Part of Speech (POS) tag sequence templates.,5 Corpus,[0],[0]
"For each template, the POS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all the visually applicable assignments.",5 Corpus,[0],[0]
"This generation process yields an overall of 237 sentences,
of which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations.",5 Corpus,[0],[0]
"Table 1 presents the corpus templates for each ambiguity class, along with the number of sentences generated from each template.
",5 Corpus,[0],[0]
The corpus videos are filmed in an indoor environment containing background objects and pedestrians.,5 Corpus,[0],[0]
"To account for the manner of performing actions, videos are shot twice with different actors.",5 Corpus,[0],[0]
"Whenever applicable, we also filmed the actions from two different directions (e.g. approach from the left, and approach from the right).",5 Corpus,[0],[0]
"Finally, all videos were shot with two cameras from two different view points.",5 Corpus,[0],[0]
"Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos.",5 Corpus,[0],[0]
"The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames).
",5 Corpus,[0],[0]
"A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities.",5 Corpus,[0],[0]
"Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed.",5 Corpus,[0],[0]
"Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013),
aim to control for more aspects of the videos than just the main action being performed but they do not provide the range of ambiguities discussed here.",5 Corpus,[0],[0]
"The closest dataset is that of Siddharth et al. (2014) as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.",5 Corpus,[0],[0]
"Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for evaluating the work described here.",5 Corpus,[0],[0]
"To perform the disambiguation task, we extend the sentence recognition model of Siddharth et al. (2014) which represents sentences as compositions of words.",6 Model,[0],[0]
"Given a sentence, its first order logic interpretation and a video, our model produces a score which determines if the sentence is depicted by the video.",6 Model,[0],[0]
It simultaneously tracks the participants in the events described by the sentence while recognizing the events themselves.,6 Model,[0],[0]
"This al-
lows it to be flexible in the presence of noise by integrating top-down information from the sentence with bottom-up information from object and property detectors.",6 Model,[0],[0]
"Each word in the query sentence is represented by an HMM (Baum et al., 1970), which recognizes tracks (i.e. paths of detections in a video for a specific object) that satisfy the semantics of the given word.",6 Model,[0],[0]
"In essence, this model can be described as having two layers, one in which object tracking occurs and one in which words observe tracks and filter tracks that do not satisfy the word constraints.
",6 Model,[0],[0]
"Given a sentence interpretation, we construct a sentence-specific model which recognizes if a video depicts the sentence as follows.",6 Model,[0],[0]
"Each predicate in the first order logic formula has a corresponding HMM, which can recognize if that predicate is true of a video given its arguments.",6 Model,[0],[0]
"Each variable has a corresponding tracker which attempts to physically locate the bounding box corresponding to that variable in each frame of a
video.",6 Model,[0],[0]
This creates a bipartite graph: HMMs that represent predicates are connected to trackers that represent variables.,6 Model,[0],[0]
"The trackers themselves are similar to the HMMs, in that they comprise a lattice of potential bounding boxes in every frame.",6 Model,[0],[0]
"To construct a joint model for a sentence interpretation, we take the cross product of HMMs and trackers, taking only those cross products dictated by the structure of the formula corresponding to the desired interpretation.",6 Model,[0],[0]
"Given a video, we employ an object detector to generate candidate detections in each frame, construct trackers which select one of these detections in each frame, and finally construct the overall model from HMMs and trackers.
",6 Model,[0],[0]
"Provided an interpretation and its corresponding formula composed of P predicates and V variables, along with a collection of object detections, bframedetection index, in each frame of a video of length T the model computes the score of the videosentence pair by finding the optimal detection for each participant in every frame.",6 Model,[0],[0]
"This is in essence the Viterbi algorithm (Viterbi, 1971), the MAP algorithm for HMMs, applied to finding optimal object detections jframevariable for each participant, and the optimal state kframepredicate for each predicate HMM, in every frame.",6 Model,[0],[0]
"Each detection is scored by its confidence from the object detector, f and each object track is scored by a motion coherence metric g which determines if the motion of the track agrees with the underlying optical flow.",6 Model,[0],[0]
"Each predicate,
p, is scored by the probability of observing a particular detection in a given state hp, and by the probability of transitioning between states ap.",6 Model,[0],[0]
"The structure of the formula and the fact that multiple predicates often refer to the same variables is recorded by θ, a mapping between predicates and their arguments.",6 Model,[0],[0]
"The model computes the MAP estimate as:
max j11 ,..., j T 1
...",6 Model,[0],[0]
"j1V ,..., j T V
max k11,..., k T 1
... k1P ,..., k T P
V∑ v=1 T∑ t=1",6 Model,[0],[0]
f(btjtv ),6 Model,[0],[0]
+ T∑ t=2 g(bt−1,6 Model,[0],[0]
"jt−1v , btjtv )+
P∑ p=1 T∑ t=1 hp(k t p, b t jt θ1p , btjt θ2p ) + T∑ t=2 ap(k t−1 p , k t p)
for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binary predicates) but is trivially extended to arbitrary arities.",6 Model,[0],[0]
"Figure 3 provides a visual overview of the model as a cross-product of tracker models and word models.
",6 Model,[0],[0]
Our model extends the approach of Siddharth et al. (2014) in several ways.,6 Model,[0],[0]
"First, we depart from the dependency based representation used in that work, and recast the model to encode first order logic formulas.",6 Model,[0],[0]
Note that some complex first order logic formulas cannot be directly encoded in the model and require additional inference steps.,6 Model,[0],[0]
"This extension enables us to represent ambiguities in which a given sentence has multiple logical interpretations for the same syntactic parse.
",6 Model,[0],[0]
"Second, we introduce several model components which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our corpus and could not be handled by the model of Siddharth et al. (2014).",6 Model,[0],[0]
"These new components are the predicate “not equal”, disjunction, and conjunction.",6 Model,[0],[0]
"The key addition among these components is support for the new predicate “not equal”, which enforces that two tracks, i.e. objects, are distinct from each other.",6 Model,[0],[0]
"For example, in the sentence “Claire and Bill moved a chair” one would want to ensure that the two movers are distinct entities.",6 Model,[0],[0]
"In earlier work, this was not required because the sentences tested in that work were designed to distinguish objects based on constraints rather than identity.",6 Model,[0],[0]
"In other words, there might have been two different people but they were distinguished in the sentence by their actions or appearance.",6 Model,[0],[0]
"To faithfully recognize that two actors are moving the chair in the earlier example, we must ensure that they are disjoint from each other.",6 Model,[0],[0]
"In order to do this we create a new HMM for this predicate, which assigns low probability to tracks that heavily overlap, forcing the model to fit two different actors in the previous example.",6 Model,[0],[0]
"By combining the new first order logic based semantic representation in lieu of a syntactic representation with a more expressive model, we can encode the sentence interpretations required to perform the disambiguation task.
",6 Model,[0],[0]
Figure 3(left) shows an example of two different interpretations of the above discussed sentence “Claire and Bill moved a chair”.,6 Model,[0],[0]
"Object trackers, which correspond to variables in the first order logic representation of the sentence interpretation, are shown in red.",6 Model,[0],[0]
"Predicates which constrain the possible bindings of the trackers, corresponding to predicates in the representation of the sentence, are shown in blue.",6 Model,[0],[0]
"Links represent the argument structure of the first order logic formula, and determine the cross products that are taken between the predicate HMMs and tracker lattices in order to form the joint model which recognizes the entire interpretation in a video.
",6 Model,[0],[0]
The resulting model provides a single unified formalism for representing all the ambiguities in table 2.,6 Model,[0],[0]
"Moreover, this approach can be tuned to different levels of specificity.",6 Model,[0],[0]
"We can create models that are specific to one interpretation of a sentence or that are generic, and accept multiple interpretations by eliding constraints that are not com-
mon between the different interpretations.",6 Model,[0],[0]
"This allows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible.",6 Model,[0],[0]
We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5.,7 Experimental Results,[0],[0]
"Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes.",7 Experimental Results,[0],[0]
"We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus.",7 Experimental Results,[0],[0]
"For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space.",7 Experimental Results,[0],[0]
The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set.,7 Experimental Results,[0],[0]
"As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity.",7 Experimental Results,[0],[0]
"Instead, any sentence which contains names was automatically converted to one which contains arbitrary “person” labels.
",7 Experimental Results,[0],[0]
The sentences in our corpus have either two or three interpretations.,7 Experimental Results,[0],[0]
"Each interpretation has one or more associated videos where the scene was shot from a different angle, carried out either by different actors, with different objects, or in different directions of motion.",7 Experimental Results,[0],[0]
"For each sentence-video pair, we performed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations of the corresponding sentence best fits that video.",7 Experimental Results,[0],[0]
"Overall chance performance on our dataset is 49.04%, slightly lower than 50% due to the 1- out-of-3 classification examples.
",7 Experimental Results,[0],[0]
The model presented here achieved an accuracy of 75.36% over the entire corpus averaged across all error categories.,7 Experimental Results,[0],[0]
This demonstrates that the model is largely capable of capturing the underlying task and that similar compositional crossmodal models may do the same.,7 Experimental Results,[0],[0]
"For each of the 3 major ambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semantic ambiguities, and 64.44% for discourse ambiguities.
",7 Experimental Results,[0],[0]
The most significant source of model failures are poor object detections.,7 Experimental Results,[0],[0]
Objects are often rotated and presented at angles that are difficult to recognize.,7 Experimental Results,[0],[0]
"Certain object classes like the telescope
are much more difficult to recognize due to their small size and the fact that hands tend to largely occlude them.",7 Experimental Results,[0],[0]
"This accounts for the degraded performance of the semantic ambiguities relative to the syntactic ambiguities, as many more semantic ambiguities involved the telescope.",7 Experimental Results,[0],[0]
Object detector performance is similarly responsible for the lower performance of the discourse ambiguities which relied much more on the accuracy of the person detector as many sentences involve only people interacting with each other without any additional objects.,7 Experimental Results,[0],[0]
"This degrades performance by removing a helpful constraint for inference, according to which people tend to be close to the objects they are manipulating.",7 Experimental Results,[0],[0]
"In addition, these sentences introduced more visual uncertainty as they often involved three actors.
",7 Experimental Results,[0],[0]
The remaining errors are due to the event models.,7 Experimental Results,[0],[0]
"HMMs can fixate on short sequences of events which seem as if they are part of an action, but in fact are just noise or the prefix of another action.",7 Experimental Results,[0],[0]
"Ideally, one would want an event model which has a global view of the action, if an object went up from the beginning to the end of the video while a person was holding it, it’s likely that the object was being picked up.",7 Experimental Results,[0],[0]
"The event models used here cannot enforce this constraint, they merely assert that the object was moving up for some number of frames; an event which can happen due to noise in the object detectors.",7 Experimental Results,[0],[0]
Enforcing such local constraints instead of the global constraint of the motion of the object over the video makes joint tracking and event recognition tractable in the framework presented here but can lead to errors.,7 Experimental Results,[0],[0]
Finding models which strike a better balance between local information and global constraints while maintaining tractable inference remains an area of future work.,7 Experimental Results,[0],[0]
We present a novel framework for studying ambiguous utterances expressed in a visual context.,8 Conclusion,[0],[0]
"In particular, we formulate a new task for resolving structural ambiguities using visual signal.",8 Conclusion,[0],[0]
"This is a fundamental task for humans, involving complex cognitive processing, and is a key challenge for language acquisition during childhood.",8 Conclusion,[0],[0]
"We release a multimodal corpus that enables to address this task, as well as support further investigation of ambiguity related phenomena in visually grounded language processing.",8 Conclusion,[0],[0]
"Finally, we
present a unified approach for resolving ambiguous descriptions of videos, achieving good performance on our corpus.
While our current investigation focuses on structural inference, we intend to extend this line of work to learning scenarios, in which the agent has to deduce the meaning of words and sentences from structurally ambiguous input.",8 Conclusion,[0],[0]
"Furthermore, our framework can be beneficial for image and video retrieval applications in which the query is expressed in natural language.",8 Conclusion,[0],[0]
"Given an ambiguous query, our approach will enable matching and clustering the retrieved results according to the different query interpretations.",8 Conclusion,[0],[0]
"This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF1231216.",Acknowledgments,[0],[0]
SU was also supported by ERC Advanced Grant 269627 Digital Baby.,Acknowledgments,[0],[0]
Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception.,abstractText,[0],[0]
"In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence.",abstractText,[0],[0]
"To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence.",abstractText,[0],[0]
We address this task by extending a vision model which determines if a sentence is depicted by a video.,abstractText,[0],[0]
"We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.",abstractText,[0],[0]
Do You See What I Mean? Visual Resolution of Linguistic Ambiguities,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1275–1284 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1275
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",text,[0],[0]
"Neural machine translation (NMT) has proven to be powerful (Sutskever et al., 2014; Bahdanau et al., 2015).",1 Introduction,[0],[0]
"It is on-par, and in some cases, even surpasses the traditional statistical MT (Luong et al., 2015) while enjoying more flexibility and significantly less manual effort for feature engineering.",1 Introduction,[0],[0]
"Despite their flexibility, most neural MT models translate sentences independently.",1 Introduction,[0],[0]
"Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a
few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017).
",1 Introduction,[0],[0]
There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps.,1 Introduction,[0],[0]
Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements.,1 Introduction,[0],[0]
"More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts.",1 Introduction,[0],[0]
"The latter two report deteriorated performance when using the target-side context.
",1 Introduction,[0],[0]
"In this paper, we present a document-level machine translation model which combines sentencebased NMT (Bahdanau et al., 2015) with memory networks (Sukhbaatar et al., 2015).",1 Introduction,[0],[0]
"We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated.",1 Introduction,[0],[0]
"We conduct experiments on three language pairs: French-English, German-English and Estonian-English.",1 Introduction,[0],[0]
"The experimental results and analysis demonstrate that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",1 Introduction,[0],[0]
"Our document NMT model is grounded on sentence-based NMT model (Bahdanau et al.,
2015) which contains an encoder to read the source sentence as well as an attentional decoder to generate the target translation.
",2.1 Neural Machine Translation (NMT),[0],[0]
Encoder It is a bidirectional RNN consisting of two RNNs running in opposite directions over the source sentence:,2.1 Neural Machine Translation (NMT),[0],[0]
"−→ hi = −−→ RNN( −→ h i−1,ES",2.1 Neural Machine Translation (NMT),[0],[0]
"[xi]), ←−",2.1 Neural Machine Translation (NMT),[0],[0]
h,2.1 Neural Machine Translation (NMT),[0],[0]
"i = ←−− RNN( ←− h i+1,ES",2.1 Neural Machine Translation (NMT),[0],[0]
"[xi])
",2.1 Neural Machine Translation (NMT),[0],[0]
"where ES [xi] is embedding of the word xi from the embedding table ES of the source language, and −→ h i and ←−",2.1 Neural Machine Translation (NMT),[0],[0]
h,2.1 Neural Machine Translation (NMT),[0],[0]
"i are the hidden states of the forward and backward RNNs which can be based on the LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) units.",2.1 Neural Machine Translation (NMT),[0],[0]
"Each word in the source sentence is then represented by the concatenation of the corresponding bidirectional hidden states, hi =",2.1 Neural Machine Translation (NMT),[0],[0]
[ −→ h i; ←− h,2.1 Neural Machine Translation (NMT),[0],[0]
"i].
",2.1 Neural Machine Translation (NMT),[0],[0]
"Decoder The generation of each word yj is conditioned on all of the previously generated words y<j via the state of the RNN decoder sj , and the source sentence via a dynamic context vector cj :
yj ∼ softmax(Wy · rj + br) rj = tanh(sj +Wrc · cj",2.1 Neural Machine Translation (NMT),[0],[0]
+,2.1 Neural Machine Translation (NMT),[0],[0]
"Wrj ·ET [yj−1]) sj = tanh(Ws · sj−1 +Wsj ·ET [yj−1] +Wsc · cj)
where ET",2.1 Neural Machine Translation (NMT),[0],[0]
"[yj ] is embedding of the word yj from the embedding table ET of the target language, and W matrices and br vector are the parameters.",2.1 Neural Machine Translation (NMT),[0],[0]
"The dynamic context vector cj is computed via cj = ∑ i αjihi, where
αj = softmax(aj) aji = v · tanh(Wae ·",2.1 Neural Machine Translation (NMT),[0],[0]
"hi +Wat · sj−1)
",2.1 Neural Machine Translation (NMT),[0],[0]
This is known as the attention mechanism which dynamically attends to relevant parts of the source necessary for generating the next target word.,2.1 Neural Machine Translation (NMT),[0],[0]
"Memory Networks (Weston et al., 2015) are a class of neural models that use external memories to perform inference based on long-range dependencies.",2.2 Memory Networks (MemNets),[0],[0]
"A memory is a collection of vectors M = {m1, ..,mK} constituting the memory cells, where each cell mk may potentially correspond to a discrete object xk.",2.2 Memory Networks (MemNets),[0],[0]
The memory is equipped with a read and optionally a write operation.,2.2 Memory Networks (MemNets),[0],[0]
"Given a query vector q, the output vector generated by reading from the memory is ∑|M | i=1",2.2 Memory Networks (MemNets),[0],[0]
"pimi, where pi represents the relevance of the query to the i-th memory cell p =
softmax(qT ·M).",2.2 Memory Networks (MemNets),[0],[0]
"For the rest of the paper, we denote the read operation by MemNet(M , q).",2.2 Memory Networks (MemNets),[0],[0]
We formulate document-wide machine translation as a structured prediction problem.,3 Document NMT as Structured Prediction,[0],[0]
"Given a set of sentences {x1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",x|d|} in a source document d, we are interested in generating the collection of their translations {y1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",y|d|} taking into account interdependencies among them imposed by the document.",3 Document NMT as Structured Prediction,[0],[0]
We achieve this by the factor graph in Figure 1 to model the probability of the target document given the source document.,3 Document NMT as Structured Prediction,[0],[0]
"Our model has two types of factors:
• fθ(yt;xt,x−t) to capture the interdependencies between the translation yt, the corresponding source sentence xt and all the other sentences in the source document x−t, and
• gθ(yt;y−t) to capture the interdependencies between the translation yt and all the other translations in the document y−t.
",3 Document NMT as Structured Prediction,[0],[0]
"Hence, the probability of a document translation given the source document is
P (y1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",y|d||x1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",x|d|) ∝",3 Document NMT as Structured Prediction,[0],[0]
"exp (∑
t
fθ(yt;xt,x−t) + gθ(yt;y−t) ) .
",3 Document NMT as Structured Prediction,[0],[0]
"The factors fθ and gθ are realised by neural architectures whose parameters are collectively denoted by θ.
Training It is challenging to train the model parameters by maximising the (regularised) likelihood since computing the partition function is hard.",3 Document NMT as Structured Prediction,[0],[0]
"This is due to the enormity of factors
gθ(yt;y−t) over a large number of translation variables yt’s (i.e., the number of sentences in the document) as well as their unbounded domain (i.e., all sentences in the target language).",3 Document NMT as Structured Prediction,[0],[0]
"Thus, we resort to maximising the pseudo-likelihood (Besag, 1975) for training the parameters:
argmax θ ∏ d∈D |d|∏ t=1 Pθ(yt|xt,y−t,x−t) (1)
whereD is the set of bilingual training documents, and |d| denotes the number of (bilingual) sentences in the document d = {(xt,yt)}|d|t=1.",3 Document NMT as Structured Prediction,[0],[0]
"We directly model the document-conditioned NMT model Pθ(yt|xt,y−t,x−t) using a neural architecture which subsumes both the fθ and gθ factors (covered in the next section).
",3 Document NMT as Structured Prediction,[0],[0]
"Decoding To generate the best translation for a document according to our model, we need to solve the following optimisation problem:
arg max y1,...,y|d| |d|∏ t=1 Pθ(yt|xt,y−t,x−t)
which is hard (due to similar reasons as mentioned earlier).",3 Document NMT as Structured Prediction,[0],[0]
We hence resort to a block coordinate descent optimisation algorithm.,3 Document NMT as Structured Prediction,[0],[0]
"More specifically, we initialise the translation of each sentence using the base neural MT model P (yt|xt).",3 Document NMT as Structured Prediction,[0],[0]
"We then repeatedly visit each sentence in the document, and update its translation using our document-context dependent NMT model P (yt|xt,y−t,x−t) while the translations of other sentences are kept fixed.",3 Document NMT as Structured Prediction,[0],[0]
"We augment the sentence-level attentional NMT model by incorporating the document context (both source and target) using memory networks when generating the translation of a sentence, as shown in Figure 2.
",4 Context Dependent NMT with MemNets,[0],[0]
"Our model generates the target translation word-by-word from left to right, similar to the vanilla attentional neural translation model.",4 Context Dependent NMT with MemNets,[0],[0]
"However, it conditions the generation of a target word not only on the previously generated words and the current source sentence (as in the vanilla NMT model), but also on all the other source sentences of the document and their translations.",4 Context Dependent NMT with MemNets,[0],[0]
"That is, the
generation process is as follows:
Pθ(yt|xt,y−t,x−t)",4 Context Dependent NMT with MemNets,[0],[0]
"= |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt,y−t,x−t)
(2)
where yt,j is the j-th word of the t-th target sentence, yt,<j are the previously generated words, and x−t and y−t are as introduced previously.
",4 Context Dependent NMT with MemNets,[0],[0]
"Our model represents the source and target document contexts as external memories, and attends to relevant parts of these external memories when generating the translation of a sentence.",4 Context Dependent NMT with MemNets,[0],[0]
"Let M [x−t] and M [y−t] denote external memories representing the source and target document context, respectively.",4 Context Dependent NMT with MemNets,[0],[0]
These contain memory cells corresponding to all sentences in the document except the t-th sentence (described shortly).,4 Context Dependent NMT with MemNets,[0],[0]
"Let ht and st be representations of the t-th source sentence and its current translation, from the encoder and decoder respectively.",4 Context Dependent NMT with MemNets,[0],[0]
"We make use of ht as the query to get the relevant context from the source external memory:
csrct = MemNet(M",4 Context Dependent NMT with MemNets,[0],[0]
"[x−t],ht)
Furthermore, for the t-th sentence, we get the relevant information from the target context:
ctrgt = MemNet(M",4 Context Dependent NMT with MemNets,[0],[0]
"[y−t], st +Wat · ht)
where the query consists of the representation of the translation st from the decoder endowed with that of the source sentence ht from the encoder to make the query robust to potential noises in the current translation and circumvent error propagation, and Wat projects the source representation into the hidden state space.
",4 Context Dependent NMT with MemNets,[0],[0]
"Now that we have representations of the relevant source and target document contexts, Eq. 2 can be re-written as:
Pθ(yt|xt,y−t,x−t)",4 Context Dependent NMT with MemNets,[0],[0]
"= |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt, ctrgt , c src t )
(3)
More specifically, the memory contexts csrct and ctrgt are incorporated into the NMT decoder as:
• Memory-to-Context in which the memory contexts are incorporated when computing the next decoder hidden state:
st,j = tanh(Ws · st,j−1 +Wsj ·ET [yt,j ] + Wsc · ct,j +Wsm · csrct +Wst · c trg t )
",4 Context Dependent NMT with MemNets,[0],[0]
"• Memory-to-Output in which the memory contexts are incorporated in the output layer:
yt,j ∼ softmax(Wy · rt,j +Wym · csrct + Wyt · ctrgt + br)
",4 Context Dependent NMT with MemNets,[0],[0]
"where Wsm, Wst, Wym, and Wyt are the new parameter matrices.",4 Context Dependent NMT with MemNets,[0],[0]
"We use only the source, only the target, or both external memories as the additional conditioning contexts.",4 Context Dependent NMT with MemNets,[0],[0]
"Furthermore, we use either the Memory-to-Context or Memory-toOutput architectures for incorporating the document contexts.",4 Context Dependent NMT with MemNets,[0],[0]
"In the experiments, we will explore these different options to investigate the most effective combination.",4 Context Dependent NMT with MemNets,[0],[0]
"We now turn our attention to the construction of the external memories for the source and target sides of a document.
",4 Context Dependent NMT with MemNets,[0],[0]
The Source Memory We make use of a hierarchical 2-level RNN architecture to construct the external memory of the source document.,4 Context Dependent NMT with MemNets,[0],[0]
"More specifically, we pass each sentence of the document through a sentence-level bidirectional RNN to get the representation of the sentence (by concatenating the last hidden states of the forward and backward RNNs).",4 Context Dependent NMT with MemNets,[0],[0]
We then pass the sentence representations through a document-level bidirectional RNN to propagate sentences’ information across the document.,4 Context Dependent NMT with MemNets,[0],[0]
"We take the hidden states
of the document-level bidirectional RNNs as the memory cells of the source external memory.
",4 Context Dependent NMT with MemNets,[0],[0]
"The source external memory is built once for each minibatch, and does not change throughout the document translation.",4 Context Dependent NMT with MemNets,[0],[0]
"To be able to fit the computational graph of the document NMT model within GPU memory limits, we pre-train the sentence-level bidirectional RNN using the language modelling training objective.",4 Context Dependent NMT with MemNets,[0],[0]
"However, the document-level bidirectional RNN is trained together with other parameters of the document NMT model by back-propagating the document translation training objective.
",4 Context Dependent NMT with MemNets,[0],[0]
The Target Memory,4 Context Dependent NMT with MemNets,[0],[0]
The memory cells of the target external memory represent the current translations of the document.,4 Context Dependent NMT with MemNets,[0],[0]
Recall from the previous section that we use coordinate descent iteratively to update these translations.,4 Context Dependent NMT with MemNets,[0],[0]
"Let {y1, . . .",4 Context Dependent NMT with MemNets,[0],[0]
",y|d|} be the current translations, and let {s|y1|, . . .",4 Context Dependent NMT with MemNets,[0],[0]
", s|y|d||} be the last states of the decoder when these translations were generated.",4 Context Dependent NMT with MemNets,[0],[0]
We use these last decoder states as the cells of the external target memory.,4 Context Dependent NMT with MemNets,[0],[0]
"We could make use of hierarchical sentencedocument RNNs to transform the document translations into memory cells (similar to what we do for the source memory); however, it would have been computationally expensive and may have resulted in error propagation.",4 Context Dependent NMT with MemNets,[0],[0]
We will show in the experiments that our efficient target memory construction is indeed effective.,4 Context Dependent NMT with MemNets,[0],[0]
Datasets.,5 Experiments and Analysis,[0],[0]
"We conducted experiments on three language pairs: French-English, German-English and Estonian-English.",5 Experiments and Analysis,[0],[0]
Table 1 shows the statistics of the datasets used in our experiments.,5 Experiments and Analysis,[0],[0]
"The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document.",5 Experiments and Analysis,[0],[0]
"The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005).",5 Experiments and Analysis,[0],[0]
"Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents.",5 Experiments and Analysis,[0],[0]
The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets.,5 Experiments and Analysis,[0],[0]
"For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1https://wit3.fbk.eu/ 2http://www.statmt.org/europarl/ 3http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz
and news-test2011 and news-test2016 as the test sets.",5 Experiments and Analysis,[0],[0]
"The news-commentary corpus has document boundaries already provided.
",5 Experiments and Analysis,[0],[0]
We pre-processed all corpora to remove very short documents and those with missing translations.,5 Experiments and Analysis,[0],[0]
"Out-of-vocabulary and rare words (frequency less than 5) are replaced by the <UNK> token, following Cohn et al. (2016).4
Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations.",5 Experiments and Analysis,[0],[0]
"We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p < 0.05, comparing to the baselines.
",5 Experiments and Analysis,[0],[0]
"Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016).",5 Experiments and Analysis,[0],[0]
"For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively.",5 Experiments and Analysis,[0],[0]
The translation model uses GRU units for the bidirectional RNN encoder and the 2-layer RNN decoder.,5 Experiments and Analysis,[0],[0]
GRUs are used instead of LSTMs to reduce the number of parameters in the main model.,5 Experiments and Analysis,[0],[0]
"The RNN hidden dimensions and word embedding sizes are set to 512 in the translation and memory components, and the alignment dimension is set to 256 in the translation model.
",5 Experiments and Analysis,[0],[0]
Training We use a stage-wise method to train the variants of our document context NMT model.,5 Experiments and Analysis,[0],[0]
"Firstly, we pre-train the Memory-toContext/Memory-to-Output models, setting their readings from the source and target memories to
4We do not split words into subwords using BPE (Sennrich et al., 2016) as that increases sentence lengths resulting in removing long documents due to GPU memory limitations, which would heavily reduce the amount of data that we have.
",5 Experiments and Analysis,[0],[0]
the zero vector.,5 Experiments and Analysis,[0],[0]
"This effectively learns parameters associated with the underlying sentence-based NMT model, which is then used as initialisation when training all parameters in the second stage (including the ones from the first stage).",5 Experiments and Analysis,[0],[0]
"For the first stage, we make use of stochastic gradient descent (SGD)5 with initial learning rate of 0.1 and a decay factor of 0.5 after the fourth epoch for a total of ten epochs.",5 Experiments and Analysis,[0],[0]
The convergence occurs in 6-8 epochs.,5 Experiments and Analysis,[0],[0]
"For the second stage, we use SGD with an initial learning rate of 0.08 and a decay factor of 0.9 after the first epoch for a total of 15 epochs6.",5 Experiments and Analysis,[0],[0]
The best model is picked based on the dev-set perplexity.,5 Experiments and Analysis,[0],[0]
"To avoid overfitting, we employ dropout with the rate 0.2 for the single memory model.",5 Experiments and Analysis,[0],[0]
"For the dual memory model, we set dropout for Document RNN to 0.2 and for the encoder and decoder to 0.5.",5 Experiments and Analysis,[0],[0]
Mini-batching is used in both stages to speed up training.,5 Experiments and Analysis,[0],[0]
"For the largest dataset, the document NMT model takes about 4.5 hours per epoch to train on a single P100 GPU, while the sentence-level model takes about 3 hours per epoch for the same settings.
",5 Experiments and Analysis,[0],[0]
"When training the document NMT model in the second stage, we need the target memory.",5 Experiments and Analysis,[0],[0]
One option would be to use the ground truth translations for building the memory.,5 Experiments and Analysis,[0],[0]
"However, this may result in inferior training, since at the test time, the decoder iteratively updates the translation of sentences based on the noisy translations of other sentences (accessed via the target memory).",5 Experiments and Analysis,[0],[0]
"Hence, while training the document NMT model, we construct the target memory from the translations generated by the pre-trained sentence-level model7.",5 Experiments and Analysis,[0],[0]
"This effectively exposes the model to its potential test-time mistakes during the training time, resulting in more robust learned parameters.",5 Experiments and Analysis,[0],[0]
"We have three variants of our model, using: (i) only the source memory (S-NMT+src mem), (ii) only the target memory (S-NMT+trg mem), or
5In our initial experiments, we found SGD to be more effective than Adam/Adagrad; an observation also made by Bahar et al. (2017).
",5.1 Main Results,[0],[0]
"6For the document NMT model training, we did some preliminary experiments using different learning rates and used the scheme which converged to the best perplexity in the least number of epochs while for sentence-level training we follow Cohn et al. (2016).
",5.1 Main Results,[0],[0]
"7We report results for two-pass decoding, i.e., we only update the translations once using the initial translations generated from the base model.",5.1 Main Results,[0],[0]
"We tried multiple passes of decoding at test-time but it was not helpful.
",5.1 Main Results,[0],[0]
(iii) both the source and target memories (SNMT+both mems).,5.1 Main Results,[0],[0]
We compare these variants against the standard sentence-level NMT model (S-NMT).,5.1 Main Results,[0],[0]
"We also compare the source memory variants of our model to the local context-NMT models8 of Jean et al. (2017) and Wang et al. (2017), which use a few previous source sentences as context, added to the decoder hidden state (similar to our Memory-to-Context model).
",5.1 Main Results,[0],[0]
Memory-to-Context We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2).,5.1 Main Results,[0],[0]
"Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs.
",5.1 Main Results,[0],[0]
We further experiment to train the target memory variants using gold translations instead of the generated ones for German-English.,5.1 Main Results,[0],[0]
"This led to −0.16 and −0.25 decrease9 in the BLEU scores for the target-only and both-memory variants, which confirms the intuition of constructing the target memory by exposing the model to its noises during training time.
",5.1 Main Results,[0],[0]
"Memory-to-Output From Table 2, we consistently see +.95/+1.00",5.1 Main Results,[0],[0]
"BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan-
8We implemented and trained the baseline local context models using the same hyperparameters and training procedure that we used for training our memory models.
",5.1 Main Results,[0],[0]
9Latter is statistically significant decrease w.r.t.,5.1 Main Results,[0],[0]
"the both memory model trained on generated target translations.
guage pairs.",5.1 Main Results,[0],[0]
"For French→English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best.",5.1 Main Results,[0],[0]
"For German→English, the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best.",5.1 Main Results,[0],[0]
"Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts.",5.1 Main Results,[0],[0]
"We attribute this to the large number of parameters in the latter architecture (Table 3) and limited amount of data.
",5.1 Main Results,[0],[0]
"We further experiment with more data for train-
BLEU-1 Fr→En De→En Et→En
NC-11NC-16
Jean et al. (2017) 52.8 30.6 39.2 51.9 Wang et al. (2017) 52.6 28.2 38.3 52.3 S-NMT 51.4 28.7 36.9 50.4
+src mem 53.0 30.5 39.1 52.6 +both mems 53.5 33.1 41.3 53.2
Table 5: Unigram BLEU for our Memory-to-Context Document NMT models vs. S-NMT and Source context NMT baselines.",5.1 Main Results,[0],[0]
bold:,5.1 Main Results,[0],[0]
"Best performance.
",5.1 Main Results,[0],[0]
ing the sentence-based NMT to investigate the extent to which document context is useful in this setting.,5.1 Main Results,[0],[0]
We randomly choose an additional 300K German-English sentence pairs from WMT’14 data to train the base NMT model in stage 1.,5.1 Main Results,[0],[0]
"In stage 2, we use the same document corpus as before to train the document-level models.",5.1 Main Results,[0],[0]
"As seen from Figure 3, the document MT variants still benefit from the document context even when the base model is trained on a larger bilingual corpus.",5.1 Main Results,[0],[0]
"For the Memory-to-Context model, we see massive improvements of +0.72 and +1.44 METEOR scores for the source memory and dual memory model respectively, when compared to the baseline.",5.1 Main Results,[0],[0]
"On the other hand, for the Memory-to-Output model, the target memory model’s METEOR score increases significantly by +1.09 compared to the baseline, slightly differing from the corresponding model using the smaller corpus (+1.2).
",5.1 Main Results,[0],[0]
"Local Source Context Models Table 4 shows comparison of our Memory-to-Context model variants to local source context-NMT models (Jean et al., 2017; Wang et al., 2017).",5.1 Main Results,[0],[0]
"For French→English, our source memory model is comparable to both baselines.",5.1 Main Results,[0],[0]
"For German→English, our S-NMT+src mem model is comparable to Jean et al. (2017) but outperforms Wang et al. (2017) for one test set according to BLEU, and for both test sets according to METEOR.",5.1 Main Results,[0],[0]
"For Estonian→English, our model outperforms Jean et al. (2017).",5.1 Main Results,[0],[0]
"Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned).",5.1 Main Results,[0],[0]
"However, the other two context baselines have access to that information, yet our
model’s performance is either better or quite close to those models.",5.1 Main Results,[0],[0]
We also look into the unigram BLEU scores to see how much our global source memory variants lead to improvement at the word-level.,5.1 Main Results,[0],[0]
"From Table 5, it can be seen that our model’s performance is better than the baselines for majority of the cases.",5.1 Main Results,[0],[0]
"The S-NMT+both mems model gives the best results for all three language pairs, showing that leveraging both source and target document context is indeed beneficial for improving MT performance.",5.1 Main Results,[0],[0]
Using Global/Local Target Context We first investigate whether using a local target context would have been equally sufficient in comparison to our global target memory model for the three datasets.,5.2 Analysis,[0],[0]
We condition the decoder on the previous target sentence representation (obtained from the last hidden state of the decoder) by adding it as an additional input to all decoder states (PrevTrg) similar to our Memory-to-Context model.,5.2 Analysis,[0],[0]
"From Table 6, we observe that for French→English and Estonian→English, using all sentences in the target context or just the previous target sentence gives comparable results.",5.2 Analysis,[0],[0]
"We may attribute this to these specific datasets, that is documents from TED talks or European Parliament Proceedings may depend more on the local than on the global context.",5.2 Analysis,[0],[0]
"However, for German→English (NC-11), the target memory model performs the best show-
ing that for documents with richer context (e.g. news articles) we do need the global target document context to improve MT performance.
",5.2 Analysis,[0],[0]
"Output Analysis To better understand the dual memory model, we look at the first sentence example in Table 7.",5.2 Analysis,[0],[0]
It can be seen that the source sentence has the noun “Qimonda” but the sentencelevel NMT model fails to attend to it when generating the translation.,5.2 Analysis,[0],[0]
"On the other hand, the single memory models are better in delivering some, if not all, of the underlying information in the source sentence but the dual memory model’s translation quality surpasses them.",5.2 Analysis,[0],[0]
"This is because the word “Qimonda” was being repeated in this specific document, providing a strong contextual signal to our global document context model while the local context model by Wang et al. (2017) is still unable to correctly translate the noun even when it has access to the word-level information of previous sentences.
",5.2 Analysis,[0],[0]
We resort to manual evaluation as there is no standard metric which evaluates document-level discourse information like consistency or pronominal anaphora.,5.2 Analysis,[0],[0]
"By manual inspection, we observe that our models can identify nouns in the source sentence to resolve coreferent pronouns, as shown in the second example of Table 7.",5.2 Analysis,[0],[0]
"Here the topic of the sentence is “the country under the dictatorship of Lukashenko” and our target and dual memory models are able to generate the appropriate pronoun/determiner as well as accurately translate the word ‘diktatuur’, hence producing much better translation as compared to both baselines.",5.2 Analysis,[0],[0]
"Apart from these improvements, our models are better in improving the readability of sentences by generating more context appropriate grammatical structures such as verbs and adverbs.
",5.2 Analysis,[0],[0]
"Furthermore, to validate that our model improves the consistency of translations, we look at five documents (roughly 70 sentences) from the test set of Estonian-English, each of which had a word being repeated in the gold translation.",5.2 Analysis,[0],[0]
Our model is able to resolve the consistency in 22 out of 32 cases as compared to the sentencebased model which only accurately translates 16 of those.,5.2 Analysis,[0],[0]
"Following Wang et al. (2017), we also investigate the extent to which our model can correct errors made by the baseline system.",5.2 Analysis,[0],[0]
We randomly choose five documents from the test set.,5.2 Analysis,[0],[0]
"Out of the 20 words/phrases which were incorrectly translated by the sentence-based model, our
model corrects 85% of them while also generating 10% new errors.",5.2 Analysis,[0],[0]
"Document-level Statistical MT There have been a few SMT-based attempts to document MT, but they are either restrictive or do not lead to significant improvements.",6 Related Work,[0],[0]
Hardmeier and Federico (2010) identify links among words in the source document using a word-dependency model to improve translation of anaphoric pronouns.,6 Related Work,[0],[0]
Gong et al. (2011) make use of a cache-based system to save relevant information from the previously generated translations and use that to enhance document-level translation.,6 Related Work,[0],[0]
"Garcia et al. (2014) propose a two-pass approach to improve the translations already obtained by a sentencelevel model.
",6 Related Work,[0],[0]
"Docent is an SMT-based document-level decoder (Hardmeier et al., 2012, 2013), which tries to modify the initial translation generated by the Moses decoder (Koehn et al., 2007) through stochastic local search and hill-climbing.",6 Related Work,[0],[0]
Garcia et al. (2015) make use of neural-based continuous word representations to incorporate distributional semantics into Docent.,6 Related Work,[0],[0]
"In another work, Garcia et al. (2017) incorporate new word embedding features into Docent to improve the lexical consistency of translations.",6 Related Work,[0],[0]
"The proposed methods fail to yield improvements upon automatic evaluation.
",6 Related Work,[0],[0]
"Larger Context Neural MT Jean et al. (2017)
extend the vanilla attention-based neural MT model (Bahdanau et al., 2015) by conditioning the decoder on the previous sentence via attention over its words.",6 Related Work,[0],[0]
Extending their model to consider the global source document context would be challenging due to the large size of computation graph over all the words in the source document.,6 Related Work,[0],[0]
"Wang et al. (2017) employ a 2-level hierarichal RNN to summarise three previous source sentences, which is then used as an additional input to the decoder hidden state.",6 Related Work,[0],[0]
Bawden et al. (2017) use multi-encoder NMT models to exploit context from the previous source and target sentence.,6 Related Work,[0],[0]
They highlight the importance of targetside context but report deteriorated BLEU scores when using it.,6 Related Work,[0],[0]
All these works consider a very local source/target context and completely ignore the global source and target document contexts.,6 Related Work,[0],[0]
We have proposed a document-level neural MT model that captures global source and target document context.,7 Conclusion,[0],[0]
Our model augments the vanilla sentence-based NMT model with external memories to incorporate documental interdependencies on both source and target sides.,7 Conclusion,[0],[0]
We show statistically significant improvements of the translation quality on three language pairs.,7 Conclusion,[0],[0]
"For future work, we intend to investigate models which incorporate specific discourse-level phenomena.",7 Conclusion,[0],[0]
The authors are grateful to André,Acknowledgments,[0],[0]
Martins and the anonymous reviewers for their helpful comments and corrections.,Acknowledgments,[0],[0]
"This work was supported by the Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) (www. massive.org.au), and partially supported by a Google Faculty Award to GH and the Australian Research Council through DP160102686.",Acknowledgments,[0],[0]
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks.,abstractText,[0],[0]
"We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document.",abstractText,[0],[0]
"The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies.",abstractText,[0],[0]
"We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent.",abstractText,[0],[0]
"Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",abstractText,[0],[0]
Document Context Neural Machine Translation with Memory Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 414–419 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
414",text,[0],[0]
Event Detection (ED) is an important subtask of event extraction.,1 Introduction,[0],[0]
It extracts event triggers from individual sentences and further identifies the type of the corresponding events.,1 Introduction,[0],[0]
"For instance, according to the ACE-2005 annotation guideline, in the sentence “Jane and John are married”, an ED system should be able to identify the word “married” as a trigger of the event “Marry”.",1 Introduction,[0],[0]
"However, it may be difficult to identify events from isolated sentences, because the same event trigger might represent different event types in different contexts.
",1 Introduction,[0],[0]
"Existing ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g., (McClosky et al., 2011; Hong et al., 2011; Li et al., 2014)) and representation-based methods (e.g., (Nguyen and Grishman, 2015; Chen et al.,
2015; Liu et al., 2016a; Chen et al., 2017)).",1 Introduction,[0],[0]
"The former mainly rely on a set of hand-designed features, while the latter employ distributed representation to capture meaningful semantic information.",1 Introduction,[0],[0]
"In general, most of these existing methods mainly exploit sentence-level contextual information.",1 Introduction,[0],[0]
"However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document.",1 Introduction,[0],[0]
"For example, there are the following sentences in ACE-2005:
...",1 Introduction,[0],[0]
I knew it was time to leave.,1 Introduction,[0],[0]
Isn’t that a great argument for term limits? ...,1 Introduction,[0],[0]
"If we only examine the first sentence, it is hard to determine whether the trigger “leave” indicates a “Transport” event meaning that he wants to leave the current place, or an “End-Position” event indicating that he will stop working for his current organization.",1 Introduction,[0],[0]
"However, if we can capture the contextual information of this sentence, it is more confident for us to label “leave” as the trigger of an “End-Position” event.",1 Introduction,[0],[0]
"Upon such observation, there have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED.",1 Introduction,[0],[0]
"However, they suffer from two major limitations.",1 Introduction,[0],[0]
"First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible.",1 Introduction,[0],[0]
"Besides, a representation-based study has been presented in (Duan et al., 2017), which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier.",1 Introduction,[0],[0]
"However, as being limited by the unsupervised training
process, the document-level representation cannot specifically capture event-related information.
",1 Introduction,[0],[0]
"In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level.",1 Introduction,[0],[0]
"This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.",1 Introduction,[0],[0]
It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences.,1 Introduction,[0],[0]
This learning process is guided by a general loss function where the loss corresponding to attention at both word and sentence levels and that of event type identification are integrated.,1 Introduction,[0],[0]
"It should be mentioned that although the attention mechanism has recently been applied effectively in various tasks, including machine translation (Zhang et al., 2017), question answering (Hao et al., 2017), document summarization (Tan et al., 2017), etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED oriented embeddings of documents.
",1 Introduction,[0],[0]
"We evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and systematically investigate the impacts of different supervised attention strategies on its performance.",1 Introduction,[0],[0]
"Experimental results show that the DEEBRNN model outperforms both feature-based and
representation-based state-of-the-art methods in terms of recall and F1-measure.",1 Introduction,[0],[0]
We formalize ED as a multi-class classification problem.,2 The Proposed Model,[0],[0]
"Given a sentence, we treat every word in it as a trigger candidate, and classify each candidate to a certain event type.",2 The Proposed Model,[0],[0]
"In the ACE-2005 dataset, there are 8 event types, further being divided into 33 subtypes, and a “Not Applicable (NA)” type.",2 The Proposed Model,[0],[0]
"Without loss of generality, in this paper we regard the 33 subtypes as 33 event types.",2 The Proposed Model,[0],[0]
"Figure 1 presents the schematic diagram of the proposed DEEB-RNN model, which contains two main modules:
1.",2 The Proposed Model,[0],[0]
"The ED Oriented Document Embedding Learning (EDODEL) module, which learns the distributed representations of documents from both word and sentence levels via the well-designed hierarchical and supervised attention mechanism.
2.",2 The Proposed Model,[0],[0]
"The Document-level Enhanced Event Detector (DEED) module, which tags each trigger candidate with an event type based on the learned embedding of documents.",2 The Proposed Model,[0],[0]
"To learn the ED oriented embedding of a document, we apply the hierarchical and supervised attention network presented in Figure 1, which consists of a word-level Bi-GRU (Schuster and Paliwal, 2002) encoder with attention on event triggers
and a sentence-level Bi-GRU encoder with attention on sentences with events.",2.1 The EDODEL Module,[0],[0]
"Given a document with L sentences, DEEB-RNN learns its embedding for detecting events in all sentences.
",2.1 The EDODEL Module,[0],[0]
"Word-level embeddings Given a sentence si (i = 1, 2, ..., L) consisting of words {wit|t = 1, 2, ..., T}.",2.1 The EDODEL Module,[0],[0]
"For each word wit, we first concatenate its embedding wit and its entity type embedding1 eit (Nguyen and Grishman, 2015) as the input git of a Bi-GRU and thus obtain the bidirectional hidden state hit:
hit = [ −−−−→ GRUw(git), ←−−−− GRUw(git)].",2.1 The EDODEL Module,[0],[0]
"(1)
We then feed hit to a perceptron with no bias to get uit = tanh(Wwhit) as a hidden representation of hit and also obtain an attention weight αit = u T itcw, which should be normalized through a softmax function.",2.1 The EDODEL Module,[0],[0]
"Here, similar to that in (Yang et al., 2016), cw is a vector representing the wordlevel context of wit, which is initialized at random.",2.1 The EDODEL Module,[0],[0]
"Finally, the embedding of the sentence si can be obtained by summing up hit with their weights:
si = T∑ t=1 αithit.",2.1 The EDODEL Module,[0],[0]
"(2)
To pay more attention to trigger words than other words, we construct the gold word-level attention signals α∗i for the sentence si, as illustrated in Figure 2a.",2.1 The EDODEL Module,[0],[0]
"We can then take the square error as the general loss of the attention at word level to supervise the learning process:
Ew(α ∗,α) = L∑ i=1",2.1 The EDODEL Module,[0],[0]
T∑ t=1 (α∗it − αit)2.,2.1 The EDODEL Module,[0],[0]
"(3)
1The words in the ACE-2005 dataset are annotated with their entity types (annotated as “NA” if they are not an entity).
",2.1 The EDODEL Module,[0],[0]
"Sentence-level embeddings Given the sentence embeddings {si|i = 1, 2, ..., L}, we first get the hidden state qi via a Bi-GRU:
qi = [ −−−→ GRUs(si), ←−−− GRUs(si)].",2.1 The EDODEL Module,[0],[0]
"(4)
Then we feed qi to a perceptron with no bias to get the hidden representation ti = tanh(Wsqi) and also obtain an attention weight βi = tTi cs to be normalized via softmax.",2.1 The EDODEL Module,[0],[0]
"Similarly, cs represents the sentence-level context of si to be randomly initialized.",2.1 The EDODEL Module,[0],[0]
"We eventually obtain the document embedding d as:
d = L∑ i=1 βisi.",2.1 The EDODEL Module,[0],[0]
"(5)
We also think that the sentences containing event should obtain more attention than other ones.",2.1 The EDODEL Module,[0],[0]
"Therefore, similar to the case at word level, we construct the gold sentence-level attention signals β∗ for the document d, as illustrated in Figure 2b, and further take the square error as the general loss of the attention at sentence level to supervise the learning process:
Es(β ∗,β) = L∑ i=1",2.1 The EDODEL Module,[0],[0]
(β∗i − βi)2.,2.1 The EDODEL Module,[0],[0]
(6),2.1 The EDODEL Module,[0],[0]
"We employ another Bi-GRU encoder and a softmax output layer to model the ED task, which can handle event triggers with multiple words.",2.2 The DEED Module,[0],[0]
"Specifically, given a sentence sj (j = 1, 2, ..., L) in document d, for each of its word wjt (t = 1, 2, ..., T ), we concatenate its word embedding wjt and entity type embedding ejt with the corresponding document embedding d as the input rjt of the Bi-GRU and thus obtain the hidden state fjt:
fjt = [ −−−→ GRUe(rjt), ←−−− GRUe(rjt)].",2.2 The DEED Module,[0],[0]
"(7)
Finally, we get the probability vector ojt with K dimensions through a softmax layer for wjt, where the k-th element, o(k)jt , of ojt indicates the probability of classifying wjt to the k-th event type.",2.2 The DEED Module,[0],[0]
"The loss function, J(y,o), can thus be defined in terms of the cross-entropy error of the real event type yjt and the predicted probability o(k)jt as follows:
J(y,o) =",2.2 The DEED Module,[0],[0]
"− L∑
j=1 T∑ t=1 K∑ k=1",2.2 The DEED Module,[0],[0]
"I(yjt = k)log o (k) jt , (8)
where I(·) is the indicator function.",2.2 The DEED Module,[0],[0]
"In the DEEB-RNN model, the above two modules are jointly trained.",2.3 Joint Training of the DEEB-RNN model,[0],[0]
"For this purpose, we define the joint loss function in the training process upon the losses specified for different modules as follows:
J(θ)= ∑ ∀d∈ϕ (J(y,o)+λEw(α ∗,α)+µEs(β ∗,β)), (9) where θ denotes, as a whole, the parameters used in DEEB-RNN, ϕ is the training document set, and λ and µ are hyper-parameters for striking a balance among J(y,o), Ew(α∗,α) and Es(β∗,β).",2.3 Joint Training of the DEEB-RNN model,[0],[0]
We validate the proposed model through comparison with state-of-the-art methods on the ACE2005 dataset.,3.1 Datasets and Settings,[0],[0]
"In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents.",3.1 Datasets and Settings,[0],[0]
"All the data preprocessing and evaluation criteria follow those in (Ghaeini et al., 2016).
",3.1 Datasets and Settings,[0],[0]
Hyper-parameters are tuned on the validation set.,3.1 Datasets and Settings,[0],[0]
"We set the dimension of the hidden layers corresponding to GRUw, GRUs, and GRUe to 300, 200, and 300, respectively, the output size of Ww and Ws to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5.",3.1 Datasets and Settings,[0],[0]
"In addition, we utilize the pre-trained word embeddings with 300 dimensions from (Mikolov et al., 2013) for initialization.",3.1 Datasets and Settings,[0],[0]
"For entity types, their embeddings are randomly initialized.",3.1 Datasets and Settings,[0],[0]
"We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout (Krizhevsky et al., 2012) for regularization.",3.1 Datasets and Settings,[0],[0]
"In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the baselines.
",3.2 Baseline Models,[0],[0]
"Sentence-level is a feature-based model proposed in (Hong et al., 2011), which regards entitytype consistency as a key feature to predict event mentions.
",3.2 Baseline Models,[0],[0]
"Joint Local is a feature-based model developed in (Li et al., 2013), which incorporates such features that explicitly capture the dependency among multiple triggers and arguments.
",3.2 Baseline Models,[0],[0]
"JRNN is a representation-based model proposed in (Nguyen et al., 2016), which exploits the inter-dependency between event triggers and argument roles via discrete structures.
",3.2 Baseline Models,[0],[0]
"Skip-CNN is a representation-based model presented in (Nguyen and Grishman, 2016), which proposes a novel convolution to exploit nonconsecutive k-grams for event detection.
",3.2 Baseline Models,[0],[0]
"ANN-S2 is a representation-based model developed in (Liu et al., 2017), which explicitly exploits argument information for event detection via supervised attention mechanisms.
",3.2 Baseline Models,[0],[0]
"Cross-event is a feature-based model proposed in (Liao and Grishman, 2010), which learns relations among event types from training corpus and futher helps predict the occurrence of events.
",3.2 Baseline Models,[0],[0]
"PSL is a feature-based model developed in (Liu et al., 2016b), which encods global information such as event-event association in the form of logic using the probabilistic soft logic model.
",3.2 Baseline Models,[0],[0]
"DLRNN is a representation-based model proposed in (Duan et al., 2017), which automatically extracts cross-sentence clues to improve sentencelevel event detection.",3.2 Baseline Models,[0],[0]
"In this section, we conduct experiments on the ACE-2005 dataset to demonstrate the effectiveness of different attention strategies.
",3.3 Impacts of Different Attention Strategies,[0],[0]
"Bi-GRU is the basic ED model, which does not employ document-level embeddings.
",3.3 Impacts of Different Attention Strategies,[0],[0]
"DEEB-RNN uses the document embeddings and computes attentions without supervision, in which hyper-parameters λ and µ are set to 0.
DEEB-RNN1/2/3 means they uses the gold attention signals as supervision information.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Specifically, DEEB-RNN1 uses only the gold word-level attention signal (λ = 1 and µ = 0), DEEB-RNN2 uses only the gold sentence-level attention signal (λ = 0 and µ = 1), whilst DEEB-RNN3 employs the gold attention signals at both word and sen-
tence levels (λ = 1 and µ = 1).",3.3 Impacts of Different Attention Strategies,[0],[0]
"Table 1 compares these methods, where we can observe that the methods with document embeddings (i.e., the last four) significantly outperform the pure Bi-GRU method, which suggests that document-level information is very beneficial for ED.",3.3 Impacts of Different Attention Strategies,[0],[0]
"An interesting phenomenon is that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance.",3.3 Impacts of Different Attention Strategies,[0],[0]
This is because of the following reasons.,3.3 Impacts of Different Attention Strategies,[0],[0]
"On one hand, as compared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers.",3.3 Impacts of Different Attention Strategies,[0],[0]
"In this way, the BiRNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers.",3.3 Impacts of Different Attention Strategies,[0],[0]
That means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers.,3.3 Impacts of Different Attention Strategies,[0],[0]
"Therefore, as compared to DEEB-RNN, the precision of DEEBRNN2 is improved; On the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Therefore, those sentences with such event triggers cannot be detected.",3.3 Impacts of Different Attention Strategies,[0],[0]
"The recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Moreover, DEEB-RNN3 shows the best performance, indicating that the gold attention signals at both word and sentence levels are useful for ED.",3.3 Impacts of Different Attention Strategies,[0],[0]
Table 2 presents the overall performance of all methods on ACE-2005.,3.4 Performance Comparison,[0],[0]
"We can see that different versions of DEEB-RNN consistently out-
perform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others.",3.4 Performance Comparison,[0],[0]
"The better performance of DEEB-RNN can be explained by the following reasons: (1) Compared with feature-based methods, including Sentencelevel, Joint Local, and representation-based methods, including JRNN, Skip-CNN and ANN-S2, our method exploits document-level information (i.e., the ED oriented document embeddings) from both word and sentence levels in a document by the supervised attention mechanism, which enhance the ability of identifying trigger words; (2) Compared with feature-based methods using document-level information, such as Cross-event, PSL, our method can automatically capture event types in documents via a end-to-end Bi-RNN based model without manually designed rules; (3) Compared with representation-based methods using document-level information, such as DLRNN, our method can learn event detection oriented embeddings of documents through the hierarchical and supervised attention based Bi-RNN network.",3.4 Performance Comparison,[0],[0]
"In this study, we proposed a hierarchical and supervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection.",4 Conclusions and Future Work,[0],[0]
We explored different strategies to construct gold word- and sentence-level attentions to focus on event information.,4 Conclusions and Future Work,[0],[0]
Experiments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as compared to the state-of-the-art methods in terms of both recall and F1-measure.,4 Conclusions and Future Work,[0],[0]
"In this paper, we can strike a balance between sentence and document embeddings by adjusting their dimensions.",4 Conclusions and Future Work,[0],[0]
"In the future, we may improve the DEEB-RNN model to automatically determine the weights of sentence and document embeddings.",4 Conclusions and Future Work,[0],[0]
"This work is supported by National Key Research and Development Program of China under grants 2016YFB1000902 and 2017YFC0820404, and National Natural Science Foundation of China under grants 61772501, 61572473, 61572469, and 91646120.",Acknowledgments,[0],[0]
"We are grateful to Dr. Liu Kang of the Institute of Automation, Chinese Academy of Sciences for very helpful discussion on event detection.",Acknowledgments,[0],[0]
Document-level information is very important for event detection even at sentence level.,abstractText,[0],[0]
"In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences.",abstractText,[0],[0]
"This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.",abstractText,[0],[0]
It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences.,abstractText,[0],[0]
"Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",abstractText,[0],[0]
Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2020",text,[0],[0]
Recurrent neural networks have become one of the most widely used models in natural language processing (NLP).,1 Introduction,[0],[0]
"A number of variants of RNNs such as Long Short-Term Memory networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit networks (GRU; Cho et al., 2014) have been designed to model text capturing long-term dependencies in problems such as language modeling.",1 Introduction,[0],[0]
"However, document modeling, a key to many natural language
∗The first three authors made equal contributions to this paper.",1 Introduction,[0],[0]
"The work was done when the second author was visiting Edinburgh.
",1 Introduction,[0],[0]
"1Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information.
understanding tasks, is still an open challenge.",1 Introduction,[0],[0]
"Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016).",1 Introduction,[0],[0]
"Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity.",1 Introduction,[0],[0]
"Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level.
",1 Introduction,[0],[0]
"It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017).",1 Introduction,[0],[0]
"In this paper, we formalize the use of external information to further guide document modeling for end goals.
",1 Introduction,[0],[0]
We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.”,1 Introduction,[0],[0]
Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor.,1 Introduction,[0],[0]
Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words.,1 Introduction,[0],[0]
"Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document.",1 Introduction,[0],[0]
"Our model explicitly biases the extractor with external cues and
implicitly biases the encoder through training.
",1 Introduction,[0],[0]
We demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information.,1 Introduction,[0],[0]
"These two problems, extractive document summarization and answer selection for machine reading comprehension, both require local and global contextual reasoning about a given document.",1 Introduction,[0],[0]
"Extractive document summarization systems aim at creating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems select the candidate sentence in a document most likely to contain the answer to a query.",1 Introduction,[0],[0]
"For document summarization, we exploit the title and image captions which often appear with documents (specifically newswire articles) as external information.",1 Introduction,[0],[0]
"For answer selection, we use word overlap features, such as the inverse sentence frequency (ISF, Trischler et al., 2016) and the inverse document frequency (IDF) together with the query, all formulated as external cues.
",1 Introduction,[0],[0]
"Our main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.
",1 Introduction,[0],[0]
"Second, while external information has been shown to be useful for summarization systems using traditional hand-crafted features (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001), our model is the first to exploit such information in deep learning-based summarization.",1 Introduction,[0],[0]
"We evaluate our models automatically (in terms of ROUGE scores) on the CNN news highlights dataset (Hermann et al., 2015).",1 Introduction,[0],[0]
"Experimental results show that our summarizer, informed with title and image captions, consistently outperforms summarizers that do not use this information.",1 Introduction,[0],[0]
We also conduct a human evaluation to judge which type of summary participants prefer.,1 Introduction,[0],[0]
"Our results overwhelmingly show that human subjects find our summaries more informative and complete.
",1 Introduction,[0],[0]
"Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often
measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).",1 Introduction,[0],[0]
Our model with ISF and IDF scores as external features achieves competitive results for answer selection.,1 Introduction,[0],[0]
"Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other.",1 Introduction,[0],[0]
"Given a document D consisting of a sequence of n sentences (s1, s2, ..., sn) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise.",2 Document Modeling For Sentence Extraction,[0],[0]
"Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017).",2 Document Modeling For Sentence Extraction,[0],[0]
"The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below.",2 Document Modeling For Sentence Extraction,[0],[0]
"The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues.
",2 Document Modeling For Sentence Extraction,[0],[0]
"Sentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations.",2 Document Modeling For Sentence Extraction,[0],[0]
We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature.,2 Document Modeling For Sentence Extraction,[0],[0]
This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length.,2 Document Modeling For Sentence Extraction,[0],[0]
We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a sentence.,2 Document Modeling For Sentence Extraction,[0],[0]
"In Figure 1, ker-
nels of size 2 (red) and 4 (blue) are applied three times each.",2 Document Modeling For Sentence Extraction,[0],[0]
The max-pooling over time operation yields two feature lists fK2 and fK4 ∈ R3.,2 Document Modeling For Sentence Extraction,[0],[0]
"The final sentence embeddings have six dimensions.
",2 Document Modeling For Sentence Extraction,[0],[0]
Document Encoder,2 Document Modeling For Sentence Extraction,[0],[0]
The document encoder composes a sequence of sentences to obtain a document representation.,2 Document Modeling For Sentence Extraction,[0],[0]
"We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997).",2 Document Modeling For Sentence Extraction,[0],[0]
"Given a document D consisting of a sequence of sentences (s1, s2, . . .",2 Document Modeling For Sentence Extraction,[0],[0]
", sn), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).
",2 Document Modeling For Sentence Extraction,[0],[0]
Sentence Extractor,2 Document Modeling For Sentence Extraction,[0],[0]
Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues.,2 Document Modeling For Sentence Extraction,[0],[0]
"It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer.",2 Document Modeling For Sentence Extraction,[0],[0]
Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder).,2 Document Modeling For Sentence Extraction,[0],[0]
"Instead, our extractor attends to a sequence of p pieces of external information E : (e1, e2, ..., ep) relevant for the task (e.g., ei is a title or an image caption for summarization) for cues.",2 Document Modeling For Sentence Extraction,[0],[0]
"At time ti, it reads sentence si and makes a binary prediction, conditioned on the document representation (obtained from the document encoder), the previously labeled sentences and the external information.",2 Document Modeling For Sentence Extraction,[0],[0]
"This way, our labeler is able to identify locally and globally important sentences within the document which correlate well with the external information.
",2 Document Modeling For Sentence Extraction,[0],[0]
"Given sentence st at time step t, it returns a probability distribution over labels as:
p(yt|st, D,E) = softmax(g(ht, h′t))",2 Document Modeling For Sentence Extraction,[0],[0]
"(1) g(ht, h ′ t) = Uo(Vhht",2 Document Modeling For Sentence Extraction,[0],[0]
+W ′ hh ′,2 Document Modeling For Sentence Extraction,[0],[0]
"t) (2)
ht = LSTM(st, ht−1)
",2 Document Modeling For Sentence Extraction,[0],[0]
h′t = p∑ i=1,2 Document Modeling For Sentence Extraction,[0],[0]
"α(t,i)ei,
where α(t,i) = exp(htei)∑",2 Document Modeling For Sentence Extraction,[0],[0]
"j exp(htej)
where g(·) is a single-layer neural network with parameters Uo, Vh and W ′h.",2 Document Modeling For Sentence Extraction,[0],[0]
"ht is an intermedi-
ate RNN state at time step t. The dynamic context vector h′t is essentially the weighted sum of the external information (e1, e2, . .",2 Document Modeling For Sentence Extraction,[0],[0]
.,2 Document Modeling For Sentence Extraction,[0],[0]
", ep).",2 Document Modeling For Sentence Extraction,[0],[0]
Figure 1 summarizes our model.,2 Document Modeling For Sentence Extraction,[0],[0]
We validate our model on two sentence extraction problems: extractive document summarization and answer selection for machine reading comprehension.,3 Sentence Extraction Applications,[0],[0]
Both these tasks require local and global contextual reasoning about a given document.,3 Sentence Extraction Applications,[0],[0]
"As such, they test the ability of our model to facilitate document modeling using external information.
",3 Sentence Extraction Applications,[0],[0]
Extractive Summarization,3 Sentence Extraction Applications,[0],[0]
An extractive summarizer aims to produce a summary S by selecting m sentences from D (where m < n).,3 Sentence Extraction Applications,[0],[0]
"In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si should be included in the summary) by assigning score p(yi|si,D ,E , θ) quantifying the relevance of si to the summary.",3 Sentence Extraction Applications,[0],[0]
"We assemble a summary S by selecting m sentences with top p(yi = 1|si,D ,E , θ) scores.
",3 Sentence Extraction Applications,[0],[0]
We formulate external information E as the sequence of the title and the image captions associated with the document.,3 Sentence Extraction Applications,[0],[0]
"We use the convolutional sentence encoder to get their sentence-level representations.
",3 Sentence Extraction Applications,[0],[0]
"Answer Selection Given a question q and a document D , the goal of the task is to select one candidate sentence si ∈ D in which the answer exists.",3 Sentence Extraction Applications,[0],[0]
"In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si contains the answer) and assign score p(yi|si,D ,E , θ) quantifying si’s relevance to the query.",3 Sentence Extraction Applications,[0],[0]
"We return as answer the sentence si with the highest p(yi = 1|si,D ,E , θ) score.
",3 Sentence Extraction Applications,[0],[0]
We treat the question q as external information and use the convolutional sentence encoder to get its sentence-level representation.,3 Sentence Extraction Applications,[0],[0]
This simplifies Eq.,3 Sentence Extraction Applications,[0],[0]
"(1) and (2) as follow:
p(yt|st, D, q) = softmax(g(ht, q)) (3) g(ht, q) = Uo(Vhht +Wqq),
where Vh and Wq are network parameters.",3 Sentence Extraction Applications,[0],[0]
"We exploit the simplicity of our model to further assimilate external features relevant for answer selection: the inverse sentence frequency (ISF, (Trischler et al., 2016)), the inverse document frequency (IDF) and a modified version of the ISF score which we call local ISF.",3 Sentence Extraction Applications,[0],[0]
"Trischler et al. (2016) have shown that a simple ISF baseline (i.e., a sentence with the highest ISF score as an answer) correlates well with the answers.",3 Sentence Extraction Applications,[0],[0]
"The ISF score αsi for the sentence si is computed as αsi =∑
w∈si∩q IDF(w), where IDF is the inverse document frequency score of word w, defined as: IDF(w) = log NNw , whereN is the total number of sentences in the training set and Nw is the number of sentences in which w appears.",3 Sentence Extraction Applications,[0],[0]
"Note that, si ∩ q
refers to the set of words that appear both in si and in q. Local ISF is calculated in the same manner as the ISF score, only with setting the total number of sentences (N ) to the number of sentences in the article that is being analyzed.
",3 Sentence Extraction Applications,[0],[0]
"More formally, this modifies Eq.",3 Sentence Extraction Applications,[0],[0]
"(3) as follows:
p(yt|st, D, q) = softmax(g(ht, q, αt, βt, γt)),(4)
where αt, βt and γt are the ISF, IDF and local ISF scores (real values) of sentence st respectively .",3 Sentence Extraction Applications,[0],[0]
"The function g is calculated as follows:
g(ht, q, αt, βt, γt) =Uo (Vhht+
Wqq +Wisf(αt · 1)+ Widf(βt · 1) +Wlisf(γt · 1) ) ,
where Wisf , Widf and Wlisf are new parameters added to the network and 1 is a vector of 1s of size equal to the sentence embedding size.",3 Sentence Extraction Applications,[0],[0]
"In Figure 1, these external feature vectors are represented as 6-dimensional gray vectors accompanied with dashed arrows.",3 Sentence Extraction Applications,[0],[0]
This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups.,4 Experiments and Results,[0],[0]
"In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation.",4 Experiments and Results,[0],[0]
"Summarization Dataset We evaluated our models on the CNN news highlights dataset (Hermann et al., 2015).2",4.1 Extractive Document Summarization,[0],[0]
"We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents).",4.1 Extractive Document Summarization,[0],[0]
"We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the
2Hermann et al. (2015) have also released the DailyMail dataset, but we do not report our results on this dataset.",4.1 Extractive Document Summarization,[0],[0]
We found that the script written by Hermann et al. to crawl DailyMail articles mistakenly extracts image captions as part of the main body of the document.,4.1 Extractive Document Summarization,[0],[0]
"As image captions often do not have sentence boundaries, they blend with the sentences of the document unnoticeably.",4.1 Extractive Document Summarization,[0],[0]
"This leads to the production of erroneous summaries.
",4.1 Extractive Document Summarization,[0],[0]
“story highlights” associated with each article are gold-standard abstractive summaries.,4.1 Extractive Document Summarization,[0],[0]
We trained our network on a named-entity-anonymized version of news articles.,4.1 Extractive Document Summarization,[0],[0]
"However, we generated deanonymized summaries and evaluated them against gold summaries to facilitate human evaluation and to make human evaluation comparable to automatic evaluation.
",4.1 Extractive Document Summarization,[0],[0]
"To train our model, we need documents annotated with sentence extraction information, i.e., each sentence in a document is labeled with 1 (summary-worthy) or 0 (not summary-worthy).",4.1 Extractive Document Summarization,[0],[0]
"We followed Nallapati et al. (2017) and automatically extracted ground truth labels such that all positively labeled sentences from an article collectively give the highest ROUGE (Lin and Hovy, 2003) score with respect to the gold summary.
",4.1 Extractive Document Summarization,[0],[0]
"We used a modified script of Hermann et al. (2015) to extract titles and image captions, and we associated them with the corresponding articles.",4.1 Extractive Document Summarization,[0],[0]
All articles get associated with their titles.,4.1 Extractive Document Summarization,[0],[0]
"The availability of image captions varies from 0 to 414 per article, with an average of 3 image captions.",4.1 Extractive Document Summarization,[0],[0]
"There are 40% CNN articles with at least one image caption.
",4.1 Extractive Document Summarization,[0],[0]
"All sentences, including titles and image captions, were padded with zeros to a sentence length of 100.",4.1 Extractive Document Summarization,[0],[0]
All input documents were padded with zeros to a maximum document length of 126.,4.1 Extractive Document Summarization,[0],[0]
"For each document, we consider a maximum of 10 image captions.",4.1 Extractive Document Summarization,[0],[0]
"We experimented with various numbers (1, 3, 5, 10 and 20) of image captions on the validation set and found that our model performed best with 10 image captions.",4.1 Extractive Document Summarization,[0],[0]
"We refer the reader to the supplementary material for more implementation details to replicate our results.
",4.1 Extractive Document Summarization,[0],[0]
Comparison Systems We compared the output of our model against the standard baseline of simply selecting the first three sentences from each document as the summary.,4.1 Extractive Document Summarization,[0],[0]
"We refer to this baseline as LEAD in the rest of the paper.
",4.1 Extractive Document Summarization,[0],[0]
We also compared our system against the sentence extraction system of Cheng and Lapata (2016).,4.1 Extractive Document Summarization,[0],[0]
"We refer to this system as POINTERNET as the neural attention architecture in Cheng and Lapata (2016) resembles the one of Pointer Networks (Vinyals et al., 2015).3 It does not exploit any external information.4 Cheng and Lap-
3The architecture of POINTERNET is closely related to our model without external information.
",4.1 Extractive Document Summarization,[0],[0]
"4Adding external information to POINTERNET is an in-
ata (2016) report only on the DailyMail dataset.",4.1 Extractive Document Summarization,[0],[0]
"We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5
Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6",4.1 Extractive Document Summarization,[0],[0]
"Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency.",4.1 Extractive Document Summarization,[0],[0]
"In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously.
",4.1 Extractive Document Summarization,[0],[0]
teresting direction of research,4.1 Extractive Document Summarization,[0],[0]
but we do not pursue it here.,4.1 Extractive Document Summarization,[0],[0]
"It requires decoding with multiple types of attentions and this is not the focus of this paper.
",4.1 Extractive Document Summarization,[0],[0]
5We are unable to compare our results to the extractive system of Nallapati et al. (2017) because they report their results on the DailyMail dataset and their code is not available.,4.1 Extractive Document Summarization,[0],[0]
"The abstractive systems of Chen et al. (2016) and Tan and Wan (2017) report their results on the CNN dataset, however, their results are not comparable to ours as they report on the full-length F1 variants of ROUGE to evaluate their abstractive summaries.",4.1 Extractive Document Summarization,[0],[0]
"We report ROUGE recall scores which is more appropriate to evaluate our extractive summaries.
",4.1 Extractive Document Summarization,[0],[0]
"6We used pyrouge, a Python package, to compute all our ROUGE scores with parameters “-a -c 95 -m -n 4 -w 1.2.”
",4.1 Extractive Document Summarization,[0],[0]
We report our results on both full length (three sentences with the top scores as the summary) and fixed length (first 75 bytes and 275 bytes as the summary) summaries.,4.1 Extractive Document Summarization,[0],[0]
"For full length summaries, our decision of selecting three sentences is guided by the fact that there are 3.11 sentences on average in the gold highlights of the training set.",4.1 Extractive Document Summarization,[0],[0]
"We conduct our ablation study on the validation set with full length ROUGE scores, but we report both fixed and full length ROUGE scores for the test set.
",4.1 Extractive Document Summarization,[0],[0]
We experimented with two types of external information: title (TITLE) and image captions (CAPTION).,4.1 Extractive Document Summarization,[0],[0]
"In addition, we experimented with the first sentence (FS) of the document as external information.",4.1 Extractive Document Summarization,[0],[0]
"Note that the latter is not external information, it is a sentence in the document.",4.1 Extractive Document Summarization,[0],[0]
"However, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries (Rush et al., 2015; Nallapati et al., 2016).",4.1 Extractive Document Summarization,[0],[0]
"XNET with FS acts as a baseline for XNET with title and image captions.
",4.1 Extractive Document Summarization,[0],[0]
We report the performance of several variants of XNET on the validation set in Table 1.,4.1 Extractive Document Summarization,[0],[0]
We also compare them against the LEAD baseline and POINTERNET.,4.1 Extractive Document Summarization,[0],[0]
These two systems do not use any additional information.,4.1 Extractive Document Summarization,[0],[0]
"Interestingly, all the variants of XNET significantly outperform LEAD and POINTERNET.",4.1 Extractive Document Summarization,[0],[0]
"When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external information.",4.1 Extractive Document Summarization,[0],[0]
"Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001).",4.1 Extractive Document Summarization,[0],[0]
The performance with TITLE and CAPTION is better than that with FS.,4.1 Extractive Document Summarization,[0],[0]
"We also tried possible combinations of TITLE, CAPTION and FS.",4.1 Extractive Document Summarization,[0],[0]
All XNET models are superior to the ones without any external information.,4.1 Extractive Document Summarization,[0],[0]
"XNET performs best when TITLE and CAPTION are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively).",4.1 Extractive Document Summarization,[0],[0]
"It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external information is useful to identify the gist of the document.",4.1 Extractive Document Summarization,[0],[0]
"We use this model for testing purposes.
",4.1 Extractive Document Summarization,[0],[0]
Our final results on the test set are shown in Table 2.,4.1 Extractive Document Summarization,[0],[0]
"It turns out that for smaller summaries (75 bytes) LEAD and POINTERNET are superior
to XNET.",4.1 Extractive Document Summarization,[0],[0]
"This result could be because LEAD (always) and POINTERNET (often) include the first sentence in their summaries, whereas, XNET is better capable at selecting sentences from various document positions.",4.1 Extractive Document Summarization,[0],[0]
"This is not captured by smaller summaries of 75 bytes, but it becomes more evident with longer summaries (275 bytes and full length) where XNET performs best across all ROUGE scores.",4.1 Extractive Document Summarization,[0],[0]
"We note that POINTERNET outperforms LEAD for 75-byte summaries, then its performance drops behind LEAD for 275-byte summaries, but then it outperforms LEAD for full length summaries on the metrics R1, R2 and RL.",4.1 Extractive Document Summarization,[0],[0]
"It shows that POINTERNET with its attention over sentences in the document is capable of exploring more than first few sentences in the document, but it is still behind XNET which is better at identifying salient sentences in the document.",4.1 Extractive Document Summarization,[0],[0]
"XNET performs significantly better than POINTERNET by 0.8 points for 275-byte summaries and by 1.9 points for full length summaries, on average for all ROUGE scores.
",4.1 Extractive Document Summarization,[0],[0]
Human Evaluation We complement our automatic evaluation results with human evaluation.,4.1 Extractive Document Summarization,[0],[0]
"We randomly selected 20 articles from the test set.
",4.1 Extractive Document Summarization,[0],[0]
Annotators were presented with a news article and summaries from four different systems.,4.1 Extractive Document Summarization,[0],[0]
"These include the LEAD baseline, POINTERNET, XNET and the human authored highlights.",4.1 Extractive Document Summarization,[0],[0]
"We followed the guidelines in Cheng and Lapata (2016), and asked our participants to rank the summaries from best (1st) to worst (4th) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?).",4.1 Extractive Document Summarization,[0],[0]
We did not allow any ties and we only sampled articles with nonidentical summaries.,4.1 Extractive Document Summarization,[0],[0]
We assigned this task to five annotators who were proficient English speakers.,4.1 Extractive Document Summarization,[0],[0]
Each annotator was presented with all 20 articles.,4.1 Extractive Document Summarization,[0],[0]
The order of summaries to rank was randomized per article.,4.1 Extractive Document Summarization,[0],[0]
"An example of summaries our subjects ranked is provided in the supplementary material.
",4.1 Extractive Document Summarization,[0],[0]
The results of our human evaluation study are shown in Table 3.,4.1 Extractive Document Summarization,[0],[0]
"As one might imagine, HUMAN gets ranked 1st most of the time (41%).",4.1 Extractive Document Summarization,[0],[0]
"However, it is closely followed by XNET which ranked 1st 28% of the time.",4.1 Extractive Document Summarization,[0],[0]
"In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places.",4.1 Extractive Document Summarization,[0],[0]
We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p < 0.01).,4.1 Extractive Document Summarization,[0],[0]
"It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN.",4.1 Extractive Document Summarization,[0],[0]
"On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN.",4.1 Extractive Document Summarization,[0],[0]
The human evaluation results corroborates our empirical results in Table 1 and Table 2: XNET is better than LEAD and POINTERNET in producing informative and fluent summaries.,4.1 Extractive Document Summarization,[0],[0]
"Question Answering Datasets We run experiments on four datasets collected for open domain question-answering tasks: WikiQA (Yang et al., 2015), SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMarco (Nguyen et al., 2016).
",4.2 Answer Selection,[0],[0]
NewsQA was especially designed to present lexical and syntactic divergence between questions and answers.,4.2 Answer Selection,[0],[0]
"It contains 119,633 questions posed by crowdworkers on 12,744 CNN articles previously collected by Hermann et al. (2015).",4.2 Answer Selection,[0],[0]
"In a similar manner, SQuAD associates 100,000+
question with a Wikipedia article’s first paragraph, for 500+ previously chosen articles.",4.2 Answer Selection,[0],[0]
WikiQA was collected by mining web-searching query logs and then associating them with the summary section of the Wikipedia article presumed to be related to the topic of the query.,4.2 Answer Selection,[0],[0]
"A similar collection procedure was followed to create MSMarco with the difference that each candidate answer is a whole paragraph from a different browsed website associated with the query.
",4.2 Answer Selection,[0],[0]
"We follow the widely used setup of leaving out unanswered questions (Trischler et al., 2016; Yang et al., 2015) and adapt the format of each dataset to our task of answer sentence selection by labeling a candidate sentence with 1 if any answer span is contained in that sentence.",4.2 Answer Selection,[0],[0]
"In the case of MSMarco, each candidate paragraph comes associated with a label, hence we treat each one as a single long sentence.",4.2 Answer Selection,[0],[0]
"Since SQuAD keeps the official test dataset hidden and MSMarco does not provide labels for its released test set, we report results on their official validation sets.",4.2 Answer Selection,[0],[0]
"For validation, we set apart 10% of each official training set.
",4.2 Answer Selection,[0],[0]
"Our dataset splits consist of 92,525, 5,165 and 5,124 samples for NewsQA; 79,032, 8,567, and 10,570 for SQuAD; 873, 122, and 237 for WikiQA; and 79,704, 9,706, and 9,650 for MSMarco, for training, validation, and testing respectively.
",4.2 Answer Selection,[0],[0]
"Comparison Systems We compared the output of our model against the ISF (Trischler et al., 2016) and LOCALISF baselines.",4.2 Answer Selection,[0],[0]
"Given an article, the sentence with the highest ISF score is selected as an answer for the ISF baseline and the sentence with the highest local ISF score for the LOCALISF baseline.",4.2 Answer Selection,[0],[0]
"We also compare our model against a neural network (PAIRCNN) that encodes (question, candidate) in an isolated manner as in previous work (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).",4.2 Answer Selection,[0],[0]
The architecture uses the sentence encoder explained in earlier sections to learn the question and candidate representations.,4.2 Answer Selection,[0],[0]
"The distribution over labels is given by p(yt|q) = p(yt|st, q) = softmax(g(st, q))",4.2 Answer Selection,[0],[0]
"where g(st, q) = ReLU(Wsq ·",4.2 Answer Selection,[0],[0]
[st; q] + bsq).,4.2 Answer Selection,[0],[0]
"In addition, we also compare our model against APCNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (Wang and Jiang, 2017), KVMemNN (Miller et al., 2016), and COMPAGGR, a state-of-the-art system by Wang et al. (2017).
",4.2 Answer Selection,[0],[0]
We experiment with several variants of our model.,4.2 Answer Selection,[0],[0]
"XNET is the vanilla version of our sen-
tence extractor conditioned only on the query q as external information (Eq.",4.2 Answer Selection,[0],[0]
(3)).,4.2 Answer Selection,[0],[0]
"XNET+ is an extension of XNET which uses ISF, IDF and local ISF scores in addition to the query q as external information (Eqn. (4)).",4.2 Answer Selection,[0],[0]
"We also experimented with a baseline XNETTOPK where we choose the top k sentences with highest ISF score, and then among them choose the one with the highest probability according to XNET.",4.2 Answer Selection,[0],[0]
"In our experiments, we set k = 5.",4.2 Answer Selection,[0],[0]
"In the end, we experimented with an ensemble network LRXNET which combines the XNET score, the COMPAGGR score and other word-overlap-based scores (tweaked and optimized for each dataset separately) for each sentence using a logistic regression classifier.",4.2 Answer Selection,[0],[0]
"It uses ISF and LocalISF scores for NewsQA, IDF and ISF scores for SQuAD, sentence length, IDF and ISF scores for WikiQA, and word overlap and ISF score for MSMarco.",4.2 Answer Selection,[0],[0]
"We refer the reader to the supplementary material for more implementation and optimization details to replicate our results.
",4.2 Answer Selection,[0],[0]
Evaluation Metrics,4.2 Answer Selection,[0],[0]
"We consider metrics that evaluate systems that return a ranked list of candidate answers: mean average precision (MAP), mean reciprocal rank (MRR), and accuracy (ACC).
",4.2 Answer Selection,[0],[0]
"Results Table 4 gives the results for the test sets of NewsQA and WikiQA, and the original validation sets of SQuAD and MSMarco.",4.2 Answer Selection,[0],[0]
"Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions,
instead of only observing each candidate in isolation.
",4.2 Answer Selection,[0],[0]
"Secondly, we can observe that ISF is indeed a strong baseline that outperforms XNET.",4.2 Answer Selection,[0],[0]
"This means that just “reading” the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering.",4.2 Answer Selection,[0],[0]
"Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR.",4.2 Answer Selection,[0],[0]
"Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets.
",4.2 Answer Selection,[0],[0]
This consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection.,4.2 Answer Selection,[0],[0]
"Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a classification technique.",4.2 Answer Selection,[0],[0]
"Using it as a hard constraint, with XNETTOPK, does not achieve the best result.",4.2 Answer Selection,[0],[0]
We believe that often the ISF score is a better indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself.,4.2 Answer Selection,[0],[0]
"As such, XNET+ is capable of using this feature in datasets with richer context.
",4.2 Answer Selection,[0],[0]
It is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern.,4.2 Answer Selection,[0],[0]
"For the SQuAD dataset, the results are comparable (less than 1%).",4.2 Answer Selection,[0],[0]
"However, the improvement for WikiQA reaches ∼3% and then the gap shrinks again for NewsQA, with an improvement of ∼1%.",4.2 Answer Selection,[0],[0]
"This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and
to an entire article for NewsQA.",4.2 Answer Selection,[0],[0]
"Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case expressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises.7
Interestingly, our model lags behind COMPAGGR on the MSMarco dataset.",4.2 Answer Selection,[0],[0]
"It turns out this is due to contextual independence between candidates in the MSMarco dataset, i.e., each candidate is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sentences from a document in the NewsQA, SQuAD and WikiQA datasets.",4.2 Answer Selection,[0],[0]
"As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly.",4.2 Answer Selection,[0],[0]
This can be observed by the fact that XNET and PAIRCNN obtain comparable results.,4.2 Answer Selection,[0],[0]
COMPAGGR performs better because comparing each candidate independently is a better strategy.,4.2 Answer Selection,[0],[0]
We describe an approach to model documents while incorporating external information that informs the representations learned for the sentences in the document.,5 Conclusion,[0],[0]
"We implement our approach through an attention mechanism of a neural network architecture for modeling documents.
",5 Conclusion,[0],[0]
"Our experiments with extractive document summarization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide document modeling for natural language understanding tasks.",5 Conclusion,[0],[0]
"Our model uses image captions and the title of the document for document summarization, and the query with word overlap features for answer selection and outperforms its counterparts that do not use this information.",5 Conclusion,[0],[0]
"Second, our external attention mechanism successfully guides the learning of the document representation for the relevant end goal.",5 Conclusion,[0],[0]
"For answer selection, we show that inserting the query with word overlap features using our external attention mechanism outperforms state-of-the-art systems that naturally also have access to this information.",5 Conclusion,[0],[0]
"We thank Jianpeng Cheng for providing us with the CNN dataset and the implementation of Point-
7See the supplementary material for an example supporting our hypothesis.
",Acknowledgments,[0],[0]
erNet.,Acknowledgments,[0],[0]
We also thank the members of the Edinburgh NLP group for participating in our human evaluation experiments.,Acknowledgments,[0],[0]
"This work greatly benefitted from discussions with Jianpeng Cheng, Annie Louis, Pedro Balage, Alfonso Mendes, Sebastião Miranda, and members of the Edinburgh NLP group.",Acknowledgments,[0],[0]
"We gratefully acknowledge the support of the European Research Council (Lapata; award number 681760), the European Union under the Horizon 2020 SUMMA project (Narayan, Cohen; grant agreement 688139), and Huawei Technologies (Cohen).",Acknowledgments,[0],[0]
Document modeling is essential to a variety of natural language understanding tasks.,abstractText,[0],[0]
We propose to use external information to improve document modeling for problems that can be framed as sentence extraction.,abstractText,[0],[0]
We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information.,abstractText,[0],[0]
We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question).,abstractText,[0],[0]
"We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.1",abstractText,[0],[0]
Document Modeling with External Attention for Sentence Extraction,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422–1432, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Schütze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012).",1 Introduction,[0],[0]
"The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document.",1 Introduction,[0],[0]
"In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn-
∗Corresponding author.",1 Introduction,[0],[0]
"1 Codes and datasets are publicly available at
http://ir.hit.edu.cn/˜dytang.
ing algorithm to build sentiment classifier.",1 Introduction,[0],[0]
"Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015).
",1 Introduction,[0],[0]
Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document.,1 Introduction,[0],[0]
This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document.,1 Introduction,[0],[0]
"However, existing studies typically fail to effectively capture such information.",1 Introduction,[0],[0]
"For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that.",1 Introduction,[0],[0]
"Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences.",1 Introduction,[0],[0]
"Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data.",1 Introduction,[0],[0]
"Essentially, they use local ngram information and do not capture semantic relations between sentences.",1 Introduction,[0],[0]
"Furthermore, a person asked to do this task will naturally carry it out in a sequential, bottom-up fashion, analyze the meanings of sentences before considering semantic relations between them.",1 Introduction,[0],[0]
"This motivates us to develop an end-to-end and bottom-up algorithm to effectively model document representation.
",1 Introduction,[0],[0]
"In this paper, we introduce a neural network approach to learn continuous document representation for sentiment classification.",1 Introduction,[0],[0]
"The method is on the basis of the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a docu-
1422
ment) depends on the meanings of its constituents.",1 Introduction,[0],[0]
"Specifically, the approach models document representation in two steps.",1 Introduction,[0],[0]
"In the first step, it uses convolutional neural network (CNN) or long short-term memory (LSTM) to produce sentence representations from word representations.",1 Introduction,[0],[0]
"Afterwards, gated recurrent neural network is exploited to adaptively encode semantics of sentences and their inherent relations in document representations.",1 Introduction,[0],[0]
These representations are naturally used as features to classify the sentiment label of each document.,1 Introduction,[0],[0]
"The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2.
",1 Introduction,[0],[0]
We conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4.,1 Introduction,[0],[0]
"We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014).",1 Introduction,[0],[0]
"Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural
2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html
3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge
network in document modeling.",1 Introduction,[0],[0]
The main contributions of this work are as follows: • We present a neural network approach to encode relations between sentences in document representation for sentiment classification.,1 Introduction,[0],[0]
"•We report empirical results on four large-scale datasets, and show that the approach outperforms state-of-the-art methods for document level sentiment classification.",1 Introduction,[0],[0]
"•We report empirical results that traditional recurrent neural network is weak in modeling document composition, while adding neural gates dramatically improves the classification performance.",1 Introduction,[0],[0]
"We introduce the proposed neural model in this section, which computes continuous vector representations for documents of variable length.",2 The Approach,[0],[0]
These representations are further used as features to classify the sentiment label of each document.,2 The Approach,[0],[0]
"An overview of the approach is displayed in Figure 1.
",2 The Approach,[0],[0]
"Our approach models document semantics based on the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a document) comes from the meanings of its constituents and the rules used to combine them.",2 The Approach,[0],[0]
"Since a document consists of a list of sentences and each sentence is made up of a list of words, the approach models document representation in two stages.",2 The Approach,[0],[0]
"It first produces continuous sentence vectors from word represen-
tations with sentence composition (Section 2.1).",2 The Approach,[0],[0]
"Afterwards, sentence vectors are treated as inputs of document composition to get document representation (Section 2.2).",2 The Approach,[0],[0]
Document representations are then used as features for document level sentiment classification (Section 2.3).,2 The Approach,[0],[0]
"We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition.
",2.1 Sentence Composition,[0],[0]
"Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003).",2.1 Sentence Composition,[0],[0]
"All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V",2.1 Sentence Composition,[0],[0]
| is vocabulary size.,2.1 Sentence Composition,[0],[0]
"These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014).",2.1 Sentence Composition,[0],[0]
"We adopt the latter strategy to make better use of semantic and grammatical associations of words.
",2.1 Sentence Composition,[0],[0]
We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition.,2.1 Sentence Composition,[0],[0]
"CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a).",2.1 Sentence Composition,[0],[0]
"They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results.",2.1 Sentence Composition,[0],[0]
"One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives.
",2.1 Sentence Composition,[0],[0]
"Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation.",2.1 Sentence Composition,[0],[0]
Figure 2 displays the method.,2.1 Sentence Composition,[0],[0]
"We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification.",2.1 Sentence Composition,[0],[0]
"For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence.",2.1 Sentence Composition,[0],[0]
"In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigram-
s and trigrams in a sentence.",2.1 Sentence Composition,[0],[0]
Each filter consists of a list of linear layers with shared parameters.,2.1 Sentence Composition,[0],[0]
"Formally, let us denote a sentence consisting of n words as {w1, w2, ...wi, ...wn}, let lc be the width of a convolutional filter, and let Wc, bc be the shared parameters of linear layers in the filter.",2.1 Sentence Composition,[0],[0]
Each word wi is mapped to its embedding representation ei ∈ Rd.,2.1 Sentence Composition,[0],[0]
"The input of a linear layer is the concatenation of word embeddings in a fixed-length window size lc, which is denoted as Ic =",2.1 Sentence Composition,[0],[0]
[ei; ei+1; ...; ei+lc−1] ∈ Rd·lc .,2.1 Sentence Composition,[0],[0]
"The output of a linear layer is calculated as
Oc = Wc · Ic + bc (1)
where Wc ∈ Rloc×d·lc , bc ∈ Rloc , loc is the output length of linear layer.",2.1 Sentence Composition,[0],[0]
"To capture global semantics of a sentence, we feed the outputs of linear layers to an average pooling layer, resulting in an output vector with fixed-length.",2.1 Sentence Composition,[0],[0]
"We further add hyperbolic tangent (tanh) to incorporate pointwise nonlinearity, and average the outputs of multiple filters to get sentence representation.
",2.1 Sentence Composition,[0],[0]
"We also try lstm as the sentence level semantic calculator, the performance comparison between these two variations is given in Section 3.",2.1 Sentence Composition,[0],[0]
The obtained sentence vectors are fed to a document composition component to calculate the document representation.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We present a gated recurrent neural network approach for document composition in this part.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Given the vectors of sentences of variable length as input, document composition produces a fixed-length document vector as output.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"To this end, a simple strategy is ignoring the order of sen-
tences and averaging sentence vectors as document vector.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Despite its computational efficiency, it fails to capture complex linguistic relations (e.g. “cause” and “contrast”) between sentences.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Convolutional neural network (Denil et al., 2014) is an alternative for document composition, which models local sentence relations with shared parameters of linear layers.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
Standard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector st with the output vector of the previous step ht−1.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
ht = tanh(Wr ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + br) (2) where Wr ∈ Rlh×(lh+loc), br ∈ Rlh , lh and loc are dimensions of hidden vector and sentence vector, respectively.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
This makes it difficult to model long-distance correlations in a sequence.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015).",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Specifically, the transition function of the gated RNN used in this work is calculated as follows.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
it = sigmoid(Wi ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
[ht−1; st] + bi) (3) ft = sigmoid(Wf ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + bf ) (4)
gt = tanh(Wr ·",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + br) (5) ht = tanh(it gt + ft ht−1) (6)
where stands for element-wise multiplication, Wi, Wf , bi, bf adaptively select and remove history vector and input vector for semantic composition.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to get a better document representation.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
Figure 3 (a) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical semantics with different granularities.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The method is illustrated in Figure 3 (b), which shares some characteristics with (Zhao et al., 2015).",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We can go one step further to use preceding histories and following evidences in the same way, and exploit bidirectional (Graves et al., 2013) gated RNN as the calculator.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
The model is embedded in Figure 1.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering.,2.3 Sentiment Classification,[0],[0]
"Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a softmax layer to convert real values to conditional probabilities, which is calculated as follows.
",2.3 Sentiment Classification,[0],[0]
"Pi = exp(xi)∑C
i′=1 exp(xi′) (7)
We conduct experiments in a supervised learning setting, where each document in the training data is accompanied with its gold sentiment label.
",2.3 Sentiment Classification,[0],[0]
Corpus #docs #s/d #w/d |V,2.3 Sentiment Classification,[0],[0]
"| #class Class Distribution
For model training, we use the cross-entropy error between gold sentiment distribution P g(d) and predicted sentiment distribution P (d) as the loss function.
loss = − ∑ d∈T",2.3 Sentiment Classification,[0],[0]
C∑ i=1,2.3 Sentiment Classification,[0],[0]
P gi (d) · log(Pi(d)),2.3 Sentiment Classification,[0],[0]
"(8)
where T is the training data, C is the number of classes, d represents a document.",2.3 Sentiment Classification,[0],[0]
"P g(d) has a 1-of-K coding scheme, which has the same dimension as the number of classes, and only the dimension corresponding to the ground truth is 1, with all others being 0.",2.3 Sentiment Classification,[0],[0]
We take the derivative of loss function through back-propagation with respect to the whole set of parameters θ =,2.3 Sentiment Classification,[0],[0]
"[Wc; bc;Wi; bi;Wf ; bf ;Wr; br;Wsoftmax, bsoftmax], and update parameters with stochastic gradient descent.",2.3 Sentiment Classification,[0],[0]
"We set the widths of three convolutional filters as 1, 2 and 3, output length of convolutional filter as 50.",2.3 Sentiment Classification,[0],[0]
"We learn 200-dimensional word embeddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U(−0.01, 0.01), and set learning rate as 0.03.",2.3 Sentiment Classification,[0],[0]
We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification.,3 Experiment,[0],[0]
We describe experimental settings and report empirical results in this section.,3 Experiment,[0],[0]
We conduct experiments on large-scale datasets consisting of document reviews.,3.1 Experimental Setting,[0],[0]
"Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and 2015.",3.1 Experimental Setting,[0],[0]
"Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of
documents.",3.1 Experimental Setting,[0],[0]
"We do not consider the cases that rating does not match with review texts (Zhang et al., 2014).
",3.1 Experimental Setting,[0],[0]
Statistical information of these datasets are given in Table 1.,3.1 Experimental Setting,[0],[0]
"We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10.",3.1 Experimental Setting,[0],[0]
We run tokenization and sentence splitting with Stanford CoreNLP,3.1 Experimental Setting,[0],[0]
"(Manning et al., 2014) on all these datasets.",3.1 Experimental Setting,[0],[0]
"We use accuracy (Manning and Schütze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance.",3.1 Experimental Setting,[0],[0]
"We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive).
",3.1 Experimental Setting,[0],[0]
"MSE = ∑N
i (goldi − predictedi)2 N
(9)",3.1 Experimental Setting,[0],[0]
"We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification.
",3.2 Baseline Methods,[0],[0]
"(1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set.
",3.2 Baseline Methods,[0],[0]
"(2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5.
",3.2 Baseline Methods,[0],[0]
"(3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al.
5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...).",3.2 Baseline Methods,[0],[0]
"However, its performance is obviously worse than SVM classifier.
(4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier.
(5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation.
",3.2 Baseline Methods,[0],[0]
"(6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling.
",3.2 Baseline Methods,[0],[0]
"(7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014).
",3.2 Baseline Methods,[0],[0]
"(8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided.",3.2 Baseline Methods,[0],[0]
Window size is tuned on the development set.,3.2 Baseline Methods,[0],[0]
Experimental results are given in Table 2.,3.3 Comparison to Other Methods,[0],[0]
"We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better).",3.3 Comparison to Other Methods,[0],[0]
"The best method in each dataset and each evaluation metric is in bold.
",3.3 Comparison to Other Methods,[0],[0]
"From Table 2, we can see that majority is the worst method because it does not capture any textual semantics.",3.3 Comparison to Other Methods,[0],[0]
"SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers
6We use Skipgram as it performs slightly better than CBOW in the experiment.",3.3 Comparison to Other Methods,[0],[0]
"We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus.
among all baseline methods.",3.3 Comparison to Other Methods,[0],[0]
"Designing complex features are also effective for document level sentiment classification, however, it does not surpass the bag-of-ngram features significantly as on Twitter corpora (Kiritchenko et al., 2014).",3.3 Comparison to Other Methods,[0],[0]
"Furthermore, the aforementioned bag-of-features are discrete and sparse.",3.3 Comparison to Other Methods,[0],[0]
"For example, the feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features.",3.3 Comparison to Other Methods,[0],[0]
"Based on them, we try to concatenate several discourse-driven features, but the classification performances remain unchanged.
",3.3 Comparison to Other Methods,[0],[0]
AverageSG,3.3 Comparison to Other Methods,[0],[0]
is a straight forward way to compose document representation without feature engineering.,3.3 Comparison to Other Methods,[0],[0]
"Unfortunately, we can see that it does not work in this scenario, which appeals for powerful semantic composition models for document level sentiment classification.",3.3 Comparison to Other Methods,[0],[0]
"We try to make better use of the sentiment information to learn a better SSWE (Tang et al., 2014), e.g. setting a large window size.",3.3 Comparison to Other Methods,[0],[0]
"However, its performance is still worse than context-based word embedding.",3.3 Comparison to Other Methods,[0],[0]
"This stems from the fact that there are many sentiment shifters (e.g. negation or contrast words) in document level reviews, while Tang et al. (2014) learn SSWE by assigning sentiment label of a text to each phrase it contains.",3.3 Comparison to Other Methods,[0],[0]
"How to learn SSWE effectively with document level sentiment supervision remains as an interesting future work.
",3.3 Comparison to Other Methods,[0],[0]
"Since JMARS outputs real-valued outputs, we only evaluate it in terms ofMSE.",3.3 Comparison to Other Methods,[0],[0]
"We can see that sophisticated baseline methods such as JMARS, paragraph vector and convolutional NN obtain significant performance boosts over AverageSG by
capturing deeper semantics of texts.",3.3 Comparison to Other Methods,[0],[0]
"Comparing between CNN and AverageSG, we can conclude that deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents.",3.3 Comparison to Other Methods,[0],[0]
"However, it is somewhat disappointing that these models do not significantly outperform discrete bag-of-ngrams and bag-of-features.",3.3 Comparison to Other Methods,[0],[0]
"The reason might lie in that semantic meanings of documents, e.g. relations between sentences, are not well captured.",3.3 Comparison to Other Methods,[0],[0]
We can see that the proposed method Conv-GRNN and LSTM-GRNN yield the best performance on all four datasets in two evaluation metrics.,3.3 Comparison to Other Methods,[0],[0]
"Compared with CNN, Conv-GRNN shows its superior power in document composition component, which encodes semantics of sentences and their relations in document representation with gated recurrent neural network.",3.3 Comparison to Other Methods,[0],[0]
We also find that LSTM (almost) consistently performs better than CNN in modeling the sentence representation.,3.3 Comparison to Other Methods,[0],[0]
"As discussed before, document composition contributes a lot to the superior performance of ConvGRNN and LSTM-GRNN.",3.4 Model Analysis,[0],[0]
"Therefore, we take Conv-GRNN as an example and compare different neural models for document composition in this part.",3.4 Model Analysis,[0],[0]
"Specifically, after obtaining sentence vectors with convolutional neural network as described in Section 2.1, we carry out experiments in following settings.
",3.4 Model Analysis,[0],[0]
(1) Average.,3.4 Model Analysis,[0],[0]
"Sentence vectors are averaged to get the document vector.
(2) Recurrent / GatedNN.",3.4 Model Analysis,[0],[0]
Sentence vectors are fed to standard (or gated) recurrent neural network in a sequential way from the beginning of the input document.,3.4 Model Analysis,[0],[0]
"The last hidden vector is regarded as document representation.
",3.4 Model Analysis,[0],[0]
(3) Recurrent Avg / GatedNN,3.4 Model Analysis,[0],[0]
Avg.,3.4 Model Analysis,[0],[0]
"We extend setting (2) by averaging hidden vectors of recurrent neural network as document vector.
(4) Bi Recurrent Avg / Bi GatedNN Avg.",3.4 Model Analysis,[0],[0]
We extend setting (3) by calculating hidden vectors from both preceding histories and following contexts.,3.4 Model Analysis,[0],[0]
"Bi-directional hidden vectors are averaged as document representation.
",3.4 Model Analysis,[0],[0]
Table 3 shows the experimental results.,3.4 Model Analysis,[0],[0]
"We can see that standard recurrent neural network (RNN) is the worst method, even worse than the simple vector average.",3.4 Model Analysis,[0],[0]
"This is because RNN suffers from the vanishing gradient problem, stating that the influence of a given input on the hidden layer decays exponentially over time on the network output.",3.4 Model Analysis,[0],[0]
"In this paper, it means that document representation encodes rare semantics of the beginning sentences.",3.4 Model Analysis,[0],[0]
This is further justified by the great improvement of Recurrent Avg over Recurrent.,3.4 Model Analysis,[0],[0]
"Bi Recurrent Avg and Recurrent Avg perform comparably, but disappointingly both of them fail to transcend Average.",3.4 Model Analysis,[0],[0]
"After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings.",3.4 Model Analysis,[0],[0]
"The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN",3.4 Model Analysis,[0],[0]
Avg and Bi GatedNN,3.4 Model Analysis,[0],[0]
Avg obtains comparable performances with GatedNN.,3.4 Model Analysis,[0],[0]
"Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002).",4 Related Work,[0],[0]
"Pang and Lee (2002; 2005)
cast this problem as a classification task, and use machine learning method in a supervised learning framework.",4 Related Work,[0],[0]
Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity.,4 Related Work,[0],[0]
"Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method.",4 Related Work,[0],[0]
Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier.,4 Related Work,[0],[0]
"Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014).
",4 Related Work,[0],[0]
"Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015).",4 Related Work,[0],[0]
"Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification.",4 Related Work,[0],[0]
Existing studies in this direction can be divided into two groups.,4 Related Work,[0],[0]
One line of research focuses on learning continuous word embedding.,4 Related Work,[0],[0]
"Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014).",4 Related Work,[0],[0]
"Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account.",4 Related Work,[0],[0]
"Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010).",4 Related Work,[0],[0]
Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function.,4 Related Work,[0],[0]
Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition.,4 Related Work,[0],[0]
"Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014).",4 Related Work,[0],[0]
Glorot et al. (2011) use stacked denoising autoencoder.,4 Related Work,[0],[0]
"Convolutional neural networks are widely used for semantic compo-
sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics.",4 Related Work,[0],[0]
Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words.,4 Related Work,[0],[0]
"Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a).
",4 Related Work,[0],[0]
"In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations.",4 Related Work,[0],[0]
"A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning.",4 Related Work,[0],[0]
They verify the effectiveness of LSTM in text generation task.,4 Related Work,[0],[0]
We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification.,5 Conclusion,[0],[0]
"The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives.",5 Conclusion,[0],[0]
We conduct extensive experiments on four review datasets with two evaluation metrics.,5 Conclusion,[0],[0]
Empirical results show that our approaches achieve state-of-the-art performances on all these datasets.,5 Conclusion,[0],[0]
"We also find that (1) traditional recurrent neural network is extremely weak in modeling document composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation.
",5 Conclusion,[0],[0]
We briefly discuss some future plans.,5 Conclusion,[0],[0]
How to effectively compose sentence meanings to document meaning is a central problem in natural language processing.,5 Conclusion,[0],[0]
"In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results.",5 Conclusion,[0],[0]
"From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc.",5 Conclusion,[0],[0]
"Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a).",5 Conclusion,[0],[0]
"However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work.",5 Conclusion,[0],[0]
"From another perspective, one could compose document
representation over discourse tree structures rather than in a sequential way.",5 Conclusion,[0],[0]
"Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms.",5 Conclusion,[0],[0]
"However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web.",5 Conclusion,[0],[0]
How to simultaneously learn document structure and composition function is an interesting future work.,5 Conclusion,[0],[0]
The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions.,Acknowledgments,[0],[0]
We also would like to thank three anonymous reviewers for their valuable comments and suggestions.,Acknowledgments,[0],[0]
"This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61133012 and No. 61273321).",Acknowledgments,[0],[0]
Duyu Tang is supported by Baidu Fellowship and IBM Ph.D. Fellowship.,Acknowledgments,[0],[0]
Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document.,abstractText,[0],[0]
"To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion.",abstractText,[0],[0]
The model first learns sentence representation with convolutional neural network or long short-term memory.,abstractText,[0],[0]
"Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network.",abstractText,[0],[0]
We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge.,abstractText,[0],[0]
Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1,abstractText,[0],[0]
Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2044–2054 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Document-level sentiment classification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010).",1 Introduction,[0],[0]
"There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc.",1 Introduction,[0],[0]
Most of reviews are very comprehensive and thus long documents.,1 Introduction,[0],[0]
Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management.,1 Introduction,[0],[0]
"Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016).",1 Introduction,[0],[0]
"However, predicting an overall score for each long document is not enough, because the document can mention dif-
ferent aspects of the corresponding product or service.",1 Introduction,[0],[0]
"For example, in Figure 1, there could be different aspects for a review of hotel.",1 Introduction,[0],[0]
These aspects help customer service better understand what are the major pros and cons of the product or service.,1 Introduction,[0],[0]
"Compared to the overall rating, users are less motivated to give aspect ratings.",1 Introduction,[0],[0]
"Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspect rather than an overall rating.
",1 Introduction,[1.0000000804648372],"['Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspect rather than an overall rating.']"
"One straightforward approach for documentlevel multi-aspect sentiment classification is multi-task learning (Caruana, 1997).",1 Introduction,[1.0],"['One straightforward approach for documentlevel multi-aspect sentiment classification is multi-task learning (Caruana, 1997).']"
"For neural networks, we can simply treat each aspect (e.g., rating from one to five) as a classification task, and let different tasks use softmax classifier to extract task-specific representations at the top layer while share the input and hidden layers to mutually enhance the prediction results (Collobert et al., 2011; Luong et al., 2016).",1 Introduction,[0],[0]
"However, such approach ignores the fact that the aspects themselves have semantic meanings.",1 Introduction,[0],[0]
"For example, as human beings, if we were asked to evaluate the aspect rating of a document, we simply read the review, and find aspect-related keywords, and see around comments.",1 Introduction,[0],[0]
"Then, we aggregate all the related snippets to make a decision.
",1 Introduction,[0],[0]
"In this paper, we propose a novel approach to treat document-level multi-aspect sentiment clas-
2044
sification as a machine comprehension (Kumar et al., 2016; Sordoni et al., 2016) problem.",1 Introduction,[0],[0]
"To mimic human’s evaluation of aspect classification, we create a list of keywords for each aspect.",1 Introduction,[1.0],"['To mimic human’s evaluation of aspect classification, we create a list of keywords for each aspect.']"
"For example, when we work on the Room aspect, we generate some keywords such as “room,” “bed,” “view,” etc.",1 Introduction,[1.0],"['For example, when we work on the Room aspect, we generate some keywords such as “room,” “bed,” “view,” etc.']"
Then we can ask pseudo questions: “How is the room?”,1 Introduction,[0],[0]
“How is the bed?”,1 Introduction,[0],[0]
“How is the view?” and provide an answer “Rating 5.”,1 Introduction,[0],[0]
"In this case, we can train a machine comprehension model to automatically attend corresponding text snippets in the review document to predict the aspect rating.",1 Introduction,[0],[0]
"Specifically, we introduce a hierarchical and iterative attention model to construct aspect-specific representations.",1 Introduction,[1.0],"['Specifically, we introduce a hierarchical and iterative attention model to construct aspect-specific representations.']"
We use a hierarchical architecture to build up different representations at both word and sentence levels interacting with aspect questions.,1 Introduction,[1.0],['We use a hierarchical architecture to build up different representations at both word and sentence levels interacting with aspect questions.']
"At each level, the model consists of input encoders and iterative attention modules.",1 Introduction,[0],[0]
The input encoder learns memories1 of documents and questions with Bi-directional LSTM (Bi-LSTM) model and non-linear mapping respectively.,1 Introduction,[0],[0]
"The iterative attention module takes into memories as input and attends them sequentially with a multiple hop mechanism, performing effective interactions between documents and aspect questions.
",1 Introduction,[0],[0]
"To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines.",1 Introduction,[1.0],"['To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines.']"
"We also analyze the effects of num-
1Following the work (Weston et al., 2015; Sukhbaatar et al., 2015), we refer the memory to a set vectors which are stacked together and could be attended.
",1 Introduction,[0],[0]
bers of the hop and aspect words on performances.,1 Introduction,[0],[0]
"Moreover, a case study for attention results is performed at both word and sentence levels.
",1 Introduction,[0.9999999844535254],"['Moreover, a case study for attention results is performed at both word and sentence levels.']"
The contributions of this paper are two-fold.,1 Introduction,[0],[0]
"First, we study the document-level multi-aspect sentiment classification as a machine comprehension problem and introduce a hierarchical iterative attention model for it.",1 Introduction,[0],[0]
"Second, we demonstrate the effectiveness of proposed model on two datasets, showing that our model outperforms classical baselines.",1 Introduction,[0],[0]
The code and data for this paper are available at https://github.com/ HKUST-KnowComp/DMSCMC.,1 Introduction,[0],[0]
"In this section, we introduce our proposed method.",2 Method,[0],[0]
We first briefly introduce the problem we work on.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Given a piece of review, our task is to predict the ratings of different aspects.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For example, in Figure 1, we predict the ratings of Cleanliness, Room, and Value.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"To achieve this, we assume that there are existing reviews with aspect ratings for machines to learn.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Formally, we denote the review document as d containing a set of Td sentences {s1, s2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
sTd}.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For the t-th sentence st, we use a set of words { w1, w2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"w|st| } to represent it, and use wi, wwi and w p",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"i as the one-hot encoding, word embedding, and phrase embedding for wi respectively.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"The phrase embedding encodes the semantics of phrases where the current word wi is the center (e.g., hidden vectors learned by Bi-LSTM shown in Section 2.2).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For each qk of K aspects
{q1, q2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
", qK}, we use Nk aspect-related keywords, { qk1 , qk2 . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"qkNk } , to represent it.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Similarly, we use qki , q w ki
as the one-hot encoding and word embedding for qki respectively.
",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"There are several sophisticated methods for choosing aspect keywords (e.g., topic model).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Here, we consider a simple way where five seeds were first manually selected for each aspect and then more words were obtained based on their cosine similarities with seeds2
As shown in Figure 2 (left), our framework follows the idea of multi-task learning, which learns different aspects simultaneously.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"In this case, all these tasks share the representations of words and architecture of semantic model for the final classifiers.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Different from straightforward neural network based multi-task learning (Collobert et al., 2011), for each document d and an aspect qk, our model uses both the content of d and all the related keywords { qk1 , qk2 . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
qkNk } as input.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Since the keywords can cover most of the semantic meanings of the aspect, and we do not know which document mentions which semantic meaning, we build an attention model to automatically decide it (introduced in Section 2.3).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Assuming that the keywords have been decided, we use a hierarchical attention model to select useful information from the review documents.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"As shown in Figure 2 (right), the hierarchical attention of keywords is applied to both sentence level (to select meaningful words) and document level (to select meaningful sentence).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Thus, our model builds aspectspecific representations in a bottom-up manner.
",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Specifically, we obtain sentence representations { sk1, sk2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
skT } using the input encoder (Section 2.2) and iterative attention module (Section 2.3) at the word level.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
Then we take sentence representations and k-th aspect as input and apply the sentence-level input encoder and attention model to generate the document representation dk for final classification.,2.1 Problem Definition and Hierarchical Framework,[1.0],['Then we take sentence representations and k-th aspect as input and apply the sentence-level input encoder and attention model to generate the document representation dk for final classification.']
"As shown in Figure 2 (right), the attention model is applied twice at different levels of the representation.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
The input module builds memory vectors for the iterative attention module and is performed both at word and sentence levels.,2.2 Input Encoder,[0],[0]
"For a document, it con-
2For example, the words “value,” “price,” “worth,” “cost,” and “$” are selected as seeds for aspect Price.",2.2 Input Encoder,[0],[0]
"The information for seeds can be found in our released resource.
",2.2 Input Encoder,[0],[0]
verts word sequence into word level memory Mdw and sentence sequence into sentence level memory Mds respectively.,2.2 Input Encoder,[0],[0]
"For an aspect question qk, it takes a set of aspect-specific words {qki}1≤i≤Nk as input and derives word level memory Mqw and sentence level memory Mqs.
To construct Mdw, we obtain word embeddings{ ww1 , ww2 , . . .",2.2 Input Encoder,[0],[0]
ww|st| } from an embedding matrix EA applied to all words shown in the corpus.,2.2 Input Encoder,[0],[0]
"Then, LSTM (Hochreiter and Schmidhuber, 1997) model is used as the encoder to produce hidden vectors of words based on the word embeddings.",2.2 Input Encoder,[0],[0]
"At each step, LSTM takes input wwt and derives a new hidden vector by ht = LSTM(wwt , ht−1).",2.2 Input Encoder,[0],[0]
"To preserve the subsequent context information for words, another LSTM is ran over word sequence in a reverse order simultaneously.",2.2 Input Encoder,[0],[0]
"Then the forward hidden vector −→ h t and backward hidden
vector ←− h t are concatenated as phrase embedding wpt .",2.2 Input Encoder,[0],[0]
We stack these phrase embeddings together as word level memory Mdw.,2.2 Input Encoder,[0],[0]
"Similarly, we feed sentence representations into another Bi-LSTM to derive the sentence level memory Mds .",2.2 Input Encoder,[0],[0]
"Note that, the sentence representations are obtained using the iterative attention module which is described as Eq. (5) in Section 2.3.
",2.2 Input Encoder,[0],[0]
"Since we have question keywords as input, to allow the interactions between questions and documents, we also build question memory in following way.",2.2 Input Encoder,[0],[0]
"We obtain Qk = { qwki }
1≤i≤Nk by looking up an embedding matrix 3 EB applied to all question keywords.",2.2 Input Encoder,[0],[0]
"Then a non-linear mapping is applied to obtain the question memory at word level:
Mqkw = tanh(QkW q w), (1)
where Wqw is the parameter matrix to adapt qk at word level.",2.2 Input Encoder,[0],[0]
"Similarly, we use another mapping to obtain the sentence level memory:
Mqks = tanh(QkW q s), (2)
where Wqs is the parameter matrix to adapt qk at sentence level.",2.2 Input Encoder,[0],[0]
"The iterative attention module (IAM) attends and reads memories of questions and documents alternatively with a multi-hop mechanism, deriving
3EA and EB are initialized by the same pre-trained embeddings but are different embedding matrices with different updates.
aspect-specific sentence and document representations.",2.3 Iterative Attention Module,[0],[0]
"As we discussed in the introduction, the set of selected question keywords may not best characterize the aspect for different documents.",2.3 Iterative Attention Module,[0],[0]
"Thus, the IAM module introduces a backward attention to use document information (word or sentence) to select useful keywords of each aspect as the document-specific question to build attention model.
",2.3 Iterative Attention Module,[0],[0]
The illustration of IAM is shown in Figure 3.,2.3 Iterative Attention Module,[0],[0]
"To obtain sentence representations, it takes Mdw and Mqw as the input and performs m iterations (hops).",2.3 Iterative Attention Module,[0],[0]
"For each iteration, IAM conducts four operations: (1) attends the question memory by the selective vector p and summarizes question memory vectors into a single vector q̂; (2) updates the selective vector by the previous one and q̂; (3) attends document (content) memory based on the updated selective vector and summarizes memory vectors in to a single vector ĉ; (4) updates the selective vector by the previous one and ĉ.
We unify operations (1) and (3) by an attention function x̂ = A(p, M), where M could be Mdw or Mqw which corresponds x̂ = ĉ or x̂ = q̂.",2.3 Iterative Attention Module,[0],[0]
"The attention function A is decomposed as:
H = tanh(MWa (1p))",2.3 Iterative Attention Module,[0],[0]
"a = softmax(HvTa )
",2.3 Iterative Attention Module,[0],[0]
x̂,2.3 Iterative Attention Module,[0],[0]
"= ∑ aiMi,
(3)
where 1 is a vector with all elements are 1, which copies the selective vector to meet the dimension requirement.",2.3 Iterative Attention Module,[0],[0]
"The Wa and va are parameters, a is attention weights for memory vectors, and Mi
means i-th row in M. Operations (2) and (4) are formulated as an update function p2i−{l} = U(x̂, p2i−{l}−1), where i is the hop index, l can be 0 or 1 which corresponds to x̂ = ĉ or x̂ = q̂ respectively.",2.3 Iterative Attention Module,[0.9999999983454736],"['The Wa and va are parameters, a is attention weights for memory vectors, and Mi means i-th row in M. Operations (2) and (4) are formulated as an update function p2i−{l} = U(x̂, p2i−{l}−1), where i is the hop index, l can be 0 or 1 which corresponds to x̂ = ĉ or x̂ = q̂ respectively.']"
We initialize p0 by a zero vector.,2.3 Iterative Attention Module,[0],[0]
"The update function U can be a recurrent neural network (Xiong et al., 2017) or other heuristic weighting functions.",2.3 Iterative Attention Module,[0],[0]
"In this paper, we introduce a simple strategy:
p2i−{l} = x̂, (4)
which ignores the previous selective vector but succeeds to obtain comparable results with other more complicated function in the initial experiments.
",2.3 Iterative Attention Module,[0],[0]
"Multi-hop mechanism attends different memory locations in different hops (Sukhbaatar et al., 2015), capturing different interactions between documents and questions.",2.3 Iterative Attention Module,[0],[0]
"In order to preserve the information of various kinds of interactions, we concatenate all ĉ’s in each hop as the final representations of sentences:
s = [ĉ1; ĉ2; · · · ĉm].",2.3 Iterative Attention Module,[0],[0]
"(5) After obtaining sentence representations, we feed them into the sentence-level input encoder, deriving the memories Mds and Mqs.",2.3 Iterative Attention Module,[0],[0]
"Then, the aspect-specific document representation dk is obtained by the sentence-level IAM in a similar way.",2.3 Iterative Attention Module,[0],[0]
"For each aspect, we obtain aspect-specific document representations {dk}1≤k≤K .",2.4 Objective Function,[1.0],"['For each aspect, we obtain aspect-specific document representations {dk}1≤k≤K .']"
"All these representations are fed into classifiers, each of which includes a softmax layer.",2.4 Objective Function,[0],[0]
"The softmax layer outputs the probability distribution over |Y| categories for the distributed representation, which is defined as:
p′(d, k) = softmax(Wclassk dk), (6)
where Wclassk is the parameter matrix.",2.4 Objective Function,[0],[0]
"We define the cross-entropy objective function between gold sentiment distribution p(d, k) and predicted sentiment distribution p′(d, k) as the classification loss function:
− ∑ d∈D K∑ k=1",2.4 Objective Function,[0],[0]
|Y|∑ i=1,2.4 Objective Function,[0],[0]
"p(d, k)log(p′(d, k)), (7)
where p(d, k) is a one-hot vector, which has the same dimension as the number of classes, and only the dimension associated with the ground truth label is one, with others being zeros.",2.4 Objective Function,[0],[0]
"In this section, we show experimental results to demonstrate our proposed algorithm.",3 Experiment,[0],[0]
"We conduct our experiments on TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016) datasets, which contain seven aspects (value, room, location, cleanliness, check in/front desk, service, and business service) and four aspects (feel, look, smell, and taste) respectively.",3.1 Datasets,[0],[0]
"We follow the processing step (Lei et al., 2016) by choosing the reviews with different aspect ratings and the new datasets are described in Table 1.",3.1 Datasets,[0],[0]
"We tokenize the datasets by Stanford corenlp4 and randomly split them into training, development, and testing sets with 80/10/10%.",3.1 Datasets,[0],[0]
"To demonstrate the effectiveness of the proposed method, we compare our model with following baselines:
Majority uses the majority sentiment label in development sets as the predicted label.
",3.2 Baseline Methods,[0],[0]
"SVM uses unigram and bigram as text features and uses Liblinear (Fan et al., 2008) for learning.
",3.2 Baseline Methods,[0],[0]
"SLDA refers to supervised latent Dirichlet allocation (Blei and Mcauliffe, 2010) which is a statistical model of labeled documents.
",3.2 Baseline Methods,[0],[0]
"NBoW is a neural bag-of-words model averaging embeddings of all words in a document and feeds the resulted embeddings into SVM classifier.
",3.2 Baseline Methods,[0],[0]
DAN is a deep averaging network model which consists of several fully connected layers with averaged word embeddings as input.,3.2 Baseline Methods,[0],[0]
"One novel word dropout strategy is employed to boost model performances (Iyyer et al., 2015).
",3.2 Baseline Methods,[0],[0]
"CNN continuously performs a convolution operation over a sentence to extract words neighboring features, then gets a fixed-sized representation by a pooling layer (Kim, 2014).
",3.2 Baseline Methods,[0],[0]
"4http://nlp.stanford.edu/software/corenlp.shtml
LSTM is one variant of recurrent neural network and has been proved to be one of state-ofthe-art models for document-level sentiment classification (Tang et al., 2015a).",3.2 Baseline Methods,[0],[0]
"We use LSTM to refer Bi-LSTM which captures both forward and backward semantic information.
",3.2 Baseline Methods,[0],[0]
"HAN means the hierarchical attention network which is proposed in (Yang et al., 2016) for document classification.",3.2 Baseline Methods,[0],[0]
"Note that, the original HAN depends GRU as the encoder.",3.2 Baseline Methods,[0],[0]
"In our experiments, LSTM-based HAN obtains slightly better results.",3.2 Baseline Methods,[0],[0]
"Thus, we report the results of HAN with LSTM as the encoder.
",3.2 Baseline Methods,[0],[0]
"We extend DAN, CNN, LSTM with the hierarchical architecture and multi-task framework, the corresponding models are MHDAN, MHCNN and MHLSTM respectively.",3.2 Baseline Methods,[0],[0]
"Besides, MHAN is also evaluated as one baseline, which is HAN with the multi-task learning.",3.2 Baseline Methods,[0],[0]
"We implement all neural models using Theano (Theano Development Team, 2016).",3.3 Implementation Details,[0],[0]
The model parameters are tuned based on the development sets.,3.3 Implementation Details,[0],[0]
"We learn 200-dimensional word embeddings with Skip-gram model (Mikolov et al., 2013) on in-domain corpus, which follows (Tang et al., 2015a).",3.3 Implementation Details,[0],[0]
The pre-trained word embeddings are used to initialize the embedding matrices EA and EB .,3.3 Implementation Details,[0],[0]
The dimensions of all hidden vectors are set to 200.,3.3 Implementation Details,[0],[0]
"For TripAdvisor dataset, the hop numbers of word-level and sentence-level iterative attention modules are set to 4 and 2 respectively.",3.3 Implementation Details,[0],[0]
"For BeerAdvocate dataset, the hop numbers are set to 6 and 2.",3.3 Implementation Details,[0],[0]
The number of selected keywords Nk = N is set to 20.,3.3 Implementation Details,[0],[0]
"To avoid model over-fitting, we use dropout and regularization as follows: (1) the regularization parameter is set to 1e-5; (2) the dropout rate is set to 0.3, which is applied to both sentence and document vectors.",3.3 Implementation Details,[0],[0]
"All parameters are trained by ADADELTA (Zeiler, 2012) without needing to set the initial learning rate.",3.3 Implementation Details,[0],[0]
"To ensure fair comparisons, we make baselines have same settings as the proposed model, such as word embeddings, dimensions of hidden vectors and optimization details and so on.",3.3 Implementation Details,[0],[0]
"We use accuracy and mean squared error (MSE) as the evaluation metrics and the results are shown in Table 2.
",3.4 Results and Analyses,[0],[0]
"Compared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effective than traditional ngram features on these two datasets.",3.4 Results and Analyses,[1.0],"['Compared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effective than traditional ngram features on these two datasets.']"
All neural network models outperform NBoW.,3.4 Results and Analyses,[0],[0]
"It shows the advantages of neural networks in the document sentiment classification.
",3.4 Results and Analyses,[0],[0]
"From the results of neural networks, we can observe that DAN performs worse than LSTM and CNN, and LSTM achieves slightly higher results than CNN.",3.4 Results and Analyses,[0],[0]
"It can be explained that the simple composition method averaging embeddings of words in a document but ignoring word order, may not be as effective as other flexible composition models, such as LSTM and CNN, on aspect classification.",3.4 Results and Analyses,[0],[0]
"Additionally, we observe that the multi-task learning and hierarchical architecture are beneficial for neural networks.",3.4 Results and Analyses,[1.0],"['Additionally, we observe that the multi-task learning and hierarchical architecture are beneficial for neural networks.']"
"Among all baselines, MHAN and MHLSTM achieve comparable results and outperform others.
",3.4 Results and Analyses,[0],[0]
"Compared with MHAN and MHLSTM, our method achieves improvements of 1.5% (3% relative improvement) and 1.0% (2.5% relative improvement) on TripAdvisor and BeerAdvocate re-
spectively, which shows that the incorporation of iterative attention mechanism helps the deep neural network based model build up more discriminative aspect-aware representation.",3.4 Results and Analyses,[0],[0]
Note that BeerAdvocate is relatively more difficult since the predicted ratings are from 1 to 10 while TripAdvisor is 1 to 5.,3.4 Results and Analyses,[0],[0]
"Moreover, t-test is conducted by randomly splitting datasets into train/dev/test sets and random initialization.",3.4 Results and Analyses,[0],[0]
The results on test sets are described in Table 3 which show performance of our model is stable.,3.4 Results and Analyses,[0],[0]
"In this section, we sample two sentences from TripAdvisor to show the visualization of attention results for case study.",3.5 Case Study for Attention Results,[1.0],"['In this section, we sample two sentences from TripAdvisor to show the visualization of attention results for case study.']"
Both word-level and sentence-level attention visualizations are shown in Figure 4.,3.5 Case Study for Attention Results,[1.0],['Both word-level and sentence-level attention visualizations are shown in Figure 4.']
"We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.
",3.5 Case Study for Attention Results,[1.000000008454324],['We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.']
"From the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect.",3.5 Case Study for Attention Results,[1.0],"['From the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect.']"
"For example, in the first sentence, the words comfortable and bed are assigned higher
weights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness.",3.5 Case Study for Attention Results,[1.000000088875046],"['For example, in the first sentence, the words comfortable and bed are assigned higher weights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness.']"
"In the second sentence, the word internet is assigned a high attention value for Business.",3.5 Case Study for Attention Results,[0],[0]
"Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable.",3.5 Case Study for Attention Results,[1.0],"['Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable.']"
"Specifically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops.",3.5 Case Study for Attention Results,[1.0],"['Specifically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops.']"
"In the second sentence, we observe that the value of word internet increases with the number of hop.",3.5 Case Study for Attention Results,[0],[0]
"Thus, we can see that the more sensible weights are obtained for words through the proposed iterative attention mechanism.",3.5 Case Study for Attention Results,[0],[0]
"Similarly, the figures (c) and (d) show that the conclusion from words is also suitable for sentences.",3.5 Case Study for Attention Results,[0],[0]
"For the first sentence, the sentence weight regarding the aspect Room is lower than Cleanliness in the first hop, but surpasses Cleanliness in the second hop.",3.5 Case Study for Attention Results,[0],[0]
"For the second sentence, the weight for Business becomes higher in the second hop.",3.5 Case Study for Attention Results,[0],[0]
"In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances.",3.6 Effects of Hop and Aspect Keywords,[1.0],"['In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances.']"
"All the experiments are conducted
on the development set.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Due to lack of space, we only present the results of TripAdvisor and the results of BeerAdvocate have a similar behavior as TripAdvisor.
",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"For the hop number, we vary m from 1 to 7 and the results are shown in Figure 5 (left).",3.6 Effects of Hop and Aspect Keywords,[1.0],"['For the hop number, we vary m from 1 to 7 and the results are shown in Figure 5 (left).']"
"We can see that: (1) at the word level, the performance increases when m ≤ 4, but shows no improvement after m > 4; (2) at the sentence level, model performs best when m = 2.",3.6 Effects of Hop and Aspect Keywords,[1.0],"['We can see that: (1) at the word level, the performance increases when m ≤ 4, but shows no improvement after m > 4; (2) at the sentence level, model performs best when m = 2.']"
"Moreover, we can see that the hop number of word level leads to larger variation than the hop number of sentence level.
",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"For the size of aspect keywords, we vary N from 0 to 35, incremented by 5.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Note that, we set a learnable vector to represent question memory when N = 0.",3.6 Effects of Hop and Aspect Keywords,[1.0],"['Note that, we set a learnable vector to represent question memory when N = 0.']"
The results are shown in Figure 5 (right).,3.6 Effects of Hop and Aspect Keywords,[1.0],['The results are shown in Figure 5 (right).']
"We observe that the performance increases when N ≤ 20, and has no improvement after N > 20.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
This indicates that a small number of keywords can help the proposed model achieve competitive results.,3.6 Effects of Hop and Aspect Keywords,[0],[0]
Multi-Aspect Sentiment Classification.,4 Related Work,[0],[0]
Multiaspect sentiment classification has been studied extensively in literature.,4 Related Work,[0],[0]
"Lu et al. (2011) used support vector regression model based on hand-
crafted features to predict aspect ratings.",4 Related Work,[0],[0]
"To handle the correlation between aspects, McAuley et al. (2012) added a dependency term in final multi-class SVM objective.",4 Related Work,[0],[0]
"There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014).",4 Related Work,[0],[0]
"However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect.",4 Related Work,[0],[0]
"Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments.",4 Related Work,[0],[0]
"With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work.",4 Related Work,[0],[0]
Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair.,4 Related Work,[0],[0]
"However, these tasks are sentencelevel.",4 Related Work,[0],[0]
Another related research field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task.,4 Related Work,[0],[0]
"This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a hierarchical structure.",4 Related Work,[0],[0]
"However, they did not work on multiple aspects.
",4 Related Work,[0],[0]
Machine Comprehension.,4 Related Work,[0],[0]
"Recently, neural network based machine comprehension (or reading) has been studied extensively in NLP, with the releases of large-scale evaluation datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016).",4 Related Work,[0],[0]
"Most of the related studies focus on attention mechanism (Bahdanau et al., 2014) which is firstly proposed in machine translating and aims to solve the long-distance dependency between words.",4 Related Work,[0],[0]
"Hermann et al. (2015) used BiLSTM to encode document and query, and proposed Attentive Reader and Impatient Reader.",4 Related Work,[0],[0]
"The first one attends document based on the query representation, and the second one attends document by the representation of each token in query with an incremental manner.",4 Related Work,[0],[0]
"Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015) attend and reason document representation in a multihop fashion, enriching interactions between documents and questions.",4 Related Work,[0],[0]
"Dynamic Memory Network (Kumar et al., 2016) updates memories of documents by re-running GRU models based on derived attention weights.",4 Related Work,[0],[0]
"Meanwhile, the query representation is refined by another GRU model.",4 Related Work,[0],[0]
"Gated-Attention Reader (Dhingra et al., 2016) proposes a novel attention mechanism, which is based on multiplicative interactions between the query embeddings and the intermediate states of a recurrent neural network document reader.",4 Related Work,[0],[0]
"BiDirectional Attention Model (Xiong et al., 2017; Seo et al., 2017) fuses co-dependent representations of queries and documents in order to focus on relevant parts of both.",4 Related Work,[0],[0]
"Iterative Attention model (Sordoni et al., 2016) attends question and document sequentially, which is related to our model.",4 Related Work,[0],[0]
"Different from Iterative Attention model, our model focuses on the document-level multiaspect sentiment classification, which is proposed
in a hierarchical architecture and has different procedures in the iterative attention module.",4 Related Work,[0],[0]
Another related research problem is visual question answering which uses an image as question context rather than a set of keywords as question.,4 Related Work,[0],[0]
"Neural network based visual question answering (Lu et al., 2016; Xiong et al., 2016) is similar as the proposed models in text comprehension.",4 Related Work,[0],[0]
"In this paper, we model the document-level multiaspect sentiment classification as a text comprehension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model.",5 Conclusion,[1.0],"['In this paper, we model the document-level multiaspect sentiment classification as a text comprehension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model.']"
Extensive experiments show that our model outperforms the other neural models with multi-task framework and hierarchical architecture.,5 Conclusion,[1.0],['Extensive experiments show that our model outperforms the other neural models with multi-task framework and hierarchical architecture.']
This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61472006 and 91646202) as well as the National Basic Research Program (973 Program No. 2014CB340405).,6 Acknowledgments,[0],[0]
"This work was also supported by NVIDIA Corporation with the donation of the Titan X GPU, Hong Kong CERG Project 26206717, China 973 Fundamental R&D Program (No.2014CB340304), and the LORELEI Contract HR0011-15-2-0025 with DARPA.",6 Acknowledgments,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,6 Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.,6 Acknowledgments,[0],[0]
Document-level multi-aspect sentiment classification is an important task for customer relation management.,abstractText,[0],[0]
"In this paper, we model the task as a machine comprehension problem where pseudo questionanswer pairs are constructed by a small number of aspect-related keywords and aspect ratings.",abstractText,[0],[0]
A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions.,abstractText,[0],[0]
"We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism.",abstractText,[0],[0]
Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines.,abstractText,[0],[0]
Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension,title,[0],[0]
