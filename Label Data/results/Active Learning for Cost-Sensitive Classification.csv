0,1,label2,summary_sentences
Deep learning has significantly advanced our ability to address a wide range of difficult machine learning and signal processing problems.,1. Introduction,[0],[0]
"Today’s machine learning landscape is dominated by deep (neural) networks (DNs), which are compositions of a large number of simple parameterized linear and nonlinear transforms.",1. Introduction,[0],[0]
"An all-too-common story of late is that of plugging a deep network into an application as a black box, training it on copious training data,
1ECE Department, Rice University, Houston, TX, USA.",1. Introduction,[0],[0]
"Correspondence to: Randall B. <randallbalestriero@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"and then significantly improving performance over classical approaches.
",1. Introduction,[0],[0]
"Despite this empirical progress, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field.",1. Introduction,[0],[0]
"Ongoing attempts to build a rigorous mathematical framework fall roughly into five camps: (i) probing and measuring DNs to visualize their inner workings (Zeiler & Fergus, 2014); (ii) analyzing their properties such as expressive power (Cohen et al., 2016), loss surface geometry (Lu & Kawaguchi, 2017; Soudry & Hoffer, 2017), nuisance management (Soatto & Chiuso, 2016), sparsification (Papyan et al., 2017), and generalization abilities; (iii) new mathematical frameworks that share some (but not all) common features with DNs (Bruna & Mallat, 2013); (iv) probabilistic generative models from which specific DNs can be derived (Arora et al., 2013; Patel et al., 2016); and (v) information theoretic bounds (Tishby & Zaslavsky, 2015).
",1. Introduction,[0],[0]
"In this paper, we build a rigorous bridge between DNs and approximation theory via spline functions and operators.",1. Introduction,[0],[0]
"We prove that a large class of DNs — including convolutional neural networks (CNNs) (LeCun, 1998), residual networks (ResNets) (He et al., 2016; Targ et al., 2016), skip connection networks (Srivastava et al., 2015), fully connected networks (Pal & Mitra, 1992), recurrent neural networks (RNNs) (Graves, 2013), and beyond — can be written as spline operators.",1. Introduction,[0],[0]
"In particular, when these networks employ current standard-practice piecewise-affine, convex nonlinearities (e.g., ReLU, max-pooling, etc.)",1. Introduction,[0],[0]
"they can be written as the composition of max-affine spline operators (MASOs) (Magnani & Boyd, 2009; Hannah & Dunson, 2013).",1. Introduction,[0],[0]
"We focus on such nonlinearities here but note that our framework applies also to non-piecewise-affine nonlinearities through a standard approximation argument.
",1. Introduction,[0],[0]
The max-affine spline connection provides a powerful portal through which to view and analyze the inner workings of a DN using tools from approximation theory and functional analysis.,1. Introduction,[0],[0]
"Here is a summary of our key contributions:
[C1] We prove that a large class of DNs can be written as a composition of MASOs, from which it follows immediately that, conditioned on the input signal, the output of a DN is a simple affine transformation of the input.",1. Introduction,[0],[0]
"We illustrate in Section 4 by deriving a closed-form expression for the
input/output mapping of a CNN.
",1. Introduction,[0],[0]
"[C2] The affine mapping formula enables us to interpret a MASO DN as constructing a set of signal-dependent, classspecific templates against which the signal is compared via a simple inner product.",1. Introduction,[0],[0]
"In Section 5 we relate DNs directly to the classical theory of optimal classification via matched filters and provide insights into the effects of data memorization (Zhang et al., 2016).
",1. Introduction,[0],[0]
[C3] We propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal to each other.,1. Introduction,[0],[0]
"In Section 6, we show that this leads to significantly improved classification performance and reduced overfitting on standard test data sets like CIFAR100 with no change to the DN architecture.
",1. Introduction,[0],[0]
"[C4] The partition of the input space induced by a MASO links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up a new geometric avenue to study how DNs cluster and organize signals in a hierarchical fashion.",1. Introduction,[0],[0]
"Section 7 studies the properties of the MASO partition.
",1. Introduction,[0],[0]
"[C5] Leveraging the fact that a DN considers two signals to be similar if they lie in the same MASO partition region, we develop a new signal distance in Section 7.3 that measures the difference between their partition encodings.",1. Introduction,[0],[0]
"The distance is easily computed via backpropagation.
",1. Introduction,[0],[0]
A number of appendices in the Supplementary Material (SM) contain the mathematical setup and proofs.,1. Introduction,[0],[0]
"A significantly extended account of these events with numerous new results is available in (Balestriero & Baraniuk, 2018).",1. Introduction,[0],[0]
A deep network (DN) is an operator fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC that maps an input signal1 x ∈ RD to an output prediction ŷ ∈ RC as fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC .,2. Background on Deep Networks,[0],[0]
"All current DNs can be written as a composition of L intermediate mappings called layers
fΘ(x) =",2. Background on Deep Networks,[0],[0]
( f (L) θ(L),2. Background on Deep Networks,[0],[0]
◦ · · · ◦,2. Background on Deep Networks,[0],[0]
f (1) θ(1) ),2. Background on Deep Networks,[0],[0]
"(x), (1)
where Θ = { θ(1), . . .",2. Background on Deep Networks,[0],[0]
", θ(L) } is the collection of the network’s parameters from each layer.",2. Background on Deep Networks,[0],[0]
"This composition of mappings is nonlinear and non-commutative, in general.
",2. Background on Deep Networks,[0],[0]
A DN layer at level ` is an operator f (`) θ(`) that takes as input the vector-valued signal z(`−1)(x) ∈ RD(`−1) and produces the vector-valued output z(`)(x) ∈ RD(`) .,2. Background on Deep Networks,[0],[0]
We will assume that x and z(`) are column vectors.,2. Background on Deep Networks,[0],[0]
We initialize with z(0)(x) =,2. Background on Deep Networks,[0],[0]
"x and denote z(L)(x) =: z for convenience.
1",2. Background on Deep Networks,[0],[0]
"For concreteness, we focus here on processing K-channel images x, such as color digital photographs.",2. Background on Deep Networks,[0],[0]
"But our analysis and techniques apply to signals of any index-dimensionality, including speech and audio signals, video signals, etc.
",2. Background on Deep Networks,[0],[0]
"The signals z(`)(x) are typically called feature maps; it is easy to see that
z(`)(x) =",2. Background on Deep Networks,[0],[0]
"( f (`)
θ(`) ◦ · · · ◦ f (1) θ(1)
) (x), ` ∈ {1, . . .",2. Background on Deep Networks,[0],[0]
", L}.",2. Background on Deep Networks,[0],[0]
"(2)
We briefly overview the basic DN operators and layers we consider in this paper; more details and additional layers are provided in (Goodfellow et al., 2016) and (Balestriero & Baraniuk, 2018).",2. Background on Deep Networks,[0],[0]
"A fully connected operator performs an arbitrary affine transformation by multiplying its input by the dense matrix W (`) ∈ RD(`)×D(`−1) and adding the arbitrary bias vector b(`)W ∈ RD (`) , as in f (`)W ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= W (`)z(`−1)(x) + b (`) W .,2. Background on Deep Networks,[0],[0]
"A convolution operator reduces the number of parameters in the affine transformation by replacing the unconstrained W (`) with a multichannel convolution matrix, as in f (`)C ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= C(`)z(`−1)(x) +,2. Background on Deep Networks,[0],[0]
"b (`) C .
",2. Background on Deep Networks,[0],[0]
"An activation operator applies a scalar nonlinear activation function σ independently to each entry of its input, as in[ f (`) σ",2. Background on Deep Networks,[0],[0]
( z(`−1)(x) ),2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
k := σ,2. Background on Deep Networks,[0],[0]
"( [z(`−1)(x)]k ) , k = 1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"Nonlinearities are crucial to DNs, since otherwise the entire network would collapse to a single global affine transform.",2. Background on Deep Networks,[0],[0]
Three popular activation functions are the rectified linear unit (ReLU) σReLU(u),2. Background on Deep Networks,[0],[0]
":= max(u, 0), the leaky ReLU σLReLU(u)",2. Background on Deep Networks,[0],[0]
":= max(ηu, u), η > 0, and the absolute value σabs(u) := |u|.",2. Background on Deep Networks,[0],[0]
These three functions are both piecewise affine and convex.,2. Background on Deep Networks,[0],[0]
Other popular activation functions include the sigmoid σsig(u) := 11+e−u and hyperbolic tangent σtanh(u) := 2σsig(2u)−1.,2. Background on Deep Networks,[0],[0]
"These two functions are neither piecewise affine nor convex.
",2. Background on Deep Networks,[0],[0]
"A pooling operator subsamples its input to reduce its dimensionality according to a sub-sampling policy ρ applied over a collection of input indices {Rk}K (`)
k=1 (typically a small patch), e.g., max pooling[ f (`) ρ ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
"k
:= max d∈R(`)k
[ z(`−1)(x) ]",2. Background on Deep Networks,[0],[0]
"d , k =
1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"See (Balestriero & Baraniuk, 2018) for the definitions of average pooling, channel pooling, skip connections, and recurrent layers.
",2. Background on Deep Networks,[0],[0]
Definition 1.,2. Background on Deep Networks,[0],[0]
"A DN layer f (`) θ(`)
comprises a single nonlinear DN operator (non-affine to be precise) composed with any preceding affine operators lying between it and the preceding nonlinear operator.
",2. Background on Deep Networks,[0],[0]
"This definition yields a single, unique layer decomposition for any DN, and the complete DN is then the composition of its layers per (1).",2. Background on Deep Networks,[0],[0]
"For example, in a standard CNN, there are two different layers types: i) convolution-activation and ii) max-pooling.
",2. Background on Deep Networks,[0],[0]
We form the prediction ŷ by feeding fΘ(x) through a final nonlinearity g : RD(L) → RD(L) as in ŷ = g(fΘ(x)).,2. Background on Deep Networks,[0],[0]
"In classification, g is typically the softmax nonlinearity, which arises naturally from posing the classification inference as a
multinomial logistic regression problem (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"In regression, typically no g is applied.
",2. Background on Deep Networks,[0],[0]
"We learn the DN parameters Θ for a particular prediction task in a supervised setting using a labeled data set D = (xn,yn) N n=1, a loss function, and a learning policy to update the parameters Θ in the predictor fΘ(x).",2. Background on Deep Networks,[0],[0]
"For classification problems, the loss function is typically the negative cross-entropy LCE(x,y) (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"For regression problems, the loss function is typically is the squared error.",2. Background on Deep Networks,[0],[0]
"Since the layer-by-layer operations in a DN are differentiable almost everywhere with respect to their parameters and inputs, we can use some flavor of first-order optimization such as gradient descent to optimize the parameters Θ with respect to the loss function.",2. Background on Deep Networks,[0],[0]
"Moreover, the gradients for all internal parameters can be computed efficiently by backpropagation (Hecht-Nielsen, 1992), which follows from the chain rule of calculus.",2. Background on Deep Networks,[0],[0]
"Approximation theory is the study of how and how well functions can best be approximated using simpler functions (Powell, 1981).",3. Background on Spline Operators,[0],[0]
"A classical example of a simpler function is a spline s : RD → R (Schmidhuber, 1994).",3. Background on Spline Operators,[0],[0]
"For concreteness, we will work exclusively with affine splines in this paper (aka “linear splines”), but our ideas generalize naturally to higher-order splines.
",3. Background on Spline Operators,[0],[0]
Multivariate Affine Splines.,3. Background on Spline Operators,[0],[0]
"Consider a partition of a domain RD into a set of regions Ω = {ω1, . . .",3. Background on Spline Operators,[0],[0]
", ωR}",3. Background on Spline Operators,[0],[0]
"and a set of local mappings Φ = {φ1, . . .",3. Background on Spline Operators,[0],[0]
", φR} that map each region in the partition to R via φr(x) := 〈[α]r,·,x〉+ [β]r for x ∈ ωr.2",3. Background on Spline Operators,[0],[0]
"The parameters are: α ∈ RR×D, a matrix of hyperplane “slopes,” and β ∈ RR, a vector of hyperplane “offsets” or “biases”.",3. Background on Spline Operators,[0],[0]
We will use the terms offset and bias interchangeably in the sequel.,3. Background on Spline Operators,[0],[0]
"The notation [α]r,· denotes the column vector formed from the rth row of α.
",3. Background on Spline Operators,[0],[0]
"With this setup, the multivariate affine spline is defined as
s[α, β,Ω](x) = R∑ r=1 (〈[α]r,·,x〉+ [β]r)1(x ∈ ωr)
",3. Background on Spline Operators,[0],[0]
"=: 〈α[x],x〉+ β[x], (3)
where 1(x ∈ ωr) is the indicator function.",3. Background on Spline Operators,[0],[0]
The second line of (3) introduces the streamlined notation α[x] =,3. Background on Spline Operators,[0],[0]
"[α]r,· when x ∈ ωr; the definition for β[x] is similar.",3. Background on Spline Operators,[0],[0]
Such a spline is piecewise affine and hence piecewise convex.,3. Background on Spline Operators,[0],[0]
"However, in general, it is neither globally affine nor globally convex unless R = 1, a case we denote as a degenerate spline, since it corresponds simply to an affine mapping.
",3. Background on Spline Operators,[0],[0]
"2 To make the connection between splines and DNs more immediately obvious, here x is interpreted as a point in RD , which plays the rôle of the space of signals in the other sections.
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Functions.,3. Background on Spline Operators,[0],[0]
"A major complication of function approximation with splines in general is the need to jointly optimize both the spline parameters α, β and the input domain partition Ω (the “knots” for a 1D spline) (Bennett & Botkin, 1985).",3. Background on Spline Operators,[0],[0]
"However, if a multivariate affine spline is constrained to be globally convex, then it can always be rewritten as a max-affine spline (Magnani & Boyd, 2009; Hannah & Dunson, 2013)
s[α, β,Ω](x) = max r=1,...,R
〈[α]r,·,x〉+ [β]r .",3. Background on Spline Operators,[0],[0]
"(4)
An extremely useful feature of such a spline is that it is completely determined by its parameters α and β without needing to specify the partition Ω.",3. Background on Spline Operators,[0],[0]
"As such, we denote a max-affine spline simply as s[α, β].",3. Background on Spline Operators,[0],[0]
"Changes in the parameters α, β of a max-affine spline automatically induce changes in the partition Ω, meaning that they are adaptive partitioning splines (Magnani & Boyd, 2009).
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Operators.,3. Background on Spline Operators,[0],[0]
"A natural extension of an affine spline function is an affine spline operator (ASO) S[A,B,ΩS ] that produces a multivariate output.",3. Background on Spline Operators,[0],[0]
It is obtained simply by concatenating K affine spline functions from (3).,3. Background on Spline Operators,[0],[0]
"The details and a more general development are provided in the SM and (Balestriero & Baraniuk, 2018).
",3. Background on Spline Operators,[0],[0]
"We are particularly interested in the max-affine spline operator (MASO) S[A,B] :",3. Background on Spline Operators,[0],[0]
RD → RK formed by concatenating K independent max-affine spline functions from (4).,3. Background on Spline Operators,[0],[0]
"A MASO with slope parameters A ∈ RK×R×D and offset parameters B ∈ RK×R is defined as
S[A,B](x) =  maxr=1,...,R〈[A]1,r,·,x〉+ [B]1,r... maxr=1,...,R〈[A]K,r,·,x〉+ [B]K,r  =: A[x]x",3. Background on Spline Operators,[0],[0]
+B[x].,3. Background on Spline Operators,[0],[0]
"(5)
The second line of (5) introduces the streamlined notation in terms of the signal-dependent matrix A[x] and signal-dependent vector B[x], where [A[x]]k,· := [A]k,rk(x),· and [B[x]]k := [B]k,rk(x) with rk(x) = arg maxr〈[A]k,r,·,x〉+ [B]k,r.
Max-affine spline functions and operators are always piecewise affine and globally convex (and hence also continuous) with respect to each output dimension.",3. Background on Spline Operators,[0],[0]
"Conversely, any piecewise affine and globally convex function/operator can be written as a max-affine spline.",3. Background on Spline Operators,[0],[0]
"Moverover, using standard approximation arguments, it is easy to show that a MASO can approximate arbitrarily closely any (nonlinear) operator that is convex in each output dimension.",3. Background on Spline Operators,[0],[0]
"While a MASO is appropriate only for approximating convex functions/operators, we now show that virtually all of
today’s DNs can be written as a composition of MASOs, one for each layer.",4. DNs are Compositions of Spline Operators,[0],[0]
"Such a composition is, in general, nonconvex and hence can approximate a much larger class of functions/operators.",4. DNs are Compositions of Spline Operators,[0],[0]
"Interestingly, under certain broad conditions, the composition remains a piecewise affine spline operator, which enables a variety of insights into DNs.",4. DNs are Compositions of Spline Operators,[0],[0]
"We now state our main theoretical results, which are proved in the SM and elaborated in (Balestriero & Baraniuk, 2018).
",4.1. DN Operators are MASOs,[0],[0]
Proposition 1.,4.1. DN Operators are MASOs,[0],[0]
An arbitrary fully connected operator f (`)W is an affine mapping and hence a degenerate MASO S,4.1. DN Operators are MASOs,[0],[0]
"[ A
(`) W , B (`) W ] , with R = 1, [A(`)W ]k,1,· = [ W (`) ]",4.1. DN Operators are MASOs,[0],[0]
"k,· and
[B (`) W ]k,1 =
[ b
(`) W ] k , leading to W (`)z(`−1)(x) + b(`)W",4.1. DN Operators are MASOs,[0],[0]
"=
A (`)",4.1. DN Operators are MASOs,[0],[0]
W,4.1. DN Operators are MASOs,[0],[0]
[x]z (`−1)(x) +B (`) W,4.1. DN Operators are MASOs,[0],[0]
[x].,4.1. DN Operators are MASOs,[0],[0]
"The same is true of a convolution operator with W (`), b(`)W replaced by C (`), b (`) C .",4.1. DN Operators are MASOs,[0],[0]
Proposition 2.,4.1. DN Operators are MASOs,[0],[0]
"Any activation operator f (`)σ using a piecewise affine and convex activation function is a MASO S [ A (`) σ ,B (`) σ ] with R = 2, [ B (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1 = [ B (`) σ ] k,2 =
0 ∀k, and for ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = 0, [ A (`) σ ] k,2,· =
ek ∀k; for leaky ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = νek, [ A (`) σ ] k,2,· =
ek ∀k, ν > 0; and for absolute value [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,·
= −ek,[ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,2,· = ek ∀k, where ek represents the kth canonical basis element of RD(`) .
",4.1. DN Operators are MASOs,[0],[0]
Proposition 3.,4.1. DN Operators are MASOs,[0],[0]
"Any pooling operator f (`)ρ that is piecewise affine and convex is a MASO S [ A (`) ρ ,B (`) ρ ]",4.1. DN Operators are MASOs,[0],[0]
.3,4.1. DN Operators are MASOs,[0],[0]
"Max-
pooling has R = #Rk (typically a constant over all output dimensions k), [ A (`) ρ ] k,·,·
= {ei, i ∈ Rk}, and[ B (`) ρ ] k,r = 0 ∀k, r. Average-pooling is a degenerate
MASO with R = 1, [ A (`) ρ ] k,1,· = 1#(Rk) ∑ i∈Rk ei, and[
B (`) ρ ] k,1 = 0 ∀k.
",4.1. DN Operators are MASOs,[0],[0]
Proposition 4.,4.1. DN Operators are MASOs,[0],[0]
"A DN layer constructed from an arbitrary composition of fully connected/convolution operators followed by one activation or pooling operator is a MASO S[A(`), B(`)] such that
f (`)(z(`−1)(x))",4.1. DN Operators are MASOs,[0],[0]
= A(`)[x]z(`−1)(x) +B(`)[x].,4.1. DN Operators are MASOs,[0],[0]
"(6)
Consequently, a large class of DNs boil down to a composition of MASOs.",4.1. DN Operators are MASOs,[0],[0]
"We prove the following in the SM and in (Balestriero & Baraniuk, 2018) for CNNs, ResNets, skip connection nets, fully connected nets, and RNNs.
3",4.1. DN Operators are MASOs,[0],[0]
"This result is agnostic to the pooling type (spatial or channel).
",4.1. DN Operators are MASOs,[0],[0]
Theorem 1.,4.1. DN Operators are MASOs,[0],[0]
"A DN constructed from an arbitrary composition of fully connected/convolution, activation, and pooling operators of the types in Propositions 1–3 is a composition of MASOs that is equivalent to a global affine spline operator.
",4.1. DN Operators are MASOs,[0],[0]
"Note carefully that, while the layers of each of the DNs stated in Theorem 1 are MASOs, the composition of several layers is not necessarily a MASO.",4.1. DN Operators are MASOs,[0],[0]
"Indeed, a composition of MASOs remains a MASO if and only if all of its component operators (except the first) are non-decreasing with respect to each of their output dimensions (Boyd & Vandenberghe, 2004).",4.1. DN Operators are MASOs,[0],[0]
"Interestingly, ReLU and max-pooling are both nondecreasing, while leaky ReLU is strictly increasing.",4.1. DN Operators are MASOs,[0],[0]
"The culprits causing non-convexity of the composition of layers are negative entries in the fully connected or convolution operators, which destroy the required non-increasing property.",4.1. DN Operators are MASOs,[0],[0]
"A DN where these culprits are thwarted is an interesting special case, because it is convex with respect to its input (Amos et al., 2016) and multiconvex (Xu & Yin, 2013) with respect to its parameters.
",4.1. DN Operators are MASOs,[0],[0]
Theorem 2.,4.1. DN Operators are MASOs,[0],[0]
"A DN whose layers ` = 2, . . .",4.1. DN Operators are MASOs,[0],[0]
", L consist of an arbitrary composition of fully connected and convolution operators with nonnegative weights, i.e., W (`)k,j ≥ 0, C (`) k,j ≥ 0; non-decreasing, piecewise-affine, and convex activation operators; and non-decreasing, piecewise-affine, and convex pooling operators is globally a MASO and thus also globally convex with respect to each of its output dimensions.
",4.1. DN Operators are MASOs,[0],[0]
"The above results pertain to DNs using convex, affine operators.",4.1. DN Operators are MASOs,[0],[0]
"Other popular non-convex DN operators (e.g., the sigmoid and arctan activation functions) can be approximated arbitrarily closely by an affine spline operator but not by a MASO.
DNs are Signal-Dependent Affine Transformations.",4.1. DN Operators are MASOs,[0],[0]
"A common theme of the above results is that, for DNs constructed from fully connected/convolution, activation, and pooling operators from Propositions 1–3, the operator/layer outputs z(`)(x) are always a signal-dependent affine function of the input x (recall (5)).",4.1. DN Operators are MASOs,[0],[0]
The particular affine mapping applied to x depends on which partition of the spline it falls in RD.,4.1. DN Operators are MASOs,[0],[0]
"More on this in Section 7 below.
",4.1. DN Operators are MASOs,[0],[0]
DN Learning and MASO Parameters.,4.1. DN Operators are MASOs,[0],[0]
"Given labeled training data (xn,yn)Nn=1, learning in a DN that meets the conditions of Theorem 1 (i.e., optimizing its parameters Θ) is equivalent to optimally approximating the mapping from input x to output ŷ =",4.1. DN Operators are MASOs,[0],[0]
g ( z(L)(x) ),4.1. DN Operators are MASOs,[0],[0]
"using an appropriate cost function (e.g., cross-entropy for classification or squared error for regression) by learning the parameters θ(`) of the layers.",4.1. DN Operators are MASOs,[0],[0]
"In general the overall optimization problem is nonconvex (it is actually piecewise multi-convex in general (Rister, 2016)).
",4.1. DN Operators are MASOs,[0],[0]
"z (L) CNN(x) = W (L)
( 1∏
`=L−1
A(`)ρ",4.1. DN Operators are MASOs,[0],[0]
[x]A (`) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (`) ) ︸,4.1. DN Operators are MASOs,[0],[0]
"︷︷ ︸
ACNN[x]
x + W (L) L−1∑",4.1. DN Operators are MASOs,[0],[0]
`=1  `+1∏ j=L−1 A(j)ρ [x]A (j) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (j) (A(`)ρ [x]A(`)σ [x]b(`)C )︸ ︷︷ ︸,4.1. DN Operators are MASOs,[0],[0]
"BCNN[x] +b (L) W
(7)",4.1. DN Operators are MASOs,[0],[0]
"Combining Propositions 1–3 and Theorem 1 and substituting (5) into (2), we can write an explicit formula for the output of any layer z(`)(x) of a DN in terms of the input x for a variety of different architectures.",4.2. Application: DN Affine Mapping Formula,[0],[0]
"The formula for a standard CNN (using ReLU activation and max-pooling) is given in (7) above; we derive this formula and analogous formulas for ResNets and RNNs in (Balestriero & Baraniuk, 2018).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"In (7), A(`)σ [x] are the signal-dependent matrices corresponding to the ReLU activations,",4.2. Application: DN Affine Mapping Formula,[0],[0]
A(`)ρ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] are the signal-dependent matrices corresponding to maxpooling, and the biases b(L)W , b (`) C arise directly from the fully connected and convolution operators.",4.2. Application: DN Affine Mapping Formula,[0],[0]
The absence of B (`) σ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x], B (`) ρ",4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] is due to the absence of bias in the ReLU (recall (2)) and max-pooling operators (recall (3)).
",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Inspection of (7) reveals the exact form of the signaldependent, piecewise affine mapping linking x to z(L)CNN(x).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Moreover, this formula can be collapsed into
z (L) CNN(x) = W (L) ( ACNN[x]x +BCNN[x] )",4.2. Application: DN Affine Mapping Formula,[0],[0]
"+ b (L) W (8)
from which we can recognize
z",4.2. Application: DN Affine Mapping Formula,[0],[0]
(L−1) CNN (x) = ACNN[x]x,4.2. Application: DN Affine Mapping Formula,[0],[0]
"+BCNN[x] (9)
as an explicit, signal-dependent, affine formula for the featurization process that aims to convert x into a set of (hopefully) linearly separable features that are then input to the linear classifier in layer ` = L with parameters W (L) and b
(L) W .",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Of course, the final prediction ŷ is formed by running z (L) CNN(x) through a softmax nonlinearity g, but this merely rescales its entries to create a probability distribution.",4.2. Application: DN Affine Mapping Formula,[0],[0]
We now dig deeper into (8) in order to bridge DNs and classical optimal classification theory.,5. DNs are Template Matching Machines,[0],[0]
"While we focus on CNNs and classification for concreteness, our analysis holds for any DN meeting the conditions of Theorem 1.",5. DNs are Template Matching Machines,[0],[0]
An alternate interpretation of (8) is that z(L)CNN(x) is the output of a bank of linear matched filters (plus a set of biases).,5.1. Template Matching,[0],[0]
"That is, the cth element of z(L)(x) equals the inner product between the signal x and the matched filter for the cth class, which is contained in the cth row of the matrix
W (L)A[x].",5.1. Template Matching,[0],[0]
"The bias W (L)B[x] + b(L)W can be used to account for the fact that some classes might be more likely than others (i.e., the prior probability over the classes).",5.1. Template Matching,[0],[0]
"It is well-known that a matched filterbank is the optimal classifier for deterministic signals in additive white Gaussian noise (Rabiner & Gold, 1975).",5.1. Template Matching,[0],[0]
"Given an input x, the class decision is simply the index of the largest element of z(L)(x).4
Yet another interpretation of (8) is that z(L)(x) is computed not in a single matched filter calculation but hierarchically as the signal propagates through the DN layers.",5.1. Template Matching,[0],[0]
Abstracting (5) to write the per-layer maximization process as z(`)(x) = maxr(`),5.1. Template Matching,[0],[0]
A (`) r(`) z(`−1)(x),5.1. Template Matching,[0],[0]
"+B (`) r(`) and cascading, we obtain a formula for the end-to-end DN mapping
z(L)(x) =",5.1. Template Matching,[0],[0]
"W (L) max r(L−1)
",5.1. Template Matching,[0],[0]
"( A
(L−1) r(L−1)
max r(2)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .
",5.1. Template Matching,[0],[0]
"max r(1)
",5.1. Template Matching,[0],[0]
"( A (1)
r(1) x + B (1) r(1)
) + B (2)
r(2)
) · · ·+ B(L−1)
r(L−1)
) + b
(L) W .
",5.1. Template Matching,[0],[0]
"(10)
",5.1. Template Matching,[0],[0]
"This formula elucidates that a DN performs a hierarchical, greedy template matching on its input, a computationally efficient yet sub-optimal template matching technique.",5.1. Template Matching,[0],[0]
Such a procedure is globally optimal when the DN is globally convex.,5.1. Template Matching,[0],[0]
Corollary 1.,5.1. Template Matching,[0],[0]
"For a DN abiding by the requirements of Theorem 2, the computation (10) collapses to the following globally optimal template matching
z(L−1)(x)",5.1. Template Matching,[0],[0]
"= W (L) max r(L−1),r(2),...,r(1)
( A
(L−1) r(L−1)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .",5.1. Template Matching,[0],[0]
"(
A (1)
r(1) x + B
(1) r(1)
) +",5.1. Template Matching,[0],[0]
"B (2)
r(2)
) · · ·+ B(L−1
r(L−1)
)",5.1. Template Matching,[0],[0]
"+ b
(L) W .
(11)",5.1. Template Matching,[0],[0]
"Since the complete DN mapping (up to the final softmax) can be expressed as in (8), given a signal x, we can compute the signal-dependent template for class c via A[x]c =",5.2. Template Visualization Examples,[0],[0]
"d[z(L)(x)]c dx , which can be efficiently computed via backpropogation (Hecht-Nielsen, 1992).5",5.2. Template Visualization Examples,[0],[0]
"Once the template A[x]c has been computed, the bias term b[x]c can be computed via b[x]c = z(L)(x)c − 〈A[x]c,·,x〉.",5.2. Template Visualization Examples,[0],[0]
"Figure 1 plots
4Again, since the softmax merely rescales the entries of z(L)(x) into a probability distribution, it does not affect the location of its largest element.
",5.2. Template Visualization Examples,[0],[0]
"5In fact, we can use the same backpropagation procedure used for computing the gradient with respect to a fully connected or
various signal-dependent templates for two CNNs trained on the MNIST and CIFAR10 datasets.",5.2. Template Visualization Examples,[0],[0]
"Under the matched filterbank interpretation of a DN developed in Section 5.1, the optimal template for an image x of class c is a scaled version of x itself.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
But what are the optimal templates for the other (incorrect) classes?,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In an idealized setting, we can answer this question.
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
Proposition 5.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Consider an idealized DN consisting of a composition of MASOs that has sufficient approximation power to span arbitrary MASO matrices A[xn] from (9) for any input xn from the training set.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"Train the DN to classify among C classes using the training data D = (xn, yn)Nn=1 with normalized inputs ‖xn‖2 = 1 ∀n and the cross-entropy loss LCE(yn, fΘ(xn)) with the addition of the regularization constraint that ∑ c ‖A[xn]c,·‖2",5.3. Collinear Templates and Data Set Memorization,[0],[0]
< α with α > 0.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"At the global minimum of this constrained optimization problem, the rows of A?[xn] (the optimal templates) have the form:
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"[A?[xn]]c,· =  + √ (C−1)α C xn, c = yn − √
α C(C−1) xn, c 6=",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"yn
(12)
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In short, the idealized CNN in the proposition will memorize a set of collinear templates whose bimodal outputs force
convolution weight but instead with the input x.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"This procedure is becoming increasingly popular in the study of adversarial examples (Szegedy et al., 2013).
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
the softmax output to a Dirac delta function (aka 1-hot representation) that peaks at the correct class.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Figure 2 confirms this bimodal behavior on the MNIST and CIFAR10 datasets.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"While a DN’s signal-dependent matched filterbank (8) is optimized for classifying signals immersed in additive white Gaussian noise, such a statistical model is overly simplistic for most machine learning problems of interest.",6. New DNs with Orthogonal Templates,[0],[0]
"In practice, errors will arise not just from random noise but also from nuisance variations in the inputs such as arbitrary rotations, positions, and modalities of the objects of interest.",6. New DNs with Orthogonal Templates,[0],[0]
The effects of these nuisances are only poorly approximated as Gaussian random errors.,6. New DNs with Orthogonal Templates,[0],[0]
"Limited work has been done on filterbanks for classification in nonGaussian noise; one promising direction involves using not matched but rather orthogonal templates (Eldar & Oppenheim, 2001).
",6. New DNs with Orthogonal Templates,[0],[0]
"For a MASO DN’s templates to be orthogonal for all inputs, it is necessary that the rows of the matrix W (L) in the final linear classifier layer be orthogonal.",6. New DNs with Orthogonal Templates,[0],[0]
"This weak constraint on the DN still enables the earlier layers to create a high-performance, class-agnostic, featurized representation (recall the discussion just below (9)).",6. New DNs with Orthogonal Templates,[0],[0]
"To create orthogonal templates during learning, we simply add to the standard (potentially regularized) cross-entropy loss function LCE a term that penalizes non-zero off-diagonal entries in the matrix W (L)(W (L))T leading to the new loss with the additional penalty
LCE + λ ∑ c1 6=c2 ∣∣∣〈[W",6. New DNs with Orthogonal Templates,[0],[0]
"(L)] c1,· , [ W (L) ]",6. New DNs with Orthogonal Templates,[0],[0]
"c2,· 〉∣∣∣2 .",6. New DNs with Orthogonal Templates,[0],[0]
"(13) The parameter λ controls the tradeoff between cross-entropy
minimization and orthogonality preservation.",6. New DNs with Orthogonal Templates,[0],[0]
"Conveniently, when minimizing (13) via backpropagation, the orthogonal rows of W (L) induce orthogonal backpropagation updates for the various classes.
",6. New DNs with Orthogonal Templates,[0],[0]
We now empirically demonstrate that orthogonal templates lead to significantly improved classification performance.,6. New DNs with Orthogonal Templates,[0],[0]
"We conducted a range of experiments with three different conventional DN architectures – smallCNN, largeCNN, and ResNet4-4 – trained on three different datasets – SVHN, CIFAR10, and CIFAR100.",6. New DNs with Orthogonal Templates,[0],[0]
"Each DN employed bias units, ReLU activations, and max-pooling as well as batchnormalization prior each ReLU.",6. New DNs with Orthogonal Templates,[0],[0]
"The full experimental details are given in the (Balestriero & Baraniuk, 2018).",6. New DNs with Orthogonal Templates,[0],[0]
"For learning, we used the Adam optimizer with an exponential learning rate decay.",6. New DNs with Orthogonal Templates,[0],[0]
All inputs were centered to zero mean and scaled to a maximum value of one.,6. New DNs with Orthogonal Templates,[0],[0]
"No further preprocessing was performed, such as ZCA whitening (Nam et al., 2014).",6. New DNs with Orthogonal Templates,[0],[0]
We assessed how the classification performance of a given DN would change as we varied the orthogonality penalty λ in (13).,6. New DNs with Orthogonal Templates,[0],[0]
"For each configuration of DN architecture, training dataset, learning rate, and penalty λ, we averaged over 15 runs to estimate the average performance and standard deviation.
",6. New DNs with Orthogonal Templates,[0],[0]
We report here on only the CIFAR100 with largeCNN experiments.,6. New DNs with Orthogonal Templates,[0],[0]
"(See (Balestriero & Baraniuk, 2018) for detailed results for all three datasets and the other architectures.",6. New DNs with Orthogonal Templates,[0],[0]
The trends for all three datasets are similar and are independent of the learning rate.),6. New DNs with Orthogonal Templates,[0],[0]
The results for CIFAR100 in Figure 3 indicate that the benefits of the orthogonality penalty emerge distinctly as soon as λ > 0.,6. New DNs with Orthogonal Templates,[0],[0]
"In addition to improved final accuracy and generalization performance, we see that template orthogonality reduces the temptation of the DN to overfit.",6. New DNs with Orthogonal Templates,[0],[0]
"(This is is especially visible in the examples in (Balestriero & Baraniuk, 2018).)",6. New DNs with Orthogonal Templates,[0],[0]
One explanation is that the orthogonal weights W (L) positively impact not only the prediction but also the backpropagation via orthogonal gradient updates with respect to each output dimension’s partial derivatives.,6. New DNs with Orthogonal Templates,[0],[0]
"Like any spline, it is the interplay between the (affine) spline mappings and the input space partition that work the magic in a MASO DN.",7. DN’s Intrinsic Multiscale Partition,[0],[0]
Recall from Section 3 that a MASO has the attractive property that it implicitly partitions its input space as a function of its slope and offset parameters.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
The induced partition Ω opens up a new geometric avenue to study how a DN clusters and organizes signals in a hierarchical fashion.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
"A DN operator at level ` directly influences the partitioning of its input space RD(`−1) and indirectly influences the partitioning of the overall signal space RD.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
A ReLU activation operator splits each of its input dimensions into two half-planes depending on the sign of the input in each dimension.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"This partitions RD(`−1) into a combinatorially large number (up to 2D (`)
) of regions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Following a fully connected or convolution operator with a ReLU simply rotates the partition in RD(`−1) .,7.1. Effect of the DN Operators on the Partition,[0],[0]
"A max-pooling operator also partitions RD(`−1) into a combinatorially large number (up to #RD (`)
) of regions, where #R is the size of the pooling region.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
This per-MASO partitioning of each layer’s input space constructs an overall partitioning of the input signal space RD.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"As each MASO is applied, it subdivides the input space RD into finer and finer partitions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"The final partition corresponds to the intersection of all of the intermediate partitions, and hence we can encode the input in terms of the ordered collection of per-layer partition regions into which it falls.",7.1. Effect of the DN Operators on the Partition,[0],[0]
This overall process can be interpreted as a hierarchical vector quantization (VQ) of the training input signals xn.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"There are thus many potential connections between DNs and optimal quantization, information theory, and clustering that we leave for future research.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"See (Balestriero & Baraniuk, 2018) for some early results.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Unfortunately there is no simple formula for the partition of the signal space.,7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"However, once can obtain the set of inputs signals xn that fall into the same partition region at each layer of a DN.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"At layer `, denote the index of the region selected by the input x (recall (6)) by[ t(`)(x) ]",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"k
= arg max r
〈",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[A(`)]k,r,·, z (`−1)(x) 〉 +",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[B(`)]k,r.
(14)
Thus, [t(`)]k ∈ {1, . . .",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
", R(`)}, with R(`) the number of partition regions in the layer’s input space.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Encoding the partition as an ordered collection of integers designating the activate hyperplane parameters from (4), we can now visualize which inputs fall into the same or nearby partitions.
",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Due to the very large number of possible regions (up to 2D (`) for a ReLU at layer `) and the limited amount of training data, in general, many partitions will be empty or contain only a single training data point.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"To validate the utility of the hierarchical intrinsic clustering induced by a DN, we define a new distance function between the signals x1 and x2 that quantifies the similarity of their position encodings t`(x1) and t`(x2) at layer ` via
d ( t(`)(x1), t (`)(x2) )
",7.3. A New Image Distance based on the DN Partition,[0],[0]
"= 1− ∑D(`) k=1 1 ( [t(`)(x1)]k = [t (`)(x2)]k )
D(`) .",7.3. A New Image Distance based on the DN Partition,[0],[0]
"(15)
For a ReLU MASO, this corresponds simply to counting how many entries of the layer inputs for x1 and x2 are positive or negative at the same positions.",7.3. A New Image Distance based on the DN Partition,[0],[0]
"For a max-pooling
MASO, this corresponds to counting how many argmax positions are the same in each patch for x1 and x2.
",7.3. A New Image Distance based on the DN Partition,[0],[0]
Figure 4 provides a visualization of the nearest neighbors of a test image under this partition-based distance measure.,7.3. A New Image Distance based on the DN Partition,[0],[0]
"Visual inspection of the figures highlights that, as we progress through the layers of the DN, similar images become closer in the new distance but further in Euclidean distance.",7.3. A New Image Distance based on the DN Partition,[0],[0]
We have used the theory of splines to build a rigorous bridge between deep networks (DNs) and approximation theory.,8. Conclusions,[0],[0]
"Our key finding is that, conditioned on the input signal, the output of a DN can be written as a simple affine transformation of the input.",8. Conclusions,[0],[0]
"This links DNs directly to the classical theory of optimal classification via matched filters and provides insights into the positive effects of data memorization.
",8. Conclusions,[0],[0]
"There are many avenues for future work, including a more in-depth analysis of the hierarchical MASO partitioning, particularly from the viewpoint of vector quantization and K-means clustering, which are unsupervised learning techniques, and information theory.",8. Conclusions,[0],[0]
The spline viewpoint also could inspire the creation of new DN layers that have certain attractive partitioning or approximation capabilities.,8. Conclusions,[0],[0]
"We have begun exploring some of these directions in (Balestriero & Baraniuk, 2018).6
6 This work was partially supported by ARO grant W911NF-151-0316, AFOSR grant FA9550-14-1-0088, ONR grants N0001417-1-2551 and N00014-18-12571, DARPA grant G001534-7500, and a DOD Vannevar Bush Faculty Fellowship (NSSEFF) grant N00014-18-1-2047.",8. Conclusions,[0],[0]
We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators.,abstractText,[0],[0]
"Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings.",abstractText,[0],[0]
"For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input.",abstractText,[0],[0]
"This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization.",abstractText,[0],[0]
"Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture.",abstractText,[0],[0]
The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion.,abstractText,[0],[0]
"As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.",abstractText,[0],[0]
A Spline Theory of Deep Networks,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis.",1 Introduction,[0],[0]
"It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006;
∗Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041).
",1 Introduction,[0],[0]
"Yoshida et al., 2014).",1 Introduction,[0],[0]
"Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016).",1 Introduction,[0],[0]
"In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014).",1 Introduction,[0],[0]
"This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs.
",1 Introduction,[0],[0]
"Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015).",1 Introduction,[0],[0]
"Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014).",1 Introduction,[0],[0]
"However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance.",1 Introduction,[0],[0]
"Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014).",1 Introduction,[0],[0]
"They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and
2263
Zhao, 2016), also including discourse parsing.",1 Introduction,[0],[0]
Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics.,1 Introduction,[0],[0]
Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance.,1 Introduction,[0],[0]
"Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving.
",1 Introduction,[0],[0]
"The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives.",1 Introduction,[0],[0]
Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments.,1 Introduction,[0],[0]
"The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments.",1 Introduction,[0],[0]
"To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015).",1 Introduction,[0],[0]
"Our method will be evaluated on the benchmark dataset against state-of-the-art methods.
",1 Introduction,[0],[0]
"The rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, and Section 4 concludes this paper.",1 Introduction,[0],[0]
"The architecture of the model, as shown in Figure 1, is straightforward.",2 Method,[0],[0]
It can be divided into three parts: 1) CNN for modeling arguments; 2) CGNN unit for feature transformation; 3) a conventional softmax layer for the final classification.,2 Method,[0],[0]
"CNN is used to obtain the vector representations for the sentences, CGNN further captures and transforms the features for the final classification.",2 Method,[0],[0]
"As CNN has been broadly adopted for modeling sentences, we will explain it in brevity.",2.1 Convolutional Neural Network,[0],[0]
"For two arguments, typical sentence modeling process
will be applied: sentence embedding (including embeddings for words and part-of-speech (POS) tags) through projection layer, convolution operations (with multiple groups of filters) through the convolution layer, obtaining the sentence representation through one-max-pooling.",2.1 Convolutional Neural Network,[0],[0]
"The two arguments will get their sentence vectors independently without any interfering, and the convolution operation will be the same by sharing parameters.",2.1 Convolutional Neural Network,[0],[0]
The final argument-pair representation will be the vector v which is concatenated from two sentence vectors and this vector will be used as the input of the CGNN unit.,2.1 Convolutional Neural Network,[0],[0]
"For implicit sense classification, the key is how to effectively capture the interactions between the two arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
"The interactions could be word pairs, phrase pairs or even the latent meaning of the two full arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
Pitler et al. (2009) has shown that word pair features are helpful.,2.2 Collaborative Gated Neural Network,[0],[0]
"To model these interactions, we have to make a full use of the sentence vectors obtained from CNN.",2.2 Collaborative Gated Neural Network,[0],[0]
"However, common neural hidden layers might be insufficient to deal with the challenge.",2.2 Collaborative Gated Neural Network,[0],[0]
"We need to seek more powerful neural models, i.e., gated neural network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"In recent years, gated mechanism has gained popularity in neural models.",2.2 Collaborative Gated Neural Network,[0],[0]
"Although it is first introduced in the cells of recurrent neural networks, like Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014), traditional feed-forward neural models such as the Highway Network could also benefit from it (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"The existing studies show that the gated mechanism in highway network serves not only a means for easier training, but also a tool to route information in a trained network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Motivated by the idea of highway network, we propose a collaborative gated neural network (CGNN) for this task.",2.2 Collaborative Gated Neural Network,[0],[0]
"The architecture of CGNN is illustrated in Figure 1, and it contains a sequence of transformations.",2.2 Collaborative Gated Neural Network,[0],[0]
"First, the inner-cell ĉ is obtained through linear transformation and non-linear activation on the input v, and this process is exactly the operation of an ordinary neural layer.
",2.2 Collaborative Gated Neural Network,[0],[0]
"ĉ = tanh(Wc · v + bc)
",2.2 Collaborative Gated Neural Network,[0],[0]
"Meanwhile, the two gates gi and go are calculated independently because they are only influenced by
the original input through different parameters:
gi = σ(W i · v + bi) go = σ(W o · v + bo)
where the σ denotes sigmoid function which guarantees the values in the gates are in [0,1].",2.2 Collaborative Gated Neural Network,[0],[0]
"Two gated operations are applied sequentially, where a gated operation indicates the element-wise multiplication of an inner-cell and a gate.",2.2 Collaborative Gated Neural Network,[0],[0]
"Between the two gated operations, a non-linear activation operation is applied.",2.2 Collaborative Gated Neural Network,[0],[0]
"The procedure could be formulated as follows:
c = ĉ gi h = tanh(c) go
where denotes element-wise multiplication, c is the second inner-cell and h is the output of CGNN unit.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Although the two gates are generated independently, they will work collaboratively because they control the information flow of the inner-cells sequentially which resembles logical AND operation in a probabilistic version.",2.2 Collaborative Gated Neural Network,[0],[0]
"In fact, the transformations after ĉ will concern only element-wise operations which might give finer controls for each dimension, and the information can only flow on the dimensions where both gates are “open”.",2.2 Collaborative Gated Neural Network,[0],[0]
"This procedure will help select the most crucial features.
",2.2 Collaborative Gated Neural Network,[0],[0]
The gates in this model are mainly used for routing information from sentence-pairs vectors.,2.2 Collaborative Gated Neural Network,[0],[0]
"When there is only one gate in our network, the model works similar to the highway network (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"After the transformation of the CGNN unit, the transformed vector h will be sent to a conventional softmax for classification.
",2.3 Output and Training,[0],[0]
"The training object J will be the cross-entropy error E with L2 regularization:
E(ŷ, y) =",2.3 Output and Training,[0],[0]
"− l∑
j
yj × log(Pr(ŷj))
",2.3 Output and Training,[0],[0]
J(θ),2.3 Output and Training,[0],[0]
"= 1
m
m∑
k
E(ŷ(k), y(k))",2.3 Output and Training,[0],[0]
+,2.3 Output and Training,[0],[0]
"λ
2 ‖θ‖2
where yj is the gold label and ŷj is the predicted one.",2.3 Output and Training,[0],[0]
"We adopt the diagonal variant of AdaGrad (Duchi et al., 2011) for the optimization process.",2.3 Output and Training,[0],[0]
"As for the benchmark dataset, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) corpus1 is used for evaluation.",3.1 Setting,[0],[0]
"In the PDTB, each discourse relation is annotated between two argument spans.
",3.1 Setting,[0],[0]
"To be consistent with the setups of prior works, we formulate the implicit relation classification task as four one-versus-other binary classification problems only using the four top level classes: COMPARISON (COMP.), CONTINGENCY (CONT.), EXPANSION (EXP.) and TEMPORAL (TEMP.).",3.1 Setting,[0],[0]
"While different works include different relations of varying specificities, all of them include these four core relations (Pitler et al., 2009).",3.1 Setting,[0],[0]
"Following dataset splitting convention of the previous works, we use sections 2-20 for training, sections 21-22 for testing and sections 0-1 for development set.",3.1 Setting,[0],[0]
"The proposed model is possible to be extended for multi-class classification of discourse parsing, but for the comparisons with most of previous works, we will follow them and focus on the binary classification problems.
",3.1 Setting,[0],[0]
"For other hyper-parameters of the model and training process, we fix the lengths of both the input arguments to be 80, and apply truncating or zero-padding when necessary.",3.1 Setting,[0],[0]
"The dimensions for word embeddings and POS embeddings are respectively 300 and 50, and the embedding layer adopts a dropout of 0.2.",3.1 Setting,[0],[0]
"The word embeddings are initialized with pre-trained word vectors using word2vec 2 (Mikolov et al., 2013) and other parameters are randomly initialized including POS embeddings.",3.1 Setting,[0],[0]
"We
1http://www.seas.upenn.edu/˜pdtb/ 2http://www.code.google.com/p/word2vec
set the starting learning rate to 0.001.",3.1 Setting,[0],[0]
"For CNN model, we utilize three groups of filters with window widths of (2, 2, 2) and their filter numbers are all set to 1024.",3.1 Setting,[0],[0]
The hyper-parameters are the same for all models and we do not tune them individually.,3.1 Setting,[0],[0]
"For transformation of sentence vectors, a simple Multilayer Perceptron (MLP) layer could be a straightforward choice, while more complex neural modules, such as LSTM and highway network, could also be considered.",3.2 Model Analysis,[0],[0]
Our model utilizes a CGNN unit with refined gated mechanism for the transformation.,3.2 Model Analysis,[0],[0]
Will the proposed CGNN really bring about further performance improvement?,3.2 Model Analysis,[0],[0]
"We now answer this question empirically.
",3.2 Model Analysis,[0],[0]
"As shown in Table 1, CNN model usually performs well on its own.",3.2 Model Analysis,[0],[0]
"Utilizing an MLP layer or a Highway layer could improve the accuracies on CONTINGENCY, EXPANSION, TEMPORARY except for COMPARISON.",3.2 Model Analysis,[0],[0]
"Though the primary motivation of Highway is to ease gradient-based training of highly deep networks through utilizing gated units, it works merely as an ordinary MLP in the proposed model, which explains the reason that it performs like MLP.",3.2 Model Analysis,[0],[0]
"Despite one of four classes, COMPARISON, not receiving performance improvement, introducing a non-linear transformation layer lets the classification benefit as a whole.",3.2 Model Analysis,[0],[0]
"“CNN+LSTM” denotes the method of using LSTM to read the convolution sequence (without pooling operation), and it even does not perform better than MLP.
",3.2 Model Analysis,[0],[0]
The CGNN achieves the best performance on all classes including COMPARISON.,3.2 Model Analysis,[0],[0]
It gains 3.97% imrovement on average F1 score using CNN only model.,3.2 Model Analysis,[0],[0]
"We assume that CGNN is well-suited to work with CNN, adaptively transforming and combining local features detected by the individual filters.",3.2 Model Analysis,[0],[0]
We show the main results in Tables 2 and 3.,3.3 Results,[0],[0]
"The metrics include precision (P), recall (R), accuracy (Acc) and F1 score.",3.3 Results,[0],[0]
"Since not all of these metrics are reported in previous work, the comparisons are correspondingly in Table 2 and 3.",3.3 Results,[0],[0]
"Some previous work merges Entrel with Expansion, which is also explored in our study and noted as EXP.+.
",3.3 Results,[0],[0]
We compare with best-performed or competitive models including both traditional linear methods and recent neural methods.,3.3 Results,[0],[0]
"For traditional methods: Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features; Zhou et al. (2010) improve the performance through predicting connective words as features; Park and Cardie (2012) propose a locallyoptimal feature set and further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up; Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the learning; Rutherford and Xue (2014) employ Brown cluster pair and coreference patterns for performance enhancement.",3.3 Results,[0],[0]
"Several neural methods have also been included for comparison: Zhang et al. (2015) propose a simplified neural network which has only
three different pooling operations (max, min, average); Ji and Eisenstein (2015) compute distributed semantics representation by composition up the syntactic parse tree through recursive neural network; Braud and Denis (2015) consider shallow lexical features and word embeddings.",3.3 Results,[0],[0]
Chen et al. (2016) replace the original words by word embeddings to overcome the data sparsity problem and they also utilize gated relevance network to capture the semantic interaction between word pairs.,3.3 Results,[0],[0]
"The gated network is different from ours but also works well.
",3.3 Results,[0],[0]
"Our model achieves F-measure improvements of 1.85% on COMPARISON, 1.56% on CONTINGENCY, 1.27% on EXPANSION, 0.94% on EXPANSION+, 4.89% on TEMPORAL, against the state-ofthe-art of each class.",3.3 Results,[0],[0]
We improve by 4.73% on average F1 score when not including ENTREL in EXPANSION as reported in Table 2 and 3.19% on average F1 score otherwise as reported in Table 3.,3.3 Results,[0],[0]
The results show that our model achieves the best performance and especially makes the most remarkable progress on TEMPORAL.,3.3 Results,[0],[0]
"In this paper, we propose a stacking gated neural architecture for implicit discourse relation classification.",4 Conclusion,[0],[0]
Our model includes convolution and collaborative gated neural network.,4 Conclusion,[0],[0]
The analysis and experiments show that CNN performs well on its own and combining CGNN provides further gains.,4 Conclusion,[0],[0]
Our evaluation on PTDB shows that the proposed model outperforms previous state-of-the-art systems.,4 Conclusion,[0],[0]
Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks.,abstractText,[0],[0]
Implicit discourse relation classification is the bottleneck for discourse parsing.,abstractText,[0],[0]
"Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred.",abstractText,[0],[0]
This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation.,abstractText,[0],[0]
Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems.,abstractText,[0],[0]
A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030–1040, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
A verb plays a primary role in conveying the meaning of a sentence.,1 Introduction,[0],[0]
"Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP.
",1 Introduction,[0],[0]
Verb classes are one such lexical resource.,1 Introduction,[0],[0]
"Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior.",1 Introduction,[0],[0]
"Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009).
",1 Introduction,[0],[0]
"There have also been many attempts to automatically acquire verb classes with the goal of ei-
ther adding frequency information to an existing resource or of inducing similar verb classes for other languages.",1 Introduction,[0],[0]
"Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013).",1 Introduction,[0],[0]
"This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses.",1 Introduction,[0],[0]
"Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008).
",1 Introduction,[0],[0]
"In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy.",1 Introduction,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,1 Introduction,[0],[0]
"By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy.
",1 Introduction,[0],[0]
"Our novel contributions are summarized as follows:
• induce both semantic frames and verb classes from a massive amount of verb uses by a scalable method,
• explicitly deal with verb polysemy, • discover effective features for each of the
clustering steps, and
• quantitatively evaluate a soft clustering of verbs.
1030",1 Introduction,[0],[0]
"As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous.",2 Related Work,[0],[0]
"A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points.",2 Related Work,[0],[0]
"As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance.
",2 Related Work,[0],[0]
"Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods.",2 Related Work,[0],[0]
Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions.,2 Related Work,[0],[0]
"They evaluated their result with a gold-standard test set, where a single class is assigned to a verb.",2 Related Work,[0],[0]
Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features.,2 Related Work,[0],[0]
Parisien and Stevenson (2011) extended their model by adding semantic features.,2 Related Work,[0],[0]
They tried to account for verb learning by children and did not evaluate the resultant verb classes.,2 Related Work,[0],[0]
"Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985).",2 Related Work,[0],[0]
"All of the above methods considered verbs to be monosemous and did not deal with verb polysemy.
",2 Related Work,[0],[0]
"Our approach also uses Bayesian methods, but is designed to capture verb polysemy.
",2 Related Work,[0],[0]
"We summarize a few studies that consider polysemy of verbs in the rest of this section.
",2 Related Work,[0],[0]
Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy.,2 Related Work,[0],[0]
"Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007).",2 Related Work,[0],[0]
"Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives.
",2 Related Work,[0],[0]
"The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs.",2 Related Work,[0],[0]
They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.,2 Related Work,[0],[0]
"In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999).",2 Related Work,[0],[0]
"However, the verb itself is still represented as a single data point.",2 Related Work,[0],[0]
"After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering.",2 Related Work,[0],[0]
They considered multiple classes only in the gold-standard data used for their evaluations.,2 Related Work,[0],[0]
"We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993).
",2 Related Work,[0],[0]
Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb.,2 Related Work,[0],[0]
"These models are approximated to condition not
on verbs but on subcategorization frames.",2 Related Work,[0],[0]
"As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy.",2 Related Work,[0],[0]
"They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models.",2 Related Work,[0],[0]
Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet.,2 Related Work,[0],[0]
This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle.,2 Related Work,[0],[0]
"Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure.
",2 Related Work,[0],[0]
"Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013).",2 Related Work,[0],[0]
LDA-frames are probabilistic semantic frames automatically induced from a raw corpus.,2 Related Work,[0],[0]
"He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles.",2 Related Work,[0],[0]
Both of these are represented as a probabilistic distribution of words across verbs.,2 Related Work,[0],[0]
"He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012).",2 Related Work,[0],[0]
"He did not evaluate the resulting frames as verb classes.
",2 Related Work,[0],[0]
"In sum, there have been no studies that quantitatively evaluate polysemous verb classes automatically induced by unsupervised methods.",2 Related Work,[0],[0]
Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches.,3.1 Overview,[0],[0]
"Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost.",3.1 Overview,[0],[0]
"For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011).",3.1 Overview,[0],[0]
"Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013).",3.1 Overview,[0],[0]
"However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models.
",3.1 Overview,[0],[0]
"In this paper, we propose a two-step approach for inducing semantic frames and verb classes.",3.1 Overview,[0],[0]
"First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)).",3.1 Overview,[0],[0]
"To do that, we induce verb-specific semantic frames by clustering verb uses.",3.1 Overview,[0],[0]
"Then, we induce verb classes by clustering these verbspecific semantic frames across verbs.",3.1 Overview,[0],[0]
"An interesting point here is that we can use exactly the same method for these two clustering steps.
",3.1 Overview,[0],[0]
"Our procedure to automatically induce verb classes from verb uses is summarized as follows:
1.",3.1 Overview,[0],[0]
"induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and
2.",3.1 Overview,[0],[0]
"induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1.
",3.1 Overview,[0],[0]
Each of these two steps is described in the following sections in detail.,3.1 Overview,[0],[0]
We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014).,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"Our semantic frames consist of case slots, each of which consists of word instances that can be filled.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"The procedure for inducing these semantic frames is as follows:
1.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses,
2.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and
1In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"To reach 1,000 iterations, which are reported to be optimum, it would take three months.
3.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames.
",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
These three steps are briefly described below.,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
We apply dependency parsing to a large raw corpus.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Then, we extract predicate-argument structures from the dependency parses.",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Dependents that have the following dependency relations to a verb are extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗
In this process, the verb and arguments are lemmatized, and only the head of an argument is preserved for compound nouns.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
Predicate-argument structures are collected for each verb and the subsequent processes are applied to the predicate-argument structures of each verb.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
These initial frames are the input of the subsequent clustering process.,3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We select an argument in the following order by considering the degree of effect on the verb sense:3
dobj, ccomp, nsubj, prep ∗, iobj.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"After this process, we discard minor initial frames that occur fewer than 10 times.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance.
",3.2.3 Clustering Method,[0],[0]
"We calculate the posterior probability of a cluster cj given an initial frame fi as follows:
P (cj |fi) ∝",3.2.3 Clustering Method,[0],[0]
"{ n(cj) N+α · P (fi|cj) cj ̸= new
α N+α · P (fi|cj) cj",3.2.3 Clustering Method,[0],[0]
"= new,
(1)
where N is the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj .",3.2.3 Clustering Method,[0],[0]
α is a hyperparameter that determines how likely it is for a new cluster to be created.,3.2.3 Clustering Method,[0],[0]
"In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi.
P (fi|cj) is defined based on the DirichletMultinomial distribution as follows:
P (fi|cj) = ∏ w∈V",3.2.3 Clustering Method,[0],[0]
"P (w|cj)count(fi,w), (2)
where V is the vocabulary in all case slots cooccurring with the verb and count(fi, w) is the number of w in the initial frame fi.",3.2.3 Clustering Method,[0],[0]
"The original method in Kawahara et al. (2014) defined w as pairs of slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not consider slot-only features, e.g., “nsubj” and “dobj,” which ignore lexical information.",3.2.3 Clustering Method,[0],[0]
"Here we experiment with both representations and compare the results.
",3.2.3 Clustering Method,[0],[0]
"P (w|cj) is defined as follows:
P (w|cj) =",3.2.3 Clustering Method,[0],[0]
"count(cj , w) + β∑ t∈V",3.2.3 Clustering Method,[0],[0]
"count(cj , t) + |V",3.2.3 Clustering Method,[0],[0]
"| · β , (3)
where count(cj , w) is the current number of w in the cluster cj , and β is a hyper-parameter of Dirichlet distribution.",3.2.3 Clustering Method,[0],[0]
"For a new cluster, this probability is uniform (1/|V |).
",3.2.3 Clustering Method,[0],[0]
"We regard each output cluster as a semantic frame, by merging the initial frames in a cluster into a semantic frame.",3.2.3 Clustering Method,[0],[0]
"In this way, semantic frames for each verb are acquired.
",3.2.3 Clustering Method,[0],[0]
We use Gibbs sampling to realize this clustering.,3.2.3 Clustering Method,[0],[0]
"To induce verb classes across verbs, we apply clustering to the induced verb-specific semantic
frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"This is because an initial frame has the same structure as a semantic frame, which is produced by merging initial frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"We regard each output cluster as a verb class this time.
",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"For the features, w, in equation (2), we try the two representations again: slot-only features and slot-word pair features.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The representation using only slots corresponds to the consideration of only syntactic argument patterns.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The other representation using the slot-word pairs means that semantic similarity based on word overlap is naturally considered by looking at lexical information.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes.,4 Experiments and Evaluations,[0],[0]
"Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations.",4 Experiments and Evaluations,[0],[0]
These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation.,4 Experiments and Evaluations,[0],[0]
"Finally, we discuss the results of our full experiments.",4 Experiments and Evaluations,[0],[0]
"We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information.",4.1 Experimental Settings,[0],[0]
"Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences.",4.1 Experimental Settings,[0],[0]
"From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words.",4.1 Experimental Settings,[0],[0]
"We focused on verbs whose frequency in the web corpus was more than 1,000.",4.1 Experimental Settings,[0],[0]
"There were 19,649 verbs, including phrasal verbs, and separating passive and active constructions.",4.1 Experimental Settings,[0],[0]
"We extracted 2,032,774,982 predicate-argument structures.
",4.1 Experimental Settings,[0],[0]
We also used the English Gigaword corpus (LDC2011T07; English Gigaword Fifth Edition).,4.1 Experimental Settings,[0],[0]
"This corpus consists of approximately 180 million sentences, which totaling four billion words.
",4.1 Experimental Settings,[0],[0]
"There were 7,356 verbs after applying the same frequency threshold as the web corpus.",4.1 Experimental Settings,[0],[0]
"We extracted 423,778,278 predicate-argument structures from this corpus.
",4.1 Experimental Settings,[0],[0]
We set the hyper-parameters α in (1) and β in (3) to 1.0.,4.1 Experimental Settings,[0],[0]
The cluster assignments for all the components were initialized randomly.,4.1 Experimental Settings,[0],[0]
We took 100 samples for each input frame and selected the cluster assignment that has the highest probability.,4.1 Experimental Settings,[0],[0]
"To measure the precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)).",4.2 Evaluation Metrics,[0],[0]
"However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4",4.2 Evaluation Metrics,[0],[0]
We propose a normalized version of modified purity and inverse purity.,4.2 Evaluation Metrics,[0],[0]
"This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013).
",4.2 Evaluation Metrics,[0],[0]
"To measure the precision of a clustering, a normalized version of modified purity is defined as follows.",4.2 Evaluation Metrics,[0],[0]
Suppose K is the set of automatically induced clusters and G is the set of gold classes.,4.2 Evaluation Metrics,[0],[0]
Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class.,4.2 Evaluation Metrics,[0],[0]
"Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb.",4.2 Evaluation Metrics,[0],[0]
"Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s classes.",4.2 Evaluation Metrics,[0],[0]
The core idea of purity is that each cluster Ki is associated with its most prevalent gold class.,4.2 Evaluation Metrics,[0],[0]
"In addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity.",4.2 Evaluation Metrics,[0],[0]
"The normalized modified purity (nmPU) can then be written as follows:
nmPU = 1 N ∑ i s.t. |Ki|>1",4.2 Evaluation Metrics,[0],[0]
"max j δKi(Ki ∩ Gj), (4)
δKi(Ki ∩ Gj) = ∑
v∈Ki∩Gj civ, (5)
",4.2 Evaluation Metrics,[0],[0]
4Korhonen et al. (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb.,4.2 Evaluation Metrics,[0],[0]
"They reported only precision measures including modified purity, and avoided extending the evaluation metrics for soft clusterings.
where N denotes the total number of verbs, |Ki| denotes the number of positive components in Ki, and civ denotes the v-th component of Ki.",4.2 Evaluation Metrics,[0],[0]
"δKi(Ki ∩ Gj) means the total mass of the set of verbs in Ki ∩Gj , given by summing up the values in Ki.",4.2 Evaluation Metrics,[0],[0]
"In case of evaluating a hard clustering, this is equal to |Ki ∩",4.2 Evaluation Metrics,[0],[0]
"Gj | because all the values of civ are equal to 1.
",4.2 Evaluation Metrics,[0],[0]
"As usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering:
niPU = 1 N ∑",4.2 Evaluation Metrics,[0],[0]
j max i δGj (Ki ∩ Gj).,4.2 Evaluation Metrics,[0],[0]
"(6)
Finally, we use the harmonic mean (F1) of nmPU and niPU as a single measure of clustering quality.",4.2 Evaluation Metrics,[0],[0]
"We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997).",4.3 Type-level Multi-class Evaluations,[0],[0]
"It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous.",4.3 Type-level Multi-class Evaluations,[0],[0]
The average number of verb classes per verb is 2.24.,4.3 Type-level Multi-class Evaluations,[0],[0]
"An excerpt from this data is shown in Table 1.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"As our baselines, we adopt two previously proposed methods.",4.3 Type-level Multi-class Evaluations,[0],[0]
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003).,4.3 Type-level Multi-class Evaluations,[0],[0]
They used the information bottleneck (IB) method for assigning probabilities of classes to each verb.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Note that Korhonen et al. (2003) actually hardened the clusterings and left
the evaluations of soft clusterings for their future work.",4.3 Type-level Multi-class Evaluations,[0],[0]
"For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies.",4.3 Type-level Multi-class Evaluations,[0],[0]
"To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities.",4.3 Type-level Multi-class Evaluations,[0],[0]
"That is, classes that have a higher class attribute probability than the threshold are output for each verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"We report the results of the following threshold values: 0.01, 0.02, 0.05 and 0.10.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"The other baseline is LDA-frames (Materna, 2012).",4.3 Type-level Multi-class Evaluations,[0],[0]
"We use the induced LDA-frames that are
5http://ilexir.co.uk/applications/valex/
available on the web site.6 This frame data was induced from the BNC and consists of 1,200 frames and 400 semantic roles.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Again, we set a threshold for frame attribute probabilities.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We report results using our methods with four feature combinations (slot-only (S) and slot-word pair (SW) features each used for both the framegeneration and verb-class clustering steps) for both the Gigaword and web corpora.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Table 2 lists evaluation results for the baseline methods and our methods.7 The results of the IB baseline and our methods are obtained by averaging five runs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We can see that “web/SW-S” achieved the best performance and obtained a higher F1 than the baselines by more than nine points.,4.3 Type-level Multi-class Evaluations,[0],[0]
“Web/SWS” uses the combination of slot-word pair features for clustering verb-specific frames and slotonly features for clustering across verbs.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Interestingly, this result indicates that slot distributions are more effective than lexical information in slotword pairs for inducing verb classes similar to the gold standard.",4.3 Type-level Multi-class Evaluations,[0],[0]
"This result is consistent with expectations, given a gold standard based on Levin’s verb classes, which are organized according to the syntactic behavior of verbs.",4.3 Type-level Multi-class Evaluations,[0],[0]
"The use of slot-word pairs for verb class induction generally merged too many frames into each class, apparently due to accidental word overlaps across verbs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
The verb classes induced from the web corpus achieved a higher F1 than those from the Gigaword corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
This can be attributed to the larger size of the web corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
"The employment of this kind of huge corpus is enabled by our scalable method.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"6http://nlp.fi.muni.cz/projekty/lda-frames/ 7Although we do not think that the classes with very small attribute probabilities are meaningful, the F1 scores for lower thresholds than 0.01 converged to about 66 in the case of LDA-frames.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Since we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"However, we wish to compare our method with previous work on the induction of a predominant (monosemous) class for each verb.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Then, we apply clustering to these frames across verbs.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For clustering features, we again compare two representations: slot-only features (S) and slot-word pair features (SW).
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al. (2003).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This data contains 110 verbs and 33 classes.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"As we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"When we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al. (2003), and LDA-frames proposed by Materna (2012).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"The
clusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To harden the clusterings of the IB method and the LDA-frames, the class with the highest probability is selected for each verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This hardening process is exactly the same as Korhonen et al. (2003).,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8
Table 3 lists accuracies of baseline methods and our methods.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
Our proposed method using the web corpus achieved comparable performance with the baseline methods on the predominant class evaluation and outperformed them on the multiple class evaluation.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"More sophisticated methods for predominant class induction, such as the method of Sun and Korhonen (2009) using selectional preferences, could produce better single-class outputs, but have difficulty in producing polysemy-aware verb classes.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"From the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F1 than those based on slot-word pair features in many cases.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This result is different from that of multi-class evaluations in Section 4.3.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of semantic frames.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We conduct token-level multi-class evaluations using 119 verbs, which appear 100 or more times in sections 02-21 of the SemLink WSJ corpus.",4.5 Token-level Multi-class Evaluations,[0],[0]
"These 119 verbs cover 102 VerbNet classes, and 48 of them are polysemous in the sense of being in more than one VerbNet class.",4.5 Token-level Multi-class Evaluations,[0],[0]
Each instance of these 119 verbs in this corpus belongs to one of 102 VerbNet classes.,4.5 Token-level Multi-class Evaluations,[0],[0]
We first add these instances to the instances from a raw corpus and apply the twostep clustering to these merged instances.,4.5 Token-level Multi-class Evaluations,[0],[0]
"Then, we compare the induced verb classes of the SemLink instances with their gold-standard VerbNet classes.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1).",4.5 Token-level Multi-class Evaluations,[0],[0]
"It is not necessary to normalize these metrics because the clustering of these instances is hard.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"8Korhonen et al. (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"For clustering features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3).",4.5 Token-level Multi-class Evaluations,[0],[0]
The results of these methods are obtained by averaging five runs.,4.5 Token-level Multi-class Evaluations,[0],[0]
"For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs.",4.5 Token-level Multi-class Evaluations,[0],[0]
Table 4 lists accuracies of these methods for the two corpora.,4.5 Token-level Multi-class Evaluations,[0],[0]
"We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”).
",4.5 Token-level Multi-class Evaluations,[0],[0]
Modi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus.,4.5 Token-level Multi-class Evaluations,[0],[0]
We also conducted the above evaluation against FrameNet frames for 75 verbs.9,4.5 Token-level Multi-class Evaluations,[0],[0]
"We achieved an F1 of 62.79% (66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F1 of 60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is difficult to directly compare these results with Modi et al. (2012), but our induced verb classes seem to have higher F1 accuracy.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We finally induce verb classes from the semantic frames of 1,667 verbs, which appear at least once in sections 02-21 of the WSJ corpus.",4.6 Full Experiments and Discussions,[0],[0]
"Based on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slotonly features.",4.6 Full Experiments and Discussions,[0],[0]
"We ended with 38,481 semantic frames and 699 verb classes from the Gigaword
9Since FrameNet frames are not assigned to all verbs of SemLink, the number of verbs is different from the evaluations against VerbNet classes.
corpus, and 61,903 semantic frames and 840 verb classes from the web corpus.",4.6 Full Experiments and Discussions,[0],[0]
"It took two days to induce verb classes from the Gigaword corpus and three days from the web corpus.
",4.6 Full Experiments and Discussions,[0],[0]
Examples of verb classes and semantic frames induced from the web corpus are shown in Table 5 and Table 6.,4.6 Full Experiments and Discussions,[0],[0]
"While there are many classes with consistent meanings, such as “Class 4” and “Class 16,” some classes have mixed meanings.",4.6 Full Experiments and Discussions,[0],[0]
"For instance, “Class 2” consists of the semantic frames “need:2” and “say:2.”",4.6 Full Experiments and Discussions,[0],[0]
"These frames were merged due to the high syntactic similarity of constituting slot distributions, which are comprised of a subject and a sentential complement.",4.6 Full Experiments and Discussions,[0],[0]
"To improve the quality of verb classes, it is necessary to develop a clustering model that can consider syntactic and lexical similarity in a balanced way.",4.6 Full Experiments and Discussions,[0],[0]
We presented a step-wise unsupervised method for inducing verb classes from instances in gigaword corpora.,5 Conclusion,[0],[0]
This method first clusters predicateargument structures to induce verb-specific semantic frames and then clusters these semantic frames across verbs to induce verb classes.,5 Conclusion,[0],[0]
"Both clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process.",5 Conclusion,[0],[0]
"The resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.10
10http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
From the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points.",5 Conclusion,[0],[0]
"This indicates that slot distributions are more effective than lexical information in slotword pairs for the induction of verb classes, when Levin-style classes are used for evaluation.",5 Conclusion,[0],[0]
"This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs.
",5 Conclusion,[0],[0]
"As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation.",5 Conclusion,[0],[0]
"For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus.",5 Conclusion,[0],[0]
"It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011).",5 Conclusion,[0],[0]
This work was supported by Kyoto University John Mung Program and JST CREST.,Acknowledgments,[0],[0]
"We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing.",Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgments,[0],[0]
We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora.,abstractText,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,abstractText,[0],[0]
"By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering.",abstractText,[0],[0]
"In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words.",abstractText,[0],[0]
The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.,abstractText,[0],[0]
A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1243–1252 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1243",text,[0],[0]
Neural architectures have taken the field of machine translation by storm and are in the process of replacing phrase-based systems.,1 Introduction,[0],[0]
"Based on the encoder-decoder framework (Sutskever et al., 2014) increasingly complex neural systems are being developed at the moment.",1 Introduction,[0],[0]
"These systems find new ways of extracting information from the source sentence and the target sentence prefix for example by using convolutions (Gehring et al., 2017) or stacked self-attention layers (Vaswani et al., 2017).",1 Introduction,[0],[0]
"These architectural changes have led to great performance improvements over classical RNN-based neural translation systems (Bahdanau et al., 2014).
∗Code and a workflow that reproduces the experiments are available at https://github.com/philschulz/ stochastic-decoder.
†Work done prior to joining Amazon.
",1 Introduction,[0],[0]
"Surprisingly, there have been almost no efforts to change the probabilistic model wich is used to train the neural architectures.",1 Introduction,[0],[0]
A notable exception is the work of Zhang et al. (2016) who introduce a sentence-level latent Gaussian variable.,1 Introduction,[0],[0]
"In this work, we propose a more expressive latent variable model that extends the attentionbased architecture of Bahdanau et al. (2014).",1 Introduction,[0],[0]
"Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.).",1 Introduction,[0],[0]
"Neural machine translation (NMT) models are incapable of capturing this variation, however.",1 Introduction,[0],[0]
"This is because their likelihood function incorporates the statistical assumption that there is one (and only one) output1 for a given source sentence, i.e.,
P (yn1 |xm1 ) = n∏
i=1
P (yi|xm1 , y<i) .",1 Introduction,[0],[0]
"(1)
Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data.",1 Introduction,[0],[0]
The noise sources are modelled as Gaussian random variables.,1 Introduction,[0],[0]
"The contributions of this work are: • The introduction of an NMT system that is capable of capturing word-level variation in translation data.
",1 Introduction,[0],[0]
• A thorough discussions of issues encountered when training this model.,1 Introduction,[0],[0]
"In particular, we motivate the use of KL scaling as introduced by Bowman et al. (2016) theoretically.
",1 Introduction,[0],[0]
1Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence.,1 Introduction,[0],[0]
"The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system.
",1 Introduction,[0],[0]
• An empirical demonstration of the improvements achievable with the proposed model.,1 Introduction,[0],[0]
The NMT system upon which we base our experiments is based on the work of Bahdanau et al. (2014).,2 Neural Machine Translation,[0],[0]
The likelihood of the model is given in Equation (1).,2 Neural Machine Translation,[0],[0]
We briefly describe its architecture.,2 Neural Machine Translation,[0],[0]
"Let xm1 = (x1, . . .",2 Neural Machine Translation,[0],[0]
", xm) be the source sentence and yn1 the target sentence.",2 Neural Machine Translation,[0],[0]
Let RNN (·) be any function computed by a recurrent neural network (we use a bi-LSTM for the encoder and an LSTM for the decoder).,2 Neural Machine Translation,[0],[0]
We call the decoder state at the ith target position ti; 1 ≤ i ≤,2 Neural Machine Translation,[0],[0]
n.,2 Neural Machine Translation,[0],[0]
The computation performed by the baseline system is summarised below.,2 Neural Machine Translation,[0],[0]
"[
h1, . . .",2 Neural Machine Translation,[0],[0]
", hm ] = RNN (xm1 ) (2a)
t̃i = RNN (ti−1, yi−1) (2b)
eij = v ⊤ a tanh ( Wa[t̃i, hj ] ⊤ + ba ) (2c) αij = exp (eij)∑m j=1 exp (eij) (2d)
ci = m∑ j=1 αijhj (2e) ti = Wt[t̃i, ci] ⊤ + bt (2f)
ϕi = softmax(Woti + bo) (2g)
",2 Neural Machine Translation,[0],[0]
"The parameters {Wa,Wt,Wo, ba, bt, bo, va} ⊆ θ are learned during training.",2 Neural Machine Translation,[0],[0]
The model is trained usingmaximum likelihood estimation.,2 Neural Machine Translation,[0],[0]
Thismeans that we employ a cross-entropy loss whose input is the probability vector returned by the softmax.,2 Neural Machine Translation,[0],[0]
This section introduces our stochastic decoder model for capturing word-level variation in translation data.,3 Stochastic Decoder,[0],[0]
Imagine an idealised translator whose translations are always perfectly accurate and fluent.,3.1 Motivation,[0],[0]
"If an MT systemwas providedwith training data from such a translator, it would still encounter variation in that data.",3.1 Motivation,[0],[0]
"After all, there are several perfectly accurate and fluent translations for each source sentence.",3.1 Motivation,[0],[0]
"These can be highly different in both their lexical as well as their syntactic realisations.
",3.1 Motivation,[0],[0]
"In practice, of course, human translators’ performance varies according to their level of education, their experience on the job, their familiarity with the textual domain and myriads of other factors.",3.1 Motivation,[0],[0]
"Even within a single translator variation may occur due to level of stress, tiredness or status of health.",3.1 Motivation,[0],[0]
"That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing onemachinegenerated translation against several human translations (see e.g. Papineni et al., 2002).
",3.1 Motivation,[0],[0]
"Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable.",3.1 Motivation,[0],[0]
"Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations.",3.1 Motivation,[0],[0]
"A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions.",3.1 Motivation,[0],[0]
"Multimodal modelling of these variations is thus called for—and our results confirm this intuition.
",3.1 Motivation,[0],[0]
An example of variation comes from free word order and agreement phenomena in morphologically rich languages.,3.1 Motivation,[0],[0]
An English sentence with rigid word order may be translated into several orderings in German.,3.1 Motivation,[0],[0]
"However, all orderings need to respect the agreement relationship between the main verb and the subject (indicated by underlining) as well as the dative case of the direct object (dashes) and the accusative of the indirect object (dots).",3.1 Motivation,[0],[0]
"The agreement requirements are fixed and independent of word order.
",3.1 Motivation,[0],[0]
1.,3.1 Motivation,[0],[0]
I can’t imagine you naked.,3.1 Motivation,[0],[0]
(a) Ich kann mir . . .,3.1 Motivation,[0],[0]
.dich,3.1 Motivation,[0],[0]
nicht nackt vorstellen.,3.1 Motivation,[0],[0]
(b) Ich kann . . . .,3.1 Motivation,[0],[0]
.dich,3.1 Motivation,[0],[0]
mir nicht nackt vorstellen.,3.1 Motivation,[0],[0]
(c) . . . .,3.1 Motivation,[0],[0]
.Dich,3.1 Motivation,[0],[0]
"kann ichmir nicht nackt vorstellen.
",3.1 Motivation,[0],[0]
"Stochastically encoding the word order variation allows the model to learn the same agreement phenomenon from different translation variants as it does not need to encode the word order and agreement relationships jointly in the decoder state.
",3.1 Motivation,[0],[0]
"Further examples of VP and NP variation from an actual translation corpus are shown in Figure 1.
",3.1 Motivation,[0],[0]
We aim to address these word-level variation phenomena with a stochastic decoder model.,3.1 Motivation,[0],[0]
The model contains a latent Gaussian variable for each target position.,3.2 Model formulation,[0],[0]
This variable depends on the previous latent states and the decoder state.,3.2 Model formulation,[0],[0]
"Through the use of recurrent networks, the conditioning context does not need to be restricted and the likelihood factorises exactly.
",3.2 Model formulation,[0],[0]
"P (yn1 |xm1 ) = ∫
dzn0 p(z0|xm1 )× n∏
i=1
p(zi|z<i, y<i, xm1 )P (yi|zi1, y<i, xm1 ) (3)
As can be seen from Equation (3), the model also contains a 0th latent variable that is meant to initialise the chain of latent variables based solely on the source sentence.",3.2 Model formulation,[0],[0]
Contrast this with the model of Zhang et al. (2016) which uses only that 0th variable.,3.2 Model formulation,[0],[0]
A graphical representation of the stochastic decoder model is given in Figure 2a.,3.2 Model formulation,[0],[0]
"Its generative story is as follows
Z0|xm1 ∼ N (µ0, σ20) (4a) Zi|z",3.2 Model formulation,[0],[0]
<,3.2 Model formulation,[0],[0]
"i, y<i, xm1 ∼ N (µi, σ2i ) (4b) Yi|zi0, y<i, xm1 ∼ Cat(ϕi) (4c)
where i = 1, . . .",3.2 Model formulation,[0],[0]
", n and both the Gaussian and the Categorical parameters are predicted by neural network architectures whose inputs vary per time step.",3.2 Model formulation,[0],[0]
This probabilistic formulation can be implemented with a multitude of different architectures.,3.2 Model formulation,[0],[0]
We present ours in the next section.,3.2 Model formulation,[0],[0]
"Since the model contains latent variables and is parametrised by a neural network, it falls into the class of deep generative models (DGMs).",3.3 Neural Architecture,[0],[0]
"We use a reparametrisation of the Gaussian variables (Kingma and Welling, 2014; Rezende et al., 2014; Titsias and Lázaro-Gredilla, 2014) to enable backpropagation inside a stochastic computation graph (Schulman et al., 2015).",3.3 Neural Architecture,[0],[0]
In order to sample ddimensional Gaussian variable z ∈,3.3 Neural Architecture,[0],[0]
Rd,3.3 Neural Architecture,[0],[0]
"with mean µ and variance σ2, we first sample from a standard Gaussian distribution and then transform the sample,
z = µ+ σ ⊙ ϵ ϵ",3.3 Neural Architecture,[0],[0]
"∼ N (0, I) .",3.3 Neural Architecture,[0],[0]
"(5)
Here µ, σ ∈ Rd and ⊙ denotes element-wise multiplication (also known as Hadamard product).",3.3 Neural Architecture,[0],[0]
See the supplement for details on the Gaussian reparametrisation.,3.3 Neural Architecture,[0],[0]
We use neural networks with one hidden layer with a tanh activation to compute the mean and standard deviation of each Gaussian distribution.,3.3 Neural Architecture,[0],[0]
A softplus transformation is applied to the output of the standard deviation’s network to ensure positivity.,3.3 Neural Architecture,[0],[0]
Let us denote the functions that these networks compute by f .,3.3 Neural Architecture,[0],[0]
"For the initial latent state z0 we compute the mean and standard deviation as
µ0 = fµ0 (hm) σ0",3.3 Neural Architecture,[0],[0]
= fσ0 (hm) .,3.3 Neural Architecture,[0],[0]
"(6)
The parameters of all other latent distributions are computed by functions fµ and fσ whose inputs vary per target position.
",3.3 Neural Architecture,[0],[0]
"µi = fµ (ti−1, zi−1) σi",3.3 Neural Architecture,[0],[0]
"= fσ (ti−1, zi−1) (7)
Using these values, each latent variable is sampled according to Equation (5).",3.3 Neural Architecture,[0],[0]
The sampled latent variables are then used to modify the update of the decoder hidden state (Equation (2b)),3.3 Neural Architecture,[0],[0]
"as follows:
t̃i = RNN (ti−1, yi−1, zi) (8)
The remaining computations stay unchanged.",3.3 Neural Architecture,[0],[0]
Notice that the latent values are used directly in updating the decoder state.,3.3 Neural Architecture,[0],[0]
This makes the decoder state a function of a random variable and thus the decoder state is itself random.,3.3 Neural Architecture,[0],[0]
"Applying this argument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic.",3.3 Neural Architecture,[0],[0]
"We use variational inference (see e.g. Blei et al., 2017) to train the model.",4 Inference and Training,[0],[0]
"In variational inference, we employ a variational distribution q(z) that approximates the true posterior p(z|x) over the latent variables.",4 Inference and Training,[0],[0]
The distribution q(z) has its own set of parameters λ that is disjoint from the set of model parameters θ.,4 Inference and Training,[0],[0]
It is used to maximise the evidence lower bound (ELBO) which is a lower bound on the marginal likelihood p(x).,4 Inference and Training,[0],[0]
The ELBO is maximised with respect to both the model parameters θ and the variational parameters λ.,4 Inference and Training,[0],[0]
"Most NLP models that use DGMs only use one latent variable (e.g. Bowman et al., 2016).",4 Inference and Training,[0],[0]
"Models
that use several variables usually employ a mean field approximation under which all latent variables are independent.",4 Inference and Training,[0],[0]
"This turns the ELBO into a sum of expectations (e.g. Zhou and Neubig, 2017).",4 Inference and Training,[0],[0]
"For our stochastic decoder we design a more flexible approximation posterior family which respects the dependencies between the latent variables,
q(zn0 )",4 Inference and Training,[0],[0]
= q(z0) n∏ i=1,4 Inference and Training,[0],[0]
q(zi|z<i) .,4 Inference and Training,[0],[0]
"(9)
Our stochastic decoder can be viewed as a stack of conditional DGMs (Sohn et al., 2015) in which the latent variables depend on one another.",4 Inference and Training,[0],[0]
"The ELBO thus consists of nested positional ELBOs,
ELBO0 + Eq(z0)[ELBO1 +Eq(z1)[ELBO2 + . . .",4 Inference and Training,[0],[0]
"]] ,
(10)
where for a given target position i the ELBO is
ELBOi = Eq(zi)",4 Inference and Training,[0],[0]
"[log p(yi|x m 1 , y<i, z<i, zi)]
−KL (q(zi) ||",4 Inference and Training,[0],[0]
"p(zi|xm1 , y<i, z<i)) .",4 Inference and Training,[0],[0]
"(11)
",4 Inference and Training,[0],[0]
The first term is often called reconstruction or likelihood term whereas the second term is called the KL term.,4 Inference and Training,[0],[0]
"Since the KL term is a function of two Gaussian distributions, and the Gaussian is an exponential family, we can compute it analytically (Michalowicz et al., 2014), without the need for sampling.",4 Inference and Training,[0],[0]
This is very similar to the hierarchical latent variable model of Rezende et al. (2014).,4 Inference and Training,[0],[0]
"Following common practice in DGM research, we employ a neural network to compute the variational distributions.",4 Inference and Training,[0],[0]
"To discriminate it from the
generative model, we call this neural net the inference model.",4 Inference and Training,[0],[0]
At training time both the source and target sentence are observed.,4 Inference and Training,[0],[0]
We exploit this by endowing our inference model with a “lookahead” mechanism.,4 Inference and Training,[0],[0]
"Concretely, samples from the inference network condition on the information available to the generation network (Section 3.3) and also on the target words that are yet to be processed by the generative decoder.",4 Inference and Training,[0],[0]
This allows the latent distribution to not only encode information about the currently modelled word but also about the target words that follow it.,4 Inference and Training,[0],[0]
The conditioning of the inference network is illustrated graphically in Figure 2b.,4 Inference and Training,[0],[0]
The inference network produces additional representations of the target sentence.,4 Inference and Training,[0],[0]
"One representation encodes the target sentence bidirectionally (12a), in analogy to the source sentence encoding.",4 Inference and Training,[0],[0]
The second representation is built by encoding the target sentence in reverse (12b).,4 Inference and Training,[0],[0]
This reverse encoding can be used to provide information about future context to the decoder.,4 Inference and Training,[0],[0]
"We use the symbols b and r for the bidirectional and reverse target encodings, respectively.",4 Inference and Training,[0],[0]
"In our experiments, we again use LSTMs to compute these encodings.",4 Inference and Training,[0],[0]
"[
b1, . . .",4 Inference and Training,[0],[0]
", bn ] = RNN (yn1 ) (12a)[
r1, . . .",4 Inference and Training,[0],[0]
", rn ] = RNN (yn1 ) (12b)
",4 Inference and Training,[0],[0]
"In analogy to the generativemodel (Section 3.3), the inference network uses single hidden layer networks to compute the mean and standard deviations of the latent variable distributions.",4 Inference and Training,[0],[0]
"We denote these functions g and again employ different functions for the initial latent state and all other latent states.
µ0 = gµ0 (hm, bn) (13a) σ0 = gσ0",4 Inference and Training,[0],[0]
"(hm, bn) (13b) µi = gµ (ti−1, zi−1, ri, yi) (13c)",4 Inference and Training,[0],[0]
σi,4 Inference and Training,[0],[0]
"= gσ (ti−1, zi−1, ri, yi) (13d)
",4 Inference and Training,[0],[0]
"As before, we use Equation (5) to sample from the variational distribution.",4 Inference and Training,[0],[0]
"During training, all samples are obtained from the inference network.",4 Inference and Training,[0],[0]
Only at test time do we sample from the generator.,4 Inference and Training,[0],[0]
"Notice that since the inference network conditions on representations produced by the generator network, a naïve application of backpropagation would update parts of the generator network with gradients computed for
the inference network.",4 Inference and Training,[0],[0]
We prevent this by blocking gradient flow from the inference net into the generator.,4 Inference and Training,[0],[0]
The training procedure as outlined above does not work well empirically.,4.1 Analysis of the Training Procedure,[0],[0]
This is because our model uses a strong generator.,4.1 Analysis of the Training Procedure,[0],[0]
By this we mean that the generation model (that is the baseline NMT model) is a very good density model in and by itself and does not need to rely on latent information to achieve acceptable likelihood values during training.,4.1 Analysis of the Training Procedure,[0],[0]
"DGMs with strong generators have a tendency to not make use of latent information (Bowman et al., 2016).",4.1 Analysis of the Training Procedure,[0],[0]
"This problem went initially unnoticed because early DGMs (Kingma and Welling, 2014; Rezende et al., 2014) used weak generators2, i.e. models that made very strong independence assumptions and were not able to capture contextual information without making use of the information encoded by the latent variable.",4.1 Analysis of the Training Procedure,[0],[0]
WhyDGMswould ignore the latent information can be understood by considering the KL-term of the ELBO.,4.1 Analysis of the Training Procedure,[0],[0]
"In order for the latent variable to be informative about the observed data, we need them to have high mutual information I(Z;Y ).
I(Z;Y ) =",4.1 Analysis of the Training Procedure,[0],[0]
"Ep(z,y) [ log p(Z, Y )
p(Z)p(Y )
]",4.1 Analysis of the Training Procedure,[0],[0]
"(14)
Observe that we can rewrite the mutual information as an expected KL divergence by applying the definition of conditional probability.
I(Z;Y ) = Ep(y)",4.1 Analysis of the Training Procedure,[0],[0]
[KL (p(Z|Y ) ||,4.1 Analysis of the Training Procedure,[0],[0]
p(Z)),4.1 Analysis of the Training Procedure,[0],[0]
"] (15)
Since we cannot compute the posterior p(z|y) exactly, we approximate it with the variational distribution q(z|y) (the joint is approximated by q(z|y)p(y) where the latter factor is the data distribution).",4.1 Analysis of the Training Procedure,[0],[0]
"To the extent that the variational distribution recovers the true posterior, the mutual information can be computed this way.",4.1 Analysis of the Training Procedure,[0],[0]
"In fact, if we take the learned prior p(z) to be an approximation of themarginal ∫ q(z|y)p(y)dy it can easily be shown that the thus computed KL term is an upper bound on mutual information (Alemi et al., 2017).",4.1 Analysis of the Training Procedure,[0],[0]
"The trouble is that the ELBO (Equation (11)) can be trivially maximised by setting the KL-term to 0 and maximising only the reconstruction term.
2The term weak generator has first been coined by Alemi et al. (2017).
",4.1 Analysis of the Training Procedure,[0],[0]
This is especially likely at the beginning of training when the variational approximation does not yet encode much useful information.,4.1 Analysis of the Training Procedure,[0],[0]
We can only hope to learn a useful variational distribution if a) the variational approximation is allowed to move away from the prior and b) the resulting increase in the reconstruction term is higher than the increase in the KL-term (i.e. the ELBO increases overall).,4.1 Analysis of the Training Procedure,[0],[0]
"Several schemes have been proposed to enable better learning of the variational distribution (Bowman et al., 2016; Kingma et al., 2016; Alemi et al., 2017).",4.1 Analysis of the Training Procedure,[0],[0]
Here we use KL scaling and increase the scale gradually until the original objective is recovered.,4.1 Analysis of the Training Procedure,[0],[0]
"This has the following effect: during the initial learning stage, the KL-term barely contributes to the objective and thus the updates to the variational parameters are driven by the signal from the reconstruction term and hardly restricted by the prior.",4.1 Analysis of the Training Procedure,[0],[0]
Once the scale factor approaches 1 the variational distribution will be highly informative to the generator (assuming sufficiently slow increase of the scale factor).,4.1 Analysis of the Training Procedure,[0],[0]
The KL-term can now be minimised by matching the prior to the variational distribution.,4.1 Analysis of the Training Procedure,[0],[0]
"Notice that up to this point, the prior has hardly been updated.",4.1 Analysis of the Training Procedure,[0],[0]
Thus moving the variational approximation back to the prior would likely reduce the reconstruction term since the standard normal prior is not useful for inference purposes.,4.1 Analysis of the Training Procedure,[0],[0]
This is in stark contrast to Bowman et al. (2016) whose prior was a fixed standard normal distribution.,4.1 Analysis of the Training Procedure,[0],[0]
"Although they used KL scaling, the KL term could only be decreased by moving the variational approximation back to the fixed prior.",4.1 Analysis of the Training Procedure,[0],[0]
This problem disappears in our model where priors are learned.,4.1 Analysis of the Training Procedure,[0],[0]
Moving the prior towards the variational approximation has another desirable effect.,4.1 Analysis of the Training Procedure,[0],[0]
The prior can now learn to emulate the variational “lookahead” mechanism without having access to future contexts itself (recall that the inference model has access to future target tokens).,4.1 Analysis of the Training Procedure,[0],[0]
At test time we can thus hope to have learned latent variable distributions that encode information not only about the output at the current position but about future outputs as well.,4.1 Analysis of the Training Procedure,[0],[0]
We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations.,5 Experiments,[0],[0]
"We trained models to
translate from English into Arabic, Czech, French and German.",5 Experiments,[0],[0]
The number of sentences for each language after preprocessing is shown in Table 1.,5 Experiments,[0],[0]
"The vocabulary was split into 50,000 subword units using Google’s sentence piece3 software in its standard settings.",5 Experiments,[0],[0]
"As our baseline NMT systems we use Sockeye (Hieber et al., 2017)4.",5 Experiments,[0],[0]
Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2.,5 Experiments,[0],[0]
"We report baselines with and without dropout (Srivastava et al., 2014).",5 Experiments,[0],[0]
For dropout a retention probability of 0.5 was used.,5 Experiments,[0],[0]
As a second baseline we use our own implementation of the model of Zhang et al. (2016) which contains a single sentence-level Gaussian latent variable (SENT).,5 Experiments,[0],[0]
Our implementation differs from theirs in three aspects.,5 Experiments,[0],[0]
"First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the inference network (Zhang et al. (2016) use the average of all states).",5 Experiments,[0],[0]
"Second, the latent variable is smaller in size than the one used by (Zhang et al., 2016).5",5 Experiments,[0],[0]
This was done to make their model and the stochastic decoder proposed here as similar as possible.,5 Experiments,[0],[0]
"Finally, their implementation was based on groundhog whereas ours builds on Sockeye.",5 Experiments,[0],[0]
Our stochastic decoder model (SDEC) is also built on top of the basic Sockeyemodel.,5 Experiments,[0],[0]
It adds the components described in Sections 3 and 4.,5 Experiments,[0],[0]
Recall that the functions that compute the means and standard deviations are implemented by neural nets with a single hidden layer with tanh activation.,5 Experiments,[0],[0]
The width of that layer is twice the size of the latent variable.,5 Experiments,[0],[0]
In our experiments we tested different latent variable sizes and used KL scaling (see Section 4.1).,5 Experiments,[0],[0]
"The scale started from 0 and was increased by 1/20,000 after each mini-batch.",5 Experiments,[0],[0]
"Thus, at iteration t the scale is min(t/20,000, 1).",5 Experiments,[0],[0]
"All models use 1028 units for the LSTM hid3https://github.com/google/sentencepiece 4https://github.com/awslabs/sockeye 5We did, however, find that increasing the latent variable size actually hurt performance in our implementation.
",5 Experiments,[0],[0]
den state (or 512 for each direction in the bidirectional LSTMs) and 256 for the attention mechansim.,5 Experiments,[0],[0]
"Training is done with Adam (Kingma and Ba, 2015).",5 Experiments,[0],[0]
In decoding we use a beam of size 5 and output the most likely word at each position.,5 Experiments,[0],[0]
We deterministically set all latent variables to their mean values during decoding.,5 Experiments,[0],[0]
"Monte Carlo decoding (Gal, 2016) is difficult to apply to our setting as it would require sampling entire translations.
",5 Experiments,[0],[0]
Results We show the BLEU scores for all models that we tested on the IWSLT data set in Table 2.,5 Experiments,[0],[0]
"The stochastic decoder dominates the Sockeye baseline across all 4 languages, and outperforms SENT on most languages.",5 Experiments,[0],[0]
"Except on German, there is a trend towards smaller latent variable sizes being more helpful.",5 Experiments,[0],[0]
This is in line with findings by Chung et al. (2015) and Fraccaro et al. (2016) who also used relatively small latent variables.,5 Experiments,[0],[0]
This observation also implies that our model does not improve simply because it has more parameters than the baseline.,5 Experiments,[0],[0]
That the margin between the SDEC and SENT models is not large was to be expected for two reasons.,5 Experiments,[0],[0]
"First, Chung et al. (2015) and Fraccaro et al. (2016) have shown that stochastic RNNs lead to enormous improvements in modelling continuous sequences but only modest increases in performance for discrete sequences (such as natural language).",5 Experiments,[0],[0]
"Second, translation performance is measured in BLEU score.",5 Experiments,[0],[0]
We observed that SDEC often reached better ELBO values than SENT indicating a better model fit.,5 Experiments,[0],[0]
"How to fully leverage the better modelling ability of stochastic RNNs when producing discrete outputs is a matter of future research.
",5 Experiments,[0],[0]
"Qualitative Analysis Finally, we would like to demonstrate that our model does indeed capture variation in translation.",5 Experiments,[0],[0]
"To this end, we randomly picked sentences from the IWSLT test set and had our model translate them several times, however, the values of the latent variables were sampled instead of fixed.",5 Experiments,[0],[0]
"Contrary to the BLEU-based evaluation, beam search was not used in this evaluation in order to avoid interaction between different latent variable samples.",5 Experiments,[0],[0]
See Figure 3 for examples of syntactic and lexical variation.,5 Experiments,[0],[0]
It is important to note that we do not sample from the categorical output distribution.,5 Experiments,[0],[0]
For each target position we pick the most likely word.,5 Experiments,[0],[0]
"A non-stochastic NMT system would always yield the same translation in
this scenario.",5 Experiments,[0],[0]
"Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system.",5 Experiments,[0],[0]
"This supports our initial point that the SENT model is likely insensitive to local variation, a problem that our model was designed to address.",5 Experiments,[0],[0]
"Like the model of Bowman et al. (2016), SENT presumably tends to ignore the latent variable.",5 Experiments,[0],[0]
The stochastic decoder is strongly influenced by previous work on stochastic RNNs.,6 Related Work,[0],[0]
The first such proposal was made by Bayer and Osendorfer (2015) who introduced i.i.d.,6 Related Work,[0],[0]
Gaussian latent variables at each output position.,6 Related Work,[0],[0]
"Since their model neglects any sequential dependence of the noise sources, it underperformed on several sequence modeling tasks.",6 Related Work,[0],[0]
Chung et al. (2015) made the latent variables depend on previous information by feeding the previous decoder state into the latent variable sampler.,6 Related Work,[0],[0]
Their inference model did not make use of future elements in the sequence.,6 Related Work,[0],[0]
Using a “look-ahead” mechanism in the inference net was proposed by Fraccaro et al. (2016) who had a separate stochastic and deterministic RNN layer which both influence the output.,6 Related Work,[0],[0]
"Since the stochastic layer in their model depends on the deterministic layer but not vice versa, they could first run the deterministic layer at inference time and then condition the inference net’s encoding of the future on the thus obtained features.",6 Related Work,[0],[0]
"Like us, they used KL scaling during training.",6 Related Work,[0],[0]
"More recently, Goyal et al. (2017) proposed an auxiliary loss that has the inference net predict future feature representations.",6 Related Work,[0],[0]
This approach yields state-of-the-art results but is still in need of a theoretical justification.,6 Related Work,[0],[0]
"Within translation, Zhang et al. (2016) were the first to incorporate Gaussian variables into an NMT model.",6 Related Work,[0],[0]
Their approach only uses one sentence-level latent variable (corresponding to our z0) and can thus not deal with word-level variation directly.,6 Related Work,[0],[0]
"Concurrently to our work, Su et al. (2018) have also proposed a recurrent latent variable model for NMT.",6 Related Work,[0],[0]
Their approach differs from ours in that they do not use a 0th latent variable nor a look-ahead mechanism during inference time.,6 Related Work,[0],[0]
"Furthermore, their underlying recurrent model is a GRU.",6 Related Work,[0],[0]
"In the wider field of NLP, deep generative mod-
els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016).",6 Related Work,[0],[0]
Wehave presented a recurrent decoder formachine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora.,7 Conclusion and Future Work,[0],[0]
Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation.,7 Conclusion and Future Work,[0],[0]
"The proposed model consistently outperforms strong baselines
on several language pairs.",7 Conclusion and Future Work,[0],[0]
"As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future.",7 Conclusion and Future Work,[0],[0]
"Here, we list the three which we believe to be most promising.
",7 Conclusion and Future Work,[0],[0]
• Latent factor models: our model only contains one source of variation per word.,7 Conclusion and Future Work,[0],[0]
"A latent factor model such as DARN (Gregor et al., 2014) would consider several sources simultaneously.",7 Conclusion and Future Work,[0],[0]
This would also allow us to perform a better analysis of the model behaviour as we could correlate the factors with observed linguistic phenomena.,7 Conclusion and Future Work,[0],[0]
•,7 Conclusion and Future Work,[0],[0]
"Richer prior and variational distributions: The diagonal Gaussian is likely too simple a
distribution to appropriately model the variation in our data.",7 Conclusion and Future Work,[0],[0]
"Richer distributions computed by normalising flows (Rezende and Mohamed, 2015; Kingma et al., 2016) will likely improve our model.",7 Conclusion and Future Work,[0],[0]
"• Extension to other architectures: Introducing latent variables into non-autoregressive translation models such as the transformer (Vaswani et al., 2017) should increase their translation ability further.",7 Conclusion and Future Work,[0],[0]
Philip Schulz and Wilker Aziz were supported by the Dutch Organisation for Scientific Research (NWO) VICI Grant nr. 277-89-002.,8 Acknowledgements,[0],[0]
Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105).,8 Acknowledgements,[0],[0]
"The process of translation is ambiguous, in that there are typically many valid translations for a given sentence.",abstractText,[0],[0]
"This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process.",abstractText,[0],[0]
"To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora.",abstractText,[0],[0]
We provide an indepth analysis of the pitfalls encountered in variational inference for training deep generative models.,abstractText,[0],[0]
Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.,abstractText,[0],[0]
A Stochastic Decoder for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4810–4815 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4810",text,[0],[0]
"Community question-answer fora are great resources, collecting answers to frequently and lessfrequently asked questions on specific topics, but these are often not moderated and contain many irrelevant answers.",1 Introduction,[0],[0]
"Community Question Answering (CQA), cast as a question relevancy ranking problem, was the topic of two shared tasks at SemEval 2016-17.",1 Introduction,[0],[0]
"This is a non-trivial retrieval task, typically evaluated using mean average precision (MAP).",1 Introduction,[0],[0]
"We present a strong baseline for this task, on par with or surpassing state-of-the-art systems.
",1 Introduction,[0],[0]
"The English subtasks of the SemEval CQA (Nakov et al., 2015, 2017) consist of QuestionQuestion Similarity, Question-Comment Similarity, and Question-External Comment Similarity.",1 Introduction,[0],[0]
"In this study, we focus on the core subtask of Question-Question similarity, defined as follows:",1 Introduction,[0],[0]
"Given a question, rank other relevant questions by their relevancy to that question.",1 Introduction,[0],[0]
This proved to be a difficult task in both SemEval-16 and SemEval17 as it is the one with the least amount of data available.,1 Introduction,[0],[0]
"The baseline was the ranking retrieved
by performing a Google search, which proved to be a strong baseline beating a large portion of the systems submitted.
",1 Introduction,[0],[0]
Contribution Our baseline is a simple multitask feed-forward neural network taking distance measures between pairs of questions as input.,1 Introduction,[0],[0]
We use a question-answer dataset as auxiliary task; but we also experiment with datasets for pairwise classification tasks such as natural language inference and fake news detection.,1 Introduction,[0],[0]
"This simple, easy-totrain model is on par or better than state-of-theart systems for question relevancy ranking.",1 Introduction,[0],[0]
We also show that this simple model outperforms a more complex model based on recurrent neural networks.,1 Introduction,[0],[0]
We present a simple baseline model for question relevancy ranking.1,2 Our Model,[0],[0]
It is a deep feed-forward network with a hidden layer that is shared with an auxiliary task model.,2 Our Model,[0],[0]
The input to the network is extremely simple and consists of five distance measures of the input question-question pair.,2 Our Model,[0],[0]
"§2.1 discusses these distance measures, and how they relate.",2 Our Model,[0],[0]
§2.2 introduces the multi-task learning architecture that we propose.,2 Our Model,[0],[0]
"We use four similarity metrics and three sentence representations (averaged word embeddings, binary unigram vectors, and trigram vectors).",2.1 Features,[0],[0]
"The cosine distance between the sentence representations of query x and query y is∑
i xiyi√∑",2.1 Features,[0],[0]
"i x 2 + √∑ i y 2
1Code available at http://anavaleriagonzalez/FAQ rank.
",2.1 Features,[0],[0]
The Manhattan distance is∑,2.1 Features,[0],[0]
i |xi,2.1 Features,[0],[0]
"− yi|
The Bhattacharya distance is
− ln( ∑ i √ xiyi)
and is a measure of divergence, and the Euclidean distance is √∑
i
(xi − yi)2
Note that the squared Euclidean distance is proportional to cosine distance and Manhattan distance.",2.1 Features,[0],[0]
"The Bhattacharya and Jaccard metrics, on the other hand, are sensitive to the number of types in the input (the `1 norm of the vector encodings).",2.1 Features,[0],[0]
"So, for example, only the cosine, Euclidean, and Manhattan distances will be the same for
x = 〈1, 1, 0, 0, 1, 0, 1, 1, 0, 1〉,y = 〈0, 0, 1, 0, 1, 0, 0, 0, 1, 1〉
and
x = 〈0, 0, 0, 0, 0, 1, 0, 0, 1, 1〉,y = 〈1, 1, 1, 1, 0, 0, 0, 0, 0, 1〉
The Jaccard index is the only metric that can only be applied to two of our representations, unigrams and n-grams: It is defined over mdimensional binary (indicator) vectors and therefore not applicable to averaged embeddings.",2.1 Features,[0],[0]
"It is defined as
x · y m
We represent each query pair by these 14 numerical features.",2.1 Features,[0],[0]
"Our architecture is a simple feed-forward, multitask learning (MTL) architecture.",2.2 MTL Architecture,[0],[0]
Our architecture is presented in Figure 1 and is a Multi-Layer Perceptron (MLP) that takes a pair of sequences as input.,2.2 MTL Architecture,[0],[0]
The sequences can be sampled from the main task or the auxiliary task.,2.2 MTL Architecture,[0],[0]
"The MLP has one shared hidden layer, a task-specific hidden layer and, finally, a task-specific classification layer for each output.",2.2 MTL Architecture,[0],[0]
"The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2.",2.2 MTL Architecture,[0],[0]
"We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model.",2.3 LSTM baseline,[0],[0]
"It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively.",2.3 LSTM baseline,[0],[0]
"The outputs are then concatenated, to enable representing the differences between the two sequences.",2.3 LSTM baseline,[0],[0]
"Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above.",2.3 LSTM baseline,[0],[0]
"For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs.",3 Datasets,[0],[0]
Below we present the datasets used for our main and auxiliary tasks.,3 Datasets,[0],[0]
"We provide some summary statistics for each dataset in Table 3.
SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related queries.",3 Datasets,[0],[0]
"As an auxiliary task, we use the data from subtask A, which is a questionrelated comment ranking task.
",3 Datasets,[0],[0]
"Natural Language Inference Natural Language Inference (NLI), consists in predicting ENTAILMENT, CONTRADICTION or NEUTRAL, given a hypothesis and a premise.",3 Datasets,[0],[0]
"We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres.",3 Datasets,[0],[0]
"Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task.
",3 Datasets,[0],[0]
Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online.,3 Datasets,[0],[0]
"This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018).",3 Datasets,[0],[0]
"Formally, the FNC task consists in, given a headline and the body of
2http://www.fakenewschallenge.org/
text which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline.",3 Datasets,[0],[0]
"There are four labels:
• AGREES:",3 Datasets,[0],[0]
The body of the article is in agreement with the headline • DISAGREES:,3 Datasets,[0],[0]
"The body of the article is in dis-
agreement with the headline • DISCUSSES:",3 Datasets,[0],[0]
"The body of the article does not
take a position • UNRELATED: the body of the article dis-
cusses a different topic
We include fake news detection as a weak auxiliary signal that can lead to better generalization of our question-question ranking model.",3 Datasets,[0],[0]
"We evaluate our performance on the main task of question relevancy ranking using the official
SemEval-2017 Task 3 evaluation scripts (Nakov et al., 2017).",3.1 Evaluation,[0],[0]
"The scripts provide a variety of metrics; however, in accordance with the shared task, we report Mean Average Precision (MAP) (the official metric for the SemEval 2016 and 2017 shared tasks); Mean Reciprocal Rank (MRR), which has being thoroughly used for IR and QA; Average Recall; and, finally, the accuracy of predicting relevant documents.",3.1 Evaluation,[0],[0]
The results from our experiments are shown in Table 1.,4 Results,[0],[0]
"We present the official metric from the SemEval task, as well as other common metrics.",4 Results,[0],[0]
"For the SemEval-16 data, our multitask MLP architecture with a question-answer auxiliary task performed best on all metrics, except accuracy, where the multi-task MLP using all auxiliary tasks performed best.",4 Results,[0],[0]
We outperform the winning systems of both the SemEval 2016 and 2017 campaigns.,4 Results,[0],[0]
"In addition, our improvements from single-task to multi-task are significant (p < 0.01).",4 Results,[0],[0]
We also outperform the official IR baseline used in the SemEval 2016 and 2017 shared tasks.,4 Results,[0],[0]
We discuss the STL-LSTM-SIM results in §5.,4 Results,[0],[0]
"Furthermore, in Table 2, we show the performance of our models when training on feature combinations, while in Table 3, we present an ablation test where we remove one feature at a time.
",4 Results,[0],[0]
"Learning curve In Figure 4, we also present our learning curves for the development set when incrementally increasing the training set size.",4 Results,[0],[0]
"We observe that when using an auxiliary task, the learning is more stable across training set size.",4 Results,[0],[0]
"For the SemEval shared tasks on CQA, several authors used complex recurrent and convolutional neural network architectures (Severyn and Moschitti, 2015; Barrón-Cedeno et al., 2016).",5 Discussion,[0],[0]
"For example, Barrón-Cedeno et al. used a convolutional neural network in combination with feature vectors representing lexical, syntactic, and semantic similarity as well as tree kernels.",5 Discussion,[0],[0]
Their performance was slightly lower than the best system (SemEval-Best for 2016 in Table 1).,5 Discussion,[0],[0]
"The best system used lexical and semantic similarity measures in combination with a ranking model based on support vector machines (SVMs) (Filice et al., 2016; Franco-Salvador et al., 2016).",5 Discussion,[0],[0]
Both systems are harder to implement and train than the model we propose here.,5 Discussion,[0],[0]
"For SemEval-17, FrancoSalvador et al. (2016), the winning team used
distributed representations of words, knowledge graphs and frames from FrameNet (Baker et al., 1998) as some of their features, and used SVMs for ranking.
",5 Discussion,[0],[0]
"For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose.",5 Discussion,[0],[0]
"This architecture is based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997).",5 Discussion,[0],[0]
"For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query.",5 Discussion,[0],[0]
We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer.,5 Discussion,[0],[0]
"This way we can evaluate the usefulness of the flexible, expressive LSTM network directly (as our MTL model becomes an ablation instance of the full, more complex architecture).",5 Discussion,[0],[0]
We use the same dropout regularization and SGD values as for the MLP.,5 Discussion,[0],[0]
"Tuning all parameters on the development data, we do not manage to outperform our proposed model, however.",5 Discussion,[0],[0]
See lines MTL-LSTM-SIM in Table 1 for results.,5 Discussion,[0],[0]
"We show that simple feature engineering, combined with an auxiliary task and a simple feedfor-
ward neural architecture is appropriate for a small dataset and manages to beat the baseline and the best performing systems for the Semeval task of question relevancy ranking.",6 Conclusion,[0],[0]
We observe that introducing pairwise classification tasks leads to significant improvements in performance and a more stable model.,6 Conclusion,[0],[0]
"Overall, our simple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data.",6 Conclusion,[0],[0]
The first author of this paper is funded by a BotXO PhD Award;3 the last author by an ERC Starting Grant.,Acknowledgments,[0],[0]
We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.,Acknowledgments,[0],[0]
The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks – a task that amounts to question relevancy ranking – involve complex pipelines and manual feature engineering.,abstractText,[0],[0]
"Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google’s search engine.",abstractText,[0],[0]
We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair.,abstractText,[0],[0]
"This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.",abstractText,[0],[0]
A strong baseline for question relevancy ranking,title,[0],[0]
"Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention.",1 Introduction,[0],[0]
"Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing.
",1 Introduction,[0],[0]
"The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction
and normalization and 2) temporal relation (also known as TLINKs (Pustejovsky et al., 2003a)) extraction.",1 Introduction,[0],[0]
"While the first task has now been well handled by the state-of-the-art systems (HeidelTime (Strötgen and Gertz, 2010), SUTime (Chang and Manning, 2012), IllinoisTime (Zhao et al., 2012), NavyTime (Chambers, 2013), UWTime (Lee et al., 2014), etc.) with end-to-end F1 scores being around 80%, the second task has long been a challenging one; even the top systems only achieved F1 scores of around 35% in the TE workshops.
",1 Introduction,[0],[0]
"The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e., events or timexes) and edges represent the TLINKs between them.",1 Introduction,[0],[0]
"The task is challenging because it often requires global considerations – considering the entire graph, the TLINK annotation is quadratic in the number of nodes and thus very expensive, and an overwhelming fraction of the temporal relations are missing in human annotation.",1 Introduction,[0],[0]
"In this paper, we propose a structured learning approach to temporal relation extraction, where local models are updated based on feedback from global inferences.",1 Introduction,[0],[0]
"The structured approach also gives rise to a semisupervised method, making it possible to take advantage of the readily available unlabeled data.",1 Introduction,[0],[0]
"As a byproduct, this approach further provides a new, effective perspective on handling those missing relations.
",1 Introduction,[0],[0]
"In the common formulations, temporal relations are categorized into three types: the E-E TLINKs (those between a pair of events), the T-T TLINKs (those between a pair of timexes), and the E-T TLINKs (those between an event and a timex).",1 Introduction,[0],[0]
"While the proposed approach can be generally applied to all three types, this paper focuses on the majority type, i.e., the E-E TLINKs.",1 Introduction,[0],[0]
"For example, consider the following snippet taken from the
ar X
iv :1
90 6.
04 94
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 Ju
n 20
19
training set provided in the TE3 workshop.",1 Introduction,[0],[0]
"We want to construct a temporal graph as in Fig. 1 for the events in boldface in Ex1.
Ex1 . . .",1 Introduction,[0],[0]
"tons of earth cascaded down a hillside, ripping two houses from their foundations.",1 Introduction,[0],[0]
"No one was hurt, but firefighters ordered the evacuation of nearby homes and said they’ll monitor the shifting ground.. . .
",1 Introduction,[0],[0]
"As discussed in existing work (Verhagen, 2004; Bramsen et al., 2006; Mani et al., 2006; Chambers and Jurafsky, 2008), the structure of a temporal graph is constrained by some rather simple rules:
1.",1 Introduction,[0],[0]
Symmetry.,1 Introduction,[0],[0]
"For example, if A is before B, then B must be after A.
2.",1 Introduction,[0],[0]
Transitivity.,1 Introduction,[0],[0]
"For example, if A is before B and B is before C, then A must be before C.
This particular structure of a temporal graph (especially the transitivity structure) makes its nodes highly interrelated, as can be seen from Fig. 1.",1 Introduction,[0],[0]
"It is thus very challenging to identify the TLINKs between them, even for human annotators: The inter-annotator agreement on TLINKs is usually about 50%-60% (Mani et al., 2006).",1 Introduction,[0],[0]
Fig. 2 shows the actual human annotations provided by TE3.,1 Introduction,[0],[0]
"Among all the ten possible pairs of nodes, only three TLINKs were annotated.",1 Introduction,[0],[0]
"Even if we only look at main events in consecutive sentences and at events in the same sentence, there are still quite a few missing TLINKs, e.g., the one between hurt and cascaded and the one between monitor and ordered.
",1 Introduction,[0],[0]
Early attempts by Mani et al. (2006); Chambers et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) studied local methods – learning models that make pairwise decisions between each pair of events.,1 Introduction,[0],[0]
"State-of-the-art local methods, including ClearTK (Bethard, 2013), UTTime
(Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results.",1 Introduction,[0],[0]
"However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph).",1 Introduction,[0],[0]
"Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs.",1 Introduction,[0],[0]
"Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005).",1 Introduction,[0],[0]
"In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline.",1 Introduction,[0],[0]
The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve.,1 Introduction,[0],[0]
"This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008).
",1 Introduction,[0],[0]
"Although L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning).",1 Introduction,[0],[0]
"In parallel to the work presented here, Leeuwenberg and Moens (2017) also proposed a structured learning approach to extracting the temporal relations.",1 Introduction,[0],[0]
"Their work focuses on a domain-specific dataset from Clinical TempEval (Bethard et al., 2016), so their work does not need to address some of the difficulties of the general problem that our work addresses.",1 Introduction,[0],[0]
"More importantly, they compared structured learning to local baselines, while we find that the comparison between structured learning and L+I is more interesting and important for
understanding the effect of global considerations in the learning phase.",1 Introduction,[0],[0]
"In difference from existing methods, we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way.",1 Introduction,[0],[0]
"Our solution targets on these issues and, as we show, achieves significant improvements on two commonly used evaluation sets.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
"Section 2 clarifies the temporal relation types and the evaluation metric of a temporal graph used in this paper, Section 3 explains the structured learning approach in detail, and Section 4 discusses the practical issue of missing relations.",1 Introduction,[0],[0]
We provide experiments and discussion in Section 5 and conclusion in Section 6.,1 Introduction,[0],[0]
"Existing corpora for temporal processing often follows the interval representation of events proposed in Allen (1984), and makes use of 13 relation types in total.",2.1 Temporal Relation Types,[0],[0]
"In many systems, vague or none is also included as another relation type when a TLINK is not clear or missing.",2.1 Temporal Relation Types,[0],[0]
"However, current systems usually use a reduced set of relation types, mainly due to the following reasons.
1.",2.1 Temporal Relation Types,[0],[0]
The non-uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others (see Table 1 in Mani et al. (2006)).,2.1 Temporal Relation Types,[0],[0]
"For example, relations such as immediately before or immediately after barely exist in a corpus compared to before and after.
2.",2.1 Temporal Relation Types,[0],[0]
"Due to the ambiguity in natural language, determining relations like before and immediately before can be a difficult task itself (Chambers et al., 2014).
",2.1 Temporal Relation Types,[0],[0]
"In this work, we follow the reduced set of temporal relation types used in CAEVO (Chambers et al., 2014): before, after, includes, is included, equal, and vague.",2.1 Temporal Relation Types,[0],[0]
"The most recent evaluation metric in TE3, i.e., the temporal awareness (UzZaman and Allen, 2011), is adopted in this work.",2.2 Quality of A Temporal Graph,[0],[0]
"Specifically, let Gsys and Gtrue be two temporal graphs from the system prediction and the ground truth, respectively.",2.2 Quality of A Temporal Graph,[0],[0]
"The
precision and recall of temporal awareness are defined as follows.
",2.2 Quality of A Temporal Graph,[0],[0]
"P = |G−sys ∩G+true| |G−sys| , R = |G−true ∩G+sys| |G−true|
where G+ is the closure of graph G, G− is the reduction of G, “∩” is the intersection between TLINKs in two graphs, and |G| is the number of TLINKs in G. The temporal awareness metric better captures how “useful” a temporal graph is.",2.2 Quality of A Temporal Graph,[0],[0]
"For example, if system 1 produces ripping is before hurt and hurt is before monitor, and system 2 adds ripping is before monitor on top of system 1.",2.2 Quality of A Temporal Graph,[0],[0]
"Since system 2 is simply a transitive closure of system 1, they would have the same evaluation scores.",2.2 Quality of A Temporal Graph,[0],[0]
Note that vague relations are usually considered as non-existing TLINKs and are not counted during evaluation.,2.2 Quality of A Temporal Graph,[0],[0]
"As shown in Fig. 1, the learning problem in temporal relation extraction is global in nature.",3 A Structured Training Approach,[0],[0]
"Even the top local method in TE3, UTTime (Laokulrat et al., 2013), only achieved F1=56.5 when presented with a pair of temporal entities (Task C– relation only (UzZaman et al., 2013)).",3 A Structured Training Approach,[0],[0]
"Since the success of an L+I method strongly relies on the quality of the local classifiers, a poor local classifier is obviously a roadblock for L+I methods.",3 A Structured Training Approach,[0],[0]
"Following the insights from Punyakanok et al. (2005), we propose to use a structured learning approach (also called “Inference Based Training” (IBT)).
",3 A Structured Training Approach,[0],[0]
"Unlike the current L+I approach, where local classifiers are trained independently beforehand without knowledge of the predictions on neighboring pairs, we train local classifiers with feedback that accounts for other relations, by performing global inference in each round of the learning process.",3 A Structured Training Approach,[0],[0]
"In order to introduce the structured learning algorithm, we first explain its most important component, the global inference step.",3 A Structured Training Approach,[0],[0]
"In a document with n pairs of events, let φi ∈ X ⊆ Rd be the extracted d-dimensional feature and yi ∈ Y be the temporal relation for the i-th pair of events, i = 1, 2, . . .",3.1 Inference,[0],[0]
", n, where Y = {rj}6j=1 is the label set for the six temporal relations we use.",3.1 Inference,[0],[0]
"Moreover, let x = {φ1, . . .",3.1 Inference,[0],[0]
", φn} ∈ X n",3.1 Inference,[0],[0]
"and y = {y1, . . .",3.1 Inference,[0],[0]
", yn} ∈ Yn be more compact representations of all the features and labels in this
document.",3.1 Inference,[0],[0]
"Given the weight vector wr of a linear classifier trained for relation r ∈ Y (i.e., using the one-vs-all scheme), the global inference step is to solve the following constrained optimization problem:
ŷ = arg max y∈C(Yn) f(x,y), (1)
where C(Yn) ⊆ Yn constrains the temporal graph to be symmetrically and transitively consistent, and f(x,y) is the scoring function:
f(x,y) = n∑ i=1",3.1 Inference,[0],[0]
fyi(φi),3.1 Inference,[0],[0]
= n∑ i=1,3.1 Inference,[0],[0]
ew T yi φi∑,3.1 Inference,[0],[0]
"r∈Y e wTr φi .
",3.1 Inference,[0],[0]
"Specifically, fyi(φi) is the probability of the i-th event pair having relation yi.",3.1 Inference,[0],[0]
"f(x, y) is simply the sum of these probabilities over all the event pairs in a document, which we think of as the confidence of assigning y = {y1, ..., yn} to this document and therefore, it needs to be maximized in Eq.",3.1 Inference,[0],[0]
"(1).
Note that when C(Yn) = Yn, Eq. (1) can be solved for each ŷi independently, which is what the so-called local methods do, but the resulting ŷ may not satisfy global consistency in this way.",3.1 Inference,[0],[0]
When C(Yn) 6=,3.1 Inference,[0],[0]
"Yn, Eq.",3.1 Inference,[0],[0]
"(1) cannot be decoupled for each ŷi and is usually formulated as an ILP problem (Roth and Yih, 2004; Chambers and Jurafsky, 2008; Do et al., 2012).",3.1 Inference,[0],[0]
"Specifically, let Ir(ij) ∈ {0, 1} be the indicator function of relation r for event i and event j and fr(ij) ∈",3.1 Inference,[0],[0]
"[0, 1] be the corresponding soft-max score.",3.1 Inference,[0],[0]
"Then the ILP objective for global inference is formulated as follows.
",3.1 Inference,[0],[0]
Î,3.1 Inference,[0],[0]
=,3.1 Inference,[0],[0]
argmax,3.1 Inference,[0],[0]
"I
∑ ij∈E ∑ r∈Y fr(ij)Ir(ij) (2)
s.t. ΣrIr(ij) = 1 (uniqueness) , Ir(ij) = Ir̄(ji), (symmetry)
Ir1(ij)",3.1 Inference,[0],[0]
"+ Ir2(jk)− ΣNm=1Irm3 (ik) ≤ 1, (transitivity)
for all distinct events i, j, and k, where E = {ij | sentence dist(i, j)≤ 1}, r̄ is the reverse of r, and N is the number of possible relations for r3 when r1 and r2 are true.
",3.1 Inference,[0],[0]
Our formulation in Eq.,3.1 Inference,[0],[0]
"(2) is different from previous work (Chambers and Jurafsky, 2008; Do et al., 2012) in two aspects: 1) We restrict our event pairs ij to a smaller set E = {ij | sentence dist(i, j)≤ 1} where pairs that are
more than one sentence away are deleted for computational efficiency and (usually) for better performance.",3.1 Inference,[0],[0]
"In fact, to make better use of global constraints, we should have allowed more event pairs in Eq.",3.1 Inference,[0],[0]
(2).,3.1 Inference,[0],[0]
"However, fr(ij) is usually more reliable when i and j are closer in text.",3.1 Inference,[0],[0]
"Many participating systems in TE3 (UzZaman et al., 2013) have used this pre-filtering strategy to balance the trade-off between confidence in fr(ij) and global constraints.",3.1 Inference,[0],[0]
"We observe that the strategy fits very well to the existing datasets: As shown in Fig. 3, annotated TLINKs barely exist if two events are two sentences away.",3.1 Inference,[0],[0]
"2) Previously, transitivity constraints were formulated as Ir1(ij) + Ir2(jk)",3.1 Inference,[0],[0]
"− Ir3(ik) ≤ 1, which is a special case when N = 1 and can be understood as “r1 and r2 determine a single r3”.",3.1 Inference,[0],[0]
"However, it was overlooked that, although some r1 and r2 cannot uniquely determine r3, they can still constrain the set of labels r3 can take.",3.1 Inference,[0],[0]
"For example, as shown in Fig. 4, when r1=before and r2=is included, r3 is not determined",3.1 Inference,[0],[0]
"but we know that r3 ∈ {before, is included}1.",3.1 Inference,[0],[0]
"This information can be easily exploited by allowing N > 1.
",3.1 Inference,[0],[0]
"With these two differences, the optimization problem (2) can still be efficiently solved using off-the-shelf ILP packages such as GUROBI
1The transitivity table in Allen (1983) shows two more possible relations, overlap and immediately before, which are not in our label set.
",3.1 Inference,[0],[0]
"(Gurobi Optimization, Inc., 2012).",3.1 Inference,[0],[0]
"With the inference solver defined above, we propose to use the structured perceptron (Collins, 2002) as a representative for the inference based training (IBT) algorithm to learn those weight vectors wr.",3.2 Learning,[0],[0]
"Specifically, let L = {xk,yk}Kk=1 be the labeled training set of K instances (usually documents).",3.2 Learning,[0],[0]
The structured perceptron training algorithm for this problem is shown in Algorithm 1.,3.2 Learning,[0],[0]
"The Illinois-SL package (Chang et al., 2010) was used in our experiments for its structured perceptron component.",3.2 Learning,[0],[0]
"In terms of the features used in this work, we adopt the same set of features designed for E-E TLINKs in Sec. 3.1 of Do et al. (2012).
",3.2 Learning,[0],[0]
"In Algorithm 1, Line 6 is the inference step as in Eq.",3.2 Learning,[0],[0]
"(1) or (2), which is augmented with a closure operation on ŷ in the following line.",3.2 Learning,[0],[0]
"In the case in which there is only one pair of events in each instance (thus no structure to take advantage of), Algorithm 1 reduces to the conventional perceptron algorithm and Line 6 simply chooses the top scoring label.",3.2 Learning,[0],[0]
"With a structured instance instead, Line 6 becomes slower to solve, but it can provide valuable information so that the perceptron learner is able to look further at other labels rather than an isolated pair.",3.2 Learning,[0],[0]
"For example in Ex1 and Fig. 1, the fact that (ripping,ordered)=before is established through two other relations: 1) ripping is an adverbial participle and thus included in cascaded and 2) cascaded is before ordered.",3.2 Learning,[0],[0]
"If (ripping,ordered)=before is presented to a local learning algorithm without knowing its predictions on (ripping,cascaded) and (cascaded,ordered), then the model either cannot support it or overfits it.",3.2 Learning,[0],[0]
"In IBT, however, if the classifier was correct in deciding (ripping,cascaded) and (cascaded,ordered), then (ripping,ordered) would be correct automatically and would not contribute to updating the classifier.",3.2 Learning,[0],[0]
The scarcity of training data and the difficulty in annotation have long been a bottleneck for temporal processing systems.,3.3 Semi-supervised Structured Learning,[0],[0]
"Given the inherent global constraints in temporal graphs, we propose to perform semi-supervised structured learning using the constraint-driven learning (CoDL) algorithm (Chang et al., 2007, 2012), as shown in Algorithm 2, where the function “Learn” in Lines 2 and 9 represents any standard learning algorithm
Algorithm 1: Structured perceptron algorithm for temporal relations
Input: Training set L = {xk,yk}Kk=1, learning rate λ
1 Perform graph closure on each yk 2",3.3 Semi-supervised Structured Learning,[0],[0]
"Initialize wr = 0, ∀r ∈ Y 3 while convergence criteria not satisfied do 4 Shuffle the examples in L 5 foreach (x,y) ∈ L do 6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 if ŷ",3.3 Semi-supervised Structured Learning,[0],[0]
6=,3.3 Semi-supervised Structured Learning,[0],[0]
"y then 9 wr = wr + λ( ∑ i:yi=r
φi−∑",3.3 Semi-supervised Structured Learning,[0],[0]
"i:ŷi=r φi), ∀r ∈ Y
10 return {wr}r∈Y
(e.g., perceptron, SVM, or even structured perceptron",3.3 Semi-supervised Structured Learning,[0],[0]
"; here we used the averaged perceptron (Freund and Schapire, 1998)) and subscript “r” means selecting the learned weight vector for relation r ∈ Y .",3.3 Semi-supervised Structured Learning,[0],[0]
"CoDL improves the model learned from a small amount of labeled data by repeatedly generating feedback through labeling unlabeled examples, which is in fact a semi-supervised version of IBT.",3.3 Semi-supervised Structured Learning,[0],[0]
"Experiments show that this scheme is indeed helpful in this problem.
",3.3 Semi-supervised Structured Learning,[0],[0]
Algorithm 2: Constraint-driven learning algorithm,3.3 Semi-supervised Structured Learning,[0],[0]
"Input: Labeled set L, unlabeled set U ,
weighting coefficient γ 1",3.3 Semi-supervised Structured Learning,[0],[0]
"Perform closure on each graph in L 2 Initialize wr = Learn(L)r, ∀ r ∈",3.3 Semi-supervised Structured Learning,[0],[0]
Y 3 while convergence criteria not satisfied do 4 T = ∅ 5 foreach x ∈ U do,3.3 Semi-supervised Structured Learning,[0],[0]
"6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 T = T ∪ {(x, ŷ)} 9 wr = γwr + (1− γ)Learn(T )",3.3 Semi-supervised Structured Learning,[0],[0]
"r,∀ r ∈ Y
10 return {wr}r∈Y",3.3 Semi-supervised Structured Learning,[0],[0]
"Since even human annotators find it difficult to annotate temporal graphs, many of the TLINKs are left unspecified by annotators (compare Fig. 2 to Fig. 1).",4 Missing Annotations,[0],[0]
"While some of these missing TLINKs can be inferred from existing ones, the vast majority still remain unknown as shown in Table 1.",4 Missing Annotations,[0],[0]
"De-
spite the existence of denser annotation schemes (e.g., Cassidy et al. (2014)), the TLINK annotation task is quadratic in the number of nodes, and it is practically infeasible to annotate complete graphs.",4 Missing Annotations,[0],[0]
"Therefore, the problem of identifying these unknown relations in training and test is a major issue that dramatically hurts existing methods.
",4 Missing Annotations,[0],[0]
We could simply use these unknown pairs (or some filtered version of them) to design rules or train classifiers to identify whether a TLINK is vague or not.,4 Missing Annotations,[0],[0]
"However, we propose to exclude both the unknown pairs and the vague classifier from the training process – by changing the structured loss function to ignore the inference feedback on vague TLINKs (see Line 9 in Algorithm 1 and Line 9 in Algorithm 2).",4 Missing Annotations,[0],[0]
"The reasons are discussed below.
",4 Missing Annotations,[0],[0]
"First, it is believed that a lot of the unknown pairs are not really vague but rather pairs that the annotators failed to look at (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014).",4 Missing Annotations,[0],[0]
"For example, (cascaded, monitor) should be annotated as before but is missing in Fig. 2.",4 Missing Annotations,[0],[0]
It is hard to exclude this noise in the data during training.,4 Missing Annotations,[0],[0]
"Second, compared to the overwhelmingly large number of unknown TLINKs (89.5% as shown in Table 1), the scarcity of non-vague TLINKs makes it hard to learn a good vague classifier.",4 Missing Annotations,[0],[0]
"Third, vague is fundamentally different from the other relation types.",4 Missing Annotations,[0],[0]
"For example, if a before TLINK can be established given a sentence, then it always holds as before regardless of other events around it, but if a TLINK is vague given a sentence, it may still change to other types afterwards if a connection can later be established through other nodes from the context.",4 Missing Annotations,[0],[0]
"This distinction emphasizes that vague is a consequence of lack of background/contextual information, rather than a concrete relation type to be trained on.",4 Missing Annotations,[0],[0]
"Fourth, without the vague classifier, the predicted temporal graph tends to become more densely connected, thus the global transitivity constraints can be more effective in correcting local mistakes (Chambers
and Jurafsky, 2008).",4 Missing Annotations,[0],[0]
"However, excluding the local classifier for vague TLINKs would undesirably assign nonvague TLINKs to every pair of events.",4 Missing Annotations,[0],[0]
"To handle this, we take a closer look at the vague TLINKs.",4 Missing Annotations,[0],[0]
We note that a vague TLINK could arise in two situations if the annotators did not fail to look at it.,4 Missing Annotations,[0],[0]
"One is that an annotator looks at this pair of events and decides that multiple relations can exist, and the other one is that two annotators disagree on the relation (similar arguments were also made in Cassidy et al. (2014)).",4 Missing Annotations,[0],[0]
"In both situations, the annotators first try to assign all possible relations to a TLINK, and then change the relation to vague if more than one can be assigned.",4 Missing Annotations,[0],[0]
"This human annotation process for vague is different from many existing methods, which either identify the existence of a TLINK first (using rules or machinelearned classifiers) and then classify, or directly include vague as a classification label along with other non-vague relations.
",4 Missing Annotations,[0],[0]
"In this work, however, we propose to mimic this mental process by a post-filtering method2.",4 Missing Annotations,[0],[0]
"Specifically, we take each TLINK produced by ILP and determine whether it is vague using its relative entropy (the Kullback-Leibler divergence) to the uniform distribution.",4 Missing Annotations,[0],[0]
"Let {rm}Mm=1 be the set of relations that the i-th pair of events can take, we filter the i-th TLINK given by ILP by:
δi = M∑ m=1 frm(φi) log (Mfrm(φi)),
where frm(φi) is the soft-max score of rm, obtained by the local classifier for rm.",4 Missing Annotations,[0],[0]
"We then compare δi to a fixed threshold τ to determine the vagueness of this TLINK; we accept its originally predicted label if δi > τ , or change it to vague otherwise.",4 Missing Annotations,[0],[0]
Using relative entropy here is intuitively appealing and empirically useful as shown in the experiments section; better metrics are of course yet to be designed.,4 Missing Annotations,[0],[0]
"The TempEval3 (TE3) workshop (UzZaman et al., 2013) provided the TimeBank (TB) (Pustejovsky et al., 2003b), AQUAINT (AQ) (Graff, 2002), Silver (TE3-SV), and Platinum (TE3-PT) datasets,
2Some systems (e.g., TARSQI (Verhagen and Pustejovsky, 2008)) employed a similar idea from a different standpoint, by thresholding TLINKs based on confidence scores.
where TB and AQ are usually for training, and TE3-PT is usually for testing.",5.1 Datasets,[0],[0]
"The TE3-SV dataset is a much larger, machine-annotated and automatically-merged dataset based on multiple systems, with the intention to see if these “silver” standard data can help when included in training (although almost all participating systems saw performance drop with TE3-SV included in training).
",5.1 Datasets,[0],[0]
Two popular augmentations on TB are the VerbClause temporal relation dataset (VC) and TimebankDense dataset (TD).,5.1 Datasets,[0],[0]
"The VC dataset has specially annotated event pairs that follow the socalled Verb-Clause structure (Bethard et al., 2007), which is usually beneficial to be included in training (UzZaman et al., 2013).",5.1 Datasets,[0],[0]
The TD dataset contains 36 documents from TB which were reannotated using the dense event ordering framework proposed in Cassidy et al. (2014).,5.1 Datasets,[0],[0]
The experiments included in this paper will involve the TE3 datasets as well as these augmentations.,5.1 Datasets,[0],[0]
"Therefore, some statistics on them are shown in Table 2 for the readers’ information.",5.1 Datasets,[0],[0]
"In addition to the state-of-the-art systems, another two baseline methods were also implemented for a better understanding of the proposed ones.",5.2 Baseline Methods,[0],[0]
"The first is the regularized averaged perceptron (AP) (Freund and Schapire, 1998) implemented in the LBJava package (Rizzolo and Roth, 2010) and is a local method.",5.2 Baseline Methods,[0],[0]
"On top of the first baseline, we performed global inference in Eq.(2), referred to as the L+I baseline (AP+ILP).",5.2 Baseline Methods,[0],[0]
"Both of them used the same feature set (i.e., as designed in Do et al. (2012))",5.2 Baseline Methods,[0],[0]
as in the proposed structured perceptron (SP) and CoDL for fair comparisons.,5.2 Baseline Methods,[0],[0]
"To clarify,
SP and CoDL are training algorithms and their immediate outputs are the weight vectors {wr}r∈Y for local classifiers.",5.2 Baseline Methods,[0],[0]
"An ILP inference was performed on top of them to yield the final output, and we refer to it as “S+I” (i.e., structured learning+inference) methods.",5.2 Baseline Methods,[0],[0]
"To show the benefit of using structured learning, we first tested one scenario where the gold pairs of events that have a non-vague TLINK were known priori.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This setup was a standard task presented in TE3, so that the difficulty of detecting vague TLINKs was ruled out.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This setup also helps circumvent the issue that TE3 penalizes systems which assign extra labels that do not exist in the annotated graph, while these extra labels may be actually correct because the annotation itself might be incomplete.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"UTTime (Laokulrat et al., 2013) was the top system in this task in TE3.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"Since UTTime is not available to us, and its performance was reported in TE3 in terms of both E-E and E-T TLINKs together, we locally trained an E-T classifier based on Do et al. (2012) and included its prediction only for fair comparison.
",5.3.1 TE3 Task C - Relation Only,[0],[0]
UTTime is a local method and was trained on TB+AQ and tested on TE3-PT.,5.3.1 TE3 Task C - Relation Only,[0],[0]
We used the same datasets for our local baseline and its performance is shown in Table 3 under the name “AP-1”.,5.3.1 TE3 Task C - Relation Only,[0],[0]
Note that the reported numbers below are the temporal awareness scores obtained from the official evaluation script provided in TE3.,5.3.1 TE3 Task C - Relation Only,[0],[0]
"We can see that UTTime is about 3% better than AP-1 in the absolute value of F1, which is expected since UTTime included more advanced features derived from syntactic parse trees.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"By adding the VC and TD datasets into the training set, we retrained our local baseline and achieved comparable performance to
UTTime (“AP-2” in Table 3).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"On top of AP-2, a global inference step enforcing symmetry and transitivity constraints (“AP+ILP”) can further improve the F1 score by 9.3%, which is consistent with previous observations (Chambers and Jurafsky, 2008; Do et al., 2012).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"SP+ILP further improved the performance in precision, recall, and F1 significantly (per the McNemar’s test (Everitt, 1992; Dietterich, 1998) with p <0.0005), reaching an F1 score of 67.2%.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This meets our expectation that structured learning can be better when the local problem is difficult (Punyakanok et al., 2005).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"In the first scenario, we knew in advance which TLINKs existed or not, so the “pre-filtering” (i.e., ignoring distant pairs as mentioned in Sec. 3.1 and “post-filtering” methods were not used when generating the results in Table 3.",5.3.2 TE3 Task C,[0],[0]
"We then tested a more practical scenario, where we only knew the events, but did not know which ones are related.",5.3.2 TE3 Task C,[0],[0]
"This setup was Task C in TE3 and the top system was ClearTK (Bethard, 2013).",5.3.2 TE3 Task C,[0],[0]
"Again, for fair comparison, we simply added the E-T TLINKs predicted by ClearTK.",5.3.2 TE3 Task C,[0],[0]
"Moreover, 10% of the training data was held out for development.",5.3.2 TE3 Task C,[0],[0]
"Corresponding results on the TE3-PT testset are shown in Table 4.
",5.3.2 TE3 Task C,[0],[0]
"From lines 2-4, all systems see significant drops in performance if compared with the same entries in Table 3.",5.3.2 TE3 Task C,[0],[0]
It confirms our assertion that how to handle vague TLINKs is a major issue for this temporal relation extraction problem.,5.3.2 TE3 Task C,[0],[0]
"The improvement of SP+ILP (line 4) over AP (line 2) was small and AP+ILP (line 3) was even worse than AP, which necessitates the use of a better approach
towards vague TLINKs.",5.3.2 TE3 Task C,[0],[0]
"By applying the postfiltering method proposed in Sec. 4, we were able to achieve better performances using SP+ILP (line 5), which shows the effectiveness of this strategy.",5.3.2 TE3 Task C,[0],[0]
"Finally, by setting U in Algorithm 2 to be the TE3-SV dataset, CoDL+ILP (line 6) achieved the best F1 score with a relative improvement over ClearTK being 14.8%.",5.3.2 TE3 Task C,[0],[0]
"Note that when using TE3SV in this paper, we did not use its annotations on TLINKs because of its well-known large noise (UzZaman et al., 2013).
",5.3.2 TE3 Task C,[0],[0]
"In UzZaman et al. (2013), we notice that the best performance of ClearTK was achieved when trained on TB+VC (line 7 is higher than its reported values in TE3 because of later changes in ClearTK), so we retrained the proposed systems on the same training set and results are shown on lines 8-9.",5.3.2 TE3 Task C,[0],[0]
"In this case, the improvement of S+I over Local was small, which may be due to the lack of training data.",5.3.2 TE3 Task C,[0],[0]
"Note that line 8 was still significantly different to line 7 per the McNemar’s test, although there was only 0.2% absolute difference in F1, which can be explained from their large differences in precision and recall.",5.3.2 TE3 Task C,[0],[0]
"The proposed structured learning approach was further compared to a recent system, a CAscading EVent Ordering architecture (CAEVO) proposed in Chambers et al. (2014) (lines 10-13).",5.3.3 Comparison with CAEVO,[0],[0]
We used the same training set and test set as CAEVO in the S+I systems.,5.3.3 Comparison with CAEVO,[0],[0]
"Again, we added the E-T TLINKs predicted by CAEVO to both S+I systems.",5.3.3 Comparison with CAEVO,[0],[0]
"In Chambers et al. (2014), CAEVO was reported on the straightforward evaluation metric including the vague TLINKs, but the temporal awareness scores
were used here, which explains the difference between line 11 in Table 4 and what was reported in Chambers et al. (2014).
ClearTK was reported to be outperformed by CAEVO on TD-Test (Chambers et al., 2014), but we observe that ClearTK on line 10 was much worse even than itself on line 7 (trained on TB+VC) and on line 1 (trained on TB+AQ+VC+TD) due to the annotation scheme difference between TD and TB/AQ/VC. ClearTK was designed mainly for TE3, aiming for high precision, which is reflected by its high precision on line 10, but it does not have enough flexibility to cope with two very different annotation schemes.",5.3.3 Comparison with CAEVO,[0],[0]
"Therefore, we have chosen CAEVO as the baseline system to evaluate the significance of the proposed ones.",5.3.3 Comparison with CAEVO,[0],[0]
"On the TD-Test dataset, all systems other than ClearTK had better F1 scores compared to their performances on TE3-PT.",5.3.3 Comparison with CAEVO,[0],[0]
"This notable difference (i.e., 48.53 vs 40.3) indicates the better quality of the dense annotation scheme that was used to create TD (Cassidy et al., 2014).",5.3.3 Comparison with CAEVO,[0],[0]
"SP+ILP outperformed CAEVO and if additional unlabeled dataset TE3-SV was used, CoDL+ILP achieved the best score with a relative improvement in F1 score being 6.3%.
",5.3.3 Comparison with CAEVO,[0],[0]
"We notice that the proposed systems often have higher recall than precision, and that this is less an issue on a densely annotated testset (TD-Test), so their low precision on TE3-PT possibly came from the missing annotations on TE3-PT.",5.3.3 Comparison with CAEVO,[0],[0]
It is still under investigation how to control precision and recall in real applications.,5.3.3 Comparison with CAEVO,[0],[0]
We develop a structured learning approach to identifying temporal relations in natural language text and show that it captures the global nature of this problem better than state-of-the-art systems do.,6 Conclusion,[0],[0]
A new perspective towards vague relations is also proved to gain from fully taking advantage of the structured approach.,6 Conclusion,[0],[0]
"In addition, the global nature of this problem gives rise to a better way of making use of the readily available unlabeled data, which further improves the proposed method.",6 Conclusion,[0],[0]
"The improved performance on both TE3-PT and TDTest, two differently annotated datasets, clearly shows the advantage of the proposed method over existing methods.",6 Conclusion,[0],[0]
We plan to build on the notable improvements shown here and expand this study to deal with additional temporal reasoning problems in natural language text.,6 Conclusion,[0],[0]
We thank all the reviewers for providing useful comments.,Acknowledgements,[0],[0]
This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizon Network; by the US Defense Advanced Research Projects Agency (DARPA) under contract FA8750-13-2-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053.,Acknowledgements,[0],[0]
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies of the U.S. Government.,Acknowledgements,[0],[0]
Identifying temporal relations between events is an essential step towards natural language understanding.,abstractText,[0],[0]
"However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events.",abstractText,[0],[0]
"Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators.",abstractText,[0],[0]
This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge.,abstractText,[0],[0]
"As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods.",abstractText,[0],[0]
"As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem.",abstractText,[0],[0]
A Structured Learning Approach to Temporal Relation Extraction,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1169–1180 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
A Structured Syntax-Semantics Interface for English-AMR Alignment
Ida Szubert Adam Lopez School of Informatics
University of Edinburgh Edinburgh, Scotland, UK
{k.i.szubert@sms, alopez@inf}.ed.ac.uk
Nathan Schneider Linguistics and Computer Science
Georgetown University Washington, DC, USA
nathan.schneider@georgetown.edu
Abstract
Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested. To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs. Our annotation explains how 97% of AMR edges are evoked by words or syntax. Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%. While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.",text,[0],[0]
"Abstract Meaning Representation (AMR; Banarescu et al., 2013) is a popular framework for annotating whole sentence meaning.",1 Introduction,[0],[0]
"An AMR annotation is a directed, usually acyclic graph in which nodes represent entities and events, and edges represent relations between them, as on the right in figure 1.1
AMR annotations include no explicit mapping between elements of an AMR and the corresponding elements of the sentence that evoke them, and this presents a challenge to developers of machine learning systems that parse sentences to AMR or generate sentences from AMR, since they must
1For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus.
first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2
This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment.",1 Introduction,[0],[0]
Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment.,1 Introduction,[0],[0]
"In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1.",1 Introduction,[0],[0]
"So edge alignments allow ISI to explain more of the AMR structure than JAMR, but in a limited way: only 23% of AMR edges are aligned in the ISI corpus.",1 Introduction,[0],[0]
"This may be be-
2Some recent neural AMR sytems require minimal or no explicit alignments (Konstas et al., 2017; van Noord and Bos, 2017).",1 Introduction,[0],[0]
"But they implicitly learn them in the form of soft attention, and we believe that a clearer understanding of alignment will benefit modeling and error analysis even in these systems.
1169
cause edges are often evoked by syntactic structure rather than words: for instance, the :ARG1 edge in figure 1 is evoked by the fact that cat is the subject of lies and not by any particular word.
",1 Introduction,[0],[0]
"Although it seems sensible to assume that all of the nodes and edges of an AMR are evoked by the words and syntax of a sentence, the existing alignment schemes do not allow for expressing that relationship.",1 Introduction,[0],[0]
We therefore propose a framework expressive enough to align AMR to syntax (§2) and use it to align a corpus of 200 AMRs to dependency parses.,1 Introduction,[0],[0]
"We analyse our corpus and show that the addition of syntactic alignments allows us account for 97% of the AMR content.
",1 Introduction,[0],[0]
"Syntactic-semantic mappings are often assumed by AMR parsing models (e.g. Wang et al., 2015; Artzi et al., 2015; Damonte et al., 2017), which is understandable since these mappings are wellstudied in linguistic theory.",1 Introduction,[0],[0]
But AMR explicitly avoids theoretical commitment to a syntaxsemantics mapping: Banarescu et al. (2013) state that “AMR is agnostic about how we might want to derive meanings from strings.”,1 Introduction,[0],[0]
"If we are going to build such an assumption into our models, we should test it empirically, which we can do by analysing our corpus.",1 Introduction,[0],[0]
"We observe some pervasive structural differences between AMR and dependency syntax (§3), despite the fact that a majority of AMR edges map easily onto dependency edges.
",1 Introduction,[0],[0]
"Since syntactic alignment can largely explain AMRs, we also develop a baseline rule-based aligner for it, and show that this new task is much more difficult than lexical alignment (§4).",1 Introduction,[0],[0]
We also show how our data can be used to analyze errors made by an AMR parser (§5).,1 Introduction,[0],[0]
We make our annotated data and aligner freely available for further research.3,1 Introduction,[0],[0]
"Our syntactic representation is dependency grammar, which represents the sentence as a rooted, directed graph where nodes are words and edges are grammatical relations between them (Kruijff, 2006).",2 Aligning AMR to dependency syntax,[0],[0]
"We use Universal Dependencies (UD), a cross-lingual dependency annotation scheme, as implemented in Stanford CoreNLP (Manning et al., 2014).",2 Aligning AMR to dependency syntax,[0],[0]
"Within the UD framework, we use enhanced dependencies (Schuster and Manning, 2016), in which dependents can have more than one head,
3https://github.com/ida-szubert/amr_ud
resulting in dependency graphs (DGs).4
Our alignment guidelines generalize ideas present in the existing frameworks.",2 Aligning AMR to dependency syntax,[0],[0]
"We want to allow many-to-many alignments, which we motivate by the observation that some phenomena cause an AMR graph to have one structure expressing the same information as multiple DG structures, and vice versa.",2 Aligning AMR to dependency syntax,[0],[0]
"For instance, in figure 2 the AMR subgraph representing Cruella de Vil aligns to two subgraphs in the dependency graph because of pronominal coreference.",2 Aligning AMR to dependency syntax,[0],[0]
"In the other direction, in figure 3 the capabilities node aligns to both capable nodes in the AMR, which is a result of the AMR treating conjoined adjectival modifiers as a case of ellipsis.",2 Aligning AMR to dependency syntax,[0],[0]
The alignments we propose hold between subgraphs of any size.,2 Aligning AMR to dependency syntax,[0],[0]
By aligning subgraphs we gain expressiveness needed to point out correspondences between semantic and syntactic structure.,2 Aligning AMR to dependency syntax,[0],[0]
"If AMR and DG were very similar in how they represent information, such correspondences would probably hold between subgraphs consisting of a single edge, as in figure 1 cat nmod:possÐÐÐÐÐ→my ∼ cat possÐÐ→I.",2 Aligning AMR to dependency syntax,[0],[0]
"However, AMR by design abstracts away from syntax and it should not be assumed that all mappings will be so clean.",2 Aligning AMR to dependency syntax,[0],[0]
"For example, the same figure has lies nmod-inÐÐÐÐ→sun caseÐÐ→in∼ lies locationÐÐÐÐ→sun.",2 Aligning AMR to dependency syntax,[0],[0]
"Moreover, AMR represents the meaning of particular words or phrases with elaborate structures, the result of which might be that the same information is expressed by a single word and a complex AMR subgraph, as in figure 3 where AMR represents general as person
ARG0-ofÐÐÐÐ→have-org-role ARG2ÐÐ→general.",2 Aligning AMR to dependency syntax,[0],[0]
An alignment is a link between subgraphs in an AMR and a DG which represent equivalent information.,2.1 Overview,[0],[0]
Given a sentence’s DG and AMR we define an alignment as a mapping between an AMR subgraph and a DG subgraph.,2.1 Overview,[0],[0]
"Lexical alignments (§2.2) hold between pairs of nodes, and nodes from either graph may participate in multiple lexical alignments.",2.1 Overview,[0],[0]
"Structural alignments (§2.3) hold between pairs of connected subgraphs where at least one of the subgraphs contains an edge.
",2.1 Overview,[0],[0]
"4We chose UD because it emphasises shallow and semantically motivated annotation, by the virtue of which it can be expected to align relatively straightforwardly to a semantic annotation such as AMR.",2.1 Overview,[0],[0]
"Aligning AMR with different versions of dependency grammar (e.g. Prague) or different syntactic frameworks (e.g. CCG, TAG) would be an interesting extension of our work.
",2.1 Overview,[0],[0]
In the following two sections we discuss the types of alignments that our framework allows.,2.1 Overview,[0],[0]
More detailed guidelines regarding how to align particular linguistic constructions can be found in appendix A.,2.1 Overview,[0],[0]
A lexical alignment should hold between a word and an AMR concept if the latter is judged to express the lexical meaning of the former.,2.2 Lexical alignments,[0],[0]
"Node labels usually reflect their lexically aligned word or its lemma, including derivational morphology (e.g. thirsty ∼ thirst-01).",2.2 Lexical alignments,[0],[0]
"Thus, string similarity is a useful heuristic for lexical alignment.5
Most AMR nodes align lexically to a single word.",2.2 Lexical alignments,[0],[0]
"Cases of one-to-many alignments include coreference, when an entity is mentioned multiple times in the sentence, and multiword expressions such as a verb-particle constructions (pay off ∼ pay-off-02) and fixed grammatical expressions (instead of ∼ instead-of-91).",2.2 Lexical alignments,[0],[0]
Occasionally an AMR node does not lexically align to any DG node.,2.2 Lexical alignments,[0],[0]
"This is true for constants indicating sentence mood such as imperative, implicit uses of and to group list items, inferred concept nodes such as entity
5Exceptions include: pronouns with noun antecedents in the sentence; the - indicating negative polarity, which lexically aligns to no, not, and negative prefixes; modal auxiliaries, e.g., can ∼ possible; normalized dates and values such as February ∼ 2 in a date-entity; and amr-unknown, which aligns to wh-words.
",2.2 Lexical alignments,[0],[0]
"types, name in named entities, and -91 frames like have-org-role-91.
",2.2 Lexical alignments,[0],[0]
"Most words are lexically aligned to a single AMR node, if they are aligned at all.",2.2 Lexical alignments,[0],[0]
"A word may align to multiple AMR nodes if it is duplicated in the AMR due to ellipsis or distributive coordination (capabilities aligns to c2 / capable and c3 / capable in figure 3), or if it is morphologically decomposed in the AMR (evildoer aligns to evil and do-02 in figure 2).",2.2 Lexical alignments,[0],[0]
"Many words are not lexically aligned to any AMR node, including punctuation tokens, articles, copulas, nonmodal auxiliaries, expletive subjects, infinitival to, complementizer that, and relative pronouns.",2.2 Lexical alignments,[0],[0]
"Structural alignments primarily reflect compositional grammatical constructions, be they syntactic or morphological.",2.3 Structural alignments,[0],[0]
Note that the structural alignments build upon the lexical ones.,2.3 Structural alignments,[0],[0]
"Structural alignments hold between two subgraphs, at least one of which is larger than a single node.",2.3 Structural alignments,[0],[0]
"If a subgraph includes any edges, it automatically includes nodes adjacent to those edges.",2.3 Structural alignments,[0],[0]
Structural alignments need not be disjoint: an edge can appear in two or more distinct alignments.,2.3 Structural alignments,[0],[0]
Nodes and edges in both AMR and DG may be unaligned.,2.3 Structural alignments,[0],[0]
"The ability to align subgraphs to subgraphs gives considerable flexibility in how the annotation task
can be interpreted.",2.3.1 Constraints on structural alignments,[0],[0]
We establish the following principles to guide the specification of alignment: Connectedness Principle.,2.3.1 Constraints on structural alignments,[0],[0]
"In an alignment d ∼ a, d must be a connected subgraph of the DG, and a must be a connected subgraph of the AMR.",2.3.1 Constraints on structural alignments,[0],[0]
Minimality Principle.,2.3.1 Constraints on structural alignments,[0],[0]
"If two alignments, d ∼ a and d′ ∼ a′, have no dependency or AMR edges in common, then their union d ∪d′ ∼",2.3.1 Constraints on structural alignments,[0],[0]
"a∪a′ is redundant, even if it is valid.",2.3.1 Constraints on structural alignments,[0],[0]
Individual alignments should be as small as possible; we believe compositionality is best captured by keeping structures minimal.,2.3.1 Constraints on structural alignments,[0],[0]
"Therefore, in figure 1 there is no alignment between subgraphs spanning My, cat, lies and i, cat, lie.",2.3.1 Constraints on structural alignments,[0],[0]
"Such subgraphs do express equivalent information, but the alignment between them decomposes neatly into smaller alignments and we record only those.",2.3.1 Constraints on structural alignments,[0],[0]
Subsumption Principle.,2.3.1 Constraints on structural alignments,[0],[0]
This principle expresses the fact that our alignments are hierarchical.,2.3.1 Constraints on structural alignments,[0],[0]
"Structural alignments need to be consistent with lexical alignments: for subgraph a to be aligned to subgraph d, all nodes lexically aligned to nodes in a must be included in d, and vice versa.",2.3.1 Constraints on structural alignments,[0],[0]
"Moreover, structural alignments need to be consistent with other structural alignments.",2.3.1 Constraints on structural alignments,[0],[0]
"A structural alignment d ∼ a is valid only if, for every connected AMR subgraph a< ⊂ a which is aligned to a DG subgraph, d′ ∼ a<, we also have that d′ is a subgraph of d—and vice versa for every d< ⊂",2.3.1 Constraints on structural alignments,[0],[0]
"d.
Further, if a contains a node n which is not lexically aligned but which is part of a structurally aligned subgraph a′ such that d′ ∼ a′, it needs to be the case that a′ ⊂ a ∧ d′ ⊂ d or
a′ ⊃ a ∧ d′ ⊃",2.3.1 Constraints on structural alignments,[0],[0]
d. (And vice versa for nodes in d.),2.3.1 Constraints on structural alignments,[0],[0]
"For example, conceal
nsubj-xsubjÐÐÐÐÐ→Cruella ∼ conceal
ARG0ÐÐ→person nameÐÐ→name op1Ð→Cruella is not a valid alignment, because the AMR side contains nodes person and name, which are not lexically aligned but which are both parts of a structural alignment marked in blue.
",2.3.1 Constraints on structural alignments,[0],[0]
Coordination Principle.,2.3.1 Constraints on structural alignments,[0],[0]
"If an alignment contains a dependency edge between two conjuncts, or between a conjunct and a coordinating conjunction, then it must also include all conjuncts and the conjunction.",2.3.1 Constraints on structural alignments,[0],[0]
This preserves the integrity of coordinate structures in alignments.,2.3.1 Constraints on structural alignments,[0],[0]
"For example, in figure 2 there is no alignment glee ccÐ→and ∼ and op1Ð→glee; only the larger structure which includes the greed nodes is aligned.
",2.3.1 Constraints on structural alignments,[0],[0]
Named Entity Principle.,2.3.1 Constraints on structural alignments,[0],[0]
Any structural alignment containing an AMR name node or any of the strings under it must contain the full subgraph rooted in the name plus the node above it specifying the entity type.,2.3.1 Constraints on structural alignments,[0],[0]
"This means that for example, in figure 2 there is no alignment conceal nsubj-xsubjÐÐÐÐÐ→Cruella ∼ conceal ARG0ÐÐ→person nameÐÐ→name op1Ð→""Cruella"".",2.3.1 Constraints on structural alignments,[0],[0]
Such an alignment would also be stopped by the Subsumption Principle provided that the blue alignment of the whole name was present.,2.3.1 Constraints on structural alignments,[0],[0]
"The Named Entity Principle is superfluous, but is provided to explicitly describe the treatment of such constructions.",2.3.1 Constraints on structural alignments,[0],[0]
"The smallest structure which can participate in a structural alignment is a single node, provided that it is aligned to a subgraph containing at least one edge.",2.3.2 Typology of structural alignments,[0],[0]
"A DG node may align to an AMR subgraph if the word is morphologically decomposed or otherwise analyzed in the AMR (e.g. in figure 2, evildoer ∼ person ARG0-ofÐÐÐÐ→do-02 ARG1ÐÐ→thing modÐ→evil).",2.3.2 Typology of structural alignments,[0],[0]
"Examples of DG structures whose meaning is expressed in a single AMR node include light verb constructions, phrasal verbs, and various other multiword expressions (e.g. in figure 2, makes dobjÐÐ→attempt ∼ attempt-01).
",2.3.2 Typology of structural alignments,[0],[0]
"Conceptually the simplest case of structural alignment is one edge to one edge, as in the blue and green alignments in figure 1.",2.3.2 Typology of structural alignments,[0],[0]
"For such an alignment to be possible, two requirements must be satisfied: nodes which are endpoints of those edges need to be aligned one-to-one; and the AMR relation and the syntactic dependency must map cleanly in terms of the relationship they express.
",2.3.2 Typology of structural alignments,[0],[0]
A one edge to multiple edges alignment arises when either of those requirements is not met.,2.3.2 Typology of structural alignments,[0],[0]
To see what happens in absence of one-to-one endpoint alignments let’s look at the relation between confident and general in figure 3.,2.3.2 Typology of structural alignments,[0],[0]
The DG general node is aligned to an AMR subgraph: general∼ person ARG0-ofÐÐÐÐ→have-org-role ARG2ÐÐ→general.,2.3.2 Typology of structural alignments,[0],[0]
All alignments which involve the general node on the DG side need to include its aligned subgraph on the AMR side.,2.3.2 Typology of structural alignments,[0],[0]
"It necessarily follows that the AMR subgraphs in those alignments will contain more edges that the DG ones; in this case the yellow subgraph in DG has 1 edge, and in AMR 3 edges.",2.3.2 Typology of structural alignments,[0],[0]
"As for the second requirement, it is possible for one graph to use multiple edges to express a relationship when the other graph needs only one.",2.3.2 Typology of structural alignments,[0],[0]
This is the case for lie nmod-inÐÐÐÐ→sun caseÐÐ→in ∼ lie locationÐÐÐÐ→sun in figure 1.,2.3.2 Typology of structural alignments,[0],[0]
"An example which combines both the node- and edge-related issues is marked in red in figure 2.
",2.3.2 Typology of structural alignments,[0],[0]
"Finally, we also allow for many edges to many edges alignments.",2.3.2 Typology of structural alignments,[0],[0]
"This may seem counterintuitive considering the assumption that we want to capture mappings between relations expressed in DG and AMR, and that we want to align minimal subgraphs.",2.3.2 Typology of structural alignments,[0],[0]
"There are cases where an alignment is actually capturing a single relation, but we need to treat a subgraph as an endpoint of the edge both in DG and AMR.",2.3.2 Typology of structural alignments,[0],[0]
"For instance, con-
sider in figure 2 the relationship that holds between Cruella de Vil and concealing, expressed syntactically as an nsubj-xsubj edge and semantically as an ARG0 edge.",2.3.2 Typology of structural alignments,[0],[0]
"One of the entities involved in that relationship, Cruella, is represented by a 2- edge DG subgraph and a 4-edge AMR subgraph.",2.3.2 Typology of structural alignments,[0],[0]
"Consequently, the alignment covering the DG and AMR edges that relate Cruella to concealing must link subgraphs consisting respectively of 3 and 5 edges.",2.3.2 Typology of structural alignments,[0],[0]
A more difficult case of many edges to many edges alignment arises when relationships between nodes are expressed so differently in the DG and AMR that given an edge in one graph it is not possible to find in the other graph a subgraph that would convey the same information without also including some other information.,2.3.2 Typology of structural alignments,[0],[0]
Coordination has this property: e.g. in figure 2 the conj-and dependency between glee and greed has no counterpart in the AMR.,2.3.2 Typology of structural alignments,[0],[0]
"There is no edge between AMR nodes aligned to those words, and the smallest AMR subgraph which contains them also contains and, which is itself lexically aligned.",2.3.2 Typology of structural alignments,[0],[0]
We cannot align glee conj-andÐÐÐÐ→greed ∼ glee op1←Ðand,2.3.2 Typology of structural alignments,[0],[0]
op2Ð→greed because of the rule that all lexically aligned nodes in one subgraph must be aligned to nodes in the other subgraph.,2.3.2 Typology of structural alignments,[0],[0]
Therefore we need to extend the DG side to and cc←Ðglee conj-andÐÐÐÐ→greed.,2.3.2 Typology of structural alignments,[0],[0]
"We annotated a corpus of 200 AMR-sentence pairs (3813 aligned structures) using the guidelines of §2 and appendix A.6
Data selection.",3 Manually aligned corpus,[0],[0]
"To create the corpus we drew a total of 200 AMR-sentence pairs: 135 from the training split of the AMR Annotation Release 1.0 (Knight et al., 2014), 55 from the training split of The Little Prince Corpus v1.6,7 and 10 sentences from the Adam part of the CHILDES Brown corpus (Brown, 1973), for which AMRs were produced by an experienced annotator.",3 Manually aligned corpus,[0],[0]
"Seventy items were selected to illustrate particular linguistic phenomena.8 The remaining 130 were selected at random.
",3 Manually aligned corpus,[0],[0]
"6We followed the precedent of previous AMR-to-sentence alignment corpora (see §4.2) in including 200 sentences in our gold standard, though ours was a different sample.
",3 Manually aligned corpus,[0],[0]
"7https://amr.isi.edu/download/ amr-bank-struct-v1.6.txt
8Namely: relative clauses, reflexive and non-reflexive pronominal anaphora, subject and object control, raising, exceptional case marking, coordination, wh-questions, dosupport questions, ellipsis, expletives, modal verbs, light verbs, comparison constructions, and quantification.
",3 Manually aligned corpus,[0],[0]
Preprocessing.,3 Manually aligned corpus,[0],[0]
"Dependency parses were obtained using Stanford CoreNLP neural network parser9 (Chen and Manning, 2014) and manually corrected.",3 Manually aligned corpus,[0],[0]
"The final parses conform to the enhanced UD guidelines,10 except they lack enhancements for ellipsis.
",3 Manually aligned corpus,[0],[0]
Inter-annotator agreement.,3 Manually aligned corpus,[0],[0]
The corpus was created by one annotator.,3 Manually aligned corpus,[0],[0]
"To assess inter-annotator agreement, a second annotator deeply familiar with UD and AMR annotated a random sample of sentences accounting for 10% of alignments in the corpus.",3 Manually aligned corpus,[0],[0]
"The overall inter-annotator F1-score was 88%, with 96% agreement on lexical alignments and 80% on structural alignments.",3 Manually aligned corpus,[0],[0]
We take this as an indication that our richly structured alignment framework as laid out in §2 is reasonably well-defined for annotators.,3 Manually aligned corpus,[0],[0]
"To assess our attempt to explain as much of the AMR as possible, we computed the proportion of AMR nodes and edges that participate in at least one alignment.",3.1 Coverage,[0],[0]
"Overall, 99.3% of nodes and 97.2% of edges in AMRs are aligned.",3.1 Coverage,[0],[0]
"We found that 81.5% of AMR graphs have full coverage, 18.5% have at least one unaligned edge, and 7.5% have one unaligned node (none had more than one; all unaligned nodes express mood or discourse-related information: interrogative, and, and say).",3.1 Coverage,[0],[0]
"We conclude that nearly all information in an AMR is evoked by lexical items or syntactic structure.
",3.1 Coverage,[0],[0]
We expected coverage of DG to be lower because punctuation and many function words are unaligned in our guidelines (§2.2).,3.1 Coverage,[0],[0]
"Indeed, only 71.4% of words and 65.2% of dependency edges are aligned.",3.1 Coverage,[0],[0]
"The similarity of AMR to syntax in examples like figure 1 invites the assumption of a close mapping, which often seems to be made in AMR parsers (Wang et al., 2015; Artzi et al., 2015; Misra and Artzi, 2016; Damonte et al., 2017) and aligners (Chu and Kurohashi, 2016; Chen and Palmer,
9The corpus is annotated with UD v1; a release of the dataset converted to UD v2 is planned for the future.",3.2 Syntactic-semantic similarity,[0],[0]
"We used the pretrained dependency parsing model provided in CoreNLP with depparse.extradependencies set to MAXIMAL, and used collapsed CCprocessed dependencies.
",3.2 Syntactic-semantic similarity,[0],[0]
10http://universaldependencies.org/u/overview/ enhanced-syntax.html,3.2 Syntactic-semantic similarity,[0],[0]
"2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017).",1:2 16 13.1 2:3 14 16.0,[0],[0]
"However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
Are AMRs and dependency graphs structurally similar?,1:2 16 13.1 2:3 14 16.0,[0],[0]
"We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
We define the size of a subgraph as the number of edges it contains.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"If a structure consists of a single node, we say its size is 0.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"The configuration of an alignment is then the pair of sizes for its AMR and DG sides; for example, an alignment with 1 AMR edge and 2 DG edges has configuration 1:2.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"We call an alignment configuration simple if at least one of the subgraphs is a single edge, indicating that there is a single relation which the alignment captures.",1:2 16 13.1 2:3 14 16.0,[0],[0]
Complex configurations cover multiple relations.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"By principle of minimality we infer that some structural difference between the graphs prevented those relations from aligning individually.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
One measure of similarity between AMR and DG graphs is the configuration of the most complex subgraph alignment between them.,1:2 16 13.1 2:3 14 16.0,[0],[0]
Configuration a:b is higher than c:d if a+b > c+d.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"However, all configurations involving 0 are lower than those which do not.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"A maximum of 1:1 means the graphs have only node-to-node, node-to-edge, and edge-toedge alignments, rendering the graphs isomorphic (ignoring edge directions and unaligned nodes).",1:2 16 13.1 2:3 14 16.0,[0],[0]
"In
11In particular, Chen and Palmer (2017) align dependency paths to AMR edges.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"However, their evaluation only considers node-to-node alignment, and their code and data are not available for comparison at the time of this writing.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
"general, if the maximum alignment configuration is a simple one, the graphs could be made isomorphic by collapsing the larger side of the alignment (e.g., in figure 2, the AMR side of the alignment evildoer ∼ person ARG0-ofÐÐÐÐ→do",1:2 16 13.1 2:3 14 16.0,[0],[0]
"ARG1ÐÐ→thing modÐ→evil could be collapsed into a node).
",1:2 16 13.1 2:3 14 16.0,[0],[0]
"In contrast, complex configurations imply serious structural dissimilarity, as in figure 3, where the cyan alignment has configuration 4:4.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
"The numbers in table 1 show that ≈33% of the sentences are simple.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
Table 2 provides a detailed breakdown of alignment configurations in the corpus.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"Phenomena which often trigger complex configurations include coordination, named entities, semantically decomposed words, attachment of negation, and preposition-based concepts encoding location, time, and quantity.12
We observe, comparing tables 1 and 2, that while simple configurations are most frequent in the corpus, the majority of sentences have at least one alignment which is complex.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"It should not be assumed that AMR and DG representations of a sentence are, or could trivially be made to be, isomorphic.",1:2 16 13.1 2:3 14 16.0,[0],[0]
It is worth noting that our analysis suggests that DG and AMR could be made more similar by applying simple transformations targeting problematic constructions like coordination and named entities.,1:2 16 13.1 2:3 14 16.0,[0],[0]
We use our annotations to measure the accuracy of AMR aligners on specific phenomena that were inexpressible in previous annotation schemes.,4 Evaluation of automatic aligners,[0],[0]
"Our experiments evaluate the JAMR heuristic aligner (Flanigan et al., 2014), the ISI statistical aligner (Pourdamghani et al., 2014), and a heuristic rulebased aligner that we developed specifically for
12An AMR concept evoked by a preposition usually dominates the structure (after op1ÐÐ→date-entity decadeÐÐÐ→nineties), which is at odds with UD’s prepositions-as-case-markers policy (nineties caseÐÐ→after).
structural alignment.",4 Evaluation of automatic aligners,[0],[0]
"Our aligner operates in two passes: one for lexical alignment and one for structural alignment.
",4.1 Rule-based aligner,[0],[0]
Lexical alignment algorithm.,4.1 Rule-based aligner,[0],[0]
"AMR concepts are cognate with English words, so we align them by lexical similarity.",4.1 Rule-based aligner,[0],[0]
This algorithm does not make use of the DG.,4.1 Rule-based aligner,[0],[0]
"Before alignment, we remove sense identifiers on AMR node labels, and lemmatize DG node labels.",4.1 Rule-based aligner,[0],[0]
"Then for every pair of nodes a from the AMR and d from the DG we align them if any of the following conditions holds:
1.",4.1 Rule-based aligner,[0],[0]
"The Levenshtein distance of a and d is 15% or less of the length of the longer word.13
2.",4.1 Rule-based aligner,[0],[0]
"The label of a is the morphological negation of d (e.g. prudent ∼ imprudent).14
3.",4.1 Rule-based aligner,[0],[0]
"The label of a is – (AMR’s annotation of negation) and the parent of a aligns to d via rule 2.
4.",4.1 Rule-based aligner,[0],[0]
"The label of a is – and d is one of no, none, not, or never.
5.",4.1 Rule-based aligner,[0],[0]
"The label of a consists of multiple words, and the label of d matches any one of them under rule 1.",4.1 Rule-based aligner,[0],[0]
"(e.g. sit ∼ sit-down, war-torn ∼ war).15
6.",4.1 Rule-based aligner,[0],[0]
Labels of a and d likely have the same morphological root.,4.1 Rule-based aligner,[0],[0]
"We determine this by segmenting each word with Morfessor (Grönroos et al., 2014) trained on Wiki data and applying rule 1 to the first morpheme of each word.
",4.1 Rule-based aligner,[0],[0]
"Note that if a word type is repeated in a sentence, each repetition is aligned to the same AMR nodes under the above rules.
",4.1 Rule-based aligner,[0],[0]
Structural alignment algorithm.,4.1 Rule-based aligner,[0],[0]
"We align subgraphs using the procedure below, first from AMR to DG, then from DG to AMR.",4.1 Rule-based aligner,[0],[0]
"For clarity, the explanation refers to the first case.
",4.1 Rule-based aligner,[0],[0]
"13Threshold was determined empirically on a 10% sample from the dataset.
",4.1 Rule-based aligner,[0],[0]
"14We use a list of morphologically negated words provided by Ulf Hermjakob.
",4.1 Rule-based aligner,[0],[0]
"15This rule misaligns some AMR-specific node types, such as government ∼ government-organization.
",4.1 Rule-based aligner,[0],[0]
Local phase.,4.1 Rule-based aligner,[0],[0]
"For every AMR edge ea whose endpoints are lexically aligned nodes a1 (aligned to d1) and a2 (aligned to d2), we attempt to align minimal and connected AMR and dependency subgraphs, a′ and d′:
1.",4.1 Rule-based aligner,[0],[0]
"If there is a DG edge ed whose endpoints are d1 and d2, then a′← ea and d′← ed .
2.",4.1 Rule-based aligner,[0],[0]
"Otherwise, let πd be the shortest undirected path between d1 and d2.",4.1 Rule-based aligner,[0],[0]
"If all lexically aligned nodes in πd are aligned to a1 or a2, then a′ ← ea and d′← πd .
3.",4.1 Rule-based aligner,[0],[0]
"Otherwise, let a′′ be the smallest subgraph covering all AMR nodes that are lexically aligned to nodes in πd .",4.1 Rule-based aligner,[0],[0]
"If all the nodes in a′′ are aligned only to nodes in πd , then a′← a′′ and d′← πd .
4.",4.1 Rule-based aligner,[0],[0]
"Otherwise, the attempt is abandoned.",4.1 Rule-based aligner,[0],[0]
5.,4.1 Rule-based aligner,[0],[0]
"Finally, if the top node of a′ has a parent node labeled with an entity type concept, extend a′ to include the parent.",4.1 Rule-based aligner,[0],[0]
"(This step is performed only in the AMR-to-DG step.)
",4.1 Rule-based aligner,[0],[0]
Global phase.,4.1 Rule-based aligner,[0],[0]
"The local phase might produce alignments that violate the Subsumption Principle (§2.3.1), so we filter them out heuristically.",4.1 Rule-based aligner,[0],[0]
"For every pair of structural alignments, πd ∼ πa and π ′d ∼ π ′a where πa overlaps with π ′a, or πd with π ′d , if the region of overlap is not itself an aligned subgraph, we prune both alignments.16",4.1 Rule-based aligner,[0],[0]
"We evaluate JAMR, ISI, and our aligner on two distinct tasks.",4.2 Experiments,[0],[0]
Lexical alignment.,4.2 Experiments,[0],[0]
"Lexical alignment involves aligning AMR nodes to words, a task all three systems can perform.",4.2 Experiments,[0],[0]
"We evaluate against three datasets: our own, the JAMR dataset (Flanigan et al., 2014), and the ISI dataset (Pourdamghani et al., 2014).17 Results (table 3) suggest that this task is already well-addressed, but also that there exist marked differences between how lexical alignment is defined in each dataset and that aligners are
16This could be order-dependent since the removal of one alignment could trigger the removal of others, but our aligner does not account for this.
",4.2 Experiments,[0],[0]
"17We remove span alignments in the JAMR dataset and edge alignments in the ISI dataset.
",4.2 Experiments,[0],[0]
fine-tuned to their dataset.,4.2 Experiments,[0],[0]
"For our aligner, errors are due to faulty morphological analysis, duplicated words, and both accidental string similarity between AMR concepts and words and occasional lack of similarity between concepts and words that should be aligned.",4.2 Experiments,[0],[0]
Structural alignment.,4.2 Experiments,[0],[0]
An important goal of our experiments is to establish baselines for the structural alignment task.,4.2 Experiments,[0],[0]
"While we cannot evaluate the JAMR and ISI aligners directly on this task, we can use the lexical alignments they output in place of the first pass of our aligner.",4.2 Experiments,[0],[0]
The only dataset for this task is our own.,4.2 Experiments,[0],[0]
"The results (table 4) evaluate accuracy of structural alignments only and do not count lexical alignments.
",4.2 Experiments,[0],[0]
"The automatic alignments have lower coverage of AMRs than the gold alignments do: our best aligner leaves 13.3% of AMR nodes and 30.0% of AMR edges unaligned, compared to 0.07% and 2.8% in the gold standard.",4.2 Experiments,[0],[0]
"The aligner also leaves 39.2% of DG nodes and 47.7% of DG edges unaligned, compared to 28.6% and 34.8% in the gold standard.",4.2 Experiments,[0],[0]
The relatively low F-score for the gold standard lexical alignments and DGs condition suggests that substantial improvements to our structural alignment algorithm are possible.,4.2 Experiments,[0],[0]
"The two most common reasons for low recall were missing one of the conjuncts in a coordinate structure and aligning structures that violate the principle of minimality.
",4.2 Experiments,[0],[0]
Our corpus gives alignments between AMRs and gold standard dependency parses.,4.2 Experiments,[0],[0]
To see how much performance degrades when such parses are not available we also evaluate on automatic parses.18 Both precision and recall are substantially worse when the aligner relies on automatic syntax.,4.2 Experiments,[0],[0]
"Our corpus of manually aligned AMRs can be used to identify linguistic constructions which cause
18We use the CoreNLP dependency parser with settings as described in §3.",5 Improving error analysis for AMR parsers,[0],[0]
problems for an AMR parser.,UD structure missed mislabeled,[0],[0]
"We parsed the sentences from our corpus with the parser of Damonte et al. (2017).19 We map the nodes of the resulting automatic AMRs to the gold AMRs using the smatch evaluation tool (Cai and Knight, 2013), and on the basis of this mapping identify those nodes and edges of the gold AMRs which are missing or mislabeled in the automatic AMRs.
",UD structure missed mislabeled,[0],[0]
We then measured the number and rate of erroneous AMR fragments associated with each UD relation or construction (table 5).,UD structure missed mislabeled,[0],[0]
"The largest proportion of recall errors were for fragments associated with the subject relation, prepositional phrases, and nominal compounds.",UD structure missed mislabeled,[0],[0]
"Focusing on the subject relation, we can further say that 69% of the missing or mislabeled edges have the gold label ARG0, 19% ARG1, and the rest are distributed amongst domain, ARG2, purpose and mod.",UD structure missed mislabeled,[0],[0]
"Inspecting the errors we see that phenomena underlying them include pronominal coreference, sharing arguments between conjoined predicates, auxiliary verb constructions, and control and raising.20
Our corpus facilitates fine-grained error analysis of AMR parsers with respect to individual syntactic constructions.",UD structure missed mislabeled,[0],[0]
We release the code for the above analysis in order to encourage syntactically-informed comparison and improvement of systems.,UD structure missed mislabeled,[0],[0]
We have presented a new framework and corpus for aligning AMRs to dependency syntax.,6 Conclusion,[0],[0]
"Our data and analysis show that the vast majority of the semantics in AMR graphs can be mapped to the lexical and syntactic structure of a sentence, though current alignment systems do not fully capture this correspondence.",6 Conclusion,[0],[0]
"The syntax–semantics
19The overall smatch score of the parser on this dataset was 0.65.
",6 Conclusion,[0],[0]
"20The missing edge counts include gold edges for which the parser failed to produce one or both endpoints.
",6 Conclusion,[0],[0]
correspondences are often structurally divergent (non-isomorphic).,6 Conclusion,[0],[0]
Simple algorithms for lexical and structural alignment establish baselines for the new alignment task; we expect statistical models will be brought to bear on this task in future work.,6 Conclusion,[0],[0]
Our framework also facilitates syntactically-based analysis of AMR parsers.,6 Conclusion,[0],[0]
We release our data and code for the benefit of the research community.,6 Conclusion,[0],[0]
"This work was supported in part by EU ERC Advanced Fellowship 249520 GRAMPLUS and EU ERC H2020 Advanced Fellowship GA 742137 SEMANTAX.
",Acknowledgments,[0],[0]
"We thank Sameer Bansal, Marco Damonte, Lucia Donatelli, Federico Fancellu, Sharon Goldwater, Andreas Grivas, Yova Kementchedjhieva, Junyi Li, Joana Ribeiro, and the anonymous reviewers for helpful discussion of this work and comments on previous drafts of the paper.",Acknowledgments,[0],[0]
A.1 Lexical alignments Names.,A Details of alignment guidelines,[0],[0]
"In proper names, individual strings denoting words in the name are lexically aligned, but the entity as a whole is structurally aligned.
",A Details of alignment guidelines,[0],[0]
Entity types.,A Details of alignment guidelines,[0],[0]
"If the entity type is based on a common noun which occurs in the sentence, it is lexically aligned: e.g., Jon, a clumsy man, has a cat would involve the alignment man ∼ man.",A Details of alignment guidelines,[0],[0]
"Most often, however, an entity type is not explicitly mentioned in the sentence and is taken from AMR’s ontology of entity types (http://www.isi.edu/ ~ulf/amr/lib/ne-types.html), in which case it will not be lexically aligned.
",A Details of alignment guidelines,[0],[0]
Case marking and prepositions.,A Details of alignment guidelines,[0],[0]
The possessive marker ’s and many prepositions participate in structural but not lexical alignments because they are inherently relational.,A Details of alignment guidelines,[0],[0]
"However, we align a preposition if it carries sufficient lexical content to be included as an AMR node (e.g., the AMR for The cat is under the table would include under
op1Ð→table).",A Details of alignment guidelines,[0],[0]
Wh-questions.,A Details of alignment guidelines,[0],[0]
The special concept amr-unknown aligns lexically to the wh-word whose referent is questioned.,A Details of alignment guidelines,[0],[0]
"For multiword wh-expressions like how much, the expression is aligned structurally (not lexically) to amr-unknown.
Sentence mood.",A Details of alignment guidelines,[0],[0]
"In AMR, non-wh questions are indicated by
modeÐÐ→interrogative, imperatives by modeÐÐ→imperative, and exclamations/interjections by
modeÐÐ→expressive.",A Details of alignment guidelines,[0],[0]
"UD parses do not encode sentence mood, which can be conveyed by noncanonical word order (subject-auxiliary inversion for questions) or argument omission (subject omission for imperatives), rather than the presence of certain relations or words.",A Details of alignment guidelines,[0],[0]
"Sometimes the sentence includes an appropriate alignment point, e.g. complementizers whether and if for interrogative, allowing for a lexical alignment.",A Details of alignment guidelines,[0],[0]
"More often the parse has no obvious alignment point, and the constant interrogative, imperative, or expressive is left unaligned.21
A.2 Structural alignments
Copulas.",A Details of alignment guidelines,[0],[0]
"In UD, copulas are treated as modifiers of a predicate nominal or adjective, which is linked directly to the subject of the sentence via an nsubj dependency.",A Details of alignment guidelines,[0],[0]
We do not align copulas or the cop edge.,A Details of alignment guidelines,[0],[0]
"Thus, in figure 3, there is a structural alignment between general nsubj←ÐÐconfident and the AMR subgraph connecting the lexically aligned nodes.
",A Details of alignment guidelines,[0],[0]
"21Among the UD community there has been discussion of possibly adding sentence-level marking of mood (https:// github.com/UniversalDependencies/docs/issues/458), which could provide a convenient alignment point.
",A Details of alignment guidelines,[0],[0]
Control.,A Details of alignment guidelines,[0],[0]
"The subject of the control verb and the controlled predicate are connected by the nsubjxsubj edge, which can be structurally aligned with the corresponding AMR argument relation, as in e.g. figure 2.
",A Details of alignment guidelines,[0],[0]
Relative clauses.,A Details of alignment guidelines,[0],[0]
"In enhanced UD the noun governing a relative clause and the embedded predicate are linked by edges in both directions: a “surface syntax” acl-relcl edge headed by the noun, and a “deep syntax” edge such as nsubj, dobj, iobj, or nmod headed by the embedded predicate.",A Details of alignment guidelines,[0],[0]
Each participates in a structural alignment with the corresponding AMR subgraph.,A Details of alignment guidelines,[0],[0]
"The relative pronoun is left unaligned.
Coordination.",A Details of alignment guidelines,[0],[0]
"Coordination does not naturally lend itself to analysis with dependencies, and different dependency grammar traditions offer different approaches (Nivre, 2005; Mareček et al., 2013).",A Details of alignment guidelines,[0],[0]
"UD follows the Stanford style, where the first conjunct serves as the head of the remaining conjuncts, and the conjunction is a dependent of one of the conjuncts.22 In AMR the conjunction heads all the conjuncts (Prague style).",A Details of alignment guidelines,[0],[0]
"In light of this mismatch, we use a subgraph alignment to group the conjunction with its conjuncts on each side.",A Details of alignment guidelines,[0],[0]
A simple example is illustrated in figure 2.,A Details of alignment guidelines,[0],[0]
A quirk of UD’s approach to coordination is that it does not distinguish modifiers of the first conjunct from modifiers of the coordinate structure as a whole.,A Details of alignment guidelines,[0],[0]
The basic UD parse of her glee and greed is therefore ambiguous.,A Details of alignment guidelines,[0],[0]
"We rely on an extra edge in the enhanced parse between her and greed to establish an alignment for the AMR edge greed
ARG0ÐÐ→person.",A Details of alignment guidelines,[0],[0]
"The coordination in figure 3 is more complex: the coordinated modifier defense and security distributes over capabilities (i.e., there are two kinds of capabilities).",A Details of alignment guidelines,[0],[0]
"In the enhanced parse, defense and security are both attached as modifiers of capabilities.",A Details of alignment guidelines,[0],[0]
"This is expressed semantically via duplicate AMR nodes labeled capable, each receiving different modifiers corresponding to different conjuncts.",A Details of alignment guidelines,[0],[0]
"Independent of coordination, the two capable nodes also share a common argument, nation.",A Details of alignment guidelines,[0],[0]
"The three syntactic modifiers give rise to three subgraph alignments, and the subgraph alignment covering the coordinate structure (cyan in the figure) envelops two of these.",A Details of alignment guidelines,[0],[0]
"Ellipsis construc-
22In UD version 1, and therefore the examples in this paper, the conjunction attaches to the first conjunct, whereas in version 2 it attaches to the next successive conjunct (http: //universaldependencies.org/v2/summary.html).
",A Details of alignment guidelines,[0],[0]
"tions can also trigger node duplication in AMR, requiring similar structural alignments.",A Details of alignment guidelines,[0],[0]
Named entities.,A Details of alignment guidelines,[0],[0]
"AMR annotates each named entity with a node representing the name, linked to the strings of the name and headed by an entity type.",A Details of alignment guidelines,[0],[0]
This full structure is aligned to the full name in the dependency parse.,A Details of alignment guidelines,[0],[0]
Coreferent mentions.,A Details of alignment guidelines,[0],[0]
Coreference often causes an AMR structure to align to multiple DG subgraphs.,A Details of alignment guidelines,[0],[0]
"For example, in figure 2, both the pronoun her and the name align to the AMR subgraph representing the entity.",A Details of alignment guidelines,[0],[0]
This mechanism suffices to represent coreference between mentions in the sentence.,A Details of alignment guidelines,[0],[0]
Light verbs.,A Details of alignment guidelines,[0],[0]
"Light verbs have no lexical alignment, but a subgraph alignment covers the light verb construction as a unit (e.g. makes dobjÐÐ→attempt∼ attempt-01 in figure 2).",A Details of alignment guidelines,[0],[0]
"All subgraph alignments which involve the light verb or its complement have to involve to whole unit, as shown in the alignment highlighted in red in figure 2.",A Details of alignment guidelines,[0],[0]
Multiword expressions.,A Details of alignment guidelines,[0],[0]
"In verb-particle constructions and fixed grammatical expressions the AMR node lexically aligns to all words in the expression, and additionally to the DG subgraph spanning the whole expression.",A Details of alignment guidelines,[0],[0]
"(e.g. pay ∼ pay-off-02, off ∼ pay-off-02, and pay compound-prtÐÐÐÐÐÐÐ→off ∼ pay-off-02).",A Details of alignment guidelines,[0],[0]
Prepositional phrases.,A Details of alignment guidelines,[0],[0]
"PP modifiers typically involve an extra dependency edge for the preposition attachment, as with lies nmod-inÐÐÐÐ→sun caseÐÐ→in ∼ lie-07 locationÐÐÐÐ→sun.
Semantically decomposed words.",A Details of alignment guidelines,[0],[0]
"When one word has multiple lexical alignments because of morphological decomposition, there also exists a structural alignment between that word and an AMR subgraph representing the decomposition: e.g., in figure 2, evildoer∼ person ARG0-ofÐÐÐÐ→do-02 ARG1ÐÐ→thing modÐ→evil, and in figure 3, general ∼ person
ARG0-ofÐÐÐÐ→have-org-role-91 ARG2ÐÐ→general.",A Details of alignment guidelines,[0],[0]
"AMR decomposes certain words by convention which must always be structurally aligned, such as ago ∼ before op1Ð→now and government ∼ government-organization
ARG0-ofÐÐÐÐ→govern-01.",A Details of alignment guidelines,[0],[0]
"Date, time, and value expressions.",A Details of alignment guidelines,[0],[0]
"These expressions are aligned similarly to named entities, even though the normalized constants may not exactly match the words in the sentence.",A Details of alignment guidelines,[0],[0]
"For example,
the DG structure 9:00 nummod←ÐÐÐÐpm would be represented in the AMR as date-entity timeÐÐ→21:00; tokens 9:00 and pm are treated as a multiword expression: each is lexically aligned to ""21:00"".",A Details of alignment guidelines,[0],[0]
"Moreover, we also align 9:00 nummod←ÐÐÐÐpm ∼ 21:00 and 9:00 nummod←ÐÐÐÐpm ∼ date-entity timeÐÐ→21:00.",A Details of alignment guidelines,[0],[0]
"Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested.",abstractText,[0],[0]
"To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs.",abstractText,[0],[0]
Our annotation explains how 97% of AMR edges are evoked by words or syntax.,abstractText,[0],[0]
"Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%.",abstractText,[0],[0]
"While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences.",abstractText,[0],[0]
"We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser.",abstractText,[0],[0]
"We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.",abstractText,[0],[0]
"Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested.",abstractText,[0],[0]
"To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs.",abstractText,[0],[0]
Our annotation explains how 97% of AMR edges are evoked by words or syntax.,abstractText,[0],[0]
"Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%.",abstractText,[0],[0]
"While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences.",abstractText,[0],[0]
"We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser.",abstractText,[0],[0]
"We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.",abstractText,[0],[0]
A Structured Syntax-Semantics Interface for English-AMR Alignment,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612–3621 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3612",text,[0],[0]
"Recently, neural machine translation (NMT) (Bahdanau et al., 2015; Hassan et al., 2018; Wu et al., 2016; He et al., 2017; Xia et al., 2016, 2017; Wu et al., 2018b,a) has become more and more popular given its superior performance without the demand of heavily hand-crafted engineering efforts.",1 Introduction,[0],[0]
"It is usually trained to maximize the likelihood of each token in the target sentence, by taking the source sentence and the preceding (ground-truth) target tokens as inputs.",1 Introduction,[0],[0]
"Such training approach is referred as maximum likelihood estimation (MLE) (Scholz, 1985).",1 Introduction,[0],[0]
"Although easy to implement, the token-level
∗This work was conducted at Microsoft Research Asia.
objective function during training is inconsistent with sequence-level evaluation metrics such as BLEU (Papineni et al., 2002).
",1 Introduction,[0],[0]
"To address the inconsistency issue, reinforcement learning (RL) methods have been adopted to optimize sequence-level objectives.",1 Introduction,[0],[0]
"For example, policy optimization methods such as REINFORCE (Ranzato et al., 2016; Wu et al., 2017b) and actorcritic (Bahdanau et al., 2017) are leveraged for sequence generation tasks including NMT.",1 Introduction,[0],[0]
"In machine translation community, a similar method is proposed with the name ‘minimum risk training’ (Shen et al., 2016).",1 Introduction,[0],[0]
"All these works demonstrate the effectiveness of RL techniques for NMT models (Wu et al., 2016).
",1 Introduction,[0],[0]
"However, effectively applying RL to real-world NMT systems has not been fulfilled by previous works.",1 Introduction,[0],[0]
"First, most of, if not all, previous works verified their methods based on shallow recurrent neural network (RNN) models.",1 Introduction,[0],[0]
"However, to obtain state-of-the-art (SOTA) performance, it is essential to leverage recently derived deep models (Gehring et al., 2017; Vaswani et al., 2017), which are much more powerful.
",1 Introduction,[0],[0]
"Second, it is not easy to make RL practically effective given quite a few widely acknowledged limitations of RL method (Henderson et al., 2018) such as high variance of gradient estimation (Weaver and Tao, 2001), and objective instability (Mnih et al., 2013).",1 Introduction,[0],[0]
"Therefore, several tricks are proposed in previous works.",1 Introduction,[0],[0]
"However, it remains unclear, and no agreement is achieved on how to use these tricks in machine translation.",1 Introduction,[0],[0]
"For example, baseline reward method (Weaver and Tao, 2001) is suggested in (Ranzato et al., 2016; Nguyen et al., 2017; Wu et al., 2016) but not leveraged in (He and Deng, 2012; Shen et al., 2016).
",1 Introduction,[0],[0]
"Third, large-scale datasets, especially monolingual datasets are shown to significantly improve translation quality (Sennrich et al., 2015a; Xia et al.,
2016) with MLE training, while it remains nearly empty on how to combine RL with monolingual data in NMT.
",1 Introduction,[0],[0]
"In this paper, we try to fulfill these gaps and study how to practically apply RL to obtain strong NMT systems with quite competitive, even stateof-the-art performance.",1 Introduction,[0],[0]
"Several comprehensive studies are conducted on different aspects of RL training to figure out how to: 1) set efficient rewards; 2) combine MLE and RL objectives with different weights, which aims to stabilize the training procedure; 3) reduce the variance of gradient estimation.
",1 Introduction,[0],[0]
"In addition, given the effectiveness of leveraging monolingual data in improving translation quality, we further propose a new method to combine the strength of both RL training and source/target monolingual data.",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work that tries to explore the power of monolingual data when training NMT model with RL method.
",1 Introduction,[0],[0]
"We obtain some useful findings through the experiments on WMT17 Chinese-English (Zh-En), WMT17 English-Chinese (En-Zh) and WMT14 English-German (En-De) translation tasks.",1 Introduction,[0],[0]
"For instance, multinomial sampling is better than beam search in reward computation, and the combination of RL and monolingual data significantly enhances the NMT model performance.",1 Introduction,[0],[0]
"Our main contributions are summarized as follows.
",1 Introduction,[0],[0]
"• We provide the first comprehensive study on different aspects of RL training, such as how to setup reward and baseline reward, on top of quite competitive NMT models.
",1 Introduction,[0],[0]
"• We propose a new method that effectively leverages large-scale monolingual data, from both the source and target side, when training NMT models with RL.
",1 Introduction,[0],[0]
"• Combined with several of our findings and method, we obtain the SOTA translation quality on WMT17 Zh-En translation task, surpassing strong baseline (Transformer big model + back translation) by nearly 1.5 BLEU points.",1 Introduction,[0],[0]
"Furthermore, on WMT14 En-De and WMT17 En-Zh translation tasks, we can also obtain strong competitive results.
",1 Introduction,[0],[0]
"We hope that our studies and findings will benefit the community to better understand and leverage reinforcement learning for developing strong
NMT models, especially in real-world scenarios faced with deep models and large amount of training data (including both parallel and monolingual data).",1 Introduction,[0],[0]
"Towards this end, we open source all our codes/dataset at https://github.com/ apeterswu/RL4NMT to provide a clear recipe for performance reproduction.",1 Introduction,[0],[0]
"In this section, we first introduce the attentionbased sequence-to-sequence learning framework for neural machine translation (NMT), and then introduce the basis of applying reinforcement learning to training NMT models.",2 Background,[0],[0]
Typical NMT models are based on the encoderdecoder framework with attention mechanism.,2.1 Neural Machine Translation,[0],[0]
The encoder first maps a source sentence x =,2.1 Neural Machine Translation,[0],[0]
"(x1, x2, ..., xn) to a set of continuous representations z = (z1, z2, ..., zn).",2.1 Neural Machine Translation,[0],[0]
"Given z, the decoder then generates a target sentence y = (y1, y2, ..., ym) of word tokens one by one.",2.1 Neural Machine Translation,[0],[0]
"At each decoding step t of model training, the probability of generating a token yt is maximized conditioned on x and y<t = (y1, ..., yt−1).",2.1 Neural Machine Translation,[0],[0]
"Given N training sentence pairs {xi, yi}Ni=1, maximum likelihood estimation (MLE) is usually adopted to optimize the model, and the training objective is defined as:
Lmle = N∑ i=1",2.1 Neural Machine Translation,[0],[0]
"log p(yi|xi)
=",2.1 Neural Machine Translation,[0],[0]
N∑ i=1,2.1 Neural Machine Translation,[0],[0]
"m∑ t=1 log p(yit|yi1, ..., yit−1, xi), (1)
where m is the length of sentence yi.",2.1 Neural Machine Translation,[0],[0]
"Among all the encoder-decoder models, the recently proposed Transformer (Vaswani et al., 2017) architecture achieves the best translation quality so far.",2.1 Neural Machine Translation,[0],[0]
"The main difference between Transformer and previous RNNSearch (Bahdanau et al., 2015) or ConvS2S (Gehring et al., 2017) is that Transformer relies entirely on self-attention (Lin et al., 2017) to compute representations of source and target side sentences, without using recurrent or convolutional operations.",2.1 Neural Machine Translation,[0],[0]
"As aforementioned, reinforcement learning (RL) is leveraged to bridge the gap between training and
inference of NMT, by directly optimizing the evaluation measure (e.g., BLEU) at training time.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"Specifically, NMT model can be viewed as an agent, which interacts with the environment (the previous words y<t and the context vector z available at each step t).",2.2 Training NMT with Reinforcement Learning,[0],[0]
"The parameters of the agent define a policy, i.e., a conditional probability p(yt|x, y<t).",2.2 Training NMT with Reinforcement Learning,[0],[0]
"The agent will pick an action , i.e., a candidate word out from the vocabulary, according to the policy.",2.2 Training NMT with Reinforcement Learning,[0],[0]
A terminal reward is observed once the agent generates a complete sequence ŷ.,2.2 Training NMT with Reinforcement Learning,[0],[0]
"The reward for machine translation is the BLEU (Papineni et al., 2002) score, denoted as R(ŷ, y), which is defined by comparing the generated ŷ with the ground-truth sentence",2.2 Training NMT with Reinforcement Learning,[0],[0]
"y. Note that here the reward R(ŷ, y) is the sentence-level reward, i.e., a scalar for each complete sentence ŷ.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"The goal of the RL training is to maximize the expected reward:
Lrl = N∑ i=1",2.2 Training NMT with Reinforcement Learning,[0],[0]
"Eŷ∼p(ŷ|xi)R(ŷ, y i)
=",2.2 Training NMT with Reinforcement Learning,[0],[0]
"N∑ i=1 ∑ ŷ∈Y p(ŷ|xi)R(ŷ, yi), (2)
where Y is the space of all candidate translation sentences, which is exponentially large due to the large vocabulary size, making it impossible to exactly maximize Lrl.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"In practice, REINFORCE (Williams, 1992) is usually leveraged to approximate the above expectation via sampling ŷ from the policy p(y|x), leading to the objective as maximizing:
L̂rl = N∑ i=1",2.2 Training NMT with Reinforcement Learning,[0],[0]
"R(ŷi, yi), ŷi ∼ p(y|xi),∀i ∈",2.2 Training NMT with Reinforcement Learning,[0],[0]
[N ].,2.2 Training NMT with Reinforcement Learning,[0],[0]
"(3)
Throughout the paper we will use REINFORCE as our policy optimization method for RL training.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"Although training NMT with RL can fill in the gap between training objectives and evaluation metrics, it is not easy to successfully put RL training into practice.",3 Strategies for RL Training,[0],[0]
"A key challenge is that RL methods are highly unstable and inefficient, due to the noise in gradient estimation and reward computation.",3 Strategies for RL Training,[0],[0]
"To our best knowledge, currently there is no consensus, or even a systematic study on how to configure different setups for RL training to avoid such problems, especially for training deep NMT models on large scale datasets.",3 Strategies for RL Training,[0],[0]
"We therefore aim to shed light
on practical applications of RL for NMT training.",3 Strategies for RL Training,[0],[0]
"For this purpose, we provide a comprehensive review of several important methods to stabilize RL training process in this section.",3 Strategies for RL Training,[0],[0]
"It is critical to set up appropriate rewards for RL training, i.e., the R(ŷ, y) in Eqn.",3.1 Reward Computation,[0],[0]
(3).,3.1 Reward Computation,[0],[0]
"There are two important aspects to consider in configuring the reward R(ŷ, y): how to sample training instance ŷ and whether to use reward shaping.
",3.1 Reward Computation,[0],[0]
Generate ŷ,3.1 Reward Computation,[0],[0]
"There are two strategies to sample ŷ for computing the BLEU reward R(ŷ, y).",3.1 Reward Computation,[0],[0]
"The first one is beam search (Sutskever et al., 2014), it is a breadth-first search method that maintains a “beam” of the top-K scoring candidates (prefix hypothesis sentences) at each generation step.",3.1 Reward Computation,[0],[0]
"Then, for each candidate sentence in the beam,K most likely words are appended, resulting in a pool of K ×K new candidates.",3.1 Reward Computation,[0],[0]
"Out from this pool, the top-K translations with largest probabilities are selected, and the beam search process continues.",3.1 Reward Computation,[0],[0]
"The second strategy is multinomial sampling (Chatterjee and Cancedda, 2010), which produces each word one by one through multinomial sampling over the model’s output distribution.",3.1 Reward Computation,[0],[0]
"Both sampling strategies terminate the expansion of a candidate sentence when an ‘end of sentence’ (<EOS>) token is met.
",3.1 Reward Computation,[0],[0]
The choice of different sampling strategies reflects the exploration-exploitation dilemma.,3.1 Reward Computation,[0],[0]
"Beam search strategy generates more accurate ŷ by exploiting the probabilistic space output via current NMT model, while multinomial sampling pays more attention to explore more diverse candidates.
",3.1 Reward Computation,[0],[0]
Whether to Use Reward Shaping From Eqn.,3.1 Reward Computation,[0],[0]
"(3) we can see that for the entire sequence ŷ, there is only one terminal reward R(ŷ, y) available for model training.",3.1 Reward Computation,[0],[0]
"Note that the agent needs to take tens of actions (with the number depending on the length of ŷ) to generate a complete sentence ŷ, but only one reward is available for all those actions.",3.1 Reward Computation,[0],[0]
"Consequently, RL training is inefficient due to the sparsity of rewards, and the model updates each token in the training sentence with the same reward value without distinction.",3.1 Reward Computation,[0],[0]
"Reward shaping (Ng et al., 1999) is a strategy to overcome this shortcoming.",3.1 Reward Computation,[0],[0]
"In reward shaping, intermediate reward at each decoding step t is imposed and denoted as rt(ŷt, y).",3.1 Reward Computation,[0],[0]
"Bahdanau et al. (2017) sets up the intermediate reward as rt(ŷt, y) = R(ŷ1...t, y)",3.1 Reward Computation,[0],[0]
"− R(ŷ1...t−1, y), where R(ŷ1...t, y) is defined as the BLEU score
of ŷ1...t with respect to y. Note that we have R(ŷ, y) = ∑m t=1 rt(ŷt, y), where m is the length
of ŷ. During RL training, the cumulative reward∑m τ=t rτ (ŷτ , y) is used to update the policy at time step t. It is verified that using the shaped reward rt instead of awarding the whole score R(ŷ, y) does not change the optimal policy (Ng et al., 1999).",3.1 Reward Computation,[0],[0]
"As mentioned before, the REINFORCE algorithm suffers from high variance in gradient estimation, mainly caused by using single sample ŷ to estimate the expectation.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"To reduce the variance, Ranzato et al. (2016) subtracts an average reward from the returned reward at each time step t, and the actual reward used to update the policy is
R(ŷ, y)− r̂t, (4)
where r̂t is the estimated average reward at step t, named as baseline reward (Weaver and Tao, 2001).",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"Together with reward shaping, the updated reward becomes ∑m τ=t rτ (ŷτ , y)− r̂t at step t.
Intuitively speaking, a baseline reward r̂t is established, which either encourages a word choice ŷt if the induced reward R satisfies R > r̂t, or discourages it if R < r̂t.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"Here R is either the terminal reward R(ŷ, y) or the cumulative reward∑m
τ=t rτ (ŷτ , y).",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"Such estimated baseline reward r̂t is designed to decrease the high variance of the gradient estimator.
",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"In practice, the baseline reward r̂t can be obtained through different approaches.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"For example, one may sample multiple sentences and use the mean terminal reward for these sentences as baseline reward.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"In our work, we adopt the function learning approach, using simple network (e.g., multi-layer perceptron) to build the learning function, which is the same as used in (Ranzato et al., 2016; Bahdanau et al., 2017).",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"The last important strategy we would like to mention is the combination of MLE training objective with RL objective, which is assumed to further stabilize RL training process (Wu et al., 2016; Li et al., 2017; Wu et al., 2017a).
",3.3 Combine MLE and RL Objectives,[0],[0]
"A simple way is to linearly combine the MLE (Eqn. (1)) and RL (Eqn. (3)) objectives as follows:
Lcom = α ∗",3.3 Combine MLE and RL Objectives,[0],[0]
"Lmle + (1− α) ∗ L̂rl, (5)
where α is the hyperparamter controlling the tradeoff between MLE and RL objectives.",3.3 Combine MLE and RL Objectives,[0],[0]
We will empirically evaluate how different values of α impact the final translation accuracy.,3.3 Combine MLE and RL Objectives,[0],[0]
Previous works typically conduct RL training with only bilingual data for NMT.,4 RL Training with Monolingual Data,[0],[0]
"Monolingual data has been proved to be able to significantly improve the performance of NMT systems (Sennrich et al., 2015a; Xia et al., 2016; Cheng et al., 2016).",4 RL Training with Monolingual Data,[0],[0]
It remains an open problem whether it is possible to combine the benefits of RL training and monolingual data such that even more competitive results can be obtained.,4 RL Training with Monolingual Data,[0],[0]
In this section we provide several solutions for combination and will study them in next section.,4 RL Training with Monolingual Data,[0],[0]
"Note that all the settings discussed in this section are semi-supervised learning, i.e., both bilingual and monolingual data are available.",4 RL Training with Monolingual Data,[0],[0]
We first provide a solution to RL training with source-side monolingual data.,4.1 With Source-Side Monolingual Data,[0],[0]
As shown in Eqn.,4.1 With Source-Side Monolingual Data,[0],[0]
"(3), in RL training we need to calculate the reward signal R(ŷ, y) for each generated sentence ŷ, and therefore the reference sentence y seems to be a must-have, which unfortunately is missing for source-side monolingual data.
",4.1 With Source-Side Monolingual Data,[0],[0]
We tackle this challenge via generating pseudo target reference y by bootstrapping with the model itself.,4.1 With Source-Side Monolingual Data,[0],[0]
"Apparently, for the source-side monolingual data, the pseudo target reference y should have good translation quality.",4.1 With Source-Side Monolingual Data,[0],[0]
"Therefore, for each source-side monolingual sentence, we use the NMT model trained from the bilingual data to beam search a target sentence and treat it as the pseudo target reference y. Afterwards ŷ is obtained via multinomial sampling to calculate the reward.",4.1 With Source-Side Monolingual Data,[0],[0]
"Although multinomial sampling is usually not as good as sampling via beam search, the combination of beam search (to get the pseudo target reference sentence) and the multinomial sampling (to generate the action sequence of the agent) achieves good exploration-exploitation trade-off, since the pseudo target reference exploits the accuracy of current NMT model while ŷ achieves better exploration.",4.1 With Source-Side Monolingual Data,[0],[0]
"For a target-side monolingual sentence, its source sentence x is missing, and consequently ŷ is unavailable since it is sampled based on",4.2 With Target-Side Monolingual Data,[0],[0]
"x. We tackle
this challenge via back translation (Sennrich et al., 2015a).",4.2 With Target-Side Monolingual Data,[0],[0]
We first train a reverse NMT model from the target language to the source language with bilingual data.,4.2 With Target-Side Monolingual Data,[0],[0]
"For each target-side monolingual sentence, using the reverse NMT model, we back translate it to get its pseudo source sentence x.",4.2 With Target-Side Monolingual Data,[0],[0]
"We then pair the target monolingual data and its backtranslated sentence as a pseudo bilingual sentence pair, which can be used for RL training in the same way as the genuine bilingual sentence pairs.",4.2 With Target-Side Monolingual Data,[0],[0]
A natural extension of previous discussions is to combine both the source-side and target-side monolingual data for RL training.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"We consider two combinations, the sequential method and the unified method.",4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
The former one sequentially leverages the source-side and target-side monolingual data for RL training.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"Specifically, we first train an MLE model using the bilingual data and source-side (or target-side) monolingual data; based on this MLE model, we then use REINFORCE for training with target-side (or source-side) monolingual data.",4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"For unified approach, we pack the paired data out from three domains together: the genuine bilingual data, the source monolingual data with its pseudo target references (introduced in subsection 4.1), and the target monolingual data with its back-translated samples (introduced in subsection 4.2).",4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
Then we treat the combined data as normal bilingual data on which the NMT model is trained via MLE or RL principles.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
Our goal is to investigate the model performance with different training data and find the best recipe of how to use these data in RL training.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
More details are introduced in next section.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"In this section, we provide a systematic study on aforementioned RL training strategies and the solutions of leveraging monolingual data.",5 Experiments,[0],[0]
"The RL training strategies are evaluated on bilingual datasets from three translation tasks, WMT14 EnglishGerman (En-De), WMT17 English-Chinese (EnZh) and WMT17 Chinese-English (Zh-En), and we further conduct the experiments to leverage monolingual data in WMT17 Zh-En translation.",5 Experiments,[0],[0]
"For the bilingual datasets, WMT17 (Bojar et al., 2017) En-Zh 1 and WMT17 Zh-En use the same dataset, which contains about 24M sentences pairs, including CWMT Corpus 2017 and UN Parallel Corpus V1.0.",5.1 Experimental Settings,[0],[0]
The Jieba2 segmenter is used to perform Chinese word segmentation.,5.1 Experimental Settings,[0],[0]
"We use byte pair encoding (BPE) (Sennrich et al., 2015b) to preprocess the source and target sentences, forming source-side and target-side dictionary with 40, 000 and 37, 000 types, respectively.",5.1 Experimental Settings,[0],[0]
We use the newsdev2017 as the dev set and newstest2017 as the test set.,5.1 Experimental Settings,[0],[0]
"For the WMT14 En-De dataset, it contains about 4.5M training pairs, newstest2012 and newstest2013 are concatenated as the dev set and newstest2014 acts as test set.",5.1 Experimental Settings,[0],[0]
"Same as (Vaswani et al., 2017), we also perform BPE to process the En-De dataset, the shared source-target vocabulary contains about 37, 000 tokens.
",5.1 Experimental Settings,[0],[0]
"For the monolingual dataset on Zh-En translation task, similar to (Sennrich et al., 2017), the Chinese monolingual data comes from LDC Chinese Gigaword (4th edition) and the English monolingual data comes from News Crawl 2016 articles.",5.1 Experimental Settings,[0],[0]
"After preprocessing (e.g., language detection and filtering sentences with more than 80 words), we keep 4M Chinese sentences and 7M English sentences.
",5.1 Experimental Settings,[0],[0]
"We adopt the Transformer model with transformer big setting as defined in (Vaswani et al., 2017) for Zh-En and En-Zh translations, which achieves SOTA translation quality in several other datasets.",5.1 Experimental Settings,[0],[0]
"For En-De translation, we utilize the transformer base v1 setting.",5.1 Experimental Settings,[0],[0]
"These settings are exactly same as used in the original paper, except we set the layer prepostprocess dropout for Zh-En and En-Zh translation to be 0.05.",5.1 Experimental Settings,[0],[0]
"The optimizer used for MLE training is Adam (Kingma and Ba, 2015) with initial learning rate is 0.1, and we follow the same learning rate schedule in (Vaswani et al., 2017).",5.1 Experimental Settings,[0],[0]
"During training, roughly 4, 096 source tokens and 4, 096 target tokens are paired in one mini batch.",5.1 Experimental Settings,[0],[0]
Each model is trained using 8 NVIDIA Tesla M40 GPUs.,5.1 Experimental Settings,[0],[0]
"For RL training, the model is initialized with parameters of the MLE model (trained with only bilingual data), and we continue training it with learning rate 0.0001.",5.1 Experimental Settings,[0],[0]
"Same as (Bahdanau et al., 2017), to calculate the BLEU reward, we start all n-gram counts from 1 instead of 0 and
1http://www.statmt.org/wmt17/ translation-task.html
2https://github.com/fxsjy/jieba
multiply the resulting score by the length of the target reference sentence.",5.1 Experimental Settings,[0],[0]
"For inference, we use beam search with width 6.",5.1 Experimental Settings,[0],[0]
We run each setting for at least 5 times and report the averaged case sensitive BLEU scores3,5.1 Experimental Settings,[0],[0]
"(Papineni et al., 2002) on test set.",5.1 Experimental Settings,[0],[0]
The test set BLEU is chosen via the best configuration based on the validation set.,5.1 Experimental Settings,[0],[0]
"We first evaluate different strategies for RL training, based only on bilingual datasets from previously introduced three translation tasks.
",5.2 Results of of RL Training Strategies,[0],[0]
"Reward Computation As reviewed in subsection 3.1, for reward computation, we need to consider how to sample ŷ",5.2 Results of of RL Training Strategies,[0],[0]
"and whether to use reward shaping.
",5.2 Results of of RL Training Strategies,[0],[0]
"The results are shown in Table 1, where “RL” stands for RL training with the REINFORCE algorithm.",5.2 Results of of RL Training Strategies,[0],[0]
We also report the performance of the pretrained NMT model with the MLE loss.,5.2 Results of of RL Training Strategies,[0],[0]
"From the table, an interesting finding is that ŷ sampled via beam search strategy is worse than that by multinomial sampling, with a gap of roughly 0.2-0.3 BLEU points on the test set (with significant test score ρ < 0.05).",5.2 Results of of RL Training Strategies,[0],[0]
"We therefore conjecture that exploration is more important than exploitation in reward computing: multinomial sampling brings more data diversity to the training of NMT model, while sentences generated by beam search are usually very similar to each other.",5.2 Results of of RL Training Strategies,[0],[0]
"Furthermore, we find that there is no big difference between the leverage of reward shaping or terminal reward, with only slightly better performance of reward shaping.",5.2 Results of of RL Training Strategies,[0],[0]
"We therefore use multinomial sampling and reward shaping in later experiments.
3Calculated by SacréBLEU toolkit, which produces exactly the same evaluation result as that in WMT17 Zh-En campaign.",5.2 Results of of RL Training Strategies,[0],[0]
https://github.com/awslabs/sockeye/ tree/master/contrib/sacrebleu,5.2 Results of of RL Training Strategies,[0],[0]
Variance Reduction of Gradient Estimation Next we evaluate the strategies for reducing variance of gradient estimation (see section3.2).,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
We want to know whether the baseline reward is necessary.,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"To compute the baseline reward, similar to (Ranzato et al., 2016; Bahdanau et al., 2017), we build a two-layer MLP regressor with Relu (Nair and Hinton, 2010) activation units.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The function takes the hidden states from decoder as input, and the parameters of the regressor are trained to minimize the mean squared loss of Eqn. (4).",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"We first pre-train the baseline function for 20k steps/minibatches, and then jointly train NMT model (with RL) and the baseline reward function.
",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
Table 2 shows that the learning of baseline reward does not help RL training.,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"This contradicts with previous observations (Ranzato et al., 2016), and seems to suggest that the variance of gradient estimation in NMT is not as large as we expected.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The reason might be that the probability mass on the target-side language space induced by the NMT model is highly concentrated, making the sampled ŷ representative enough in terms of estimating the expectation.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"Therefore, for the economic perspective, it is not necessary to add the additional steps of using baseline reward on RL training for NMT.
",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"Combine MLE and RL Objectives As shown in Eqn. (5), the hyperparameter α controls the trade-off between MLE and RL objectives.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"For comparison, we set α to be [0, 0.1, 0.3, 0.5, 0.7, 0.9] in our experiments.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The results are presented in Figure 1.
",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The results show that combining the MLE objective with the RL objective achieves better performance (27.48 for En-De, 34.63 for En-Zh and 25.04 for Zh-En with α = 0.3).",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"This indicates that MLE objective is helpful to stabilize the training and improve the model performance, as we expected.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"However, further increasing α does not bring more gain.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
The best trade-off between MLE and RL objectives in our experiment is α = 0.3.,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"Therefore, we set α = 0.3 in the following experiments.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"In this subsection, we report the results on both valid and test set of RL training using bilingual and monolingual data in Zh-En translation.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 3 to Table 6, “RL” denotes the model trained with RL using multinomial sampling, reward shaping, no baseline reward, and combined objective, based on the observations in the last subsection.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"“B” denotes bilingual data, “Ms” denotes sourceside monolingual data and “Mt” denotes target-side monolingual data, “&” denotes data combination.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"With Source-Side Monolingual Data As discussed before, we use beam search with beam width 4 to sample the pseudo target sentence y for each monolingual sentence x.",5.3 Results of RL Training with Monolingual Data,[0],[0]
We consider several settings for RL training: 1) only source-side monolingual data; 2) the combination of bilingual and source-side monolingual data.,5.3 Results of RL Training with Monolingual Data,[0],[0]
"We first train an MLE model using the augmented dataset combining the genuine bilingual data with the pseudo bilingual data generated from the monolingual data, and then perform RL training on this combined dataset.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"The results are shown in Table 3.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"With Target-Side Monolingual Data For target-side monolingual data, we first pre-train a translation model from English to Chinese 4, and use it to back translate target-side monolingual
4The BLEU score of the En-Zh model is 34.12.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
sentence y to get pseudo source sentence,5.3 Results of RL Training with Monolingual Data,[0],[0]
x.,5.3 Results of RL Training with Monolingual Data,[0],[0]
"Similarly, we consider several settings for RL training: 1) only target-side monolingual data; 2) the combination of bilingual data and target-side monolingual data.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"We train an MLE model using both the genuine and the generated pseudo bilingual data, and then perform RL training on this data.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"The results are presented in Table 4.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 3 and 4, we have several observations.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"First, monolingual data helps RL training, improving BLEU score from 25.04 to 25.22 (ρ < 0.05) in Table 3.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"Second, when we only add monolingual data for RL training, the model achieves similar performance compared to MLE training with bilingual and monolingual data (e.g., 25.15 vs. 25.24 (ρ < 0.05) in Table 4).
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"With both Source-Side and Target-Side Monolingual Data We have two approaches to use both source-side and target-side monolingual data, as described in subsection 4.3.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"The results are reported in Table 5 and Table 6.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 5, we can observe that the sequen-
tial training of monolingual data can benefit the model performance.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"Taking the last three rows as an example, the BLEU score of the MLE model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, RL training using the source-side monolingual data further improves the model performance by 0.7 (ρ < 0.01)",5.3 Results of RL Training with Monolingual Data,[0],[0]
BLEU points.,5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 6, we can observe on top of a quite strong MLE baseline (26.13), through the unified RL training, we can still improve the test set by 0.6 points to 26.73 (ρ < 0.01), which shows the effectiveness of combining source/target monolingual data and reinforcement learning.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"At last, as a summary of our empirical results, we compare several representataive end-to-end NMT systems to our work in Table 7, which includes the Transformer (Vaswani et al., 2017) model, with/without back-translation (Sennrich et al., 2015a) and the best NMT system in WMT17 Chinese-English translation challenge5 (SougouKnowing-ensemble).",5.4 Comparison with Other Models,[0],[0]
"The results clearly show that after combing both source-side and target-side monolingual data with RL training, we obtain the state-of-the-art BLEU score 26.73, even surpassing the best ensemble model in WMT17 Zh-En translation challenge.",5.4 Comparison with Other Models,[0],[0]
Our work is mainly related with the literature of using reinforcement learning to directly optimize the evaluation measure for neural machine translation.,6 Related Work,[0],[0]
"Several representative works are (Ranzato et al.,
5http://matrix.statmt.org/matrix/ systems_list/1878
2016; Shen et al., 2016; Bahdanau et al., 2017).",6 Related Work,[0],[0]
"In (Ranzato et al., 2016), the authors propose to train a neural translation model with the objective gradually shifting from maximizing token-level likelihood to optimizing the sentence-level BLEU score.",6 Related Work,[0],[0]
"Shen et al. (2016) proposes to adopt minimum risk training (Goel and Byrne, 2000) to minimize the task specific expected loss (i.e., induced by BLEU score) on NMT training data.",6 Related Work,[0],[0]
"Instead of the REINFORCE (Williams, 1992) algorithm used in the above two works, Bahdanau et al. (2017) further optimizes the policy by actor-critic algorithm.",6 Related Work,[0],[0]
"Wu et al. (2016) introduces a simple RL based method to optimize the stacked LSTM model for NMT, achieving better BLEU scores on English-French translation but not on English-German.",6 Related Work,[0],[0]
"Edunov et al. (2017) presents a comparative study of several classical structural prediction losses for NMT model, which also includes sequence-level loss but not exactly the same as RL.
",6 Related Work,[0],[0]
"Our work is also related with the research works that leverage monolingual data for improving NMT models (Zhang and Zong, 2016; Sennrich et al., 2015a; Wang et al., 2018; Xia et al., 2016; Cheng et al., 2016).",6 Related Work,[0],[0]
Zhang and Zong (2016) exploits the source-side monolingual data in NMT.,6 Related Work,[0],[0]
Sennrich et al. (2015a) proposes back-translation method to leverage target-side monolingual data for NMT.,6 Related Work,[0],[0]
"Xia et al. (2016) formulates the machine translation as a communication game, which leverages the power of two directional translation models and source/target monolingual data.",6 Related Work,[0],[0]
Cheng et al. (2016) proposes a similar semi-supervised approach.,6 Related Work,[0],[0]
"However, none of these works have explored the power of monolingual data in the context of training NMT model with reinforcement learning.",6 Related Work,[0],[0]
"In this work, we presented a study of how to effectively train NMT models using reinforcement learning.",7 Conclusion,[0],[0]
"Different RL strategies were evaluated in German-English, English-Chinese and ChineseEnglish translation tasks on large-scale bilingual datasets.",7 Conclusion,[0],[0]
"We found that (1) multinomial sampling is better than beam search, (2) several previous tricks such as reward shaping and baseline reward does not make significant difference, and (3) the combination of the MLE and RL objectives is important.",7 Conclusion,[0],[0]
"In addition, we explored the source/target monolingual data for RL training.",7 Conclusion,[0],[0]
"By combing the power of RL and monolingual data, we achieve the state-of-the-art BLEU score on WMT17 ChineseEnglish translation task.",7 Conclusion,[0],[0]
We hope that our study and results can benefit the community and bring some insights on how to train deep NMT models with reinforcement learning and big data.,7 Conclusion,[0],[0]
Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system.,abstractText,[0],[0]
"However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged.",abstractText,[0],[0]
"In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning.",abstractText,[0],[0]
"We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training.",abstractText,[0],[0]
"Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data.",abstractText,[0],[0]
"By integrating all our findings, we obtain competitive results on WMT14 EnglishGerman, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",abstractText,[0],[0]
A Study of Reinforcement Learning for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2814–2819 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Automatically analyzing and generating natural language requires capturing not only what is said, but also how to say it.",1 Introduction,[0],[0]
Consider the sentences “anybody hurt?”,1 Introduction,[0],[0]
and “is someone wounded?”.,1 Introduction,[0],[0]
"The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used.",1 Introduction,[0],[0]
"Such differences in formality have been identified as an important dimension of style (Trudgill, 1992) or tone (Halliday, 1978) variation.
",1 Introduction,[0],[0]
"In this paper, we build on prior computational work that has focused on analyzing formality of texts (Lahiri and Lu, 2011; Brooke and Hirst, 2013; Pavlick and Nenkova, 2015; Pavlick and Tetreault, 2016) with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation.",1 Introduction,[0],[0]
"Human translators translate a document for a specific audience (Nida and Taber Charles, 1969), and often ask what is the expected tone of the content when taking a new translation job.",1 Introduction,[0],[0]
We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input.,1 Introduction,[0],[0]
"While ultimately we would like systems to preserve the formality of the source, this is a
challenging task that requires not only automatically inferring the formality of the source, but also understanding how formality differs across languages and cultures.",1 Introduction,[0],[0]
"As a first step, we therefore limit our study to the scenario where the expected output formality is given to the MT system as an additional input.
",1 Introduction,[0],[0]
We first select a formality model providing the most accurate scores on intrinsic formality datasets.,1 Introduction,[0],[0]
We compare existing lexical formality models and novel variants based on inducing formality dimensions or subspaces in vector space models.,1 Introduction,[0],[0]
We then turn to machine translation and show that a lexical formality model can have a positive impact when used to control the formality of machine translation output.,1 Introduction,[0],[0]
"When the expected formality matches the reference, we obtain improvement of translation quality evaluated by automatic metrics (BLEU).",1 Introduction,[0],[0]
A human assessment also verified the effectiveness of our proposed system in generating translations at diverse levels of formality.,1 Introduction,[0],[0]
Our goal is to provide systems with the ability to generate language across a range of formality style.,2 Formality-Sensitive MT,[0],[0]
"We propose a Formality-Sensitive Machine Translation (FSMT) scenario where the system takes two inputs: (1) text in the source language to be translated, and (2) a desired formality level capturing the intended audience of the translation.",2 Formality-Sensitive MT,[0],[0]
We propose to implement it as n-best re-ranking within a standard phrase-based MT architecture.,2 Formality-Sensitive MT,[0],[0]
"Unlike domain adaptation approaches, which aim to produce domain-specific or potentially formality-specific systems, our goal is to obtain a single system trained on diverse data which can adaptively produce output for a range of styles.
",2 Formality-Sensitive MT,[0],[0]
"We therefore introduce a formality-scoring fea-
2814
ture for re-ranking.",2 Formality-Sensitive MT,[0],[0]
"For each translation hypothesis h, given the formality level ` as a parameter:
f(h; `) = |Formality(h)−",2 Formality-Sensitive MT,[0],[0]
"`|
where Formality(h) is the sentence-level formality score for h. f(h; `), along with standard model features, is fed into a standard re-ranking model.",2 Formality-Sensitive MT,[0],[0]
"When training the re-ranking model, the parameter ` is set to the actual formality score of the reference translation for each instance.",2 Formality-Sensitive MT,[0],[0]
"At test time, ` is provided by the user.",2 Formality-Sensitive MT,[0],[0]
The re-scoring weights help promote candidate sentences whose formality scores approach the expected level.,2 Formality-Sensitive MT,[0],[0]
The FSMT system requires quantifying the formality level of a sentence.,3 Formality Modeling,[0],[0]
"Following prior work, we define sentence-level formality based on lexical formality scores (Brooke et al., 2010; Pavlick and Nenkova, 2015).",3 Formality Modeling,[0],[0]
"We conduct an empirical comparison of existing techniques that can be adapted as lexical formality models, and introduce a sentence-level formality scheme based on weighted average.",3 Formality Modeling,[0],[0]
"State-of-the-art lexical formality models (Brooke et al., 2010; Brooke and Hirst, 2014) are based on vector space models of word meaning, and a set of pre-selected seed words that are representative of formal and informal language.
",3.1 Lexical Formality,[0],[0]
SimDiff Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known formality using cosine similarity.,3.1 Lexical Formality,[0],[0]
"Intuitively, w is more likely formal if it is semantically closer to formal seed words than to informal seed words.",3.1 Lexical Formality,[0],[0]
"Formally, given a formal word set Sf and an informal word set Si, SimDiff scores a word w by
score(w) = 1 |Sf | ∑ v∈Sf cos(ew, ev)− 1|Si| ∑ v∈Si cos(ew, ev)
",3.1 Lexical Formality,[0],[0]
Turning this difference into a formality score requires further manipulation.,3.1 Lexical Formality,[0],[0]
A neutral word r has to be manually selected to anchor the midpoint of the formality score range.,3.1 Lexical Formality,[0],[0]
"In other words, the final formality score for r is enforced to be zero:
Formality(w) =",3.1 Lexical Formality,[0],[0]
score(w)− score(r),3.1 Lexical Formality,[0],[0]
"normalizer(w, r)
",3.1 Lexical Formality,[0],[0]
The neutral word is typically selected from function words.,3.1 Lexical Formality,[0],[0]
We select “at” because it appears in nearly every document and appears with nearly equivalent probabilities in formal/informal corpora.,3.1 Lexical Formality,[0],[0]
"Finally, a normalizer which is maximized among the whole vocabulary ensures that scores cover the entire [−1, 1] range.
",3.1 Lexical Formality,[0],[0]
"Instead of using cosine diff as the score function score(w), other standard techniques can be also applied under this framework.
",3.1 Lexical Formality,[0],[0]
SVM,3.1 Lexical Formality,[0],[0]
"As an alternative to the model proposed by Brooke and Hirst (2014), we propose to train an Support Vector Machine (SVM) model to find a hyperplane that separates formal and informal words and define the score function as the distance to the hyperplane.
",3.1 Lexical Formality,[0],[0]
Formality Subspace Another category of methods consists in identifying a subspace that captures formality within the original vector space.,3.1 Lexical Formality,[0],[0]
Lexical scores can then simply be obtained by projecting word representations onto the formality subspace.,3.1 Lexical Formality,[0],[0]
One example is training a Principal Component Analysis (PCA) model on word representations of all seeds.,3.1 Lexical Formality,[0],[0]
This method is based on the assumption that representative formal/informal words principally vary along the direction of formality.,3.1 Lexical Formality,[0],[0]
"Alternatively, inspired by DENSIFIER (Rothe et al., 2016), we can learn a subspace that aims at separating words in Sf vs. words in Si and grouping words in the same set.",3.1 Lexical Formality,[0],[0]
"While previous work scored sentence by averaging word scores (Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), we propose a weighted average scheme for word sequences W to downgrade the formality contribution of neutral words:
Formality(W ) =∑ wi∈W |Formality(wi)| · Formality(wi)∑
wi∈W |Formality(wi)|",3.2 From Word to Sentence Formality,[0],[0]
"Before evaluating our FSMT framework, we evaluate the formality models at the sentence level.",3.3 Evaluation,[0],[0]
"Lahiri (2015) and Pavlick and Tetreault (2016) collected 5-way human scores for 11,263 sentences in the genres of blog, email, answers and news.",3.3 Evaluation,[0],[0]
"Following Pavlick and Tetreault (2016), we averaged human scores for each sentence as the
gold standard.",3.3 Evaluation,[0],[0]
"As in prior work, the score quality was evaluated by the Spearman correlation.
",3.3 Evaluation,[0],[0]
A large mixed-topic corpus is required to train vector space models.,3.3 Evaluation,[0],[0]
"As suggested by Brooke et al. (2010), we used the ICWSM 2009 Spinn3r dataset (English tier-1) which consists of about 1.6 billion words (Burton et al., 2009).",3.3 Evaluation,[0],[0]
We also compared the term-document association model,3.3 Evaluation,[0],[0]
"Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and the term-term association model word2vec (W2V) (Mikolov et al., 2013).",3.3 Evaluation,[0],[0]
"We used the same 105 formal seeds and 138 informal seeds as Brooke et al. (2010).
",3.3 Evaluation,[0],[0]
"Followed Brooke et al. (2010), to achieve best performance, we used a small dimensionality (10) for training LSA and word2vec.",3.3 Evaluation,[0],[0]
"In practice, we normalized the LSA word vectors to make them have unit length for SVM and PCA, but did not applied it to word2vec.",3.3 Evaluation,[0],[0]
"This suggests that the magnitude of LSA word vectors is harmful for formality modeling.
",3.3 Evaluation,[0],[0]
"We also compared formality models based on word representations to a baseline that relies on unigram models to compare word statistics in corpora representative of formal vs. informal language (Pavlick and Nenkova, 2015).",3.3 Evaluation,[0],[0]
This method requires language examples of diverse formality.,3.3 Evaluation,[0],[0]
"Conversational transcripts are generally considered as casual text, so we concatenated corpora such as Fisher (Cieri et al., 2004), Switchboard (Godfrey et al., 1992), SBCSAE (Bois et al., 2000- 2005), CallHome1, CallFriend2, BOLT",3.3 Evaluation,[0],[0]
"SMS/Chat (Song et al., 2014) and NPS Chatroom (Forsythand and Martell, 2007).",3.3 Evaluation,[0],[0]
"As the formal counterpart, we extracted comparable size of text from Europarl (Koehn, 2005).",3.3 Evaluation,[0],[0]
"This results in 30 Million tokens of formal corpora (1.1M segments) and 29 Million tokens of informal corpora (2.7M segments).
",3.3 Evaluation,[0],[0]
Table 1 shows that all models based on the vector space achieve similar performance in terms of Spearman’s ρ (except SVM-W2V which yields lower performance).,3.3 Evaluation,[0],[0]
The baseline method based on unigram models was outperformed by 0.1+ point.,3.3 Evaluation,[0],[0]
"So we select DENSIFIER-LSA as a representative for our FSMT system.
",3.3 Evaluation,[0],[0]
"1https://catalog.ldc.upenn.edu/ LDC97S42
2https://talkbank.org/access/CABank/ CallFriend/",3.3 Evaluation,[0],[0]
Set-up We evaluate this approach on a French to English translation task.,4 Evaluation of the FSMT System,[0],[0]
"Two parallel FrenchEnglish corpora are used: (1) MultiUN (Eisele and Chen, 2010), which is extracted from the United Nations website, and can be considered to be formal text; (2) OpenSubtitles2016 (Lison and Tiedemann, 2016), which is extracted from movie and TV subtitles, covers a wider spectrum of styles, but overall tends to be informal since it primarily contains conversations.",4 Evaluation of the FSMT System,[0],[0]
"Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments).",4 Evaluation of the FSMT System,[0],[0]
"Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles.",4 Evaluation of the FSMT System,[0],[0]
"Moses (Koehn et al., 2007) is used to build our phrase-based MT system.",4 Evaluation of the FSMT System,[0],[0]
"We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic.",4 Evaluation of the FSMT System,[0],[0]
"We used 4-gram language models, trained using KenLM (Heafield, 2011).",4 Evaluation of the FSMT System,[0],[0]
"Model weights were tuned using batch MIRA (Cherry and Foster, 2012).
",4 Evaluation of the FSMT System,[0],[0]
We used constant size n=1000 for n-best lists in all experiments.,4 Evaluation of the FSMT System,[0],[0]
The re-ranking is a log-linear model trained using batch MIRA.,4 Evaluation of the FSMT System,[0],[0]
"4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011).
",4 Evaluation of the FSMT System,[0],[0]
"FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different
3http://www.statmt.org/moses/?n=Moses.",4 Evaluation of the FSMT System,[0],[0]
"Baseline
4https://github.com/moses-smt/ mosesdecoder/tree/master/scripts/ nbest-rescore
formality for each source sentence.",4 Evaluation of the FSMT System,[0],[0]
"Since such references are not available, we construct three sets of test data where instances are divided according to the formality level of the available reference translation.",4 Evaluation of the FSMT System,[0],[0]
"The formality distribution in the tuning set shows that 97% reference translations fall into the range of [−0.6, 0.6].",4 Evaluation of the FSMT System,[0],[0]
"We therefore set three formality bins – informal [−1,−0.2), neutral formality",4 Evaluation of the FSMT System,[0],[0]
"[−0.2, 0.2], and formal (0.2, 1] – and split the test set into these bins.",4 Evaluation of the FSMT System,[0],[0]
"We use DENSIFIER-LSA and training setting described above to translate the entire test set three times, with three different formality levels: low (-0.4), neutral (0) and high (0.4).",4 Evaluation of the FSMT System,[0],[0]
"We first report standard automatic evaluation results using the BLEU score to compare FSMT output given different desired formality level on each bins (See Table 2).
",4.1 Automatic Evaluation,[0],[0]
"The best BLEU scores for each formality level are obtained when the level of formality given as input to the MT system matches the nature of the text being translated, as can be seen in the scores along the diagonal in Table 2.",4.1 Automatic Evaluation,[0],[0]
"Comparing with the baseline system, which produces the top translation from each n-best list, translation quality improves by +0.5 BLEU on informal text, +0.3 BLEU on neutral text, and remains constant on formal text.",4.1 Automatic Evaluation,[0],[0]
The impact increases with the distance to formal language increases.,4.1 Automatic Evaluation,[0],[0]
"This can be explained by the fact that more formal sentences tend to be longer, and the impact of alternate lexical choice for a small number of words per sentence is smaller in longer sentences.",4.1 Automatic Evaluation,[0],[0]
"In addition, the formal sentences are mostly drawn from UN data which is sufficiently different from the other genres in the heterogeneous training corpus that the informal examples do not affect baseline per-
formance on formal data.",4.1 Automatic Evaluation,[0],[0]
Automatic evaluation is limited to comparing output to a single reference: lower BLEU scores conflate translation errors and stylistic mismatch.,4.2 Human Assessment,[0],[0]
"Therefore, we conduct a human study of the formality vs. the quality.
",4.2 Human Assessment,[0],[0]
We conducted a manual evaluation of the output of our FSMT system taking low/high formality levels (-0.4/0.4) as parameters.,4.2 Human Assessment,[0],[0]
42 translation pairs were randomly selected and were annotated by 15 volunteers.,4.2 Human Assessment,[0],[0]
"For each pair of segments, the volunteers were asked to select the segment that would be more appropriate in a formal setting (e.g., a job interview) than in a casual setting (e.g., chatting with friends).",4.2 Human Assessment,[0],[0]
"A default option of “neither of them is more formal or hard to say” was also available.
",4.2 Human Assessment,[0],[0]
"By majority voting, 20 pairs were annotated as “N”, indicating the two translations has no distinctions w.r.t. formality.",4.2 Human Assessment,[0],[0]
"For example, “A: how can they do this” vs. “B: how can they do that”.",4.2 Human Assessment,[0],[0]
"Given that the translations were restricted to the nbest list, not all sentences could be translated into stylistically different language.
",4.2 Human Assessment,[0],[0]
"Of the remaining 21 pairs where annotators judged one output more formal than the other, in all but one case the translation produced by our FSMT system with high formality level parameter was judged to be more formal.",4.2 Human Assessment,[0],[0]
"Overall this indicates that our formality scoring and ranking procedure are effective.
To determine whether re-ranking based on formality might have a detrimental effect on quality, we also had annotators rate the fluency and adequacy of the segments.",4.2 Human Assessment,[0],[0]
"Inspired by Graham et al. (2013), annotators were first asked to assess fluency without a reference and separately adequacy with a reference.",4.2 Human Assessment,[0],[0]
Both assessments used a sliding scale.,4.2 Human Assessment,[0],[0]
Each segment was evaluated by an average of 7 annotators.,4.2 Human Assessment,[0],[0]
"After rescaling the ratings into the [0, 1] range, we observed a 0.75 level of fluency for informal translations and 0.70 for formal ones.",4.2 Human Assessment,[0],[0]
This slight difference fits our expectation that more casual language may feel more fluent while more formal language may feel more stilted.,4.2 Human Assessment,[0],[0]
"The adequacy ratings were 0.65 and 0.64 for informal and translations respectively, indicating that adjusting the level of formality had minimal effect on the adequacy of the result.
",4.2 Human Assessment,[0],[0]
Some examples are listed in Table 3.,4.2 Human Assessment,[0],[0]
"Occa-
sionally, the n-best list had no translation hypotheses with diverse formality, so the FSMT system dropped necessary words, appended inessential words, or selected improper or even incorrect words to fit the target formality level.",4.2 Human Assessment,[0],[0]
"In the case of ’how do you do’, the translation that was meant to be more casual was rated more formal.",4.2 Human Assessment,[0],[0]
"Because the system measures formality on the lexical level, it was not able to recognize this idiomatically formal phrase made up of words that are not inherently formal.",4.2 Human Assessment,[0],[0]
"Despite these issues, most of the output were formality-variant translations of the same French source segment, as expected.",4.2 Human Assessment,[0],[0]
"We presented a framework for formality-sensitive machine translation, where a system produces translations at a desired formality level.",5 Conclusion,[0],[0]
Our evaluation shows the effectiveness of this system in controlling language formality without loss in translation quality.,5 Conclusion,[0],[0]
"Stylistic variations of language, such as formality, carry speakers’ intention beyond literal meaning and should be conveyed adequately in translation.",abstractText,[0],[0]
We propose to use lexical formality models to control the formality level of machine translation output.,abstractText,[0],[0]
"We demonstrate the effectiveness of our approach in empirical evaluations, as measured by automatic metrics and human assessments.",abstractText,[0],[0]
A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 231–240 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
231",text,[0],[0]
"The media and the public are currently discussing the recent phenomenon of “fake news” and its potential role in swaying elections, how it may affect society, and what can and should be done about it.",1 Introduction,[0],[0]
"Prone to misunderstanding and misue, the term “fake news” arose from the observation that, in social media, a certain kind of ‘news’ spreads much more successfully than others, and this kind of ‘news’ is typically extremely one-sided (hyperpartisan), inflammatory, emotional, and often riddled with untruths.",1 Introduction,[0],[0]
"Although traditional yellow press has been spreading ‘news’ of varying de-
grees of truthfulness long before the digital revolution, its amplification over real news within social media gives many people pause.",1 Introduction,[0],[0]
"The fake news hype caused a widespread disillusionment about social media, and many politicians, news publishers, IT companies, activists, and scientists concur that this is where to draw the line.",1 Introduction,[0],[0]
"For all their good intentions, however, it must be drawn very carefully (if at all), since nothing less than free speech is at stake—a fundamental right of every free society.
",1 Introduction,[0],[0]
Many favor a two-step approach where fake news items are detected and then countermeasures are implemented to foreclose rumors and to discourage repetition.,1 Introduction,[0],[0]
"While some countermeasures are already tried in practice, such as displaying warnings and withholding ad revenue, fake news detection is still in its infancy.",1 Introduction,[0],[0]
"At any rate, a nearreal time reaction is crucial: once a fake news item begins to spread virally, the damage is done and undoing it becomes arduous.",1 Introduction,[0],[0]
"Since knowledge-based and context-based approaches to fake news detection can only be applied after publication, i.e., as news events unfold and as social interactions occur, they may not be fast enough.
",1 Introduction,[0],[0]
"We have identified style-based approaches as a viable alternative, allowing for instantaneous reactions, albeit not to fake news, but to hyperpartisanship.",1 Introduction,[0],[0]
"In this regard we contribute (1) a large news corpus annotated by experts with respect to veracity and hyperpartisanship, (2) extensive experiments on discriminating fake news, hyperpartisan news, and satire based solely on writing style, and (3) validation experiments to verify our finding that the writing style of the left and the right have more in common than any of the two have with the mainstream, applying Unmasking in a novel way.
",1 Introduction,[0],[0]
"After a review of related work, Section 3 details the corpus and its construction, Section 4 introduces our methodology, and Section 5 reports the results of the aforementioned experiments.",1 Introduction,[0],[0]
"Approaches to fake news detection divide into three categories (Figure 1): they can be knowledge-based (by relating to known facts), context-based (by analyzing news spread in social media), and stylebased (by analyzing writing style).
",2 Related Work,[0],[0]
Knowledge-based fake news detection.,2 Related Work,[0],[0]
Methods from information retrieval have been proposed early on to determine the veracity of web documents.,2 Related Work,[0],[0]
"For example, Etzioni et al. (2008) propose to identify inconsistencies by matching claims extracted from the web with those of a document in question.",2 Related Work,[0],[0]
"Similarly, Magdy and Wanas (2010) measure the frequency of documents that support a claim.",2 Related Work,[0],[0]
"Both approaches face the challenges of web data credibility, namely expertise, trustworthiness, quality, and reliability (Ginsca et al., 2015).
",2 Related Work,[0],[0]
"Other approaches rely on knowledge bases, including the semantic web and linked open data.",2 Related Work,[0],[0]
"Wu et al. (2014) “perturb” a claim in question to query knowledge bases, using the result variations as indicator of the support a knowledge base offers for the claim.",2 Related Work,[0],[0]
"Ciampaglia et al. (2015) use the shortest path between concepts in a knowledge graph, whereas Shi and Weninger (2016) use a link prediction algorithm.",2 Related Work,[0],[0]
"However, these approaches are unsuited for new claims without corresponding entries in a knowledge base, whereas knowledge bases can be manipulated (Heindorf et al., 2016).
",2 Related Work,[0],[0]
Context-based fake news detection.,2 Related Work,[0],[0]
"Here, fake news items are identified via meta information and spread patterns.",2 Related Work,[0],[0]
"For example, Long et al. (2017) show that author information can be a useful feature for fake news detection, and Derczynski et al. (2017) attempt to determine the veracity of a claim based on the conversation it sparks on Twitter as one of the RumourEval tasks.",2 Related Work,[0],[0]
"The Facebook analysis of Mocanu et al. (2015) shows that unsubstantiated claims spread as widely as well-established ones, and that user groups predisposed to conspiracy theories are more open to sharing the former.",2 Related Work,[0],[0]
"Similarly, Acemoglu et al. (2010), Kwon et al. (2013), Ma et al. (2017), and Volkova et al. (2017) model the spread of (mis-)information, while Budak et al. (2011) and Nguyen et al. (2012) propose algorithms to limit its spread.",2 Related Work,[0],[0]
The efficacy of countermeasures like debunking sites is studied by Tambuscio et al. (2015).,2 Related Work,[0],[0]
"While achieving good results, context-based approaches suffer from working only a posteriori, requiring large amounts of data, and disregarding the actual news content.
",2 Related Work,[0],[0]
"Fake news detection
Style-based fake news detection.",2 Related Work,[0],[0]
"Deception detection originates from forensic linguistics and builds on the Undeutsch hypothesis—a result from forensic psychology which asserts that memories of reallife, self-experienced events differ in content and quality from imagined events (Undeutsch, 1967).",2 Related Work,[0],[0]
The hypothesis led to the development of forensic tools to assess testimonies at the statement level.,2 Related Work,[0],[0]
"Some approaches operationalize deception detection at scale to detect uncertainty in social media posts, for example Wei et al. (2013) and Chen et al. (2015).",2 Related Work,[0],[0]
"In this regard, Rubin et al. (2015) use rhetorical structure theory as a measure of story coherence and as an indicator for fake news.",2 Related Work,[0],[0]
"Recently, Wang (2017) collected a large dataset consisting of sentence-length statements along their veracity from the fact-checking site PolitiFact.com, and then used style features to detect false statements.",2 Related Work,[0],[0]
"A related task is stance detection, where the goal is to detect the relation between a claim about an article, and the article itself (Bourgonje et al., 2017).",2 Related Work,[0],[0]
"Most prominently, stance detection was the task of the Fake News Challenge1 which ran in 2017 and received 50 submissions, albeit hardly any participants published their approach.",2 Related Work,[0],[0]
"1http://www.fakenewschallenge.org/
Where deception detection focuses on single statements, style-based text categorization as proposed by Argamon-Engelson et al. (1998) assesses entire texts.",2 Related Work,[0],[0]
"Common applications are author profiling (age, gender, etc.) and genre classification.",2 Related Work,[0],[0]
"Though susceptible to authors who can modify their writing style, such obfuscations may be detectable (e.g., Afroz et al. (2012)).",2 Related Work,[0],[0]
"As an early precursor to fake news detection, Badaskar et al. (2008) train models to identify news items that were automatically generated.",2 Related Work,[0],[0]
"Currently, text categorization methods for fake news detection focus mostly on satire detection (e.g., Rubin et al. (2016), Yang et al. (2017)).",2 Related Work,[0],[0]
"Rashkin et al. (2017) perform a statistical analysis of the stylistic differences between real, satire, hoax, and propaganda news.",2 Related Work,[0],[0]
"We make use of their results by incorporating the bestperforming style features identified.
",2 Related Work,[0],[0]
"Finally, two preprint papers have been recently shared.",2 Related Work,[0],[0]
Horne and Adali (2017) use style features for fake news detection.,2 Related Work,[0],[0]
"However, the relatively high accuracies reported must be taken with a grain of salt: their two datasets comprise only 70 news articles each, whose ground-truth is based on where an article came from, instead of resulting from a per-article expert review as in our case; their final classifier uses only 4 features (number of nouns, type-token ratio, word count, number of quotes), which can be easily manipulated; and based on their experimental setup, it cannot be ruled out that the classifier simply differentiates news portals rather than fake and real articles.",2 Related Work,[0],[0]
We avoid this problem by testing our classifiers on articles from portals which were not represented in the training data.,2 Related Work,[0],[0]
"Similarly, Pérez-Rosas et al. (2017) also report on constructing two datasets comprising around 240 and 200 news article excerpts (i.e., the 5-sentence lead) with a balanced distribution of fake vs. real.",2 Related Work,[0],[0]
"The former was collected via crowdsourcing, asking workers to write a fake news item based on a real news item, the latter was collected from the web.",2 Related Work,[0],[0]
"For style analysis, the former dataset may not be suitable, since the authors note themselves that “workers succeeded in mimicking the reporting style from the original news”.",2 Related Work,[0],[0]
"The latter dataset encompasses only celebrity news (i.e., yellow press), which introduces a bias.",2 Related Work,[0],[0]
"Their feature selection follows that of Rubin et al. (2016), which is covered by our experiments, but also incorporates topic features, rendering the resulting classifier not generalizable.",2 Related Work,[0],[0]
"This section introduces the BuzzFeed-Webis Fake News Corpus 2016, detailing its construction and annotation by professional journalists employed at BuzzFeed, as well as key figures and statistics.2",3 The BuzzFeed-Webis Fake News Corpus,[0],[0]
"The corpus encompasses the output of 9 publishers on 7 workdays close to the US presidential elections 2016, namely September 19 to 23, 26, and 27.",3.1 Corpus Construction,[0],[0]
Table 1 gives an overview.,3.1 Corpus Construction,[0],[0]
"Among the selected publishers are six prolific hyperpartisan ones (three left-wing and three right-wing), and three mainstream ones.",3.1 Corpus Construction,[0],[0]
"All publishers earned Facebook’s blue checkmark , indicating authenticity and an elevated status within the network.",3.1 Corpus Construction,[0],[0]
"Every post and linked news article has been fact-checked by 4 BuzzFeed journalists, including about 19% of posts forwarded from third parties.",3.1 Corpus Construction,[0],[0]
"Having checked a total of 2,282 posts, 1,145 mainstream, 471 leftwing, and 666 right-wing, Silverman et al. (2016) reported key insights as a data journalism article.",3.1 Corpus Construction,[0],[0]
The annotations were published alongside the article.3,3.1 Corpus Construction,[0],[0]
"However, this data only comprises URLs to the original Facebook posts.",3.1 Corpus Construction,[0],[0]
"To construct our corpus, we archived the posts, the linked articles, and attached media as well as relevant meta data to ensure long-term availability.",3.1 Corpus Construction,[0],[0]
"Due to the rapid pace at which the publishers change their websites, we were able to recover only 1,627 articles, 826 mainstream, 256 left-wing, and 545 right-wing.
",3.1 Corpus Construction,[0],[0]
Manual fact-checking.,3.1 Corpus Construction,[0],[0]
"A binary distinction between fake and real news turned out to be infeasible, since hardly any piece of fake news is entirely false, and pieces of real news may not be flawless.",3.1 Corpus Construction,[0],[0]
"Therefore, posts were rated “mostly true,” “mixture of true and false,” “mostly false,” or, if the post was opinion-driven or otherwise lacked a factual claim, “no factual content.”",3.1 Corpus Construction,[0],[0]
"Four BuzzFeed journalists worked on the manual fact-checks of the news articles: to minimize costs, each article was reviewed only once and articles were assigned round robin.",3.1 Corpus Construction,[0],[0]
"The ratings “mixture of true and false” and “mostly false” had to be justified, and, when in doubt about a rating, a second opinion was collected, whereas disagreements were resolved by a third one.",3.1 Corpus Construction,[0],[0]
"Finally, all news rated “mostly false” underwent a final check to ensure the rating was justified, lest the respective publishers would contest it.",3.1 Corpus Construction,[0],[0]
"2Corpus download: https://doi.org/10.5281/zenodo.1239675 3http://github.com/BuzzFeedNews/2016-10-facebook-fact-check
",3.1 Corpus Construction,[0],[0]
The journalists were given the following guidance: Mostly true: The post and any related link or image are based on factual information and portray it accurately.,3.1 Corpus Construction,[0],[0]
"The authors may interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up.",3.1 Corpus Construction,[0],[0]
"This rating does not allow for unsupported speculation or claims.
",3.1 Corpus Construction,[0],[0]
"Mixture of true and false (mix, for short): Some elements of the information are factually accurate, but some elements or claims are not.",3.1 Corpus Construction,[0],[0]
"This rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate.",3.1 Corpus Construction,[0],[0]
It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link.,3.1 Corpus Construction,[0],[0]
"Finally, use this rating for news articles that are based on unconfirmed information.
",3.1 Corpus Construction,[0],[0]
Mostly false: Most or all of the information in the post or in the link being shared is inaccurate.,3.1 Corpus Construction,[0],[0]
"This should also be used when the central claim being made is false.
",3.1 Corpus Construction,[0],[0]
"No factual content (n/a, for short):",3.1 Corpus Construction,[0],[0]
"This rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim.",3.1 Corpus Construction,[0],[0]
This is also the category to use for posts that are of the “Like this if you think...” variety.,3.1 Corpus Construction,[0],[0]
"Given the significant workload (i.e., costs) required to carry out the aforementioned annotations, the corpus is restricted to the given temporal period and biased toward the US culture and political landscape, comprising only English news articles from a limited number of publishers.",3.2 Limitations,[0],[0]
"Annotations were recorded at the article level, not at statement level.",3.2 Limitations,[0],[0]
"For text categorization, this is sufficient.",3.2 Limitations,[0],[0]
"At the time of writing, our corpus is the largest of its kind that has been annotated by professional journalists.",3.2 Limitations,[0],[0]
Table 1 shows the fact-checking results and some key statistics per article.,3.3 Corpus Statistics,[0],[0]
"Unsurprisingly, none of the mainstream articles are mostly false, whereas 8 across all three publishers are a mixture of true and false.",3.3 Corpus Statistics,[0],[0]
"Disregarding non-factual articles, a little more than a quarter of all hyperpartisan left-wing articles were found faulty: 15 articles mostly false, and 51 a mixture of true and false.",3.3 Corpus Statistics,[0],[0]
"Publisher “The Other 98%” sticks out by achieving an almost per-
Orientation Fact-checking results Key statistics per article Publisher
true mix false n/a Σ Paras.",3.3 Corpus Statistics,[0],[0]
"Links Words
extern all quoted all
Mainstream 806 8 0 12 826 20.1 2.2 3.7 18.1 692.0 ABC News 90 2 0 3 95 21.1 1.0 4.8 21.0 551.9 CNN 295 4 0 8 307 19.3 2.4 2.5 15.3 588.3 Politico 421 2 0 1 424 20.5 2.3 4.3 19.9 798.5
Left-wing 182 51 15 8 256 14.6 4.5 4.9 28.6 423.2 Addicting Info 95 25 8 7 135 15.9 4.4 4.5 30.5 430.5 Occupy Democrats 55 23 6 0 91 10.9 4.1 4.7 29.0 421.7",3.3 Corpus Statistics,[0],[0]
"The Other 98% 32 3 1 1 30 20.2 6.4 7.2 21.2 394.5
Right-wing 276 153 72 44 545 14.1 2.5 3.1 24.6 397.4 Eagle Rising 107 47 25 36 214 12.9 2.6 2.8 17.3 388.3 Freedom Daily 48 24 22 4 99 14.6 2.2 2.3 23.5 419.3 Right Wing News 121 82 25 4 232 15.0 2.5 3.6 33.6 396.6
Σ 1264 212 87 64 1627 17.2 2.7 3.7 20.6 551.0
Table 1: The BuzzFeed-Webis Fake News Corpus 2016 at a glance (“Paras.”",3.3 Corpus Statistics,[0],[0]
"short for “paragraphs”).
",3.3 Corpus Statistics,[0],[0]
fect score.,3.3 Corpus Statistics,[0],[0]
"By contrast, almost 45% of the rightwing articles are a mixture of true and false (153) or mostly false (72).",3.3 Corpus Statistics,[0],[0]
"Here, publisher “Right Wing News” sticks out by supplying more than half of mixtures of true and false alone, whereas mostly false articles are equally distributed.
",3.3 Corpus Statistics,[0],[0]
"Regarding key statistics per article, it is interesting that the articles from all mainstream publishers are on average about 20 paragraphs long with word counts ranging from 550 words on average at ABC News to 800 at Politico.",3.3 Corpus Statistics,[0],[0]
"Except for one publisher, left-wing articles and right-wing articles are shorter on average in terms of paragraphs as well as word count, averaging at about 420 words and 400 words, respectively.",3.3 Corpus Statistics,[0],[0]
"Left-wing articles quote on average about 10 words more than the mainstream, and right-wing articles 6 words more.",3.3 Corpus Statistics,[0],[0]
"When articles comprise links, they are usually external ones, whereas ABC News rather uses internal links, and only half of the links found at Politico articles are external.",3.3 Corpus Statistics,[0],[0]
Left-wing news articles stick out by containing almost double the amount of links across publishers than mainstream and right-wing ones.,3.3 Corpus Statistics,[0],[0]
"In our experiments, we operationalize the category of fake news by joining the articles that were rated mostly false with those rated a mixture of true and false.",3.4 Operationalizing Fake News,[0],[0]
"Arguably, the latter may not be exactly what is deemed “fake news” (as in: a complete fabrication), however, practice shows fake news are hardly ever devoid of truth.",3.4 Operationalizing Fake News,[0],[0]
"More often, true facts are misconstrued or framed badly.",3.4 Operationalizing Fake News,[0],[0]
"In our experiments, we hence call mostly true articles real news, mostly false plus mixtures of true and false—except for satire—fake news, and disregard all articles rated non-factual.",3.4 Operationalizing Fake News,[0],[0]
"This section covers our methodology, including our feature set to capture writing style, and a brief recap of Unmasking by Koppel et al. (2007), which we employ for the first time to distinguish genre styles as opposed to author styles.",4 Methodology,[0],[0]
"For sake of reproducibility, all our code has been published.4",4 Methodology,[0],[0]
Our writing style model incorporates common features as well as ones specific to the news domain.,4.1 Style Features and Feature Selection,[0],[0]
"The former are n-grams, n in [1, 3], of characters, stop words, and parts-of-speech.",4.1 Style Features and Feature Selection,[0],[0]
"Further, we employ 10 readability scores5 and dictionary features, each indicating the frequency of words from a tailor-made dictionary in a document, using the General Inquirer Dictionaries as a basis (Stone et al., 1966).",4.1 Style Features and Feature Selection,[0],[0]
"The domain-specific features include ratios of quoted words and external links, the number of paragraphs, and their average length.
",4.1 Style Features and Feature Selection,[0],[0]
"In each of our experiments, we carefully select from the aforementioned features the ones worthwhile using: all features are discarded that are hardly represented in our corpus, namely word tokens that occur in less than 2.5% of the documents, and n-gram features that occur in less than 10% of the documents.",4.1 Style Features and Feature Selection,[0],[0]
"Discarding these features prevents overfitting and improves the chances that our model will generalize.
",4.1 Style Features and Feature Selection,[0],[0]
"If not stated otherwise, our experiments share a common setup.",4.1 Style Features and Feature Selection,[0],[0]
"In order to avoid biases from the respective training sets, we balance them using oversampling.",4.1 Style Features and Feature Selection,[0],[0]
"Furthermore, we perform 3-fold cross-validation where each fold comprises one publisher from each orientation, so that the classifier does not learn a publisher’s style.",4.1 Style Features and Feature Selection,[0],[0]
For nonUnmasking experiments we use WEKA’s random forest implementation with default settings.,4.1 Style Features and Feature Selection,[0],[0]
"Unmasking, as proposed by Koppel et al. (2007), is a meta learning approach for authorship verification.",4.2 Unmasking Genre Styles,[0],[0]
"We study for the first time whether it can be used to assess the similarity of more broadly defined style categories, such as left-wing vs. rightwing vs. mainstream news.",4.2 Unmasking Genre Styles,[0],[0]
"This way, we uncover relations between the writing styles that people may involuntarily adopt as per their political orientation.",4.2 Unmasking Genre Styles,[0],[0]
"4Code download: http://www.github.com/webis-de/ACL-18 5Automated Readability Index, Coleman Liau Index, Flesh Kincaid Grade Level and Reading Ease, Gunning Fog Index, LIX, McAlpine EFLAW Score, RIX, SMOG Grade, Strain Index
Originally, Unmasking takes two documents as input and outputs its confidence whether they have been written by the same author.",4.2 Unmasking Genre Styles,[0],[0]
"Three steps are taken to accomplish this: first, each document is chunked into a set of at least 500-word long chunks; second, classification errors are measured while iteratively removing the most discriminative features of a style model consisting of the 250 most frequent words, separating the two chunk sets with a linear classifier; and third, the resulting classification accuracy curves are analyzed with regard to their slope.",4.2 Unmasking Genre Styles,[0],[0]
"A steep decrease is more likely than a shallow decrease if the two documents have been written by the same author, since there are presumably less discriminating features between documents written by the same author than between documents written by different authors.",4.2 Unmasking Genre Styles,[0],[0]
"Training a classifier on many examples of error curves obtained from same-author document pairs and differentauthor document pairs yields an effective authorship verifier—at least for long documents that can be split up into a sufficient number of chunks.
",4.2 Unmasking Genre Styles,[0],[0]
It turns out that what applies to the style of authors also applies to genre styles.,4.2 Unmasking Genre Styles,[0],[0]
"We adapt Unmasking by skipping its first step and using two sets of documents (e.g., left-wing articles and rightwing articles) as input.",4.2 Unmasking Genre Styles,[0],[0]
"When plotting classification error curves for visual inspection, steeper decreases in these plots, too, indicate higher style similarity of the two input document sets, just as with chunk sets of two documents written by the same author.",4.2 Unmasking Genre Styles,[0],[0]
"We employ four baseline models: a topic-based bag of words model, often used in the literature, but less practical since news topics change frequently and drastically; a model using only the domain-specific news style features to check whether the differences between categories measured as corpus statistics play a significant role; and naive baselines that classify all items into one of the categories in question, relating our results to the class distributions.",4.3 Baselines,[0],[0]
"Classification performance is measured as accuracy, and class-wise precision, recall, and F1.",4.4 Performance Measures,[0],[0]
"We favor these measures over, e.g., areas under the ROC curve or the precision recall curve for simplicity sake.",4.4 Performance Measures,[0],[0]
"Also, the tasks we are tackling are new, so that little is known to date about user preferences.",4.4 Performance Measures,[0],[0]
This is also why we chose the evenly-balanced F1.,4.4 Performance Measures,[0],[0]
"We report on the results of two series of experiments that investigate style differences and similarities between hyperpartisan and mainstream news, and between fake, real, and satire news, shedding light on the following questions:
1.",5 Experiments,[0],[0]
Can (left/right) hyperpartisanship be distinguished from the mainstream?,5 Experiments,[0],[0]
2.,5 Experiments,[0],[0]
Is style-based fake news detection feasible?,5 Experiments,[0],[0]
3.,5 Experiments,[0],[0]
Can fake news be distinguished from satire?,5 Experiments,[0],[0]
"Our first experiment addressing the first question uncovered an odd behavior of our classifier: it would often misjudge left-wing for right-wing news, while being much better at distinguishing both combined from the mainstream.",5 Experiments,[0],[0]
"To explain this behavior, we hypothesized that maybe the writing style of the hyperpartisan left and right are more similar to one another than to the mainstream.",5 Experiments,[0],[0]
"To investigate this hypothesis, we devised two additional validation experiments, yielding three sources of evidence instead of just one.",5 Experiments,[0],[0]
A. Predicting orientation.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Table 2 shows the classification performance of a ternary classifier trained to discriminate left, right, and mainstream—an obvious first experiment for our dataset.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Separating the left and right orientation from the mainstream does not work too well: the topic baseline outperforms the style-based models with regard to accuracy, whereas the results for class-wise precision and recall are a mixed bag.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
The left-wing articles are apparently significantly more difficult to be identified compared to articles from the other two orientations.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"When we inspected the confusion matrix (not shown), it turned out that 66% of misclassifications of left-wing articles are falsely classified as right-wing articles, whereas 60% of all misclassified right-wing articles are classified as mainstream articles.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Misclassified mainstream articles spread almost evenly across the other classes.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The poor performance of the domain-specific news style features by themselves demonstrate that orientation cannot be discriminated based on the basic corpus characteristics observed with respect to paragraphs, quotations, and hyperlinks.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"This holds for all subsequent experiments.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
B. Predicting hyperpartisanship.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Given the apparent difficulty of telling apart individual orientations, we did not frantically add features or switch classifiers to make it work.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Rather, we trained a binary
Features Accuracy Precision Recall F1 all left right main.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
left right main.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"left right main.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Style 0.60 0.21 0.56 0.75 0.20 0.59 0.74 0.20 0.57 0.75,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Topic 0.64 0.24 0.62 0.72 0.15 0.54 0.86 0.19 0.58,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"0.79 News style 0.39 0.09 0.35 0.59 0.14 0.36 0.49 0.11 0.36 0.53
classifier to discriminate hyperpartisanship in general from the mainstream.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Table 3 shows the performance values.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"This time, the best classification accuracy of 0.75 at a remarkable 0.89 recall for the hyperpartisan class is achieved by the style-based classifier, outperforming the topic baseline.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Comparing Table 2 and Table 3, we were left with a riddle: all other things being equal, how could it be that hyperpartisanship in general can be much better discriminated from the mainstream than individual orientation?",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Attempts to answer this question gave rise to our aforementioned hypothesis that, perhaps, the writing style of hyperpartisan left and right are not altogether different, despite their opposing agendas.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Or put another way, if style and topic are orthogonal concepts, then being an extremist should not exert a different style dependent on political orientation.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Excited, we sought ways to independently disprove the hypothesis, and found two: Experiments C and D.
C. Validation using leave-out classification.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"If leftwing and right-wing articles have a more similar style than either of them compared to mainstream articles, then what class would a binary classifier assign to a left-wing article, if it were trained to distinguish only the right-wing from the mainstream, and vice versa?",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Table 4 shows the results of this experiment.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"As indicated by proportions well above 0.50, full style-based classifiers have a tendency of clas-
sifying left as right and right as left.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The topic baseline, though, gets confused especially when omitting right articles from the training set with performance close to random.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The fact that the topic baseline works better when omitting left from the training set may be explainable: leading up to the elections, the hyperpartisan left was often merely reacting to topics prompted by the hyperpartisan right, instead of bringing up their own.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
D. Validation using Unmasking.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Based on Koppel et al.’s original approach in the context of authorship verification, for the first time, we generalize Unmasking to assess genre styles: just like author style similarity, genre style similarity will be characterized by the slope of a given Unmasking curve, where a steeper decrease indicates higher similarity.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"We apply Unmasking as described in Section 4.2 onto pairs of sets of left, right, and mainstream articles.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Figure 2 shows the resulting Unmasking curves (Unmasking is symmetrical, hence three curves).",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The curves are averaged over 5 runs, where each run comprised sets of 100 articles from each orientation.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"In case of the left-wing orientation, where less than 500 articles are available in our corpus, once all of them had been used, they were shuffled again to select articles for the remainder of the runs.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"As can be seen, the curve comparing left vs. right has a distinctly steeper slope than either of the others.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"This result hence matches the findings of the previous experiments.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"With caution, we conclude that the evidence gained from our three independent experimental setups supports our hypothesis that the hyperpartisan left and the hyperpartisan right have more in common in terms of writing style than any of the two have with the mainstream.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Another more tangible (e.g., practical) outcome of Experiment B is the finding that hyperpartisan news can apparently be
discriminated well from the mainstream: in particular the high recall of 0.89 at a reasonable precision of 0.69 gives us confidence that, with some further effort, a practical classifier can be built that detects hyperpartisan news at scale and in real time, since an article’s style can be assessed immediately without referring to external information.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
This series of experiments targets research questions (2) and (3).,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Again, we conduct three experiments, where the first is about predicting veracity, and the last two about discriminating satire.
",5.2 Fake vs. Real (vs. Satire),[0],[0]
A. Predicting veracity.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"When taking into account that the mainstream news publishers in our corpus did not publish any news items that are mostly false, and only very few instances that are mixtures of true and false, we may safely disregard them for the task of fake news detection.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"A reliable classifier for hyperpartisan news can act as a prefilter for a subsequent, more in-depth fake news detection approach, which may in turn be tailored to a much more narrowly defined classification task.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We hence use only the left-wing articles and the right-wing articles of our corpus for our attempt at a style-based fake news classifier.
Table 5 shows the performance values for a generic classifier that predicts fake news across orientations, and orientation-specific classifiers that have been individually trained on articles from either orientation.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Although all classifiers outperform the naive baselines of classifying everything into one of the classes in terms of precision, the slight increase comes at the cost of a large decrease in recall.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"While the orientation-specific classifiers are slightly better for most metrics, none of them outperform the naive baselines regarding the F - Measure.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We conclude that style-based fake news classification simply does not work in general.
B. Predicting satire.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Yet, not all fake news are the same.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"One should distinguish satire from the rest, which takes the form of news but lies more or less obviously to amuse its readers.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Regardless the problems that spreading fake news may cause, satire should never be filtered, but be discriminated from other fakes.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Table 6 shows the performance values of our classifier in the satire-detection setting used by Rubin et al. (2016) (the S-n-L News DB corpus), distinguishing satire from real news.",5.2 Fake vs. Real (vs. Satire),[0],[0]
This setting uses a balanced 3:1 training-to-test set split over 360 articles (180 per class).,5.2 Fake vs. Real (vs. Satire),[0],[0]
"As can be seen, our style-based model significantly outperforms all baselines across the board, achieving an accuracy of 0.82, and an F score of 0.81.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"It clearly improves over topic classification, but does not outperform Rubin et al.’s classifier, which includes features based on topic, absurdity, grammar, and punctuation.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We argue that incorporating topic into satire detection is not appropriate, since the topics of satire change along the topics of news.",5.2 Fake vs. Real (vs. Satire),[0],[0]
A classifier with topic features therefore does not generalize.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Apparently, a style-based model is competitive, and we believe that satire can be detected at scale this way, so as to prevent other fake news detection technology from falsely filtering it.
",5.2 Fake vs. Real (vs. Satire),[0],[0]
C. Unmasking satire.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Given the above results on stylistic similarities between left and right news, the question remains how satire fits into the picture.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We assess the style similarity of satire from Rubin et al.’s corpus compared to fake news and real news from ours, again applying Unmasking to compare pairs of the three categories of news as described above.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Figure 3 shows the resulting Un-
masking curves.",5.2 Fake vs. Real (vs. Satire),[0],[0]
The curve for the pair of fake vs. real news drops faster compared to the other two pairs.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Apparently, the style of fake news has more in common with that of real news than either of the two have with satire.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"These results are encouraging: satire is distinct enough from fake and real news, so that, just like with hyperpartisan news compared to mainstream news, it can be discriminated with reasonable accuracy.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Fact-checking for fake news detection poses an interdisciplinary challenge: technology is required to extract factual statements from text, to match facts with a knowledge base, to dynamically retrieve and maintain knowledge bases from the web, to reliably assess the overall veracity of an entire article rather than individual statements, to do so in real time as news events unfold, to monitor the spread of fake news within and across social media, to measure the reputation of information sources, and to raise awareness in readers.",6 Conclusion,[0],[0]
"These are only the most salient things that need be done to tackle the problem, and as our cross-section of related work shows, a large body of work must be covered.",6 Conclusion,[0],[0]
"Notwithstanding the many attacks on fake news by developing one way or another of fact-checking, we believe it worthwhile to mount our attack from another angle: writing style.
",6 Conclusion,[0],[0]
We show that news articles conveying a hyperpartisan world view can be distinguished from more balanced news by writing style alone.,6 Conclusion,[0],[0]
"Moreover, for the first time, we found quantifiable evidence that the writing styles of news of the two opposing orientations are in fact very similar: there appears to be a common writing style of left and right extremism.",6 Conclusion,[0],[0]
"We further show that satire can be distinguished well from other news, ensuring that humor will not be outcast by fake news detection technology.",6 Conclusion,[0],[0]
"All of these results offer new, tangible, short-term avenues of development, lest large-scale fact-checking is still far out of reach.",6 Conclusion,[0],[0]
"Employed as pre-filtering technologies to separate hyperpartisan news from mainstream news, our approach allows for directing the attention of human fact checkers to the most likely sources of fake news.",6 Conclusion,[0],[0]
"We thank Craig Silverman, Lauren Strapagiel, Hamza Shaban, Ellie Hall, and Jeremy Singer-Vine from BuzzFeed for making their data available, enabling our research.",Acknowledgements,[0],[0]
We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news.,abstractText,[0],[0]
"A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan.",abstractText,[0],[0]
"We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1=0.78), and satire from both (F1=0.81).",abstractText,[0],[0]
But stylometry is no silver bullet as style-based fake news detection does not work (F1=0.46).,abstractText,[0],[0]
We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream.,abstractText,[0],[0]
"This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way.",abstractText,[0],[0]
Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.,abstractText,[0],[0]
A Stylometric Inquiry into Hyperpartisan and Fake News,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 440–450 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1041",text,[0],[0]
"Every programmer has experienced the situation where they know what they want to do, but do not have the ability to turn it into a concrete implementation.",1 Introduction,[0],[0]
"For example, a Python programmer may want to “sort my list in descending order,” but not be able to come up with the proper syntax sorted(my list, reverse=True) to realize his intention.",1 Introduction,[0],[0]
"To resolve this impasse, it is common for programmers to search the web in natural language (NL), find an answer, and modify it into the desired form (Brandt et al., 2009, 2010).",1 Introduction,[0],[0]
"However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages (Balzer, 1985; Little and Miller, 2009; Gvero and Kuncak, 2015).
",1 Introduction,[0],[0]
"In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers.",1 Introduction,[0],[0]
"These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015; Misra et al., 2015), among others.",1 Introduction,[0],[0]
"While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple.
",1 Introduction,[0],[0]
"Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java.",1 Introduction,[0],[0]
"This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input descriptions.",1 Introduction,[0],[0]
"However, unlike most work in semantic parsing, it does not consider the fact that code has to be well-defined programs in the target syntax.
",1 Introduction,[0],[0]
"In this work, we propose a data-driven syntaxbased neural network model tailored for generation of general-purpose PLs like Python.",1 Introduction,[0],[0]
"In order to capture the strong underlying syntax of the PL, we define a model that transduces an NL statement into an Abstract Syntax Tree (AST; Fig. 1(a), § 2) for the target PL.",1 Introduction,[0],[0]
"ASTs can be deterministically generated for all well-formed programs using standard parsers provided by the PL, and thus give us a way to obtain syntax information with minimal engineering.",1 Introduction,[0],[0]
"Once we generate an AST, we can use deterministic generation tools to convert the AST into surface code.",1 Introduction,[0],[0]
"We hypothesize
440
that such a structured approach has two benefits.",1 Introduction,[0],[0]
"First, we hypothesize that structure can be used to constrain our search space, ensuring generation of well-formed code.",1 Introduction,[0],[0]
"To this end, we propose a syntax-driven neural code generation model.",1 Introduction,[0],[0]
"The backbone of our approach is a grammar model (§ 3) which formalizes the generation story of a derivation AST into sequential application of actions that either apply production rules (§ 3.1), or emit terminal tokens (§ 3.2).",1 Introduction,[0],[0]
The underlying syntax of the PL is therefore encoded in the grammar model a priori as the set of possible actions.,1 Introduction,[0],[0]
"Our approach frees the model from recovering the underlying grammar from limited training data, and instead enables the system to focus on learning the compositionality among existing grammar rules.",1 Introduction,[0],[0]
"Xiao et al. (2016) have noted that this imposition of structure on neural models is useful for semantic parsing, and we expect this to be even more important for general-purpose PLs where the syntax trees are larger and more complex.
",1 Introduction,[0],[0]
"Second, we hypothesize that structural information helps to model information flow within the neural network, which naturally reflects the recursive structure of PLs.",1 Introduction,[0],[0]
"To test this, we extend a standard recurrent neural network (RNN) decoder to allow for additional neural connections which reflect the recursive structure of an AST (§ 4.2).",1 Introduction,[0],[0]
"As an example, when expanding the node ?",1 Introduction,[0],[0]
"in Fig. 1(a), we make use of the information from both its parent and left sibling (the dashed rectangle).",1 Introduction,[0],[0]
"This enables us to locally pass information of relevant code segments via neural network connections, resulting in more confident predictions.
",1 Introduction,[0],[0]
"Experiments (§ 5) on two Python code generation tasks show 11.7% and 9.3% absolute improvements in accuracy against the state-of-the-art system (Ling et al., 2016).",1 Introduction,[0],[0]
"Our model also gives competitive performance on a standard semantic parsing benchmark1.
1Implementation available at https://github.",1 Introduction,[0],[0]
com/neulab/NL2code,1 Introduction,[0],[0]
"Given an NL description x, our task is to generate the code snippet c in a modern PL based on the intent of x. We attack this problem by first generating the underlying AST.",2 The Code Generation Problem,[0],[0]
We define a probabilistic grammar model of generating an AST y given x: p(y|x).,2 The Code Generation Problem,[0],[0]
"The best-possible AST ŷ is then given by
ŷ = arg max y
p(y|x).",2 The Code Generation Problem,[0],[0]
"(1)
ŷ is then deterministically converted to the corresponding surface code c.2",2 The Code Generation Problem,[0],[0]
"While this paper uses examples from Python code, our method is PLagnostic.
",2 The Code Generation Problem,[0],[0]
"Before detailing our approach, we first present a brief introduction of the Python AST and its underlying grammar.",2 The Code Generation Problem,[0],[0]
"The Python abstract grammar contains a set of production rules, and an AST is generated by applying several production rules composed of a head node and multiple child nodes.",2 The Code Generation Problem,[0],[0]
"For instance, the first rule in Tab. 1 is used to generate the function call sorted(·) in Fig. 1(a).",2 The Code Generation Problem,[0],[0]
"It consists of a head node of type Call, and three child nodes of type expr, expr* and keyword*, respectively.",2 The Code Generation Problem,[0],[0]
Labels of each node are noted within brackets.,2 The Code Generation Problem,[0],[0]
"In an AST, non-terminal nodes sketch the general structure of the target code, while terminal nodes can be categorized into two types: operation terminals and variable terminals.",2 The Code Generation Problem,[0],[0]
Operation terminals correspond to basic arithmetic operations like AddOp.,2 The Code Generation Problem,[0],[0]
Variable terminal nodes store values for variables and constants of built-in data types3.,2 The Code Generation Problem,[0],[0]
"For instance, all terminal nodes in Fig. 1(a) are variable terminal nodes.",2 The Code Generation Problem,[0],[0]
"Before detailing our neural code generation method, we first introduce the grammar model at its core.",3 Grammar Model,[0],[0]
Our probabilistic grammar model defines the generative story of a derivation AST.,3 Grammar Model,[0],[0]
"We fac-
2We use astor library to convert ASTs into Python code.",3 Grammar Model,[0],[0]
"3bool, float, int, str.
torize the generation process of an AST into sequential application of actions of two types:
• APPLYRULE[r] applies a production rule r to the current derivation tree;
• GENTOKEN[v] populates a variable terminal node by appending a terminal token v.
Fig. 1(b) shows the generation process of the target AST in Fig. 1(a).",3 Grammar Model,[0],[0]
Each node in Fig. 1(b) indicates an action.,3 Grammar Model,[0],[0]
Action nodes are connected by solid arrows which depict the chronological order of the action flow.,3 Grammar Model,[0],[0]
"The generation proceeds in depth-first, left-to-right order (dotted arrows represent parent feeding, explained in § 4.2.1).
",3 Grammar Model,[0],[0]
"Formally, under our grammar model, the probability of generating an AST y is factorized as:
p(y|x)",3 Grammar Model,[0],[0]
"= TY
t=1
p(at|x, a<t), (2)
where at is the action taken at time step t, and a<t is the sequence of actions before t. We will explain how to compute the action probabilities p(at|·) in Eq.",3 Grammar Model,[0],[0]
"(2) in § 4. Put simply, the generation process begins from a root node at t0, and proceeds by the model choosing APPLYRULE actions to generate the overall program structure from a closed set of grammar rules, then at leaves of the tree corresponding to variable terminals, the model switches to GENTOKEN actions to generate variables or constants from the open set.",3 Grammar Model,[0],[0]
We describe this process in detail below.,3 Grammar Model,[0],[0]
"APPLYRULE actions generate program structure, expanding the current node (the frontier node at
time step t: nft) in a depth-first, left-to-right traversal of the tree.",3.1 APPLYRULE Actions,[0],[0]
"Given a fixed set of production rules, APPLYRULE chooses a rule r from the subset that has a head matching the type of nft , and uses r to expand nft by appending all child nodes specified by the selected production.",3.1 APPLYRULE Actions,[0],[0]
"As an example, in Fig. 1(b), the rule Call 7! expr. . .",3.1 APPLYRULE Actions,[0],[0]
"expands the frontier node Call at time step t4, and its three child nodes expr, expr* and keyword* are added to the derivation.
",3.1 APPLYRULE Actions,[0],[0]
APPLYRULE actions grow the derivation AST by appending nodes.,3.1 APPLYRULE Actions,[0],[0]
"When a variable terminal node (e.g., str) is added to the derivation and becomes the frontier node, the grammar model then switches to GENTOKEN actions to populate the variable terminal with tokens.
",3.1 APPLYRULE Actions,[0],[0]
"Unary Closure Sometimes, generating an AST requires applying a chain of unary productions.",3.1 APPLYRULE Actions,[0],[0]
"For instance, it takes three time steps (t9 t11) to generate the sub-structure expr* 7! expr 7!",3.1 APPLYRULE Actions,[0],[0]
Name 7! str in Fig. 1(a).,3.1 APPLYRULE Actions,[0],[0]
This can be effectively reduced to one step of APPLYRULE action by taking the closure of the chain of unary productions and merging them into a single rule: expr* 7!⇤ str.,3.1 APPLYRULE Actions,[0],[0]
"Unary closures reduce the number of actions needed, but would potentially increase the size of the grammar.",3.1 APPLYRULE Actions,[0],[0]
In our experiments we tested our model both with and without unary closures (§ 5).,3.1 APPLYRULE Actions,[0],[0]
"Once we reach a frontier node nft that corresponds to a variable type (e.g., str), GENTOKEN actions are used to fill this node with values.",3.2 GENTOKEN Actions,[0],[0]
"For generalpurpose PLs like Python, variables and constants have values with one or multiple tokens.",3.2 GENTOKEN Actions,[0],[0]
"For in-
stance, a node that stores the name of a function (e.g., sorted) has a single token, while a node that denotes a string constant (e.g., a=‘hello world’) could have multiple tokens.",3.2 GENTOKEN Actions,[0],[0]
Our model copes with both scenarios by firing GENTOKEN actions at one or more time steps.,3.2 GENTOKEN Actions,[0],[0]
"At each time step, GENTOKEN appends one terminal token to the current frontier variable node.",3.2 GENTOKEN Actions,[0],[0]
A special </n>,3.2 GENTOKEN Actions,[0],[0]
token is used to “close” the node.,3.2 GENTOKEN Actions,[0],[0]
"The grammar model then proceeds to the new frontier node.
",3.2 GENTOKEN Actions,[0],[0]
"Terminal tokens can be generated from a predefined vocabulary, or be directly copied from the input NL.",3.2 GENTOKEN Actions,[0],[0]
This is motivated by the observation that the input description often contains out-ofvocabulary (OOV) variable names or literal values that are directly used in the target code.,3.2 GENTOKEN Actions,[0],[0]
"For instance, in our running example the variable name my list can be directly copied from the the input at t12.",3.2 GENTOKEN Actions,[0],[0]
We give implementation details in § 4.2.2.,3.2 GENTOKEN Actions,[0],[0]
We estimate action probabilities in Eq.,4 Estimating Action Probabilities,[0],[0]
(2) using attentional neural encoder-decoder models with an information flow structured by the syntax trees.,4 Estimating Action Probabilities,[0],[0]
"For an NL description x consisting of n words {wi}ni=1, the encoder computes a context sensitive embedding hi for each wi using a bidirectional Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), similar to the setting in (Bahdanau et al., 2014).",4.1 Encoder,[0],[0]
See supplementary materials for detailed equations.,4.1 Encoder,[0],[0]
The decoder uses an RNN to model the sequential generation process of an AST defined as Eq.,4.2 Decoder,[0],[0]
(2).,4.2 Decoder,[0],[0]
Each action step in the grammar model naturally grounds to a time step in the decoder RNN.,4.2 Decoder,[0],[0]
"Therefore, the action sequence in Fig. 1(b) can be interpreted as unrolling RNN time steps, with solid arrows indicating RNN connections.",4.2 Decoder,[0],[0]
"The RNN maintains an internal state to track the generation process (§ 4.2.1), which will then be used to compute action probabilities p(at|x, a<t) (§ 4.2.2).",4.2 Decoder,[0],[0]
"Our implementation of the decoder resembles a vanilla LSTM, with additional neural connections (parent feeding, Fig. 1(b)) to reflect the topological structure of an AST.",4.2.1 Tracking Generation States,[0],[0]
"The decoder’s internal hidden state at time step t, st, is given by:
st = fLSTM([at 1 : ct : pt :",4.2.1 Tracking Generation States,[0],[0]
"nft ], st 1), (3)
where fLSTM(·) is the LSTM update function.",4.2.1 Tracking Generation States,[0],[0]
[:] denotes vector concatenation.,4.2.1 Tracking Generation States,[0],[0]
"st will then be used to compute action probabilities p(at|x, a<t) in Eq.",4.2.1 Tracking Generation States,[0],[0]
(2).,4.2.1 Tracking Generation States,[0],[0]
"Here, at 1 is the embedding of the previous action.",4.2.1 Tracking Generation States,[0],[0]
ct is a context vector retrieved from input encodings {hi} via soft attention.,4.2.1 Tracking Generation States,[0],[0]
pt is a vector that encodes the information of the parent action.,4.2.1 Tracking Generation States,[0],[0]
"nft denotes the node type embedding of the current frontier node nft
4.",4.2.1 Tracking Generation States,[0],[0]
"Intuitively, feeding the decoder the information of nft helps the model to keep track of the frontier node to expand.",4.2.1 Tracking Generation States,[0],[0]
"Action Embedding at We maintain two action embedding matrices, WR and WG.",4.2.1 Tracking Generation States,[0],[0]
Each row in WR (WG) corresponds to an embedding vector for an action APPLYRULE[r] (GENTOKEN[v]).,4.2.1 Tracking Generation States,[0],[0]
Context Vector ct,4.2.1 Tracking Generation States,[0],[0]
The decoder RNN uses soft attention to retrieve a context vector ct from the input encodings {hi} pertain to the prediction of the current action.,4.2.1 Tracking Generation States,[0],[0]
We follow Bahdanau et al. (2014) and use a Deep Neural Network (DNN) with a single hidden layer to compute attention weights.,4.2.1 Tracking Generation States,[0],[0]
Parent Feeding pt,4.2.1 Tracking Generation States,[0],[0]
Our decoder RNN uses additional neural connections to directly pass information from parent actions.,4.2.1 Tracking Generation States,[0],[0]
"For instance, when computing s9, the information from its parent action step t4 will be used.",4.2.1 Tracking Generation States,[0],[0]
"Formally, we define the parent action step pt as the time step at which the frontier node nft is generated.",4.2.1 Tracking Generation States,[0],[0]
"As an example, for t9, its parent action step p9 is t4, since nf9 is the node ?, which is generated at t4 by the APPLYRULE[Call7!. .",4.2.1 Tracking Generation States,[0],[0]
.],4.2.1 Tracking Generation States,[0],[0]
"action.
",4.2.1 Tracking Generation States,[0],[0]
"We model parent information pt from two sources: (1) the hidden state of parent action spt , and (2) the embedding of parent action apt .",4.2.1 Tracking Generation States,[0],[0]
pt is the concatenation.,4.2.1 Tracking Generation States,[0],[0]
"The parent feeding schema en-
4We maintain an embedding for each node type.
",4.2.1 Tracking Generation States,[0],[0]
ables the model to utilize the information of parent code segments to make more confident predictions.,4.2.1 Tracking Generation States,[0],[0]
Similar approaches of injecting parent information were also explored in the SEQ2TREE model in Dong and Lapata (2016)5.,4.2.1 Tracking Generation States,[0],[0]
"In this section we explain how action probabilities p(at|x, a<t) are computed based on st.",4.2.2 Calculating Action Probabilities,[0],[0]
"APPLYRULE The probability of applying rule r as the current action at is given by a softmax6:
p(at = APPLYRULE[r]|x, a<t) = softmax(WR ·",4.2.2 Calculating Action Probabilities,[0],[0]
"g(st))| · e(r) (4) where g(·) is a non-linearity tanh(W ·st+b), and e(r)",4.2.2 Calculating Action Probabilities,[0],[0]
"the one-hot vector for rule r. GENTOKEN As in § 3.2, a token v can be generated from a predefined vocabulary or copied from the input, defined as the marginal probability:
p(at = GENTOKEN[v]|x, a<t) =",4.2.2 Calculating Action Probabilities,[0],[0]
"p(gen|x, a<t)p(v|gen, x, a<t)
+ p(copy|x, a<t)p(v|copy, x, a<t).",4.2.2 Calculating Action Probabilities,[0],[0]
The selection probabilities p(gen|·) and p(copy|·) are given by softmax(WS · st).,4.2.2 Calculating Action Probabilities,[0],[0]
"The probability of generating v from the vocabulary, p(v|gen, x, a<t), is defined similarly as Eq. (4), except that we use the GENTOKEN embedding matrix WG, and we concatenate the context vector ct with st as input.",4.2.2 Calculating Action Probabilities,[0],[0]
"To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al., 2015) to compute the probability of copying the i-th word from the input by attending to input representations {hi}:
p(wi|copy, x, a<t) =",4.2.2 Calculating Action Probabilities,[0],[0]
"exp(!(hi, st, ct))Pn
i0=1 exp(!(hi0 , st, ct)) ,
where !",4.2.2 Calculating Action Probabilities,[0],[0]
(·) is a DNN with a single hidden layer.,4.2.2 Calculating Action Probabilities,[0],[0]
"Specifically, if wi is an OOV word (e.g., the variable name my list), which is represented by a special <unk> token during encoding, we then directly copy the actual word wi from the input description to the derivation.",4.2.2 Calculating Action Probabilities,[0],[0]
"Given a dataset of pairs of NL descriptions xi and code snippets ci, we parse ci into its AST yi and
5SEQ2TREE generates tree-structured outputs by conditioning on the hidden states of parent non-terminals, while our parent feeding uses the states of parent actions.
",4.3 Training and Inference,[0],[0]
"6We do not show bias terms for all softmax equations.
",4.3 Training and Inference,[0],[0]
"decompose yi into a sequence of oracle actions, which explains the generation story of yi under the grammar model.",4.3 Training and Inference,[0],[0]
The model is then optimized by maximizing the log-likelihood of the oracle action sequence.,4.3 Training and Inference,[0],[0]
"At inference time, given an NL description, we use beam search to approximate the best AST ŷ in Eq.",4.3 Training and Inference,[0],[0]
(1).,4.3 Training and Inference,[0],[0]
See supplementary materials for the pseudo-code of the inference algorithm.,4.3 Training and Inference,[0],[0]
"HEARTHSTONE (HS) dataset (Ling et al., 2016) is a collection of Python classes that implement cards for the card game HearthStone.",5.1 Datasets and Metrics,[0],[0]
"Each card comes with a set of fields (e.g., name, cost, and description), which we concatenate to create the input sequence.",5.1 Datasets and Metrics,[0],[0]
"This dataset is relatively difficult: input descriptions are short, while the target code is in complex class structures, with each AST having 137 nodes on average.",5.1 Datasets and Metrics,[0],[0]
"DJANGO dataset (Oda et al., 2015) is a collection of lines of code from the Django web framework, each with a manually annotated NL description.",5.1 Datasets and Metrics,[0],[0]
"Compared with the HS dataset where card implementations are somewhat homogenous, examples in DJANGO are more diverse, spanning a wide variety of real-world use cases like string manipulation, IO operations, and exception handling.",5.1 Datasets and Metrics,[0],[0]
"IFTTT dataset (Quirk et al., 2015) is a domainspecific benchmark that provides an interesting side comparison.",5.1 Datasets and Metrics,[0],[0]
"Different from HS and DJANGO which are in a general-purpose PL, programs in IFTTT are written in a domain-specific language used by the IFTTT task automation
App.",5.1 Datasets and Metrics,[0],[0]
"Users of the App write simple instructions (e.g., If Instagram.",5.1 Datasets and Metrics,[0],[0]
"AnyNewPhotoByYou Then Dropbox.AddFileFromURL) with NL descriptions (e.g., “Autosave your Instagram photos to Dropbox”).",5.1 Datasets and Metrics,[0],[0]
"Each statement inside the If or Then clause consists of a channel (e.g., Dropbox) and a function (e.g., AddFileFromURL)7.",5.1 Datasets and Metrics,[0],[0]
This simple structure results in much more concise ASTs (7 nodes on average).,5.1 Datasets and Metrics,[0],[0]
"Because all examples are created by ordinary Apps users, the dataset is highly noisy, with input NL very loosely connected to target ASTs.",5.1 Datasets and Metrics,[0],[0]
"The authors thus provide a high-quality filtered test set, where each example is verified by at least three annotators.",5.1 Datasets and Metrics,[0],[0]
We use this set for evaluation.,5.1 Datasets and Metrics,[0],[0]
"Also note IFTTT’s grammar has more productions (Tab. 2), but this does not imply that its grammar is more complex.",5.1 Datasets and Metrics,[0],[0]
"This is because for HS and DJANGO terminal tokens are generated by GENTOKEN actions, but for IFTTT, all the code is generated directly by APPLYRULE actions.",5.1 Datasets and Metrics,[0],[0]
"Metrics As is standard in semantic parsing, we measure accuracy, the fraction of correctly generated examples.",5.1 Datasets and Metrics,[0],[0]
"However, because generating an exact match for complex code structures is nontrivial, we follow Ling et al. (2016), and use tokenlevel BLEU-4 with as a secondary metric, defined as the averaged BLEU scores over all examples.8",5.1 Datasets and Metrics,[0],[0]
Preprocessing All input descriptions are tokenized using NLTK.,5.2 Setup,[0],[0]
"We perform simple canonicalization for DJANGO, such as replacing quoted strings in the inputs with place holders.",5.2 Setup,[0],[0]
See supplementary materials for details.,5.2 Setup,[0],[0]
We extract unary closures whose frequency is larger than a threshold k,5.2 Setup,[0],[0]
(k = 30 for HS and 50 for DJANGO).,5.2 Setup,[0],[0]
"Configuration The size of all embeddings is 128, except for node type embeddings, which is 64.",5.2 Setup,[0],[0]
"The dimensions of RNN states and hidden layers are 256 and 50, respectively.",5.2 Setup,[0],[0]
"Since our datasets are relatively small for a data-hungry neural model, we impose strong regularization using recurrent
7Like Beltagy and Quirk (2016), we strip function parameters since they are mostly specific to users.
",5.2 Setup,[0],[0]
"8These two metrics are not ideal: accuracy only measures exact match and thus lacks the ability to give credit to semantically correct code that is different from the reference, while it is not clear whether BLEU provides an appropriate proxy for measuring semantics in the code generation task.",5.2 Setup,[0],[0]
"A more intriguing metric would be directly measuring semantic/functional code equivalence, for which we present a pilot study at the end of this section (cf.",5.2 Setup,[0],[0]
Error Analysis).,5.2 Setup,[0],[0]
"We leave exploring more sophisticated metrics (e.g. based on static code analysis) as future work.
dropouts (Gal and Ghahramani, 2016) for all recurrent networks, together with standard dropout layers added to the inputs and outputs of the decoder RNN.",5.2 Setup,[0],[0]
"We validate the dropout probability from {0, 0.2, 0.3, 0.4}.",5.2 Setup,[0],[0]
"For decoding, we use a beam size of 15.",5.2 Setup,[0],[0]
Evaluation results for Python code generation tasks are listed in Tab. 3.,5.3 Results,[0],[0]
Numbers for our systems are averaged over three runs.,5.3 Results,[0],[0]
"We compare primarily with two approaches: (1) Latent Predictor Network (LPN), a state-of-the-art sequenceto-sequence code generation model (Ling et al., 2016), and (2) SEQ2TREE, a neural semantic parsing model (Dong and Lapata, 2016).",5.3 Results,[0],[0]
"SEQ2TREE generates trees one node at a time, and the target grammar is not explicitly modeled a priori, but implicitly learned from data.",5.3 Results,[0],[0]
"We test both the original SEQ2TREE model released by the authors and our revised one (SEQ2TREE–UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015).",5.3 Results,[0],[0]
"For completeness, we also compare with a strong neural machine translation (NMT) system (Neubig, 2015) using a standard encoder-decoder architecture with attention and unknown word replacement9, and include numbers from other baselines used in Ling et al. (2016).",5.3 Results,[0],[0]
"On the HS dataset, which has relatively large ASTs, we use unary closure for our model and SEQ2TREE, and for DJANGO we do not.
",5.3 Results,[0],[0]
"9For NMT, we also attempted to find the best-scoring syntactically correct predictions in the size-5 beam, but this did not yield a significant improvement over the NMT results in Tab. 3.
",5.3 Results,[0],[0]
"System Comparison As in Tab. 3, our model registers 11.7% and 9.3% absolute improvements over LPN in accuracy on HS and DJANGO.",5.3 Results,[0],[0]
This boost in performance strongly indicates the importance of modeling grammar in code generation.,5.3 Results,[0],[0]
"For the baselines, we find LPN outperforms NMT and SEQ2TREE in most cases.",5.3 Results,[0],[0]
"We also note that SEQ2TREE achieves a decent accuracy of 13.6% on HS, which is due to the effect of unknown word replacement, since we only achieved 1.5% without it.",5.3 Results,[0],[0]
"A closer comparison with SEQ2TREE is insightful for understanding the advantage of our syntax-driven approach, since both SEQ2TREE and our system output ASTs: (1) SEQ2TREE predicts one node each time step, and requires additional “dummy” nodes to mark the boundary of a subtree.",5.3 Results,[0],[0]
The sheer number of nodes in target ASTs makes the prediction process error-prone.,5.3 Results,[0],[0]
"In contrast, the APPLYRULE actions of our grammar model allows for generating multiple nodes at a single time step.",5.3 Results,[0],[0]
"Empirically, we found that in HS, SEQ2TREE takes more than 300 time steps on average to generate a target AST, while our model takes only 170 steps.",5.3 Results,[0],[0]
"(2) SEQ2TREE does not directly use productions in the grammar, which possibly leads to grammatically incorrect ASTs and thus empty code outputs.",5.3 Results,[0],[0]
"We observe that the ratio of grammatically incorrect ASTs predicted by SEQ2TREE on HS and DJANGO are 21.2% and 10.9%, respectively, while our system guarantees grammaticality.
",5.3 Results,[0],[0]
Ablation Study We also ablated our bestperforming models to analyze the contribution of each component.,5.3 Results,[0],[0]
“–frontier embed.” removes the frontier node embedding nft from the decoder RNN inputs (Eq.,5.3 Results,[0],[0]
(3)).,5.3 Results,[0],[0]
"This yields worse results on DJANGO while gives slight improvements in ac-
curacy on HS.",5.3 Results,[0],[0]
"This is probably because that the grammar of HS has fewer node types, and thus the RNN is able to keep track of nft without depending on its embedding.",5.3 Results,[0],[0]
"Next, “–parent feed.” removes the parent feeding mechanism.",5.3 Results,[0],[0]
"The accuracy drops significantly on HS, with a marginal deterioration on DJANGO.",5.3 Results,[0],[0]
"This result is interesting because it suggests that parent feeding is more important when the ASTs are larger, which will be the case when handling more complicated code generation tasks like HS.",5.3 Results,[0],[0]
"Finally, removing the pointer network (“–copy terminals”) in GENTOKEN actions gives poor results, indicating that it is important to directly copy variable names and values from the input.
",5.3 Results,[0],[0]
"The results with and without unary closure demonstrate that, interestingly, it is effective on HS but not on DJANGO.",5.3 Results,[0],[0]
"We conjecture that this is because on HS it significantly reduces the number of actions from 173 to 142 (c.f., Tab. 2), with the number of productions in the grammar remaining unchanged.",5.3 Results,[0],[0]
"In contrast, DJANGO has a broader domain, and thus unary closure results in more productions in the grammar (237 for DJANGO vs. 100 for HS), increasing sparsity.",5.3 Results,[0],[0]
Performance by the size of AST We further investigate our model’s performance w.r.t.,5.3 Results,[0],[0]
the size of the gold-standard ASTs in Figs. 3 and 4.,5.3 Results,[0],[0]
"Not surprisingly, the performance drops when the size of the reference ASTs increases.",5.3 Results,[0],[0]
"Additionally, on the HS dataset, the BLEU score still remains at around 50 even when the size of ASTs grows to 200, indicating that our proposed syntax-driven approach is robust for long code segments.",5.3 Results,[0],[0]
Domain Specific Code Generation,5.3 Results,[0],[0]
"Although this is not the focus of our work, evaluation on IFTTT brings us closer to a standard semantic parsing set-
ting, which helps to investigate similarities and differences between generation of more complicated general-purpose code and and more limiteddomain simpler code.",5.3 Results,[0],[0]
"Tab. 4 shows the results, following the evaluation protocol in (Beltagy and Quirk, 2016) for accuracies at both channel and full parse tree (channel + function) levels.",5.3 Results,[0],[0]
"Our full model performs on par with existing neural network-based methods, while outperforming other neural models in full tree accuracy (82.0%).",5.3 Results,[0],[0]
"This score is close to the best classical method (LR), which is based on a logistic regression model with rich hand-engineered features (e.g., brown clusters and paraphrase).",5.3 Results,[0],[0]
Also note that the performance between NMT and other neural models is much closer compared with the results in Tab. 3.,5.3 Results,[0],[0]
"This suggests that general-purpose code generation is more challenging than the simpler IFTTT setting, and therefore modeling structural information is more helpful.",5.3 Results,[0],[0]
Case Studies,5.3 Results,[0],[0]
We present output examples in Tab. 5.,5.3 Results,[0],[0]
"On HS, we observe that most of the time our model gives correct predictions by filling learned code templates from training data with arguments (e.g., cost) copied from input.",5.3 Results,[0],[0]
This is in line with the findings in Ling et al. (2016).,5.3 Results,[0],[0]
"However, we do find interesting examples indicating that the model learns to generalize beyond trivial
copying.",5.3 Results,[0],[0]
"For instance, the first example is one that our model predicted wrong — it generated code block A instead of the gold B (it also missed a function definition not shown here).",5.3 Results,[0],[0]
"However, we find that the block A actually conveys part of the input intent by destroying all, not some, of the minions.",5.3 Results,[0],[0]
"Since we are unable to find code block A in the training data, it is clear that the model has learned to generalize to some extent from multiple training card examples with similar semantics or structure.
",5.3 Results,[0],[0]
The next two examples are from DJANGO.,5.3 Results,[0],[0]
"The first one shows that the model learns the usage of common API calls (e.g., os.path.join), and how to populate the arguments by copying from inputs.",5.3 Results,[0],[0]
"The second example illustrates the difficulty of generating code with complex nested structures like lambda functions, a scenario worth further investigation in future studies.",5.3 Results,[0],[0]
More examples are attached in supplementary materials.,5.3 Results,[0],[0]
"Error Analysis To understand the sources of errors and how good our evaluation metric (exact match) is, we randomly sampled and labeled 100 and 50 failed examples (with accuracy=0) from DJANGO and HS, respectively.",5.3 Results,[0],[0]
We found that around 2% of these examples in the two datasets are actually semantically equivalent.,5.3 Results,[0],[0]
These examples include: (1) using different parameter names when defining a function; (2) omitting (or adding) default values of parameters in function calls.,5.3 Results,[0],[0]
"While the rarity of such examples suggests that our exact match metric is reasonable, more advanced evaluation metrics based on statistical code analysis are definitely intriguing future work.
",5.3 Results,[0],[0]
"For DJANGO, we found that 30% of failed cases were due to errors where the pointer network failed to appropriately copy a variable name into the correct position.",5.3 Results,[0],[0]
25% were because the generated code only partially implemented the required functionality.,5.3 Results,[0],[0]
"10% and 5% of errors were due to malformed English inputs and pre-processing errors, respectively.",5.3 Results,[0],[0]
"The remaining 30% of examples were errors stemming from multiple sources, or errors that could not be easily categorized into the above.",5.3 Results,[0],[0]
"For HS, we found that all failed card examples were due to partial implementation errors, such as the one shown in Table 5.",5.3 Results,[0],[0]
"Code Generation and Analysis Most works on code generation focus on generating code for domain specific languages (DSLs) (Kushman and
Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Liu et al., 2016; Parisotto et al., 2016; Balog et al., 2016).",6 Related Work,[0],[0]
"For general-purpose code generation, besides the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al., 2016).",6 Related Work,[0],[0]
"A similar line is to use NL queries for code retrieval (Wei et al., 2015; Allamanis et al., 2015).",6 Related Work,[0],[0]
"The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).",6 Related Work,[0],[0]
"Finally, our work falls into the broad field of probabilistic modeling of source code (Maddison and Tarlow, 2014; Nguyen et al., 2013).",6 Related Work,[0],[0]
"Our approach of factoring an AST using probabilistic models is closely related to Allamanis et al. (2015), which uses a factorized model to measure the semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task.
",6 Related Work,[0],[0]
"Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms.",6 Related Work,[0],[0]
The target logical forms can be viewed as DSLs.,6 Related Work,[0],[0]
"The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016).",6 Related Work,[0],[0]
"Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016).",6 Related Work,[0],[0]
"Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kociský et al., 2016; Jia and Liang, 2016).",6 Related Work,[0],[0]
Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering.,6 Related Work,[0],[0]
"Finally, the structured prediction approach proposed by Xiao et al. (2016) is closely related to our model in using the underlying grammar as prior knowledge to constrain the generation process of derivation trees, while our method is based on a unified grammar model which jointly captures production rule application and terminal symbol generation, and scales to general purpose code generation tasks.",6 Related Work,[0],[0]
This paper proposes a syntax-driven neural code generation approach that generates an abstract syntax tree by sequentially applying actions from a grammar model.,7 Conclusion,[0],[0]
Experiments on both code generation and semantic parsing tasks demonstrate the effectiveness of our proposed approach.,7 Conclusion,[0],[0]
We are grateful to Wang Ling for his generous help with LPN and setting up the benchmark.,Acknowledgment,[0],[0]
We thank I. Beltagy for providing the IFTTT dataset.,Acknowledgment,[0],[0]
We also thank Li Dong for helping with SEQ2TREE and insightful discussions.,Acknowledgment,[0],[0]
We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python.,abstractText,[0],[0]
Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language.,abstractText,[0],[0]
"Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge.",abstractText,[0],[0]
"Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",abstractText,[0],[0]
A Syntactic Neural Model for General-Purpose Code Generation,title,[0],[0]
"Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right.",1 Introduction,[0],[0]
"These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007).",1 Introduction,[0],[0]
"However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy.
",1 Introduction,[0],[0]
"The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree.",1 Introduction,[0],[0]
A discriminative model is then trained to simulate the oracle’s behavior.,1 Introduction,[0],[0]
A parsing oracle is deterministic if it returns a single canonical transition.,1 Introduction,[0],[0]
"Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake.",1 Introduction,[0],[0]
Oracles that are both deterministic and partial are called static.,1 Introduction,[0],[0]
"Traditionally, only static oracles have been exploited in training of transition-based parsers.
",1 Introduction,[0],[0]
"Recently, Goldberg and Nivre (2012; 2013) showed that the accuracy of greedy parsers can be substantially improved without affecting their parsing speed.",1 Introduction,[0],[0]
"This improvement relies on the introduction of novel oracles that are nondeterministic
and complete.",1 Introduction,[0],[0]
"An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser.",1 Introduction,[0],[0]
"Oracles that are both nondeterministic and complete are called dynamic.
Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers.",1 Introduction,[0],[0]
The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition.,1 Introduction,[0],[0]
"They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system.",1 Introduction,[0],[0]
"In this article, we develop one such oracle (§4) and prove its correctness (§5).
",1 Introduction,[0],[0]
"An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and allows mixing of bottom-up and top-down strategies.",1 Introduction,[0],[0]
"This parser, called here the LR-spine parser, achieves state-ofthe-art results for greedy parsing.",1 Introduction,[0],[0]
"Like the arc-standard system, the LR-spine parser is not arc-decomposable, and a dynamic oracle for this system was not known.",1 Introduction,[0],[0]
"We extend our oracle for the arc-standard system to work for the LR-spine system as well (§6).
",1 Introduction,[0],[0]
The dynamic oracles developed by Goldberg and Nivre (2013) for arc-decomposable systems are based on local properties of computations.,1 Introduction,[0],[0]
"In contrast, our novel dynamic oracle algorithms rely on arguably more complex structural properties of computations, which are computed through dynamic programming.",1 Introduction,[0],[0]
"This leaves open the question of whether a machine-learning model can learn to effectively simulate such complex processes: will the
119
Transactions of the Association for Computational Linguistics, 2 (2014) 119–130.",1 Introduction,[0],[0]
Action Editor: Ryan McDonald.,1 Introduction,[0],[0]
Submitted 11/2013; Revised 2/2014; Published 4/2014.,1 Introduction,[0],[0]
"c©2014 Association for Computational Linguistics.
benefit of training with the dynamic oracle carry over to the arc-standard and LR-spine systems?",1 Introduction,[0],[0]
"We show experimentally that this is indeed the case (§8), and that using the training-with-exploration method of (Goldberg and Nivre, 2013) with our dynamic programming based oracles yields superior parsing accuracies on many languages.",1 Introduction,[0],[0]
"In this section we introduce the arc-standard parser of Nivre (2004), which is the model that we use in this article.",2 Arc-Standard Parser,[0],[0]
"To keep the notation at a simple level, we only discuss the unlabeled version of the parser; however, a labeled extension is used in §8 for our experiments.",2 Arc-Standard Parser,[0],[0]
The set of non-negative integers is denoted as N0.,2.1 Preliminaries and Notation,[0],[0]
"For i, j ∈ N0 with i ≤ j, we write [i, j] to denote the set {i, i + 1, . . .",2.1 Preliminaries and Notation,[0],[0]
", j}.",2.1 Preliminaries and Notation,[0],[0]
"When i > j, [i, j] denotes the empty set.
",2.1 Preliminaries and Notation,[0],[0]
"We represent an input sentence as a string w = w0 · · ·wn, n ∈ N0, where token w0 is a special root symbol, and each wi with i ∈",2.1 Preliminaries and Notation,[0],[0]
"[1, n] is a lexical token.",2.1 Preliminaries and Notation,[0],[0]
"For i, j ∈",2.1 Preliminaries and Notation,[0],[0]
"[0, n] with i ≤ j, we write w[i, j] to denote the substring wiwi+1 · · ·wj of w.
We write i → j to denote a grammatical dependency of some unspecified type between lexical tokens wi and wj , where wi is the head and wj is the dependent.",2.1 Preliminaries and Notation,[0],[0]
"A dependency tree for w is a directed, ordered tree t =",2.1 Preliminaries and Notation,[0],[0]
"(Vw, A), such that Vw = [0, n] is the set of nodes, A ⊆ Vw×Vw is the set of arcs, and node 0 is the root.",2.1 Preliminaries and Notation,[0],[0]
"Arc (i, j) encodes a dependency i → j, and we will often use the latter notation to denote arcs.",2.1 Preliminaries and Notation,[0],[0]
We assume the reader is familiar with the formal framework of transition-based dependency parsing originally introduced by Nivre (2003); see Nivre (2008) for an introduction.,2.2 Transition-Based Dependency Parsing,[0],[0]
"We only summarize here our notation.
",2.2 Transition-Based Dependency Parsing,[0],[0]
"Transition-based dependency parsers use a stack data structure, where each stack element is associated with a tree spanning (generating) some substring of the input w.",2.2 Transition-Based Dependency Parsing,[0],[0]
"The parser processes the input string incrementally, from left to right, applying at each step a transition that updates the stack and/or
consumes one token from the input.",2.2 Transition-Based Dependency Parsing,[0],[0]
"Transitions may also construct new dependencies, which are added to the current configuration of the parser.
",2.2 Transition-Based Dependency Parsing,[0],[0]
We represent the stack data structure as an ordered sequence σ =,2.2 Transition-Based Dependency Parsing,[0],[0]
"[σd, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", σ1], d ∈ N0, of nodes",2.2 Transition-Based Dependency Parsing,[0],[0]
"σi ∈ Vw, with the topmost element placed at the right.",2.2 Transition-Based Dependency Parsing,[0],[0]
"When d = 0, we have the empty stack σ =",2.2 Transition-Based Dependency Parsing,[0],[0]
[].,2.2 Transition-Based Dependency Parsing,[0],[0]
"Sometimes we use the vertical bar to denote the append operator for σ, and write σ = σ′|σ1 to indicate that σ1 is the topmost element of σ.
",2.2 Transition-Based Dependency Parsing,[0],[0]
The parser also uses a buffer to store the portion of the input string still to be processed.,2.2 Transition-Based Dependency Parsing,[0],[0]
We represent the buffer as an ordered sequence β =,2.2 Transition-Based Dependency Parsing,[0],[0]
"[i, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", n] of nodes from Vw, with i the first element of the buffer.",2.2 Transition-Based Dependency Parsing,[0],[0]
In this way β always encodes a (non-necessarily proper) suffix of w. We denote the empty buffer as β =,2.2 Transition-Based Dependency Parsing,[0],[0]
[].,2.2 Transition-Based Dependency Parsing,[0],[0]
"Sometimes we use the vertical bar to denote the append operator for β, and write β = i|β′ to indicate that i is the first token of β; consequently, we have β′ =",2.2 Transition-Based Dependency Parsing,[0],[0]
"[i+ 1, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", n",2.2 Transition-Based Dependency Parsing,[0],[0]
"].
When processing w, the parser reaches several states, technically called configurations.",2.2 Transition-Based Dependency Parsing,[0],[0]
"A configuration of the parser relative to w is a triple c = (σ, β,A), where σ and β are a stack and a buffer, respectively, and A ⊆ Vw × Vw is a set of arcs.",2.2 Transition-Based Dependency Parsing,[0],[0]
"The initial configuration for w is ([], [0, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", n], ∅).",2.2 Transition-Based Dependency Parsing,[0],[0]
"For the purpose of this article, a configuration is final if it has the form ([0], [], A), and in a final configuration arc set A always defines a dependency tree for w.
The core of a transition-based parser is the set of its transitions, which are specific to each family of parsers.",2.2 Transition-Based Dependency Parsing,[0],[0]
A transition is a binary relation defined over the set of configurations of the parser.,2.2 Transition-Based Dependency Parsing,[0],[0]
"We use symbol ` to denote the union of all transition relations of a parser.
",2.2 Transition-Based Dependency Parsing,[0],[0]
"A computation of the parser on w is a sequence c0, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", cm, m ∈ N0, of configurations (defined relative to w) such that ci−1 ` ci for each",2.2 Transition-Based Dependency Parsing,[0],[0]
i ∈,2.2 Transition-Based Dependency Parsing,[0],[0]
"[1,m].",2.2 Transition-Based Dependency Parsing,[0],[0]
We also use the reflexive and transitive closure relation `∗ to represent computations.,2.2 Transition-Based Dependency Parsing,[0],[0]
A computation is called complete whenever c0 is initial and cm is final.,2.2 Transition-Based Dependency Parsing,[0],[0]
"In this way, a complete computation is uniquely associated with a dependency tree for w.",2.2 Transition-Based Dependency Parsing,[0],[0]
"The arc-standard model uses the three types of transitions formally specified in Figure 1
(σ, i|β,A) `sh (σ|i, β, A) (σ|i|j, β,A) `la (σ|j, β,A ∪ {j → i}) (σ|i|j, β,A) `ra (σ|i, β, A ∪ {i→ j})
",2.3 Arc-Standard Parser,[0],[0]
Notation We sometimes use the functional notation for a transition τ ∈,2.3 Arc-Standard Parser,[0],[0]
"{sh, la, ra}, and write τ(c) = c′ in place of c",2.3 Arc-Standard Parser,[0],[0]
`τ c′.,2.3 Arc-Standard Parser,[0],[0]
"Naturally, sh applies only when the buffer is not empty, and la,ra require two elements on the stack.",2.3 Arc-Standard Parser,[0],[0]
We denote by valid(c) the set of valid transitions in a given configuration.,2.3 Arc-Standard Parser,[0],[0]
"Goldberg and Nivre (2013) show how to derive dynamic oracles for any transition-based parser which has the arc decomposition property, defined below.",2.4 Arc Decomposition,[0],[0]
"They also show that the arc-standard parser is not arc-decomposable.
",2.4 Arc Decomposition,[0],[0]
"For a configuration c, we write Ac to denote the associated set of arcs.",2.4 Arc Decomposition,[0],[0]
"A transition-based parser is arc-decomposable if, for every configuration c and for every set of arcs A that can be extended to a projective tree, we have
∀(i→ j) ∈ A,∃c′[c",2.4 Arc Decomposition,[0],[0]
"`∗ c′ ∧ (i→ j) ∈ Ac′ ] ⇒ ∃c′′[c `∗ c′′ ∧A ⊆ Ac′′ ] .
",2.4 Arc Decomposition,[0],[0]
"In words, if each arc in A is individually derivable from c, then the set A in its entirety can be derived from c as well.",2.4 Arc Decomposition,[0],[0]
"The arc decomposition property is useful for deriving dynamic oracles because it is relatively easy to investigate derivability for single arcs and then, using this property, draw conclusions about the number of gold-arcs that are simultaneously derivable from the given configuration.
",2.4 Arc Decomposition,[0],[0]
"Unfortunately, the arc-standard parser is not arcdecomposable.",2.4 Arc Decomposition,[0],[0]
"To see why, consider a configuration with stack σ",2.4 Arc Decomposition,[0],[0]
=,2.4 Arc Decomposition,[0],[0]
"[i, j, k].",2.4 Arc Decomposition,[0],[0]
"Consider also arc set A = {(i, j), (i, k)}.",2.4 Arc Decomposition,[0],[0]
"The arc (i, j) can be derived through the transition sequence ra, ra, and the arc (i, k) can be derived through the alternative transition sequence la, ra.",2.4 Arc Decomposition,[0],[0]
"Yet, it is easy to see that a configuration containing both arcs cannot be reached.
",2.4 Arc Decomposition,[0],[0]
"As we cannot rely on the arc decomposition property, in order to derive a dynamic oracle for the arcstandard model we need to develop more sophisticated techniques which take into account the interaction among the applied transitions.",2.4 Arc Decomposition,[0],[0]
We aim to derive a dynamic oracle for the arc-standard (and related) system.,3 Configuration Loss and Dynamic Oracles,[0],[0]
"This is a function that takes a configuration c and a gold tree tG and returns a set of transitions that are “optimal” for c with respect to tG. As already mentioned in the introduction, a dynamic oracle can be used to improve training of greedy transition-based parsers.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"In this section we provide a formal definition for a dynamic oracle.
",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Let t1 and t2 be two dependency trees over the same stringw, with arc setsA1 andA2, respectively.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"We define the loss of t1 with respect to t2 as
L(t1, t2) = |A1 \A2| .",3 Configuration Loss and Dynamic Oracles,[0],[0]
"(1)
Note that L(t1, t2) = L(t2, t1), since |A1| = |A2|.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Furthermore L(t1, t2) = 0",3 Configuration Loss and Dynamic Oracles,[0],[0]
"if and only if t1 and t2 are the same tree.
",3 Configuration Loss and Dynamic Oracles,[0],[0]
Let c be a configuration of our parser relative to input string w. We write D(c) to denote the set of all dependency trees that can be obtained in a computation of the form c,3 Configuration Loss and Dynamic Oracles,[0],[0]
"`∗ cf , where cf is some final configuration.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"We extend the loss function in (1) to configurations by letting
L(c, t2) = min t1∈D(c) L(t1, t2) .",3 Configuration Loss and Dynamic Oracles,[0],[0]
"(2)
Assume some reference (desired) dependency tree tG for w, which we call the gold tree.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Quantity L(c, tG) can be used to compute a dynamic oracle relating a parser configuration c to a set of optimal actions by setting
oracle(c, tG)",3 Configuration Loss and Dynamic Oracles,[0],[0]
"=
{τ | L(τ(c), tG)− L(c, tG) = 0} .",3 Configuration Loss and Dynamic Oracles,[0],[0]
"(3)
We therefore need to develop an algorithm for computing (2).",3 Configuration Loss and Dynamic Oracles,[0],[0]
"We will do this first for the arc-standard parser, and then for an extension of this model.
",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Notation We also apply the loss function L(t, tG) in (1) when t is a dependency tree for a substring of w.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"In this case the nodes of t are a subset of the nodes of tG, and L(t, tG) provides a count of the nodes of t that are assigned a wrong head node, when tG is considered as the reference tree.",3 Configuration Loss and Dynamic Oracles,[0],[0]
Throughout this section we assume an arc-standard parser.,4 Main Algorithm,[0],[0]
Our algorithm takes as input a projective gold tree tG and a configuration c =,4 Main Algorithm,[0],[0]
"(σL, β, A).",4 Main Algorithm,[0],[0]
"We call σL the left stack, in contrast with a right stack whose construction is specified below.",4 Main Algorithm,[0],[0]
The algorithm consists of two steps.,4.1 Basic Idea,[0],[0]
"Informally, in the first step we compute the largest subtrees, called here tree fragments, of the gold tree tG that have their span entirely included in the buffer β.",4.1 Basic Idea,[0],[0]
"The root nodes of these tree fragments are then arranged into a stack data structure, according to the order in which they appear in β and with the leftmost root in β being the topmost element of the stack.",4.1 Basic Idea,[0],[0]
"We call this structure the right stack σR. Intuitively, σR can be viewed as the result of pre-computing β by applying all sequences of transitions that match tG and that can be performed independently of the stack in the input configuration c, that is, σL.
",4.1 Basic Idea,[0],[0]
"In the second step of the algorithm we use dynamic programming techniques to simulate all computations of the arc-standard parser starting in a configuration with stack σL and with a buffer consisting of σR, with the topmost token of σR being the first token of the buffer.",4.1 Basic Idea,[0],[0]
"As we will see later, the search space defined by these computations includes the dependency trees for w that are reachable from the input configuration c and that have minimum loss.",4.1 Basic Idea,[0],[0]
"We then perform a Viterbi search to pick up such value.
",4.1 Basic Idea,[0],[0]
"The second step is very similar to standard implementations of the CKY parser for context-free grammars (Hopcroft and Ullman, 1979), running on an input string obtained as the concatenation of σL and σR. The main difference is that we restrict ourselves to parse only those constituents in σLσR that dominate the topmost element of σL (the rightmost ele-
ment, if σL is viewed as a string).",4.1 Basic Idea,[0],[0]
"In this way, we account for the additional constraint that we visit only those configurations of the arc-standard parser that can be reached from the input configuration c. For instance, this excludes the reduction of two nodes in σL that are not at the two topmost positions.",4.1 Basic Idea,[0],[0]
"This would also exclude the reduction of two nodes in σR: this is correct, since the associated tree fragments have been chosen as the largest such fragments in β.
",4.1 Basic Idea,[0],[0]
"The above intuitive explanation will be made mathematically precise in §5, where the notion of linear dependency tree is introduced.",4.1 Basic Idea,[0],[0]
"In the first step we process β and construct a stack σR, which we call the right stack associated with c and tG.",4.2 Construction of the Right Stack,[0],[0]
"Each node of σR is the root of a tree t which satisfies the following properties
• t is a tree fragment of the gold tree tG",4.2 Construction of the Right Stack,[0],[0]
"having span entirely included in the buffer β;
• t is bottom-up complete for tG, meaning that for each node i of t different from t’s root, the dependents of i in tG cannot be in σL;
• t is maximal for tG, meaning that every supertree of t in tG violates the above conditions.
",4.2 Construction of the Right Stack,[0],[0]
The stack σR is incrementally constructed by processig β from left to right.,4.2 Construction of the Right Stack,[0],[0]
"Each node i is copied into σR if it satisfies any of the following conditions
• the parent node of i in tG is not in β;
• some dependent of i in tG is in σL or has already been inserted in σR.
It is not difficult to see that the nodes in σR are the roots of tree fragments of tG that satisfy the condition of bottom-up completeness and the condition of maximality defined above.",4.2 Construction of the Right Stack,[0],[0]
We start with some notation.,4.3 Computation of Configuration Loss,[0],[0]
Let `L = |σL| and `R = |σR|.,4.3 Computation of Configuration Loss,[0],[0]
We write σL[i] to denote the i-th element of σL and t(σL[i]) to denote the corresponding tree fragment; σR[i] and t(σR[i]) have a similar meaning.,4.3 Computation of Configuration Loss,[0],[0]
"In order to simplify the specification of the algorithm, we assume below that σL[1] = σR[1].
",4.3 Computation of Configuration Loss,[0],[0]
Algorithm 1 Computation of the loss function for the arc-standard parser 1: T,4.3 Computation of Configuration Loss,[0],[0]
"[1, 1](σL[1])← L(t(σL[1]), tG) 2: for d← 1 to `L + `R − 1 do .",4.3 Computation of Configuration Loss,[0],[0]
"d is the index of a sub-anti-diagonal 3: for j ← max{1, d− `L + 1} to min{d, `R} do .",4.3 Computation of Configuration Loss,[0],[0]
j is the column index 4: i← d− j + 1 .,4.3 Computation of Configuration Loss,[0],[0]
i is the row index 5:,4.3 Computation of Configuration Loss,[0],[0]
if i <,4.3 Computation of Configuration Loss,[0],[0]
`L then .,4.3 Computation of Configuration Loss,[0],[0]
"expand to the left 6: for each h ∈ ∆i,j do 7: T",4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1, j](h)← min{T",4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1, j](h), T [i, j](h) + δG(h→",4.3 Computation of Configuration Loss,[0],[0]
σL[i+ 1])} 8:,4.3 Computation of Configuration Loss,[0],[0]
T,4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1,",4.3 Computation of Configuration Loss,[0],[0]
j](σL[i+ 1])← min{T,4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1, j](σL[i+ 1]), T [i, j](h) +",4.3 Computation of Configuration Loss,[0],[0]
δG(σL[i+ 1]→ h)} 9: if j < `R then .,4.3 Computation of Configuration Loss,[0],[0]
"expand to the right 10: for each h ∈ ∆i,j do 11:",4.3 Computation of Configuration Loss,[0],[0]
T,4.3 Computation of Configuration Loss,[0],[0]
"[i, j + 1](h)← min{T",4.3 Computation of Configuration Loss,[0],[0]
"[i, j + 1](h), T [i, j](h) + δG(h→ σR[j + 1])} 12: T [i, j+1](σR[j + 1])←",4.3 Computation of Configuration Loss,[0],[0]
min{T,4.3 Computation of Configuration Loss,[0],[0]
"[i, j+1](σR[j + 1]), T [i, j](h)+δG(σR[j + 1]→ h)} 13: return T",4.3 Computation of Configuration Loss,[0],[0]
"[`L, `R](0) + ∑ i∈[1,`L] L(t(σL[i]), tG)
",4.3 Computation of Configuration Loss,[0],[0]
"Therefore the elements of σR which have been constructed in §4.2 are σR[i], i ∈",4.3 Computation of Configuration Loss,[0],[0]
"[2, `R].
Algorithm 1 uses a two-dimensional array T of size `L × `R, where each entry T",4.3 Computation of Configuration Loss,[0],[0]
"[i, j] is an association list from integers to integers.",4.3 Computation of Configuration Loss,[0],[0]
"An entry T [i, j](h) stores the minimum loss among dependency trees rooted at h that can be obtained by running the parser on the first i elements of stack σL and the first j elements of buffer σR. More precisely, let
∆i,j = {σL[k] | k ∈",4.3 Computation of Configuration Loss,[0],[0]
"[1, i]} ∪ {σR[k] | k ∈",4.3 Computation of Configuration Loss,[0],[0]
"[1, j]} .",4.3 Computation of Configuration Loss,[0],[0]
"(4)
",4.3 Computation of Configuration Loss,[0],[0]
"For each h ∈ ∆i,j , the entry T [i, j](h) is the minimum loss among all dependency trees defined as above and with root h.",4.3 Computation of Configuration Loss,[0],[0]
"We also assume that T [i, j](h) is initialized to +∞ (not reported in the algorithm).
",4.3 Computation of Configuration Loss,[0],[0]
"Algorithm 1 starts at the top-left corner of T , visiting each individual sub-anti-diagonal of T in ascending order, and eventually reaching the bottomright corner of the array.",4.3 Computation of Configuration Loss,[0],[0]
For each entry T,4.3 Computation of Configuration Loss,[0],[0]
"[i, j], the left expansion is considered (lines 5 to 8) by combining with tree fragment σL[i+ 1], through a left or a right arc reduction.",4.3 Computation of Configuration Loss,[0],[0]
This results in the update of T,4.3 Computation of Configuration Loss,[0],[0]
"[i + 1, j](h), for each h ∈ ∆i+1,j , whenever a smaller value of the loss is achieved for a tree with root h.",4.3 Computation of Configuration Loss,[0],[0]
The Kronecker-like function used at line 8 provides the contribution of each single arc to the loss of the current tree.,4.3 Computation of Configuration Loss,[0],[0]
"Denoting with AG the set of
arcs of tG, such a function is defined as
δG(i→ j) = {
0, if (i→ j) ∈ AG; 1, otherwise.",4.3 Computation of Configuration Loss,[0],[0]
"(5)
A symmetrical process is implemented for the right expansion of T",4.3 Computation of Configuration Loss,[0],[0]
"[i, j] through tree fragment σR[j + 1] (lines 9 to 12).
",4.3 Computation of Configuration Loss,[0],[0]
"As we will see in the next section, quantity T [`L, `R](0) is the minimal loss of a tree composed only by arcs that connect nodes in σL and σR. By summing the loss of all tree fragments t(σL[i]) to the loss in T",4.3 Computation of Configuration Loss,[0],[0]
"[`L, `R](0), at line 13, we obtain the desired result, since the loss of each tree fragment t(σR[j]) is zero.",4.3 Computation of Configuration Loss,[0],[0]
"Throughout this section we let w, tG, σL, σR and c = (σL, β, A) be defined as in §4, but we no longer assume that σL[1] = σR[1].",5 Formal Properties,[0],[0]
"To simplify the presentation, we sometimes identify the tokens in w with the associated nodes in a dependency tree for w.",5 Formal Properties,[0],[0]
"Algorithm 1 explores all dependency trees that can be reached by an arc-standard parser from configuration c, under the condition that (i) the nodes in the buffer β are pre-computed into tree fragments and collapsed into their root nodes in the right stack σR, and (ii) nodes in σR cannot be combined together prior to their combination with other nodes in the left stack",5.1 Linear Trees,[0],[0]
"σL. This set of dependency trees is char-
acterized here using the notion of linear tree, to be used later in the correctness proof.
",5.1 Linear Trees,[0],[0]
Consider two nodes σL[i] and σL[j] with j >,5.1 Linear Trees,[0],[0]
i,5.1 Linear Trees,[0],[0]
> 1.,5.1 Linear Trees,[0],[0]
"An arc-standard parser can construct an arc between σL[i] and σL[j], in any direction, only after reaching a configuration in which σL[i] is at the top of the stack and σL[j] is at the second topmost position.",5.1 Linear Trees,[0],[0]
In such configuration we have that σL[i] dominates σL[1].,5.1 Linear Trees,[0],[0]
"Furthermore, consider nodes σR[i] and σR[j] with j > i ≥ 1.",5.1 Linear Trees,[0],[0]
"Since we are assuming that tree fragments t(σR[i]) and t(σR[j]) are bottom-up complete and maximal, as defined in §4.2, we allow the construction of an arc between σR[i] and σR[j], in any direction, only after reaching a configuration in which σR[i] dominates node σL[1].
",5.1 Linear Trees,[0],[0]
The dependency trees satisfying the restrictions above are captured by the following definition.,5.1 Linear Trees,[0],[0]
"A linear tree over (σL, σR) is a projective dependency tree t for string σLσR satisfying both of the additional conditions reported below.",5.1 Linear Trees,[0],[0]
"The path from t’s root to node σL[1] is called the spine of t.
•",5.1 Linear Trees,[0],[0]
"Every node of t not in the spine is a dependent of some node in the spine.
",5.1 Linear Trees,[0],[0]
• For each arc,5.1 Linear Trees,[0],[0]
"i → j in t with j in the spine, no dependent of i can be placed in between i and j within string σLσR.
An example of a linear tree is depicted in Figure 2.",5.1 Linear Trees,[0],[0]
"Observe that the second condition above forbids the reduction of two nodes i and j, in case none of these dominates node σL[1].",5.1 Linear Trees,[0],[0]
"For instance, the ra reduction of nodes i3 and i2 would result in arc i3 → i2 replacing arc i1 → i2 in Figure 2.",5.1 Linear Trees,[0],[0]
"The new dependency tree is not linear, because of a violation of the
second condition above.",5.1 Linear Trees,[0],[0]
"Similarly, the la reduction of nodes j3 and j4 would result in arc j4 → j3 replacing arc i3 → j3 in Figure 2, again a violation of the second condition above.
",5.1 Linear Trees,[0],[0]
"Lemma 1 Any tree t ∈ D(c) can be decomposed into trees t(σL[i]), i ∈",5.1 Linear Trees,[0],[0]
"[1, `L], trees tj , j ∈",5.1 Linear Trees,[0],[0]
"[1, q] and q ≥ 1, and a linear tree tl over (σL, σR,t), where σR,t = r1 · · · rq and each rj is the root node of tj . 2 PROOF (SKETCH)",5.1 Linear Trees,[0],[0]
"Trees t(σL[i]) are common to every tree in D(c), since the arc-standard model can not undo the arcs already built in the current configuration c. Similar to the construction in §4.2 of the right stack σR from tG, we let tj , j ∈",5.1 Linear Trees,[0],[0]
"[1, q], be tree fragments of t that cover only nodes associated with the tokens in the buffer β",5.1 Linear Trees,[0],[0]
and that are bottomup complete and maximal for t. These trees are indexed according to their left to right order in β.,5.1 Linear Trees,[0],[0]
"Finally, tl is implicitly defined by all arcs of t that are not in trees t(σL[i]) and tj .",5.1 Linear Trees,[0],[0]
"It is not difficult to see that tl has a spine ending with node σL[1] and is a linear tree over (σL, σR,t).",5.1 Linear Trees,[0],[0]
"Our proof of correctness for Algorithm 1 is based on a specific dependency tree t∗ for w, which we define below.",5.2 Correctness,[0],[0]
Let SL = {σL[i] | i ∈,5.2 Correctness,[0],[0]
"[1, `L]} and letDL be the set of nodes that are descendants of some node in SL.",5.2 Correctness,[0],[0]
"Similarly, let SR = {σR[i] | i ∈",5.2 Correctness,[0],[0]
"[1, `R]} and let DR be the set of descendants of nodes in SR.",5.2 Correctness,[0],[0]
"Note that sets SL, SR, DL and DR provide a partition of Vw.
",5.2 Correctness,[0],[0]
"We choose any linear tree t∗l over (σL, σR) having root 0, such that L(t∗l , tG)",5.2 Correctness,[0],[0]
"= mint L(t, tG), where t ranges over all possible linear trees over (σL, σR) with root 0.",5.2 Correctness,[0],[0]
"Tree t∗ consists of the set of nodes Vw and the set of arcs obtained as the union of the set of arcs of t∗l and the set of arcs of all trees t(σL[i]), i ∈",5.2 Correctness,[0],[0]
"[1, `L], and t(σR[j]), j ∈",5.2 Correctness,[0],[0]
"[1, `R].",5.2 Correctness,[0],[0]
Lemma 2 t∗ ∈ D(c).,5.2 Correctness,[0],[0]
2 PROOF (SKETCH),5.2 Correctness,[0],[0]
All tree fragments t(σL[i]) have already been parsed and are available in the stack associated with c.,5.2 Correctness,[0],[0]
"Each tree fragment t(σR[j]) can later be constructed in the computation, when a configuration c′ is reached with the relevant segment of w at the start of the buffer.",5.2 Correctness,[0],[0]
"Note also that parsing of t(σR[j]) can be done in a way that does not depend on the content of the stack in c′.
Finally, the parsing of the tree fragments t(σR[j]) is interleaved with the construction of the arcs from the linear tree t∗l , which are all of the form (i → j) with i, j ∈ (SL ∪ SR).",5.2 Correctness,[0],[0]
"More precisely, if (i → j) is an arc from t∗l , at some point in the computation nodes i and j will become available at the two topmost positions in the stack.",5.2 Correctness,[0],[0]
"This follows from the second condition in the definition of linear tree.
",5.2 Correctness,[0],[0]
"We now show that tree t∗ is “optimal” within the set D(c) and with respect to tG. Lemma 3 L(t∗, tG)",5.2 Correctness,[0],[0]
"= L(c, tG).",5.2 Correctness,[0],[0]
2 PROOF Consider an arbitrary tree t ∈ D(c).,5.2 Correctness,[0],[0]
"Assume the decomposition of t defined in the proof of Lemma 1, through trees t(σL[i]), i ∈",5.2 Correctness,[0],[0]
"[1, `L], trees tj , j ∈",5.2 Correctness,[0],[0]
"[1, q], and linear tree tl over (σL, σR,t).
",5.2 Correctness,[0],[0]
Recall that an arc,5.2 Correctness,[0],[0]
"i → j denotes an ordered pair (i, j).",5.2 Correctness,[0],[0]
"Let us consider the following partition for the set of arcs of any dependency tree for w
A1 = (SL ∪DL)×DL , A2 = (SR ∪DR)×DR , A3 = (Vw × Vw) \",5.2 Correctness,[0],[0]
"(A1 ∪A2) .
",5.2 Correctness,[0],[0]
"In what follows, we compare the losses L(t, tG) and L(t∗, tG) by separately looking into the contribution to such quantities due to the arcs in A1, A2 and A3.
",5.2 Correctness,[0],[0]
"Note that the arcs of trees t(σL[i]) are all in A1, the arcs of trees t(σR[j]) are all in A2, and the arcs of tree t∗l are all in A3.",5.2 Correctness,[0],[0]
"Since t and t
∗ share trees t(σL[i]), when restricted to arcs in A1 quantities L(t, tG) and L(t∗, tG) are the same.",5.2 Correctness,[0],[0]
"When restricted to arcs in A2, quantity L(t∗, tG) is zero, by construction of the trees t(σR[j]).",5.2 Correctness,[0],[0]
"Thus L(t, tG) can not be smaller thanL(t∗, tG) for these arcs.",5.2 Correctness,[0],[0]
"The difficult part is the comparison of the contribution to L(t, tG) and L(t∗, tG) due to the arcs in A3.",5.2 Correctness,[0],[0]
"We deal with this below.
",5.2 Correctness,[0],[0]
"LetAS,G be the set of all arcs from tG that are also in set (SL × SR) ∪ (SR × SL).",5.2 Correctness,[0],[0]
"In words, AS,G represents gold arcs connecting nodes in SL and nodes in SR, in any direction.",5.2 Correctness,[0],[0]
"Within tree t, these arcs can only be found in the tl component, since nodes in SL are all placed within the spine of tl, or else at the left of that spine.
",5.2 Correctness,[0],[0]
Let us consider an arc (j → i) ∈,5.2 Correctness,[0],[0]
"AS,G with j ∈ SL",5.2 Correctness,[0],[0]
"and i ∈ SR, and let us assume that (j → i) is in t∗l .",5.2 Correctness,[0],[0]
"If token ai does not occur in σR,t, node i is not
in tl",5.2 Correctness,[0],[0]
and (j → i) can not be an arc of t.,5.2 Correctness,[0],[0]
"We then have that (j → i) contributes one unit to L(t, tG) but does not contribute to L(t∗, tG).",5.2 Correctness,[0],[0]
"Similarly, let (i → j) ∈",5.2 Correctness,[0],[0]
"AS,G be such that i ∈ SR and j ∈ SL, and assume that (i→ j) is in t∗l .",5.2 Correctness,[0],[0]
"If token ai does not occur in σR,t, arc (i → j) can not be in t.",5.2 Correctness,[0],[0]
"We then have that (i → j) contributes one unit to L(t, tG) but does not contribute to L(t∗, tG).
",5.2 Correctness,[0],[0]
"Intuitively, the above observations mean that the winning strategy for trees in D(c) is to move nodes from SR as much as possible into the linear tree component tl, in order to make it possible for these nodes to connect to nodes in SL, in any direction.",5.2 Correctness,[0],[0]
"In this case, arcs fromA3 will also move into the linear tree component of a tree inD(c), as it happens in the case of t∗.",5.2 Correctness,[0],[0]
"We thus conclude that, when restricted to the set of arcs in A3, quantity L(t, tG) is not smaller than L(t∗, tG), because stack σR has at least as many tokens corresponding to nodes in SR as stack σR,t, and because t∗l has the minimum loss among all the linear trees over (σL, σR).
",5.2 Correctness,[0],[0]
"Putting all of the above observations together, we conclude that L(t, tG) can not be smaller than L(t∗, tG).",5.2 Correctness,[0],[0]
"This concludes the proof, since t has been arbitrarily chosen in D(c).",5.2 Correctness,[0],[0]
"Theorem 1 Algorithm 1 computes L(c, tG).",5.2 Correctness,[0],[0]
2 PROOF (SKETCH),5.2 Correctness,[0],[0]
"Algorithm 1 implements a Viterbi search for trees with smallest loss among all linear trees over (σL, σR).",5.2 Correctness,[0],[0]
"Thus T [`L, `R](0) = L(t∗l , tG).",5.2 Correctness,[0],[0]
The loss of the tree fragments t(σR[j]) is 0 and the loss of the tree fragments t(σL[i]) is added at line 13 in the algorithm.,5.2 Correctness,[0],[0]
"Thus the algorithm returns L(t∗, tG), and the statement follows from Lemma 2 and Lemma 3.",5.2 Correctness,[0],[0]
"Following §4.2, the right stack σR can be easily constructed in time O(n), n the length of the input string.",5.3 Computational Analysis,[0],[0]
We now analyze Algorithm 1.,5.3 Computational Analysis,[0],[0]
For each entry T,5.3 Computational Analysis,[0],[0]
"[i, j] and for each h ∈ ∆i,j , we update T [i, j](h) a number of times bounded by a constant which does not depend on the input.",5.3 Computational Analysis,[0],[0]
Each updating can be computed in constant time as well.,5.3 Computational Analysis,[0],[0]
We thus conclude that Algorithm 1 runs in time O(`L · `R · (`L + `R)).,5.3 Computational Analysis,[0],[0]
"Quantity `L+`R is bounded by n, but in practice the former is significantly smaller.",5.3 Computational Analysis,[0],[0]
"When measured over the sentences in the Penn
Treebank, the average value of `L+`Rn is 0.29.",5.3 Computational Analysis,[0],[0]
"In terms of runtime, training is 2.3 times slower when using our oracle instead of a static oracle.",5.3 Computational Analysis,[0],[0]
"In this section we consider the transition-based parser proposed by Sartorio et al. (2013), called here the LR-spine parser.",6 Extension to the LR-Spine Parser,[0],[0]
This parser is not arcdecomposable: the same example reported in §2.4 can be used to show this fact.,6 Extension to the LR-Spine Parser,[0],[0]
We therefore extend to the LR-spine parser the algorithm developed in §4.,6 Extension to the LR-Spine Parser,[0],[0]
Let t be a dependency tree.,6.1 The LR-Spine Parser,[0],[0]
"The left spine of t is an ordered sequence 〈i1, . . .",6.1 The LR-Spine Parser,[0],[0]
", ip〉, p ≥ 1, consisting of all nodes in a descending path from the root of t taking the leftmost child node at each step.",6.1 The LR-Spine Parser,[0],[0]
The right spine of t is defined symmetrically.,6.1 The LR-Spine Parser,[0],[0]
"We use ⊕ to denote sequence concatenation.
",6.1 The LR-Spine Parser,[0],[0]
"In the LR-spine parser each stack element σ[i] denotes a partially built subtree t(σ[i]) and is represented by a pair (lsi, rsi), with lsi and rsi the left and the right spine, respectively, of t(σ[i]).",6.1 The LR-Spine Parser,[0],[0]
"We write lsi[k] (rsi[k]) to represent the k-th element of lsi (rsi, respectively).",6.1 The LR-Spine Parser,[0],[0]
"We also write r(σ[i]) to denote the root of t(σ[i]), so that r(σ[i]) =",6.1 The LR-Spine Parser,[0],[0]
"lsi[1] = rsi[1].
Informally, the LR-spine parser uses the same transition typologies as the arc-standard parser.",6.1 The LR-Spine Parser,[0],[0]
"However, an arc (h → d) can now be created with the head node h chosen from any node in the spine of the involved tree.",6.1 The LR-Spine Parser,[0],[0]
"The transition types of the LRspine parser are defined as follows.
",6.1 The LR-Spine Parser,[0],[0]
"• Shift (sh) removes the first node from the buffer and pushes into the stack a new element, consisting of the left and right spines of the associated tree
(σ, i|β,A) `sh (σ|(〈i〉, 〈i〉), β, A) .
•",6.1 The LR-Spine Parser,[0],[0]
Left-Arc k (lak) creates a new arc h → d from the k-th node in the left spine of the topmost tree in the stack to the head of the second element in the stack.,6.1 The LR-Spine Parser,[0],[0]
"Furthermore, the two topmost stack elements are replaced by a new element associated with the resulting tree
(σ′|σ[2]|σ[1], β, A) `lak (σ′|σlak , β, A ∪ {h→ d}) where we have set h = ls1[k], d = r(σ[2]) and σlak = (〈ls1[1], . . .",6.1 The LR-Spine Parser,[0],[0]
", ls1[k]〉 ⊕ ls2, rs1).
",6.1 The LR-Spine Parser,[0],[0]
•,6.1 The LR-Spine Parser,[0],[0]
"Right-Arc k (rak for short) is defined symmetrically with respect to lak
(σ′|σ[2]|σ[1], β, A) `rak (σ′|σrak , β, A ∪ {h→ d})
where we have set h = rs2[k], d = r(σ[1]) and σrak = (ls2, 〈rs2[1], . . .",6.1 The LR-Spine Parser,[0],[0]
", rs2[k]〉 ⊕ rs1).
",6.1 The LR-Spine Parser,[0],[0]
"Note that, at each configuration in the LR-spine parser, there are |ls1| possible lak transitions, one for each choice of a node in the left spine of t(σ[1]); similarly, there are |rs2| possible rak transitions, one for each choice of a node in the right spine of t(σ[2]).",6.1 The LR-Spine Parser,[0],[0]
"We only provide an informal description of the extended algorithm here, since it is very similar to the algorithm in §4.
",6.2 Configuration Loss,[0],[0]
"In the first phase we use the procedure of §4.2 for the construction of the right stack σR, considering only the roots of elements in σL and ignoring the rest of the spines.",6.2 Configuration Loss,[0],[0]
"The only difference is that each element σR[j] is now a pair of spines (lsR,j , rsR,j).",6.2 Configuration Loss,[0],[0]
"Since tree fragment t(σR[j]) is bottom-up complete (see §4.1), we now restrict the search space in such a way that only the root node r(σR[j]) can take dependents.",6.2 Configuration Loss,[0],[0]
"This is done by setting lsR,j = rsR,j = 〈r(σR[j])〉 for each j ∈",6.2 Configuration Loss,[0],[0]
"[1, `R].",6.2 Configuration Loss,[0],[0]
"In order to simplify the presentation we also assume σR[1] = σL[1], as done in §4.3.
In the second phase we compute the loss of an input configuration using a two-dimensional array T , defined as in §4.3.",6.2 Configuration Loss,[0],[0]
"However, because of the way transitions are defined in the LR-spine parser, we now need to distinguish tree fragments not only on the basis of their roots, but also on the basis of their left and right spines.",6.2 Configuration Loss,[0],[0]
"Accordingly, we define each entry T",6.2 Configuration Loss,[0],[0]
"[i, j] as an association list with keys of the form (ls, rs).",6.2 Configuration Loss,[0],[0]
"More specifically, T [i, j](ls, rs) is the minimum loss of a tree with left and right spines ls and rs, respectively, that can be obtained by running the parser on the first i elements of stack σL and the first j elements of buffer σR.
We follow the main idea of Algorithm 1 and expand each tree in T",6.2 Configuration Loss,[0],[0]
"[i, j] at its left side, by combining with tree fragment t(σL[i+ 1]), and at its right side, by combining with tree fragment t(σR[j + 1]).
",6.2 Configuration Loss,[0],[0]
"Tree combination deserves some more detailed discussion, reported below.
",6.2 Configuration Loss,[0],[0]
We consider the combination of a tree ta from T,6.2 Configuration Loss,[0],[0]
"[i, j] and tree t(σL[i+ 1])",6.2 Configuration Loss,[0],[0]
by means of a left-arc transition.,6.2 Configuration Loss,[0],[0]
All other cases are treated symmetrically.,6.2 Configuration Loss,[0],[0]
"Let (lsa, rsa) be the spine pair of ta, so that the loss of ta is stored in T",6.2 Configuration Loss,[0],[0]
"[i, j](lsa, rsa).",6.2 Configuration Loss,[0],[0]
"Let also (lsb, rsb) be the spine pair of t(σL[i+ 1]).",6.2 Configuration Loss,[0],[0]
"In case there exists a gold arc in tG connecting a node from lsa to r(σL[i+ 1]), we choose the transition lak, k ∈",6.2 Configuration Loss,[0],[0]
"[1, |lsa|], that creates such arc.",6.2 Configuration Loss,[0],[0]
"In case such gold arc does not exists, we choose the transition lak with the maximum possible value of k, that is, k = |lsa|.",6.2 Configuration Loss,[0],[0]
"We therefore explore only one of the several possible ways of combining these two trees by means of a left-arc transition.
",6.2 Configuration Loss,[0],[0]
We remark that the above strategy is safe.,6.2 Configuration Loss,[0],[0]
"In fact, in case the gold arc exists, no other gold arc can ever involve the nodes of lsa eliminated by lak (see the definition in §6.1), because arcs can not cross each other.",6.2 Configuration Loss,[0],[0]
"In case the gold arc does not exist, our choice of k = |lsa| guarantees that we do not eliminate any element from lsa.
",6.2 Configuration Loss,[0],[0]
"Once a transition lak is chosen, as described above, the reduction is performed and the spine pair (ls, rs) for the resulting tree is computed from (lsa, rsa) and (lsb, rsb), as defined in §6.1.",6.2 Configuration Loss,[0],[0]
"At the same time, the loss of the resulting tree is computed, on the basis of the loss T",6.2 Configuration Loss,[0],[0]
"[i, j](lsa, rsa), the loss of tree t(σL[i+ 1]), and a Kronecker-like function defined below.",6.2 Configuration Loss,[0],[0]
This loss is then used to update T,6.2 Configuration Loss,[0],[0]
"[i+ 1, j](ls, rs).
",6.2 Configuration Loss,[0],[0]
Let ta and tb be two trees that must be combined in such a way that tb becomes the dependent of some node in one of the two spines of ta.,6.2 Configuration Loss,[0],[0]
"Let also pa = (lsa, rsa) and pb = (lsb, rsb) be spine pairs for ta and tb, respectively.",6.2 Configuration Loss,[0],[0]
Recall that AG is the set of arcs of tG.,6.2 Configuration Loss,[0],[0]
"The new Kronecker-like function for the computation of the loss is defined as
δG(pa, pb) =    0, if r(ta)",6.2 Configuration Loss,[0],[0]
< r(tb)∧,6.2 Configuration Loss,[0],[0]
"∃k[(rska → r(tb)) ∈ AG]; 0, if r(ta) > r(tb)∧",6.2 Configuration Loss,[0],[0]
"∃k[(lska → r(tb)) ∈ AG];
1, otherwise.",6.2 Configuration Loss,[0],[0]
The algorithm in §6.2 has an exponential behaviour.,6.3 Efficiency Improvement,[0],[0]
"To see why, consider trees in T",6.3 Efficiency Improvement,[0],[0]
"[i, j].",6.3 Efficiency Improvement,[0],[0]
These trees are produced by the combination of trees in T,6.3 Efficiency Improvement,[0],[0]
"[i − 1, j] with tree t(σL[i]), or by the combination of trees in T",6.3 Efficiency Improvement,[0],[0]
"[i, j",6.3 Efficiency Improvement,[0],[0]
− 1] with tree t(σR[j]).,6.3 Efficiency Improvement,[0],[0]
"Since each combination involves either a left-arc or a right-arc transition, we obtain a recursive relation that resolves into a number of trees in T",6.3 Efficiency Improvement,[0],[0]
"[i, j] bounded by 4i+j−2.
",6.3 Efficiency Improvement,[0],[0]
We introduce now two restrictions to the search space of our extended algorithm that result in a huge computational saving.,6.3 Efficiency Improvement,[0],[0]
"For a spine s, we write N (s) to denote the set of all nodes in s. We also let ∆i,j be the set of all pairs (ls, rs) such that T",6.3 Efficiency Improvement,[0],[0]
"[i, j](ls, rs) 6=",6.3 Efficiency Improvement,[0],[0]
"+∞.
• Every time a new pair (ls, rs) is created in ∆[i, j], we remove from ls all nodes different from the root that do not have gold dependents in {r(σL[k])",6.3 Efficiency Improvement,[0],[0]
| k <,6.3 Efficiency Improvement,[0],[0]
"i}, and we remove from rs all nodes different from the root that do not have gold dependents in {r(σR[k])",6.3 Efficiency Improvement,[0],[0]
"| k > j}.
•",6.3 Efficiency Improvement,[0],[0]
"A pair pa = (lsa, rsa) is removed from ∆[i, j] if there exists a pair pb = (lsb, rsb) in ∆[i, j] with the same root node as pa and with (lsa, rsa) 6= (lsb, rsb), such that N (lsa) ⊆ N (lsb), N (rsa) ⊆ N (rsb), and T",6.3 Efficiency Improvement,[0],[0]
"[i, j](pa) ≥ T",6.3 Efficiency Improvement,[0],[0]
"[i, j](pb).
",6.3 Efficiency Improvement,[0],[0]
The first restriction above reduces the size of a spine by eliminating a node if it is irrelevant for the computation of the loss of the associated tree.,6.3 Efficiency Improvement,[0],[0]
"The second restriction eliminates a tree ta if there is a tree tb with smaller loss than ta, such that in the computations of the parser tb provides exactly the same context as ta.",6.3 Efficiency Improvement,[0],[0]
"It is not difficult to see that the above restrictions do not affect the correctness of the algorithm, since they always leave in our search space some tree that has optimal loss.
",6.3 Efficiency Improvement,[0],[0]
A mathematical analysis of the computational complexity of the extended algorithm is quite involved.,6.3 Efficiency Improvement,[0],[0]
"In Figure 3, we plot the worst case size of T",6.3 Efficiency Improvement,[0],[0]
"[i, j] for each value of j + i",6.3 Efficiency Improvement,[0],[0]
"− 1, computed over all configurations visited in the training phase (see §7).",6.3 Efficiency Improvement,[0],[0]
We see that |T,6.3 Efficiency Improvement,[0],[0]
"[i, j]| grows linearly with j + i− 1, leading to the same space requirements of Algorithm 1.",6.3 Efficiency Improvement,[0],[0]
"Empirically, training with the dynamic
Algorithm 2 Online training for greedy transitionbased parsers
1: w← 0 2: for k iterations do 3: shuffle(corpus) 4: for sentencew and gold tree tG in corpus do 5: c← INITIAL(w) 6: while not FINAL(c) do 7: τp ← argmaxτ∈valid(c)w · φ(c, τ) 8: τo ← argmaxτ∈oracle(c,tG)w·φ(c, τ) 9: if τp 6∈ oracle(c, tG) then
10: w←",6.3 Efficiency Improvement,[0],[0]
"w + φ(c, τo)− φ(c, τp)
11: τ",6.3 Efficiency Improvement,[0],[0]
"← { τp if EXPLORE τo otherwise 12: c← τ(c) return averaged(w)
oracle is only about 8 times slower than training with the oracle of Sartorio et al. (2013) without exploring incorrect configurations.",6.3 Efficiency Improvement,[0],[0]
"We follow the training procedure suggested by Goldberg and Nivre (2013), as described in Algorithm 2.",7 Training,[0],[0]
The algorithm performs online learning using the averaged perceptron algorithm.,7 Training,[0],[0]
"A weight vector w (initialized to 0) is used to score the valid transitions in each configuration based on a feature representation φ, and the highest scoring transition τp is predicted.",7 Training,[0],[0]
"If the predicted transition is not optimal according to the oracle, the weights w are updated away from the predicted transition and to-
wards the highest scoring oracle transition τo.",7 Training,[0],[0]
"The parser then moves to the next configuration, by taking either the predicted or the oracle transition.",7 Training,[0],[0]
"In the “error exploration” mode (EXPLORE is true), the parser follows the predicted transition, and otherwise the parser follows the oracle transition.",7 Training,[0],[0]
"Note that the error exploration mode requires the completeness property of a dynamic oracle.
",7 Training,[0],[0]
"We consider three training conditions: static, in which the oracle is deterministic (returning a single canonical transition for each configuration) and no error exploration is performed; nondet, in which we use a nondeterministic partial oracle (Sartorio et al., 2013), but do not perform error exploration; and explore in which we use the dynamic oracle and perform error exploration.",7 Training,[0],[0]
The static setup mirrors the way greedy parsers are traditionally trained.,7 Training,[0],[0]
The nondet setup allows the training procedure to choose which transition to take in case of spurious ambiguities.,7 Training,[0],[0]
"The explore setup increases the configuration space explored by the parser during training, by exposing the training procedure to non-optimal configurations that are likely to occur during parsing, together with the optimal transitions to take in these configurations.",7 Training,[0],[0]
"It was shown by Goldberg and Nivre (2012; 2013) that the nondet setup outperforms the static setup, and that the explore setup outperforms the nondet setup.",7 Training,[0],[0]
"Datasets Performance evaluation is carried out on CoNLL 2007 multilingual dataset, as well as on the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006).",8 Experimental Evaluation,[0],[0]
"For the CoNLL datasets we use gold part-of-speech tags, while for the PTB we use automatically assigned tags.",8 Experimental Evaluation,[0],[0]
"As usual, the PTB parser is trained on sections 2-21 and tested on section 23.
",8 Experimental Evaluation,[0],[0]
"Setup We train labeled versions of the arc-standard (std) and LR-spine (lrs) parsers under the static, nondet and explore setups, as defined in §7.",8 Experimental Evaluation,[0],[0]
In the nondet setup we use a nondeterministic partial oracle and in the explore setup we use the nondeterministic complete oracles we present in this paper.,8 Experimental Evaluation,[0],[0]
In the static setup we resolve oracle ambiguities and choose a canonic transition sequence by attaching arcs as soon as possible.,8 Experimental Evaluation,[0],[0]
"In the explore setup,
from the first round of training onward, we always follow the predicted transition (EXPLORE is true).",8 Experimental Evaluation,[0],[0]
"For all languages, we deal with non-projectivity by skipping the non-projective sentences during training but not during test.",8 Experimental Evaluation,[0],[0]
"For each parsing system, we use the same feature templates across all languages.1 The arc-standard models are trained for 15 iterations and the LR-spine models for 30 iterations, after which all the models seem to have converged.
Results In Table 1 we report the labeled (LAS) and unlabeled (UAS) attachment scores.",8 Experimental Evaluation,[0],[0]
"As expected, the LR-spine parsers outperform the arc-standard parsers trained under the same setup.",8 Experimental Evaluation,[0],[0]
"Training with the dynamic oracles is also beneficial: despite the arguable complexity of our proposed oracles, the trends are consistent with those reported by Goldberg and Nivre (2012; 2013).",8 Experimental Evaluation,[0],[0]
For the arc-standard model we observe that the move from a static to a nondeterministic oracle during training improves the accuracy for most of languages.,8 Experimental Evaluation,[0],[0]
Making use of the completeness of the dynamic oracle and moving to the error exploring setup further improve results.,8 Experimental Evaluation,[0],[0]
"The only exceptions are Basque, that has a small dataset with more than 20% of non-projective sentences, and Chinese.",8 Experimental Evaluation,[0],[0]
"For Chinese we observe a reduction of accuracy in the nondet setup, but an increase in the explore setup.
",8 Experimental Evaluation,[0],[0]
"For the LR-spine parser we observe a practically constant increase of performance by moving from
1Our complete code, together with the description of the feature templates, is available on the second author’s homepage.
",8 Experimental Evaluation,[0],[0]
the static to the nondeterministic and then to the error exploring setups.,8 Experimental Evaluation,[0],[0]
"We presented dynamic oracles, based on dynamic programming, for the arc-standard and the LRspine parsers.",9 Conclusions,[0],[0]
"Empirical evaluation on 10 languages showed that, despite the apparent complexity of the oracle calculation procedure, the oracles are still learnable, in the sense that using these oracles in the error exploration training algorithm presented in (Goldberg and Nivre, 2012) considerably improves the accuracy of the trained parsers.
",9 Conclusions,[0],[0]
Our algorithm computes a dynamic oracle using dynamic programming to explore a forest of dependency trees that can be reached from a given parser configuration.,9 Conclusions,[0],[0]
"For the arc-standard parser, the computation takes cubic time in the size of the largest of the left and right input stacks.",9 Conclusions,[0],[0]
Dynamic programming for the simulation of arc-standard parsers have been proposed by Kuhlmann et al. (2011).,9 Conclusions,[0],[0]
"That algorithm could be adapted to compute minimum loss for a given configuration, but the running time is O(n4), n the size of the input string: besides being asymptotically slower by one order of magnitude, in practice n is also larger than the stack size above.
",9 Conclusions,[0],[0]
Acknowledgments We wish to thank the anonymous reviewers.,9 Conclusions,[0],[0]
"In particular, we are indebted to one of them for two important technical remarks.",9 Conclusions,[0],[0]
The third author has been partially supported by MIUR under project PRIN,9 Conclusions,[0],[0]
No. 2010LYA9RH 006.,9 Conclusions,[0],[0]
"We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013).",abstractText,[0],[0]
We experimentally show that using these oracles during training yields superior parsing accuracies on many languages.,abstractText,[0],[0]
A Tabular Method for Dynamic Oracles in Transition-Based Parsing,title,[0],[0]
"proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.",text,[0],[0]
"Driven by massive data and computational resources, modern convolutional neural networks (CNNs) and other network architectures have achieved many outstanding results, such as image recognition (Krizhevsky et al., 2012), neural machine translation (Sutskever et al., 2014), and playing Go games (Silver et al., 2016), etc.",1. Introduction,[0],[0]
"Despite their extensive applications, these neural networks are always considered as black boxes.",1. Introduction,[0],[0]
"Interpretability used to be for its own sake; now, due to safety-critical applications such as self-driving cars and tumor diagnosis, it is no longer satisfying to have a black box that is unaccountable for its decisions.",1. Introduction,[0],[0]
"The demand for explainable artificial intelligence (XAI) (Gunning, 2017) – human interpretable explanations of model decisions – has driven the development of visualization techniques, including image synthesis via activation
1Department of Electrical and Computer Engineering, Rice University, Houston, USA. 2Department of Computer Science, Rice University, Houston, USA.",1. Introduction,[0],[0]
"3Department of Neuroscience, Baylor College of Medicine, Houston, USA.",1. Introduction,[0],[0]
"Correspondence to: Weili Nie <wn8@rice.edu>, Ankit B. Patel <abp4@rice.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
maximization (Simonyan et al., 2013; Johnson et al., 2016; Nguyen et al., 2016) and backpropagation-based visualizations (Simonyan et al., 2013; Zeiler & Fergus, 2014; Springenberg et al., 2014; Shrikumar et al., 2017; Kindermans et al., 2017).
",1. Introduction,[0],[0]
The basic idea of backpropagation-based visualizations is to highlight class-relevant pixels by propagating the network output back to the input image space.,1. Introduction,[0],[0]
The intensity changes of these pixels have the most significant impact on network decisions.,1. Introduction,[0],[0]
"Specifically, (Simonyan et al., 2013) visualizes the spatial support of a given class in a given image, i.e. saliency map, by using the true gradient which masks out negative entries of bottom data via the forward ReLU.",1. Introduction,[0],[0]
"Despite its simplicity, the results of saliency map are normally very noisy which makes the interpretation difficult.",1. Introduction,[0],[0]
"(Zeiler & Fergus, 2014) visualize the reverse mapping from feature activities back to the input pixel space with the deconvolutional network (DeconvNet) method.",1. Introduction,[0],[0]
The basic idea of DeconvNet is to mask out negative entries of the top gradients by resorting to the backward ReLU.,1. Introduction,[0],[0]
"(Springenberg et al., 2014) proposed the Guided Backpropagation (GBP) method which combines the above two methods: by considering both the forward and backward ReLUs, it masks out the values for which either top gradients or bottom data are negative and produces sharper visualizations.",1. Introduction,[0],[0]
"More recently, DeepLift (Shrikumar et al., 2017) and PatternNet (Kindermans et al., 2017) have been proposed to further improve the visual quality of backpropagation-based methods.
",1. Introduction,[0],[0]
"This class of backpropagation-based visualizations, in particular GBP and DeconvNet, has attracted a lot of attention in both the deep learning community and other fields (Szegedy et al., 2013; Dosovitskiy & Brox, 2016; Selvaraju et al., 2016; Fong & Vedaldi, 2017; Kraus et al., 2016).",1. Introduction,[0],[0]
"Despite their good visual quality, the question of how they are actually related to the decision-making has remained largely unexplored.",1. Introduction,[0],[0]
Do the pretty visualizations actually tell us reliably about what the network is doing internally?,1. Introduction,[0],[0]
"Our experiments have confirmed previous observations (Mahendran & Vedaldi, 2016; Selvaraju et al., 2016; Samek et al., 2017) that saliency map is indeed very sensitive to the change of class labels, while GBP and DeconvNet, though their visualization results are much cleaner than saliency map, remain almost the same given different class labels.",1. Introduction,[0],[0]
"It seems that
the visual quality improvement of backpropagation-based methods is sacrificing the ability of highlighting important pixels to a specific output class.",1. Introduction,[0],[0]
"In this sense, GBP and DeconvNet may be unreliable in interpreting how deep neural networks make classification decisions.
",1. Introduction,[0],[0]
"The most commonly used explanation for these visualizations is to approximate the neural networks with a linear function (Simonyan et al., 2013; Kindermans et al., 2017), where the derivative of output with respect to input image is just the weight vector of the model.",1. Introduction,[0],[0]
"In such sense, the backpropagation-based methods can be regarded as visualizing the learned weights.",1. Introduction,[0],[0]
But apparently the approximate linear model is too simplistic to reflect the highly nonlinear property of deep neural networks.,1. Introduction,[0],[0]
"For example, GBP and DeconvNet essentially apply the same algorithm as saliency map, but treat ReLU, the nonlinear activation, differently.",1. Introduction,[0],[0]
"The linear model explanation thus cannot answer questions regarding why GBP and DeconvNet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear model.",1. Introduction,[0],[0]
"Therefore, we need a more complex model, which should at least capture the impact of both forward ReLU and backward ReLU, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, we can extract from these visualizations.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
We provide a theoretical explanation for why GBP and DeconvNet generate more humaninterpretable but less class-sensitive visualizations than saliency map.,1. Introduction,[0],[0]
"Specifically, our analysis reveals that GBP and DeconvNet are essentially doing (partial) image recovery instead of highlighting class-relevant pixels or visualizing the learned weights, which means in principle they are unrelated to the decision-making of neural networks.",1. Introduction,[0],[0]
"We also find that it is the backward ReLU introduced by either GBP or DeconvNet, together with the local connections in CNNs that results in crisp visualizations.",1. Introduction,[0],[0]
"In particular, we explain how DeconvNet also relies on the max-pooling to recover the input.",1. Introduction,[0],[0]
"Finally, we do extensive experiments to support our theory and further reveal more detailed properties of these backpropagation-based visualizations1.",1. Introduction,[0],[0]
"In this section, we first give formal definitions of backpropagation-based visualizations: saliency map, DeconvNet and GBP, and then compare their empirical behaviors.",2. Backpropagation-based Visualizations,[0],[0]
"The key difference of backpropagation-based methods is the way they propagate the output score back through the
1Code is available at https://github.com/weilinie/BackpropVis
ReLU activations.",2.1. Formal Definitions,[0],[0]
"As illustrated by Figure 1, we consider the i-th ReLU activation in the l-th layer with its input",2.1. Formal Definitions,[0],[0]
y (l) i and its output,2.1. Formal Definitions,[0],[0]
o,2.1. Formal Definitions,[0],[0]
"(l) i and denote by σ(t) = max(t, 0) the ReLU activation.",2.1. Formal Definitions,[0],[0]
"Also, denote by R (l) i the top gradient before activation, i.e., gradient of the output score with respect to o (l) i and denote by T (l) i the (modified) gradient after activation, i.e., gradient of the output score with respect to y (l) i .",2.1. Formal Definitions,[0],[0]
"Then in the gradient calculations, the corresponding forward ReLU could be formally defined as a function
σ",2.1. Formal Definitions,[0],[0]
"(l) f,i(t) , I
(
y (l) i
)
t
where I(·) is the indicator function and the corresponding backward ReLU could be formally defined as a function
σ",2.1. Formal Definitions,[0],[0]
"(l) b,i(t) , I
(
R (l) i
)
",2.1. Formal Definitions,[0],[0]
"t
Therefore, the formal definition of backpropagation-based methods for propagating the output score back through the i-th ReLU activation in the l-th layer is
T (l) i =

   
   
σ",2.1. Formal Definitions,[0],[0]
"(l) f,i
(
R (l) i
)
for saliency map
σ",2.1. Formal Definitions,[0],[0]
"(l) b,i
(
R (l) i
)
for DeconvNet
σ",2.1. Formal Definitions,[0],[0]
"(l) f,i
(
σ (l) b,i
(
R (l) i
))
",2.1. Formal Definitions,[0],[0]
"for GBP
which can be further uniformly formulated as
T (l) i = h
(
R (l) i
)",2.1. Formal Definitions,[0],[0]
∂g,2.1. Formal Definitions,[0],[0]
"(
y (l) i
)
∂y (l) i
(1)
where the two functions h(·) and g(·) are defined as
h(t) =
{
t for saliency map
σ(t) for DeconvNet and GBP
g(t) =
{
t for DeconvNet
σ(t) for saliency map and GBP
(2)",2.1. Formal Definitions,[0],[0]
"To be a good visualization method, a clean and visually human-interpretable result is very desirable.",2.2. Empirical Observations,[0],[0]
"More importantly, it should also reveal how the neural networks make decisions.",2.2. Empirical Observations,[0],[0]
"Based on this, we provide the empirical behaviors of the backpropagation-based visualizations for a pretrained VGG-16 net (Simonyan & Zisserman, 2014) in Figure 2.",2.2. Empirical Observations,[0],[0]
"Without loss of generality, the visualizations are obtained by choosing one of the class logits (i.e. the unnormalized class probability output right before the softmax function) as the output score to be taken derivative with respect to the input image.
",2.2. Empirical Observations,[0],[0]
"For the visual quality, saliency map is very noisy while DeconvNet and GBP produce human-interpretable visualizations with a subtle difference: DeconvNet unexpectedly produces some kind of texture-like pattern, and GBP is cleaner with some background information filtered out.",2.2. Empirical Observations,[0],[0]
"For the class-sensitivity, saliency map changes greatly for different class logits while DeconvNet and GBP are almost invariant to which class logit we choose.",2.2. Empirical Observations,[0],[0]
"This, together with more experiments, suggests that after introducing the backward ReLU, both DeconvNet and GBP modify the true gradient in a way that they create much cleaner results but their functionality as an indicator of important pixels to a specific class has disappeared.",2.2. Empirical Observations,[0],[0]
"In the next section, we will explain these empirical behaviors and discuss the reason why GBP and DeconvNet differ greatly from saliency map.",2.2. Empirical Observations,[0],[0]
"We first analyze the backpropagation-based methods in a three-layer CNN with random Gaussian weights, which is then extended to more complicated models such as CNNs with max-pooling and deep CNNs.",3. Theoretical Explanations,[0],[0]
"Besides, we also investigate their behaviors in well-trained CNNs.",3. Theoretical Explanations,[0],[0]
"Consider a three-layer CNN, consisting of an input layer and a convolutional hidden layer, followed by a ReLU activation function and a fully connected layer of which its output is called class logits.",3.1. A Random Three-Layer CNN,[0],[0]
"Formally, let x ∈ Rd be a normalized input image with dimension d and ‖x‖ = 1, and let W ∈ Rp×N be N convolutional filters where each column w(i) denotes the i-th filter with size p.",3.1. A Random Three-Layer CNN,[0],[0]
"Note that here we use vectors to represent images and filters for simplicity, and the analysis also works for the more practical two-dimensional case.",3.1. A Random Three-Layer CNN,[0],[0]
"Then, we let Y ∈",3.1. A Random Three-Layer CNN,[0],[0]
Rp×J,3.1. A Random Three-Layer CNN,[0],[0]
"be J image patches extracted from x, and each column y(j) with size p is generated by a linear function y(j) =",3.1. A Random Three-Layer CNN,[0],[0]
"Djx where Dj , [ 0p×(j−1)b Ip×p 0p×(d−(j−1)b−p) ] with b being the stride size2.",3.1. A Random Three-Layer CNN,[0],[0]
"For example, given a filter with size 3 and stride 1, the resulting j-th patch y(j) is made of the j-th to (j + 2)-th consecutive pixels.",3.1. A Random Three-Layer CNN,[0],[0]
The weights in the fullyconnected layer can be represented by V ∈ RNJ×K with K being the number of output logits.,3.1. A Random Three-Layer CNN,[0],[0]
"Therefore, the k-th logit is represented by
fk(x) =
N ∑
i=1
J ∑
j=1
Vqij ,kσ(w (i)T y(j))",3.1. A Random Three-Layer CNN,[0],[0]
"(3)
where the index qij denotes the ((i− 1)J + j)-th entry in every column vector of weight matrix V .
Assume every entry of V and W is sampled from an i.i.d.",3.1. A Random Three-Layer CNN,[0],[0]
"Gaussian distribution N (0, c2).",3.1. A Random Three-Layer CNN,[0],[0]
The following lemma provides the formula for backpropagation-based visualizations in a random three-layer CNN.,3.1. A Random Three-Layer CNN,[0],[0]
"Note that the norm of the final results will be in the range of [0, 1] as we apply the normalization during visualizations.
",3.1. A Random Three-Layer CNN,[0],[0]
Lemma 1.,3.1. A Random Three-Layer CNN,[0],[0]
"The backpropagation-based visualizations for the k-th logit in a random three-layer CNN is formalized as
sk(x) = 1
",3.1. A Random Three-Layer CNN,[0],[0]
"Zk
J ∑
j=1
Dj T
N ∑
i=1
h(Vqij ,k)w̃",3.1. A Random Three-Layer CNN,[0],[0]
"(i,j) (4)
where Zk is the normalization coefficient to ensure ‖sk(x)‖ ∈",3.1. A Random Three-Layer CNN,[0],[0]
"[0, 1], h(·) is given by Eq.",3.1. A Random Three-Layer CNN,[0],[0]
"(2) and
w̃(i,j) =
{
w(i) for DeconvNet w(i)I ( w(i)T y(j) )",3.1. A Random Three-Layer CNN,[0],[0]
"for saliency map and GBP
2Here we assume a VALID padding method implicitly, and other padding methods do not impact our analysis.
",3.1. A Random Three-Layer CNN,[0],[0]
Proof.,3.1. A Random Three-Layer CNN,[0],[0]
"See Appendix A.
Next, we can analyze the different behaviors of these backpropagation-based methods case by case.",3.1. A Random Three-Layer CNN,[0],[0]
"First, the behavior of GBP is given as follows.
Theorem 1.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"In a random three-layer CNN, if the number of filters N is sufficiently large, GBP at the k-th logit can be approximated as
sGBPk (x)",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"≈ x (5)
Proof.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"See Appendix B.
The above theorem shows that after introducing the backward ReLU, the input image can be approximately recovered by GBP in a random three-layer CNN, regardless of the class label.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"However, according to the linear model explanation, backpropagation-based methods are visualizing learned weights, which should be random noise as they are all sampled from i.i.d Gaussians.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"Obviously, it is inconsistent with the actual behavior of GBP.
",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"As the approximation in Eq. (5) builds on an assumption that the number of filters N is sufficiently large, a key question is: How many filters are needed to guarantee an accurate recovery?",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"From (Lugosi & Mendelson, 2017), we can set N =",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
Õ( p ǫ2 ),3.1.1. GUIDED BACKPROPAGATION,[0],[0]
such that with high probability ‖ 1 N ∑N i=1,3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"w̃ (i,j) − E[w̃(i,j)]‖",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"< ǫ, where p denotes the filter size and Õ(·) hides some other factors.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"As an upper bound, it reveals that the number of convolutional filters needed heavily depends on the filter size p.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"As the filter size intrinsically determined by the local connections in CNNs is usually small, we could use a mild number of convolutional filters to recover the input image.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"For example, given a filter size 3× 3× 3, we need at most O(103) filters to achieve an estimation error ǫ less than 0.1.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"This strongly suggests that GBP visualizations are human-interpretable in most of the CNNs, and thus the local connections property is another key factor underlying crisp visualizations.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"Here we show the behaviors of saliency map and DeconvNet in a random three-layer CNN are largely different from GBP.
",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
Theorem 2.,3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"In a random three-layer CNN, if the number of filters N is sufficiently large, saliency map and DeconvNet are approximated as Gaussian random variables satisfying
sSalk (x), s Deconv k (x) ∼ N (0, I)
",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
Proof.,3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"See Appendix C.
The above theorem shows that both saliency map and DeconvNet visualizations will yield random noise, conveying
little information about the input image and class logits.",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"For saliency map, it is easily understood since saliency map represents the true gradient of the class logit, which heavily depends on the weights.",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"For DeconvNet, although its behavior appears similar to saliency map in this simplistic scenario, we will show later on that it behaves more similarly to GBP, in particular with the existence of max-pooling.",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"In this section, we extend our analysis of a simple random three-layer CNN to other more realistic cases, including the max-pooling, deeper nets and trained weights.",3.2. Extensions to More Realistic Models,[0],[0]
"If we add a max-pooling layer between the ReLU and the fully-connected layer, the k-th logit becomes
fk(x) =
N ∑
i=1
J ∑
j=1
Vq̃ij ,kδ(σ(w (i)T y(j)))
where δ(·) denotes the max-pooling, which successively selects the maximum value in a fixed-size pooling window, and the new index q̃ij is the down-sampled version of qij .",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"Then the backpropagation-based visualizations for the k-th logit can be formulated as
sk(x) = 1
Zk
J ∑
j=1
Dj T
N ∑
i=1
h(δ′(oij)Vq̃ij ,k)w̃",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"(i,j) (6)
where oij , σ(w (i)T y(j)) is the output of each ReLU activation and δ′(oij) denotes the derivative of δ(·) evaluated at oij , which is
δ′(oij) =
{
1 if oij is chosen by max-pooling 0",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"otherwise
Since oij ≥ 0 with equality holds for w (i)T y(j) ≤ 0, given a proper pooling window size, it is highly possible that oij is chosen by the max-pooling if and only if w(i)T y(j)",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
> 0.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"It means with high probability, Eq. (6) is approximated as
sk(x)",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"≈ 1
Zk
J ∑
j=1
Dj T
N ∑
i=1
h(Vq̃ij ,k)w̃",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"(i,j) I(w(i)T y(j))
(7)
For saliency map and GBP, we know w̃(i,j)I(w(i)T y(j))",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"= w̃(i,j) and thus Eq.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
(7) is further reduced to Eq.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"(4), which means the behaviors of saliency map and GBP remain the same after introducing the max-pooling.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"However, with high probability, DeconvNet at the k-th logit becomes
sDeconvk (x)",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"≈ 1
Zk
J ∑
j=1
Dj T
N ∑
i=1
σ(Vq̃ij ,k)w (i) I(w(i)T y(j))
",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations
which is exactly the form of GBP in Eq.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
(4).,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"Therefore, adding the max-pooling makes the DeconvNet behave like GBP – doing nothing but image recovery.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"This also explains and extends the previous intuitive claims in (Samek et al., 2017; Odena et al., 2016) that the image-specific information in DeconvNet comes from the max-pooling.
",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
Note that that the approximation from Eq. (6) to Eq.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
(7) in DeconvNet with the max-pooling is essentially different from the approximations used in GBP.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"For GBP, the approximate gap can be made arbitrarily small by increasing the hidden layer size N , leading to a perfect recovery of the input.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"However, for DeconvNet, given any pooling window size, there might always exist at least one of the following two contradictory cases: it is possible that aij is chosen by the max-pooling if w(i)T y(j) ≤ 0, and also possible that aij is not chosen if w
(i)T y(j) >",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
0.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"This makes DeconvNet (with max-pooling), in theory, never recover input perfectly, which might explain why the unusual texture-like artifacts appear in the DeconvNet visualizations.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
The analysis for a three-layer CNN can be generalized to the multi-layer (or deeper) case.,3.2.2. DEEP CNNS,[0],[0]
"For clarity, we formulate the k-th logit of an L-layer deep CNN in a matrix form:
fk(x) = Γ (L)T k σ
( Γ(L−1)T · · ·σ ( Γ(1)Tx ))
where Γ(l) ∈ Rdl×dl+1 denotes either the convolutional or fully-connected operator matrix in the l-th layer and Γ (L) k is the k-th column of Γ (L).",3.2.2. DEEP CNNS,[0],[0]
"Denote by o(l) the output of ReLU activations in the l-th layer, i.e. o(l) = σ",3.2.2. DEEP CNNS,[0],[0]
"( Γ(l)T o(l−1) )
, ∀l ∈ {1, · · · , L−1} with o(0) , x.",3.2.2. DEEP CNNS,[0],[0]
"Then backpropagation-based visualizations at the k-th logit in an L-layer deep CNN can be formulated as
sk(x) = 1
Zk
∂õ(1)
∂x · h(V̂
(1) ·,k )
(a) =
1
Zk
J ∑
j=1
Dj T
N ∑
i=1
h(V̂ (1) qij ,k
)",3.2.2. DEEP CNNS,[0],[0]
"w̃(i,j) (8)
with ∀l ∈ {1, · · · , L− 1},
V̂ (l) ·,k =
∂õ(l+1)
∂o(l) ·",3.2.2. DEEP CNNS,[0],[0]
"h
(
∂õ(l+2) ∂o(l+1) · · ·h
(
∂õ(L−1)
∂o(L−2) h ( Γ (L) k )
))
where in (a) we rewrite sk(x) in an expanded form, õ (l) , g ( Γ(l)T o(l−1) )
, w(i) is the i-th filter encoded in Γ(1) and N is the number of filters in the first convolutional layer.",3.2.2. DEEP CNNS,[0],[0]
"Also, h(·), g(·) and w̃(i,j) are defined in Eq.",3.2.2. DEEP CNNS,[0],[0]
"(2) and Lemma 1.
",3.2.2. DEEP CNNS,[0],[0]
"First, the approximate property of V̂ (1) ·,k in the random deep CNN is given in the following proposition.
",3.2.2. DEEP CNNS,[0],[0]
"w(i)
y(j)
",3.2.2. DEEP CNNS,[0],[0]
Proposition 1.,3.2.2. DEEP CNNS,[0],[0]
For a random deep CNN where weights are i.i.d.,3.2.2. DEEP CNNS,[0],[0]
"Gaussians with zero mean, we can also approximate every entry of V̂ (1) ·,k as i.i.d.",3.2.2. DEEP CNNS,[0],[0]
"Gaussian with zero mean.
",3.2.2. DEEP CNNS,[0],[0]
Proof.,3.2.2. DEEP CNNS,[0],[0]
"See Appendix D.
Based on Proposition 1, we can see that the statistical properties of V̂ (1) qij ,k in Eq.",3.2.2. DEEP CNNS,[0],[0]
"(8) are approximately the same with those of Vqij ,k in Eq.",3.2.2. DEEP CNNS,[0],[0]
"(4), which means the analysis of backpropagation-based visualizations in a shallow threelayer CNN also applies to the deep CNN case.",3.2.2. DEEP CNNS,[0],[0]
"Therefore, the behaviors of these visualizations will barely change when increasing the depth of neural networks.",3.2.2. DEEP CNNS,[0],[0]
The previous analysis for random CNNs does not apply to the trained case directly since the weights here may not be i.i.d.,3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
Gaussian distributed.,3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"For saliency map, which uses the true gradient, the trained weights are likely to impose a stronger bias towards some specific subset of the input pixels, and so they can highlight class-relevant pixels rather than producing random noise.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"For GBP and DeconvNet, the analysis is a little more involved.
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"On the one hand, the trained weights w(i) will only lie in a small subspace of the whole image patch space which will create some “dead zones”, as illustrated in Figure 3 (a).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"That
is, all image patches lying in the “dead zone” will be filtered out by the forward ReLU.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"For example, it is well-known that the trained weights in the first convolutional layer are Gabor-like filters to detect the image patches containing edges (Yosinski et al., 2014; Zeiler & Fergus, 2014).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"That is, image patches without edges will probably be filtered out by the first convolutional layer.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Also, the higher convolutional layers keep filtering out more image patches with certain patterns (e.g. Figure 9).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"See the supplementary material for a comparison between GBP and a linear edge detector.
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"On the other hand, as shown in Figure 3 (b) and (c), the histograms of weights connected to the respective one of any two different neurons in the first fully connected layer (called “fc1”) of the trained VGG-16 net are very similar to each other.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Approximately, they form two very similar Gaussians with a small standard deviation, which means the (modified) gradients at any two different neurons in the layer “fc1” with respect to the input image are almost the same.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Namely, ∂õ (fc1)
∂x in Eq. (8) for GBP and DeconvNet
(with max-pooling) satisfies
∂õ (fc1) m
∂x ≈ Fconv(x), ∀m ∈ {1, · · · ,M}
where õ (fc1) m is the m-th entry of õ (fc1) and Fconv(·) :",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
R d → R d denotes the (normalized) overall filtering effect of the convolutional layers and M is the number of neurons in the layer “fc1”.,3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Thus, Eq. (8) for GBP and DeconvNet (with max-pooling) in the trained CNN can be approximated as
sk(x) = 1
Zk
∂õ(fc1)
∂x · h(V̂
(fc1) ·,k )
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"= 1
Zk
M ∑
m=1
∂õ (fc1) m
∂x · h(V̂
(fc1) m,k )
(a) ≈ Fconv(x)
(9)
where (a) follows from setting the normalization coefficient to be Zk = 1 ∑
M m=1 h(V̂ (fc1) m,k
) .
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"It shows that GBP and DeconvNet (with max-pooling) in a trained CNN are actually doing the partial image recovery, where the trained weights control which image patch could form an active path to the class logit.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"More importantly, this filtering process is not class sensitive (e.g. the edge detector).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"In the end, only these “active” image patches are combined in the first fully connected layer to form the final visualization results.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"As the right side of (9) does not depend on k, it illustrates why the GBP and DecovNet visualizations in the trained VGG are not class-sensitive.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully-
connected network (FCN) and a VGG-16 net.",4. Experiments,[0],[0]
"For a random network, their weights are all sampled from the truncated Gaussians with a zero-mean and standard deviation 0.1.",4. Experiments,[0],[0]
"Unless stated otherwise, the input is the image “tabby” from the ImageNet dataset (Deng et al., 2009) with size 224×224×3.",4. Experiments,[0],[0]
"See the supplementary materials for more results on other images and other neural network such as ResNet (He et al., 2016).",4. Experiments,[0],[0]
"In the three-layer CNN, the filter size is 7× 7× 3, the number of filters is N = 256, and the stride is 2.",4. Experiments,[0],[0]
"In the three-layer FCN, the hidden layer size is set to Nh = 4096.",4. Experiments,[0],[0]
"By default, the backpropagation-based visualizations are calculated with respect to the maximum class logit.",4. Experiments,[0],[0]
"Figure 4 shows the backpropagation-based visualizations on a random three-layer CNN and a random three-layer FCN, respectively.",4.1. Impact of Local Connections,[0],[0]
"We can see only GBP in the CNN can produce a human-interpretable visualization, while DeconvNet and saliency map in the CNN get random noise, which verifies our theoretical analysis in the section 3.1.",4.1. Impact of Local Connections,[0],[0]
"In contrast, as local connections do not exist in the FCN and the input size (e.g. 224 × 224 × 3) is extremely large, all the backpropagation-based methods (including GBP) in the FCN generate random noise.",4.1. Impact of Local Connections,[0],[0]
"Particularly for GBP, the number of hidden neurons Nh = 4096 is still not large enough to recover the image.
",4.1. Impact of Local Connections,[0],[0]
"To further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters N in the CNN and the number of hidden neurons Nh in the FCN, respectively, while keep other parameters fixed.",4.1. Impact of Local Connections,[0],[0]
The results are given in Figure 5.,4.1. Impact of Local Connections,[0],[0]
"Note that in the FCN, we have downsampled the input image to be of size 64 × 64 × 3 due to computational limitations.",4.1. Impact of Local Connections,[0],[0]
We can see that as the number of filters N increases (resp.,4.1. Impact of Local Connections,[0],[0]
"the hidden layer size Nh), the vi-
sual quality of GBP in the CNN (resp.",4.1. Impact of Local Connections,[0],[0]
in the FCN) becomes better.,4.1. Impact of Local Connections,[0],[0]
"Interestingly, even by setting Nh = 70000, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with N = 64.",4.1. Impact of Local Connections,[0],[0]
"Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP.",4.1. Impact of Local Connections,[0],[0]
"To show the impact of the max-pooling in backpropagationbased visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in Figure 6 (top row).",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"As compared with the visualizations in Figure 4 (top row), neither GBP or saliency map is impacted by the max-pooling, whereas the DeconvNet visualization has now become human interpretable instead of being the random noise as before.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"It confirms that the max-pooling is critical in helping DeconvNet produce human-interpretable visualizations via image recovery, as predicted by our theoretical analysis in the section 3.2.1.
",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"To show the impact of network depth, we also apply backpropagation-based visualizations in a random VGG-16 net, which also includes the max-pooling but is much deeper than the three-layer CNN.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
Figure 6 (bottom row) shows that only saliency map generates random noise while both GBP and DeconvNet could produce human-interpretable visualizations.,4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"Though there are subtle visual differences between the top row and bottom row of Figure 6, the behaviors of backpropagation-based methods are basically unchanged after increasing the network depth.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"In addition, both GBP and DeconvNet reconstruct every fine-grained detail of the input image in the random VGG , which is different from the trained VGG in Figure 2 where only those “active” image patches are preserved.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"To quantitatively describe how backpropagation-based visualizations change with respect to different class logits, we also provide the average l2 distance statistics as shown in Figure 7.",4.3. Average l2 Distance Statistics,[0],[0]
Our results are obtained by first calculating the l2 distance of two visualization results given two different class logits for each input image and then taking an average of those l2 distances based on 10K images from the ImageNet test set.,4.3. Average l2 Distance Statistics,[0],[0]
The process is repeated for all backpropagationbased methods in both random and trained cases.,4.3. Average l2 Distance Statistics,[0],[0]
"As we can see, the average l2 distance of saliency map is much larger than that of both GBP and DeconvNet in either a random VGG or a trained VGG, which clearly demonstrates that saliency map is class-sensitive but GBP and DeconvNet are not.",4.3. Average l2 Distance Statistics,[0],[0]
"Interestingly, in the trained VGG-16 net, the average l2 distance of DeconvNet is slightly larger than that of GBP.",4.3. Average l2 Distance Statistics,[0],[0]
It shows that the class insensitivity is exchanged for further improvement of visual quality.,4.3. Average l2 Distance Statistics,[0],[0]
Adversarial attack provides another way of directly testing whether visualizations are class-sensitive or doing image recovery.,4.4. Adversarial Attack on VGG,[0],[0]
"The class-sensitive visualizations should change drastically as both the predicted class label and ReLU states of intermediate layers have changed, while the visualizations doing image recovery should change little as only a tiny adversarial perturbation is added into the input image.",4.4. Adversarial Attack on VGG,[0],[0]
"In this experiment, we first generate an adversarial example “busby” via the fast gradient sign method (FGSM) (Goodfellow et al., 2014) by feeding the image “panda” into the pretrained VGG-16 net.",4.4. Adversarial Attack on VGG,[0],[0]
"Next, we apply the backpropagationbased visualizations to the original image “panda” and its adversary “busby” in the trained VGG-16 net.",4.4. Adversarial Attack on VGG,[0],[0]
"As shown in Figure 8, the saliency map visualization changes significantly whereas the GBP and DeconvNet visualizations remain almost unchanged after replacing “panda” by its adversary “busby”.",4.4. Adversarial Attack on VGG,[0],[0]
"Therefore, it further confirms that saliency map is class-sensitive in that it highlights important pixels in making classification decisions.",4.4. Adversarial Attack on VGG,[0],[0]
"However, GBP and DeconvNet are doing nothing but (partial) image recovery.",4.4. Adversarial Attack on VGG,[0],[0]
"There exist some differences for backpropagation-based visualizations, GBP and DeconvNet in particular, between the random and trained cases.",4.5. VGG with Partly Trained Weights,[0],[0]
"We take GBP as an example here to investigate the contributions of different layers in the trained VGG-16 net to these visual differences.
",4.5. VGG with Partly Trained Weights,[0],[0]
"First, to isolate the impact of later layers, we load the trained weights up to a given layer and leave later layers randomly initialized.",4.5. VGG with Partly Trained Weights,[0],[0]
"As shown in Figure 9 (top row), from “Conv11*” to “Conv5-1*” GBP keeps filtering out more image patches as the number of trained convolutional layers increases.",4.5. VGG with Partly Trained Weights,[0],[0]
"However, from “Conv5-1*” to “FC3*” (i.e., the
fully-trained case) GBP behaves almost the same, no matter weights in the dense layers are random or trained.",4.5. VGG with Partly Trained Weights,[0],[0]
"Therefore, it is the trained weights in the convolutional layers rather than those in the dense layers that account for filtering out image patches.",4.5. VGG with Partly Trained Weights,[0],[0]
"Also, it further confirms that GBP is class-insensitive.",4.5. VGG with Partly Trained Weights,[0],[0]
"Furthermore, to reveal the impact of each layer, we load the trained weights for the whole VGG-16 net except for a given layer which is randomly initialized instead.",4.5. VGG with Partly Trained Weights,[0],[0]
The results are shown in Figure 9 (bottom row).,4.5. VGG with Partly Trained Weights,[0],[0]
"We can see that the GBP visualization is blurry for “Conv1-1⋄”, clean with much background information for “Conv3-1⋄” and clean without background information for “Conv5-1⋄”, respectively.",4.5. VGG with Partly Trained Weights,[0],[0]
It means that the earlier convolutional layer has more important impact in the GBP visualization than the later convolutional layer.,4.5. VGG with Partly Trained Weights,[0],[0]
"In this paper, we proposed a theoretical explanation for backpropagation-based visualizations, where we started from a random three-layer CNN and later generalized it to more realistic cases.",5. Conclusions,[0],[0]
"We showed that unlike saliency map, both GBP and DeconvNet are essentially doing (partial) image recovery, which verified their class-insensitive properties.",5. Conclusions,[0],[0]
"We revealed that it is the backward ReLU, used by both GBP and DeconvNet, along with the local connections in CNNs, that is responsible for human-interpretable visualizations.",5. Conclusions,[0],[0]
We also explained how DeconvNet also relies on the max-pooling to recover the input.,5. Conclusions,[0],[0]
Our analysis was supported by extensive experiments.,5. Conclusions,[0],[0]
"Finally, we hope our analysis can provide useful insights into developing better visualization methods for deep neural networks.",5. Conclusions,[0],[0]
A future direction is to understand how the GBP visualizations in the trained CNNs filter out image patches layer by layer.,5. Conclusions,[0],[0]
Thanks to the anonymous reviewers for useful comments.,Acknowledgements,[0],[0]
"WN, YZ and AB were supported by IARPA via DoI/IBC contract D16PC00003.",Acknowledgements,[0],[0]
"Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map.",abstractText,[0],[0]
"Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions.",abstractText,[0],[0]
"Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations.",abstractText,[0],[0]
Extensive experiments are provided that support the theoretical analysis.,abstractText,[0],[0]
A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1127–1138 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1104",text,[0],[0]
"Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004).",1 Introduction,[0],[0]
"It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015).",1 Introduction,[0],[0]
"It has also proven useful for machine translation evaluation (Birch et al., 2016).",1 Introduction,[0],[0]
UCCA differs from syntactic schemes in terms of content and formal structure.,1 Introduction,[0],[0]
"It exhibits reentrancy,
discontinuous nodes and non-terminals, which no single existing parser supports.",1 Introduction,[0],[0]
"Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses.
",1 Introduction,[0],[0]
"We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA.",1 Introduction,[0],[0]
"Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016).",1 Introduction,[0],[0]
"We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokgöz and Eryiğit, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016).
",1 Introduction,[0],[0]
"We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings.",1 Introduction,[0],[0]
"To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees.",1 Introduction,[0],[0]
"Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1
The rest of the paper is structured as follows:
1All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa.
1127
Section 2 describes UCCA in more detail.",1 Introduction,[0],[0]
Section 3 introduces TUPA.,1 Introduction,[0],[0]
Section 4 discusses the data and experimental setup.,1 Introduction,[0],[0]
Section 5 presents the experimental results.,1 Introduction,[0],[0]
"Section 6 summarizes related work, and Section 7 concludes the paper.",1 Introduction,[0],[0]
"UCCA graphs are labeled, directed acyclic graphs (DAGs), whose leaves correspond to the tokens of the text.",2 The UCCA Scheme,[0],[0]
A node (or unit) corresponds to a terminal or to several terminals (not necessarily contiguous) viewed as a single entity according to semantic or cognitive considerations.,2 The UCCA Scheme,[0],[0]
"Edges bear a category, indicating the role of the sub-unit in the parent relation.",2 The UCCA Scheme,[0],[0]
"Figure 1 presents a few examples.
",2 The UCCA Scheme,[0],[0]
"UCCA is a multi-layered representation, where each layer corresponds to a “module” of semantic distinctions.",2 The UCCA Scheme,[0],[0]
"UCCA’s foundational layer, targeted in this paper, covers the predicate-argument structure evoked by predicates of all grammatical categories (verbal, nominal, adjectival and others), the inter-relations between them, and other major linguistic phenomena such as coordination and multi-word expressions.",2 The UCCA Scheme,[0],[0]
"The layer’s basic notion is the scene, describing a state, action, movement or some other relation that evolves in time.",2 The UCCA Scheme,[0],[0]
"Each scene contains one main relation (marked as either a Process or a State), as well as one or more Participants.",2 The UCCA Scheme,[0],[0]
"For example, the sentence “After graduation, John moved to Paris” (Figure 1a) contains two scenes, whose main relations are “graduation” and “moved”.",2 The UCCA Scheme,[0],[0]
"“John” is a Participant in both scenes, while “Paris” only in the latter.",2 The UCCA Scheme,[0],[0]
"Further categories account for inter-scene relations and the internal structure of complex arguments and relations (e.g. coordination, multi-word expressions and modification).
",2 The UCCA Scheme,[0],[0]
"One incoming edge for each non-root node is marked as primary, and the rest (mostly used for implicit relations and arguments) as remote edges, a distinction made by the annotator.",2 The UCCA Scheme,[0],[0]
"The primary edges thus form a tree structure, whereas the remote edges enable reentrancy, forming a DAG.
",2 The UCCA Scheme,[0],[0]
"While parsing technology in general, and transition-based parsing in particular, is wellestablished for syntactic parsing, UCCA has several distinct properties that distinguish it from syntactic representations, mostly UCCA’s tendency to abstract away from syntactic detail that do not affect argument structure.",2 The UCCA Scheme,[0],[0]
"For instance, consider the following examples where the concept of a scene
(a) After
L
graduation P
H
, U
John
A
moved
P
to R
Paris
C
A
H
A
(b) John
A
gave
C
everything up
C
P
A P process A participant H linked scene C center R relator N connector L scene linker U punctuation F function unit
(c)
John
C
and
N
Mary
C
’s
F
A
trip
P
home
A
Figure 1: UCCA structures demonstrating three structural properties exhibited by the scheme.",2 The UCCA Scheme,[0],[0]
"(a) includes a remote edge (dashed), resulting in “John” having two parents.",2 The UCCA Scheme,[0],[0]
(b) includes a discontinuous unit (“gave ... up”).,2 The UCCA Scheme,[0],[0]
(c) includes a coordination construction (“John and Mary”).,2 The UCCA Scheme,[0],[0]
Pre-terminal nodes are omitted for brevity.,2 The UCCA Scheme,[0],[0]
"Right: legend of edge labels.
has a different rationale from the syntactic concept of a clause.",2 The UCCA Scheme,[0],[0]
"First, non-verbal predicates in UCCA are represented like verbal ones, such as when they appear in copula clauses or noun phrases.",2 The UCCA Scheme,[0],[0]
"Indeed, in Figure 1a, “graduation” and “moved” are considered separate events, despite appearing in the same clause.",2 The UCCA Scheme,[0],[0]
"Second, in the same example, “John” is marked as a (remote) Participant in the graduation scene, despite not being overtly marked.",2 The UCCA Scheme,[0],[0]
"Third, consider the possessive construction in Figure 1c.",2 The UCCA Scheme,[0],[0]
"While in UCCA “trip” evokes a scene in which “John and Mary” is a Participant, a syntactic scheme would analyze this phrase similarly to “John and Mary’s shoes”.
",2 The UCCA Scheme,[0],[0]
"These examples demonstrate that a UCCA parser, and more generally semantic parsers, face an additional level of ambiguity compared to their syntactic counterparts (e.g., “after graduation” is formally very similar to “after 2pm”, which does not evoke a scene).",2 The UCCA Scheme,[0],[0]
"Section 6 discusses UCCA in the context of other semantic schemes, such as AMR (Banarescu et al., 2013).
",2 The UCCA Scheme,[0],[0]
"Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6).",2 The UCCA Scheme,[0],[0]
"One such property is reentrancy, namely the sharing of semantic units between predicates.",2 The UCCA Scheme,[0],[0]
"For instance, in Figure 1a, “John” is an argument of both “gradu-
ation” and “moved”, yielding a DAG rather than a tree.",2 The UCCA Scheme,[0],[0]
"A second property is discontinuity, as in Figure 1b, where “gave up” forms a discontinuous semantic unit.",2 The UCCA Scheme,[0],[0]
"Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al., 2014).",2 The UCCA Scheme,[0],[0]
"Finally, unlike most dependency schemes, UCCA uses non-terminal nodes to represent units comprising more than one word.",2 The UCCA Scheme,[0],[0]
"The use of non-terminal nodes is motivated by constructions with no clear head, including coordination structures (e.g., “John and Mary” in Figure 1c), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases (either the preposition or the head noun can serve as the constituent’s head).",2 The UCCA Scheme,[0],[0]
"To our knowledge, no existing parser supports all structural properties required for UCCA parsing.",2 The UCCA Scheme,[0],[0]
We now turn to presenting TUPA.,3 Transition-based UCCA Parsing,[0],[0]
"Building on previous work on parsing reentrancies, discontinuities and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties.
",3 Transition-based UCCA Parsing,[0],[0]
"Transition-based parsers (Nivre, 2003) scan the text from start to end, and create the parse incrementally by applying a transition at each step to the parser’s state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V,E, `) of constructed nodes and edges, where V is the set of nodes, E is the set of edges, and ` : E → L is the label function, L being the set of possible labels.",3 Transition-based UCCA Parsing,[0],[0]
"Some states are marked as terminal, meaning that G is the final output.",3 Transition-based UCCA Parsing,[0],[0]
A classifier is used at each step to select the next transition based on features encoding the parser’s current state.,3 Transition-based UCCA Parsing,[0],[0]
"During training, an oracle creates training instances for the classifier, based on gold-standard annotations.
",3 Transition-based UCCA Parsing,[0],[0]
Transition Set.,3 Transition-based UCCA Parsing,[0],[0]
"Given a sequence of tokens w1, . . .",3 Transition-based UCCA Parsing,[0],[0]
", wn, we predict a UCCA graph G over the sequence.",3 Transition-based UCCA Parsing,[0],[0]
"Parsing starts with a single node on the stack (an artificial root node), and the input tokens in the buffer.",3 Transition-based UCCA Parsing,[0],[0]
"Figure 2 shows the transition set.
",3 Transition-based UCCA Parsing,[0],[0]
"In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), adding the NODE transition for creating new non-terminal nodes.",3 Transition-based UCCA Parsing,[0],[0]
"For every X ∈ L, NODEX creates a new node on the buffer as a par-
ent of the first element on the stack, with an Xlabeled edge.",3 Transition-based UCCA Parsing,[0],[0]
"LEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively.",3 Transition-based UCCA Parsing,[0],[0]
"As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed if the child node already has an incoming primary edge.",3 Transition-based UCCA Parsing,[0],[0]
"LEFTREMOTEX and RIGHT-REMOTEX do not have this restriction, and the created edge is additionally marked as remote.",3 Transition-based UCCA Parsing,[0],[0]
We distinguish between these two pairs of transitions to allow the parser to create remote edges without the possibility of producing invalid graphs.,3 Transition-based UCCA Parsing,[0],[0]
"To support the prediction of multiple parents, node and edge transitions leave the stack unchanged, as in other work on transition-based dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokgöz and Eryiğit, 2015).",3 Transition-based UCCA Parsing,[0],[0]
"REDUCE pops the stack, to allow removing a node once all its edges have been created.",3 Transition-based UCCA Parsing,[0],[0]
"To handle discontinuous nodes, SWAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015).",3 Transition-based UCCA Parsing,[0],[0]
"Finally, FINISH pops the root node and marks the state as terminal.
Classifier.",3 Transition-based UCCA Parsing,[0],[0]
"The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016).",3 Transition-based UCCA Parsing,[0],[0]
"To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models.
1.",3 Transition-based UCCA Parsing,[0],[0]
"Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron algorithm (Collins and Roark, 2004) and MINUPDATE (Goldberg and Elhadad, 2011): each feature requires a minimum number of updates in training to be included in the model.2
2.",3 Transition-based UCCA Parsing,[0],[0]
"Changing the model to a feedforward neural network with dense embedding features, TUPAMLP (“multi-layer perceptron”), uses an architecture similar to that of Chen and Manning (2014), but with two rectified linear layers
2We also experimented with a linear model using dense embedding features, trained with the averaged structured perceptron algorithm.",3 Transition-based UCCA Parsing,[0],[0]
"It performed worse than the sparse perceptron model and was hence discarded.
instead of one layer with cube activation.",3 Transition-based UCCA Parsing,[0],[0]
"The embeddings and classifier are trained jointly.
3.",3 Transition-based UCCA Parsing,[0],[0]
"Finally, TUPABiLSTM uses a bidirectional LSTM for feature representation, on top of the dense embedding features, an architecture similar to Kiperwasser and Goldberg (2016).",3 Transition-based UCCA Parsing,[0],[0]
"The BiLSTM runs on the input tokens in forward and backward directions, yielding a vector representation that is then concatenated with dense features representing the parser state (e.g., existing edge labels and previous parser actions; see below).",3 Transition-based UCCA Parsing,[0],[0]
This representation is then fed into a feedforward network similar to TUPAMLP.,3 Transition-based UCCA Parsing,[0],[0]
"The feedforward layers, BiLSTM and embeddings are all trained jointly.
",3 Transition-based UCCA Parsing,[0],[0]
"For all classifiers, inference is performed greedily, i.e., without beam search.",3 Transition-based UCCA Parsing,[0],[0]
"Hyperparameters are tuned on the development set (see Section 4).
",3 Transition-based UCCA Parsing,[0],[0]
Features.,3 Transition-based UCCA Parsing,[0],[0]
"TUPASparse uses binary indicator features representing the words, POS tags, syntactic dependency labels and existing edge labels related to the top four stack elements and the next three buffer elements, in addition to their children and grandchildren in the graph.",3 Transition-based UCCA Parsing,[0],[0]
"We also use bi- and trigram features based on these values (Zhang and Clark, 2009; Zhu et al., 2013), features related to discontinuous nodes (Maier, 2015, including separating punctuation and gap type), features representing existing edges and the number of parents and children, as well as the past actions taken by the parser.",3 Transition-based UCCA Parsing,[0],[0]
"In addition, we use use a novel, UCCAspecific feature: number of remote children.3
For TUPAMLP and TUPABiLSTM, we replace all indicator features by a concatenation of the vector embeddings of all represented elements: words,
3See Appendix A for a full list of used feature templates.
",3 Transition-based UCCA Parsing,[0],[0]
"POS tags, syntactic dependency labels, edge labels, punctuation, gap type and parser actions.",3 Transition-based UCCA Parsing,[0],[0]
These embeddings are initialized randomly.,3 Transition-based UCCA Parsing,[0],[0]
"We additionally use external word embeddings initialized with pre-trained word2vec vectors (Mikolov et al., 2013),4 updated during training.",3 Transition-based UCCA Parsing,[0],[0]
"In addition to dropout between NN layers, we apply word dropout (Kiperwasser and Goldberg, 2016): with a certain probability, the embedding for a word is replaced with a zero vector.",3 Transition-based UCCA Parsing,[0],[0]
"We do not apply word dropout to the external word embeddings.
",3 Transition-based UCCA Parsing,[0],[0]
"Finally, for all classifiers we add a novel realvalued feature to the input vector, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too.
Training.",3 Transition-based UCCA Parsing,[0],[0]
"For training the transition classifiers, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state.",3 Transition-based UCCA Parsing,[0],[0]
"For example, the oracle would predict a NODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a RIGHT-EDGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created.",3 Transition-based UCCA Parsing,[0],[0]
"The transition predicted by the classifier is deemed correct and is applied to the parser state to reach the subsequent state, if the transition is included in the set of optimal transitions.",3 Transition-based UCCA Parsing,[0],[0]
"Otherwise, a random optimal transition is applied, and for the perceptronbased parser, the classifier’s weights are updated
4https://goo.gl/6ovEhC
according to the perceptron update rule.",3 Transition-based UCCA Parsing,[0],[0]
"POS tags and syntactic dependency labels are extracted using spaCy (Honnibal and Johnson, 2015).5",3 Transition-based UCCA Parsing,[0],[0]
"We use the categorical cross-entropy objective function and optimize the NN classifiers with the Adam optimizer (Kingma and Ba, 2014).",3 Transition-based UCCA Parsing,[0],[0]
Data.,4 Experimental Setup,[0],[0]
"We conduct our experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as outof-domain data.6 Table 1 presents some statistics for the two corpora.",4 Experimental Setup,[0],[0]
"We use passages of indices up to 676 of the Wiki corpus as our training set, passages 688–808 as development set, and passages 942–1028 as in-domain test set.",4 Experimental Setup,[0],[0]
"While
5https://spacy.io 6http://cs.huji.ac.il/˜oabend/ucca.html
UCCA edges can cross sentence boundaries, we adhere to the common practice in semantic parsing and train our parsers on individual sentences, discarding inter-relations between them (0.18% of the edges).",4 Experimental Setup,[0],[0]
We also discard linkage nodes and edges (as they often express inter-sentence relations and are thus mostly redundant when applied at the sentence level) as well as implicit nodes.7,4 Experimental Setup,[0],[0]
"In the out-of-domain experiments, we apply the same parsers (trained on the Wiki training set) to the 20K Leagues corpus without parameter re-tuning.
",4 Experimental Setup,[0],[0]
Implementation.,4 Experimental Setup,[0],[0]
"We use the DyNet package (Neubig et al., 2017) for implementing the NN classifiers.",4 Experimental Setup,[0],[0]
"Unless otherwise noted, we use the default values provided by the package.",4 Experimental Setup,[0],[0]
"See Appendix C for the hyperparameter values we found by tuning on the development set.
",4 Experimental Setup,[0],[0]
Evaluation.,4 Experimental Setup,[0],[0]
"We define a simple measure for comparing UCCA structures Gp = (Vp, Ep, `p) and Gg = (Vg, Eg, `g), the predicted and goldstandard graphs, respectively, over the same sequence of terminals W = {w1, . . .",4 Experimental Setup,[0],[0]
", wn}.",4 Experimental Setup,[0],[0]
"For an edge e = (u, v) in either graph, u being the parent and v the child, its yield y(e) ⊆ W is the set of terminals in W that are descendants of v. Define the set of mutual edges between Gp and Gg:
M(Gp, Gg) =
{(e1, e2) ∈",4 Experimental Setup,[0],[0]
Ep × Eg | y(e1),4 Experimental Setup,[0],[0]
= y(e2),4 Experimental Setup,[0],[0]
"∧ `p(e1) = `g(e2)}
Labeled precision and recall are defined by dividing |M(Gp, Gg)| by |Ep| and |Eg|, respectively, and F-score by taking their harmonic mean.
",4 Experimental Setup,[0],[0]
"7Appendix B further discusses linkage and implicit units.
",4 Experimental Setup,[0],[0]
"We report two variants of this measure: one where we consider only primary edges, and another for remote edges (see Section 2).",4 Experimental Setup,[0],[0]
"Performance on remote edges is of pivotal importance in this investigation, which focuses on extending the class of graphs supported by statistical parsers.
",4 Experimental Setup,[0],[0]
We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if Gp and Gg are trees.,4 Experimental Setup,[0],[0]
"Punctuation is excluded from the evaluation, but not from the datasets.
",4 Experimental Setup,[0],[0]
Comparison to bilexical graph parsers.,4 Experimental Setup,[0],[0]
"As no direct comparison with existing parsers is possible, we compare TUPA to bilexical dependency graph parsers, which support reentrancy and discontinuity but not non-terminal nodes.
",4 Experimental Setup,[0],[0]
"To facilitate the comparison, we convert our training set into bilexical graphs (see examples in Figure 4), train each of the parsers, and evaluate them by applying them to the test set and then reconstructing UCCA graphs, which are compared with the gold standard.",4 Experimental Setup,[0],[0]
"The conversion to bilexical graphs is done by heuristically selecting a head terminal for each non-terminal node, and attaching all terminal descendents to the head terminal.",4 Experimental Setup,[0],[0]
"In the inverse conversion, we traverse the bilexical graph in topological order, creating non-terminal parents for all terminals, and attaching them to the previously-created non-terminals corresponding to the bilexical heads.8
",4 Experimental Setup,[0],[0]
"In Section 5 we report the upper bounds on the achievable scores due to the error resulting from the removal of non-terminal nodes.
",4 Experimental Setup,[0],[0]
Comparison to tree parsers.,4 Experimental Setup,[0],[0]
"For completeness, and as parsing technology is considerably more
8See Appendix D for a detailed description of the conversion procedures.
mature for tree (rather than graph) parsing, we also perform a tree approximation experiment, converting UCCA to (bilexical) trees and evaluating constituency and dependency tree parsers on them (see examples in Figure 5).",4 Experimental Setup,[0],[0]
"Our approach is similar to the tree approximation approach used for dependency graph parsing (Agić et al., 2015; Fernández-González and Martins, 2015), where dependency graphs were converted into dependency trees and then parsed by dependency tree parsers.",4 Experimental Setup,[0],[0]
"In our setting, the conversion to trees consists simply of removing remote edges from the graph, and then to bilexical trees by applying the same procedure as for bilexical graphs.
",4 Experimental Setup,[0],[0]
Baseline parsers.,4 Experimental Setup,[0],[0]
"We evaluate two bilexical graph semantic dependency parsers: DAGParser (Ribeyre et al., 2014), the leading transition-based parser in SemEval 2014 (Oepen et al., 2014) and TurboParser (Almeida and Martins, 2015), a graph-based parser from SemEval 2015 (Oepen et al., 2015); UPARSE (Maier and Lichte, 2016), a transition-based constituency parser supporting discontinuous constituents; and two bilexical tree parsers: MaltParser (Nivre et al., 2007), and the stack LSTM-based parser of Dyer et al. (2015, henceforce “LSTM Parser”).",4 Experimental Setup,[0],[0]
"Default settings are used in all cases.9 DAGParser and UPARSE use beam search by default, with a beam size of 5 and 4 respectively.",4 Experimental Setup,[0],[0]
The other parsers are greedy.,4 Experimental Setup,[0],[0]
"Table 2 presents our main experimental results, as well as upper bounds for the baseline parsers, re-
9For MaltParser we use the ARCEAGER transition set and SVM classifier.",5 Results,[0],[0]
"Other configurations yielded lower scores.
",5 Results,[0],[0]
"flecting the error resulting from the conversion.10
DAGParser and UPARSE are most directly comparable to TUPASparse, as they also use a perceptron classifier with sparse features.",5 Results,[0],[0]
"TUPASparse considerably outperforms both, where DAGParser does not predict any remote edges in the out-ofdomain setting.",5 Results,[0],[0]
"TurboParser fares worse in this comparison, despite somewhat better results on remote edges.",5 Results,[0],[0]
"The LSTM parser of Dyer et al. (2015) obtains the highest primary F-score among the baseline parsers, with a considerable margin.
",5 Results,[0],[0]
"Using a feedforward NN and embedding features, TUPAMLP obtains higher scores than TUPASparse, but is outperformed by the LSTM parser on primary edges.",5 Results,[0],[0]
"However, using better input encoding allowing virtual look-ahead and look-behind in the token representation, TUPABiLSTM obtains substantially higher scores than TUPAMLP and all other parsers, on both primary and remote edges, both in the in-domain and out-of-domain settings.",5 Results,[0],[0]
"Its performance in absolute terms, of 73.5% F-score on primary edges, is encouraging in light of UCCA’s inter-annotator agreement of 80–85% F-score on them (Abend and Rappoport, 2013).
",5 Results,[0],[0]
"The parsers resulting from tree approximation 10The low upper bound for remote edges is partly due to the removal of implicit nodes (not supported in bilexical representations), where the whole sub-graph headed by such nodes, often containing remote edges, must be discarded.
are unable to recover any remote edges, as these are removed in the conversion.11 The bilexical DAG parsers are quite limited in this respect as well.",5 Results,[0],[0]
"While some of the DAG parsers’ difficulty can be attributed to the conversion upper bound of 58.3%, this in itself cannot account for their poor performance on remote edges, which is an order of magnitude lower than that of TUPABiLSTM.",5 Results,[0],[0]
"While earlier work on anchored12 semantic parsing has mostly concentrated on shallow semantic analysis, focusing on semantic role labeling of verbal argument structures, the focus has recently shifted to parsing of more elaborate representations that account for a wider range of phenomena (Abend and Rappoport, 2017).
",6 Related Work,[0],[0]
Grammar-Based Parsing.,6 Related Work,[0],[0]
"Linguistically expressive grammars such as HPSG (Pollard and Sag, 1994), CCG (Steedman, 2000) and TAG (Joshi and Schabes, 1997) provide a theory of the syntax-semantics interface, and have been used as a basis for semantic parsers by defining com-
11We also experimented with a simpler version of TUPA lacking REMOTE transitions, obtaining an increase of up to 2 labeled F-score points on primary edges, at the cost of not being able to predict remote edges.
",6 Related Work,[0],[0]
"12By anchored we mean that the semantic representation directly corresponds to the words and phrases of the text.
",6 Related Work,[0],[0]
"positional semantics on top of them (Flickinger, 2000; Bos, 2005, among others).",6 Related Work,[0],[0]
"Depending on the grammar and the implementation, such semantic parsers can support some or all of the structural properties UCCA exhibits.",6 Related Work,[0],[0]
"Nevertheless, this line of work differs from our approach in two important ways.",6 Related Work,[0],[0]
"First, the representations are different.",6 Related Work,[0],[0]
UCCA does not attempt to model the syntaxsemantics interface and is thus less coupled with syntax.,6 Related Work,[0],[0]
"Second, while grammar-based parsers explicitly model syntax, our approach directly models the relation between tokens and semantic structures, without explicit composition rules.
",6 Related Work,[0],[0]
Broad-Coverage Semantic Parsing.,6 Related Work,[0],[0]
"Most closely related to this work is Broad-Coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014, 2015).",6 Related Work,[0],[0]
"Like UCCA parsing, SDP addresses a wide range of semantic phenomena, and supports discontinuous units and reentrancy.",6 Related Work,[0],[0]
"In SDP, however, bilexical dependencies are used, and a head must be selected for every relation—even in constructions that have no clear head, such as coordination (Ivanova et al., 2012).",6 Related Work,[0],[0]
The use of non-terminal nodes is a simple way to avoid this liability.,6 Related Work,[0],[0]
"SDP also differs from UCCA in the type of distinctions it makes, which are more tightly coupled with syntactic considerations, where UCCA aims to capture purely semantic cross-linguistically applicable notions.",6 Related Work,[0],[0]
"For instance, the “poss” label in the DM target representation is used to annotate syntactic possessive constructions, regardless of whether they correspond to semantic ownership (e.g., “John’s dog”) or other semantic relations, such as marking an argument of a nominal predicate (e.g., “John’s kick”).",6 Related Work,[0],[0]
"UCCA reflects the difference between these constructions.
",6 Related Work,[0],[0]
"Recent interest in SDP has yielded numerous works on graph parsing (Ribeyre et al., 2014; Thomson et al., 2014; Almeida and Martins, 2015; Du et al., 2015), including tree approximation (Agić and Koller, 2014; Schluter et al., 2014) and joint syntactic/semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016).
",6 Related Work,[0],[0]
Abstract Meaning Representation.,6 Related Work,[0],[0]
"Another line of work addresses parsing into AMRs (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which, like UCCA, abstract away from syntactic distinctions and represent meaning directly, using OntoNotes predi-
cates (Weischedel et al., 2013).",6 Related Work,[0],[0]
"Events in AMR may also be evoked by non-verbal predicates, including possessive constructions.
",6 Related Work,[0],[0]
"Unlike in UCCA, the alignment between AMR concepts and the text is not explicitly marked.",6 Related Work,[0],[0]
"While sharing much of this work’s motivation, not anchoring the representation in the text complicates the parsing task, as it requires the alignment to be automatically (and imprecisely) detected.",6 Related Work,[0],[0]
"Indeed, despite considerable technical effort (Flanigan et al., 2014; Pourdamghani et al., 2014; Werling et al., 2015), concept identification is only about 80%–90% accurate.",6 Related Work,[0],[0]
"Furthermore, anchoring allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (Fernández-González and Martins, 2015; Birch et al., 2016).
",6 Related Work,[0],[0]
"Several transition-based AMR parsers have been proposed: CAMR assumes syntactically parsed input, processing dependency trees into AMR (Wang et al., 2015a,b, 2016; Goodman et al., 2016).",6 Related Work,[0],[0]
"In contrast, the parsers of Damonte et al. (2017) and Zhou et al. (2016) do not require syntactic pre-processing.",6 Related Work,[0],[0]
"Damonte et al. (2017) perform concept identification using a simple heuristic selecting the most frequent graph for each token, and Zhou et al. (2016) perform concept identification and parsing jointly.",6 Related Work,[0],[0]
UCCA parsing does not require separately aligning the input tokens to the graph.,6 Related Work,[0],[0]
"TUPA creates non-terminal units as part of the parsing process.
",6 Related Work,[0],[0]
"Furthermore, existing transition-based AMR parsers are not general DAG parsers.",6 Related Work,[0],[0]
"They are only able to predict a subset of reentrancies and discontinuities, as they may remove nodes before their parents have been predicted (Damonte et al., 2017).",6 Related Work,[0],[0]
"They are thus limited to a sub-class of AMRs in particular, and specifically cannot produce arbitrary DAG parses.",6 Related Work,[0],[0]
"TUPA’s transition set, on the other hand, allows general DAG parsing.13",6 Related Work,[0],[0]
"We present TUPA, the first parser for UCCA.",7 Conclusion,[0],[0]
"Evaluated in in-domain and out-of-domain settings, we show that coupled with a NN classifier and BiLSTM feature extractor, it accurately predicts UCCA graphs from text, outperforming a variety of strong baselines by a margin.
",7 Conclusion,[0],[0]
"Despite the recent diversity of semantic pars-
13See Appendix E for a proof sketch for the completeness of TUPA’s transition set.
ing work, the effectiveness of different approaches for structurally and semantically different schemes is not well-understood (Kuhlmann and Oepen, 2016).",7 Conclusion,[0],[0]
"Our contribution to this literature is a general parser that supports multiple parents, discontinuous units and non-terminal nodes.
",7 Conclusion,[0],[0]
"Future work will evaluate TUPA in a multilingual setting, assessing UCCA’s cross-linguistic applicability.",7 Conclusion,[0],[0]
"We will also apply the TUPA transition scheme to different target representations, including AMR and SDP, exploring the limits of its generality.",7 Conclusion,[0],[0]
"In addition, we will explore different conversion procedures (Kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation.
",7 Conclusion,[0],[0]
"A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016).",7 Conclusion,[0],[0]
"We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015).",7 Conclusion,[0],[0]
"This work was supported by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).",Acknowledgments,[0],[0]
The first author was supported by a fellowship from the Edmond and Lily Safra Center for Brain Sciences.,Acknowledgments,[0],[0]
"We thank Wolfgang Maier, Nathan Schneider, Elior Sulem and the anonymous reviewers for their helpful comments.",Acknowledgments,[0],[0]
"We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation.",abstractText,[0],[0]
"UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units.",abstractText,[0],[0]
"To our knowledge, the conjunction of these formal properties is not supported by any existing parser.",abstractText,[0],[0]
"Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.",abstractText,[0],[0]
A Transition-Based Directed Acyclic Graph Parser for UCCA,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4772–4777 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4772",text,[0],[0]
"Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of target words (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).",1 Introduction,[0],[0]
"While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information.",1 Introduction,[0],[0]
"Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei-
1Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch.
",1 Introduction,[0],[0]
"ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017).",1 Introduction,[0],[0]
"However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time.",1 Introduction,[0],[0]
"One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees.",1 Introduction,[0],[0]
"Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure trees, which have been used more widely in other works on syntax-based machine translation.",1 Introduction,[0],[0]
"One potential reason for the dearth of work on syntactic decoders is that such parse tree structures are not friendly to recurrent neural networks (RNNs).
",1 Introduction,[0],[0]
"In this paper, we propose TrDec, a method for incorporating tree structures in NMT.",1 Introduction,[0],[0]
"TrDec simultaneously generates a target-side tree topology and a translation, using the partially-generated tree to guide the translation process (§ 2).",1 Introduction,[0],[0]
"TrDec employs two RNNs: a rule RNN, which tracks the topology of the tree based on rules defined by a Context Free Grammar (CFG), and a word RNN, which tracks words at the leaves of the tree (§ 3).",1 Introduction,[0],[0]
"This model is similar to neural models of tree-structured data from syntactic and semantic parsing (Dyer et al., 2016; Alvarez-Melis and Jaakkola, 2017; Yin and Neubig, 2017), but with the addition of the word RNN, which is especially important for MT where fluency of transitions over the words is critical.
",1 Introduction,[0],[0]
TrDec can generate any tree structure that can be represented by a CFG.,1 Introduction,[0],[0]
"These structures include linguistically-motivated syntactic tree representations, e.g. constituent parse trees, as well as syntaxfree tree representations, e.g. balanced binary trees (§ 4).",1 Introduction,[0],[0]
"This flexibility of TrDec allows us to com-
pare and contrast different structural representations for NMT.
",1 Introduction,[0],[0]
"In our experiments (§ 5), we evaluate TrDec using both syntax-driven and syntax-free tree representations.",1 Introduction,[0],[0]
"We benchmark TrDec on three tasks: Japanese-English and German-English translation with medium-sized datasets, and Oromo-English translation with an extremely small dataset.",1 Introduction,[0],[0]
"Our findings are surprising – TrDec performs well, but it performs the best with balanced binary trees constructed without any linguistic guidance.",1 Introduction,[0],[0]
TrDec simultaneously generates the target sequence and its corresponding tree structure.,2 Generation Process,[0],[0]
"We first discuss the high-level generation process using an example, before describing the prediction model (§ 3) and the types of trees used by TrDec (§ 4).
",2 Generation Process,[0],[0]
Fig. 1 illustrates the generation process of the sentence “_The _cat _eat s _fi sh _.”,2 Generation Process,[0],[0]
", where the sentence is split into subword units, delimited by the underscore “_” (Sennrich et al., 2016).",2 Generation Process,[0],[0]
"The example uses a syntactic parse tree as the intermediate tree representation, but the process of generating with other tree representations, e.g. syntax-free trees, follows the same procedure.
",2 Generation Process,[0],[0]
"Trees used in TrDec have two types of nodes: terminal nodes, i.e. the leaf nodes that represent subword units; and nonterminal nodes, i.e. the nonleaf nodes that represent a span of subwords.",2 Generation Process,[0],[0]
"Additionally, we define a preterminal node to be a nonterminal node whose children are all terminal nodes.",2 Generation Process,[0],[0]
"In Fig. 1 Left, the green squares represent preterminal nodes.
",2 Generation Process,[0],[0]
"TrDec generates a tree in a top-down, left-toright order.",2 Generation Process,[0],[0]
"The generation process is guided by a CFG over target trees, which is constructed by taking all production rules extracted from the trees of all sentences in the training corpus.",2 Generation Process,[0],[0]
"Specifically, a rule RNN first generates the top of the tree structure, and continues until a preterminal is reached.",2 Generation Process,[0],[0]
"Then, a word RNN fills out the words under the preterminal.",2 Generation Process,[0],[0]
The model switches back to the rule RNN after the word RNN finishes.,2 Generation Process,[0],[0]
This process is illustrated in Fig. 1,2 Generation Process,[0],[0]
Right.,2 Generation Process,[0],[0]
Details are as follows: Step 1.,2 Generation Process,[0],[0]
"The source sentence is encoded by a sequential RNN encoder, producing the hidden states.",2 Generation Process,[0],[0]
Step 2.,2 Generation Process,[0],[0]
The generation starts with a derivation tree with only a Root node.,2 Generation Process,[0],[0]
"A rule RNN, initialized by the last encoder hidden state computes the probability distribution over all CFG rules whose left hand side (LHS) is Root, and selects a rule to apply to the derivation.",2 Generation Process,[0],[0]
"In our example, the rule RNN selects ROOT 7!",2 Generation Process,[0],[0]
S. Step 3.,2 Generation Process,[0],[0]
"The rule RNN applies production rules to the derivation in a top-down, left-to-right order, expanding the current opening nonterminal using a CFG rule whose LHS is the opening nonterminal.",2 Generation Process,[0],[0]
"In the next two steps, TrDec applies the rules S 7!",2 Generation Process,[0],[0]
NP VP PUNC and NP 7!,2 Generation Process,[0],[0]
"pre to the opening nonterminals S and NP, respectively.",2 Generation Process,[0],[0]
Note that after these two steps a preterminal node pre is created.,2 Generation Process,[0],[0]
Step 4a.,2 Generation Process,[0],[0]
"Upon seeing a preterminal node as the current opening nonterminal, TrDec switches to using a word RNN, initialized by the last state of the encoder, to populate this empty preterminal with phrase tokens, similar to a seq2seq decoder.",2 Generation Process,[0],[0]
"For example the subword units _The and _cat are generated by the word RNN, ending with a special end-of-phrase token, i.e. heopi.",2 Generation Process,[0],[0]
Step 4b.,2 Generation Process,[0],[0]
"While the word RNN generates subword units, the rule RNN also updates its own hidden states, as illustrated by the blue cells in Fig.",2 Generation Process,[0],[0]
1 Right.,2 Generation Process,[0],[0]
Step 5.,2 Generation Process,[0],[0]
"After the word RNN generates heopi, TrDec switches back to the rule RNN to continue generating the derivation from where the tree left
off.",2 Generation Process,[0],[0]
"In our example, this next stage is the opening nonterminal node VP.",2 Generation Process,[0],[0]
"From here, TrDec chooses the rule VP 7! pre NP.
",2 Generation Process,[0],[0]
"TrDec repeats the process above, intermingling the rule RNN and the word RNN as described, and halts when the rule RNN generates the end-ofsentence token heosi, completing the derivation.",2 Generation Process,[0],[0]
We now describe the computations during the generation process discussed in § 2.,3 Model,[0],[0]
"At first, a source sentence x, which is split into subwords, is encoded using a standard bi-directional Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997).",3 Model,[0],[0]
"This bi-directional LSTM outputs a set of hidden states, which TrDec will reference using an attention function (Bahdanau et al., 2015).
",3 Model,[0],[0]
"As discussed, TrDec uses two RNNs to generate a target parse tree.",3 Model,[0],[0]
"In our work, both of these RNNs use LSTMs, but with different parameters.
",3 Model,[0],[0]
Rule RNN.,3 Model,[0],[0]
"At any time step t in the rule RNN, there are two possible actions.",3 Model,[0],[0]
"If at the previous time step t 1, TrDec generated a CFG rule, then the state street is computed by:
s tree t = LSTM([y CFG t 1 ; ct 1; s tree p ; s word t ], s tree t 1)
where yCFGt 1 is the embedding of the CFG rule at time step t 1; ct 1 is the context vector computed by attention at street 1, i.e. input feeding (Luong et al., 2015); streep is the hidden state at the time step that generates the parent of the current node in the partial tree; swordt is the hidden state of the most recent time step before t that generated a subword (note that swordt comes from the word RNN, discussed below); and [·] denotes a concatenation.
",3 Model,[0],[0]
"Meanwhile, if at the previous time step t 1, TrDec did not generate a CFG rule, then the update at time step t must come from a subword being generated by the word RNN.",3 Model,[0],[0]
"In that case, we also update the rule RNN similarly by replacing the embedding of the CFG rule with the embedding of the subword.
",3 Model,[0],[0]
Word RNN.,3 Model,[0],[0]
"At any time step t, if the word RNN is invoked, its hidden state swordt is:
s word t = LSTM([s tree p ;wt 1; ct 1], s word t 1 ),
where streep is the hidden state of rule RNN that generated the CFG rule above the current terminal; wt 1 is the embedding of the word generated at time step t 1; and ct 1 is the attention context computed at the previous word RNN time step t 1.
Softmax.",3 Model,[0],[0]
"At any step t, our softmax logits are W · tanh",3 Model,[0],[0]
"[street , swordt ], where W varies depending on whether a rule or a subword unit is needed.",3 Model,[0],[0]
"Unlike prior work on syntactic decoders designed for utilizing a specific type of syntactic information (Wu et al., 2017), TrDec is a flexible NMT model that can utilize any tree structure.",4 Tree Structures,[0],[0]
"Here we consider two categories of tree structures:
Syntactic Trees are generated using a thirdparty parser, such as Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007).",4 Tree Structures,[0],[0]
Fig. 2,4 Tree Structures,[0],[0]
Top Left illustrates an example constituency parse tree.,4 Tree Structures,[0],[0]
"We also consider a variation of standard constituency parse trees where all of their nonterminal tags are replaced by a null tag, which is visualized in Fig. 2",4 Tree Structures,[0],[0]
Top Right.,4 Tree Structures,[0],[0]
"In addition to constituency parse trees, TrDec can also utilize dependency parse trees via a simple procedure that converts a dependency tree into a constituency tree.",4 Tree Structures,[0],[0]
"Specifically, this procedure creates a parent node with null tag for each word, and then attaches each word to the parent node of its head word while preserving the word order.",4 Tree Structures,[0],[0]
"An example of this procedure is provided in Fig. 3.
",4 Tree Structures,[0],[0]
Balanced Binary Trees are syntax-free trees constructed without any linguistic guidance.,4 Tree Structures,[0],[0]
We use two slightly different versions of binary trees.,4 Tree Structures,[0],[0]
Version 1 (Fig. 2 Bottom Left) is constructed by recursively splitting the target sentence in half and creating left and right subtrees from the left and right halves of the sentence respectively.,4 Tree Structures,[0],[0]
"Version 2 (Fig. 2 Bottom Right), is constructed by applying Version 1 on a list of nodes where consecutive words are combined together.",4 Tree Structures,[0],[0]
All tree nodes in both versions have the null tag.,4 Tree Structures,[0],[0]
"We discuss these construction processes in more detail in Appendix A.1.
",4 Tree Structures,[0],[0]
"In the experiments detailed later, we evaluated TrDec with four different settings of tree structures: 1) the fully syntactic constituency parse trees; 2) constituency parse trees with null tags; 3) dependency parse trees; 4) a concatenation of both version 1 and version 2 of the binary trees, (which effectively doubles the amount of the training data and leads to slight increases in accuracy).",4 Tree Structures,[0],[0]
Datasets.,5 Experiments,[0],[0]
"We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011), which consists of Japanese-English Wikipedia articles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset2, which largely consists of texts from the Bible.",5 Experiments,[0],[0]
Details are in Tab. 1.,5 Experiments,[0],[0]
"English sentences are parsed using Ckylark (Oda et al., 2015) for the constituency parse trees, and Stanford Parser (de Marneffe et al., 2006; Chen and Manning, 2014) for the dependency parse trees.",5 Experiments,[0],[0]
"We use byte-pair encoding (Sennrich et al., 2016) with 8K merge operations on ja-en, 4K merge operations on or-en, and 24K merge operations on de-en.
Baselines.",5 Experiments,[0],[0]
"We compare TrDec against three baselines: 1) seq2seq: the standard seq2seq model with attention; 2) CCG: a syntax-aware translation model that interleaves Combinatory Categorial Grammar (CCG) tags with words on the target side
2LDC2017E29
of a seq2seq model (Nadejde et al., 2017);",5 Experiments,[0],[0]
"3) CCGnull: the same model with CCG, but all syntactic tags are replaced by a null tag; and 4) LIN: a standard seq2seq model that generates linearized parse trees on the target side (Aharoni and Goldberg, 2017).
Results.",5 Experiments,[0],[0]
Tab. 2 presents the performance of our model and the three baselines.,5 Experiments,[0],[0]
"For our model, we report the performance of TrDec-con, TrDec-connull, TrDec-dep, and TrDec-binary (settings 1,2,3,4 in § 4).",5 Experiments,[0],[0]
"On the low-resource or-en dataset, we observe a large variance with different random seeds, so we run each model with 6 different seeds, and report the mean and standard deviation of these runs.",5 Experiments,[0],[0]
"TrDec-con-null and TrDec-con achieved comparable results, indicating that the syntactic labels have neither a large positive nor negative impact on TrDec.",5 Experiments,[0],[0]
"For ja-en and or-en, syntax-free TrDec outperforms all baselines.",5 Experiments,[0],[0]
"On de-en, TrDec loses to CCG-null, but the difference is not statistically significant (p > 0.1).
",5 Experiments,[0],[0]
Length Analysis.,5 Experiments,[0],[0]
"We performed a variety of analyses to elucidate the differences between the translations of different models, and the most conclusive results were through analysis based on the length of the translations.",5 Experiments,[0],[0]
"First, we categorize the ja-en test set into buckets by length of the reference sentences, and compare the models for each length category.",5 Experiments,[0],[0]
Fig. 4 shows the gains in BLEU score over seq2seq for the tree-based models.,5 Experiments,[0],[0]
"Since TrDec-con outperforms TrDec-dep for all datasets, we only focus on TrDec-con for analyzing TrDec’s performance with syntactic trees.",5 Experiments,[0],[0]
The relative performance of CCG decreases on long sentences.,5 Experiments,[0],[0]
"However, TrDec, with both parse trees and syntaxfree binary trees, delivers more improvement on longer sentences.",5 Experiments,[0],[0]
"This indicates that TrDec is bet-
ter at capturing long-term dependencies during decoding.",5 Experiments,[0],[0]
"Surprisingly, TrDec-binary, which does not utilize any linguistic information, outperforms TrDec-con for all sentence length categories.
",5 Experiments,[0],[0]
"Second, Fig. 5 shows a histogram of translations by the length difference between the generated output and the reference.",5 Experiments,[0],[0]
This provides an explanation of the difficulty of using parse trees.,5 Experiments,[0],[0]
"Ideally, this distribution will be focused around zero, indicating that the MT system is generating translations about the same length as the reference.",5 Experiments,[0],[0]
"However, the distribution of TrDec-con is more spread out than TrDec-binary, which indicates that it is more difficult for TrDec-con to generate sentences with appropriate target length.",5 Experiments,[0],[0]
"This is probably because constituency parse trees of sentences with similar number of words can have very different depth, and thus larger variance in the number of generation steps, likely making it difficult for the MT model to plan the sentence structure a-prior before actually generating the child sentences.",5 Experiments,[0],[0]
"We propose TrDec, a novel tree-based decoder for NMT, that generates translations along with the target side tree topology.",6 Conclusion,[0],[0]
"We evaluate TrDec on both linguistically-inspired parse trees and synthetic, syntax-free binary trees.",6 Conclusion,[0],[0]
"Our model, when used with synthetic balanced binary trees, outperforms CCG, the existing state-of-the-art in incorporating syntax in NMT models.
",6 Conclusion,[0],[0]
"The interesting result that syntax-free trees outperform their syntax-driven counterparts elicits a
natural question for future work: how do we better model syntactic structure in these models?",6 Conclusion,[0],[0]
It would also be interesting to study the effect of using source-side syntax together with the target-side syntax supported by TrDec.,6 Conclusion,[0],[0]
"This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No. HR0011-15-C0114, and the National Science Foundation under Grant No. 1815287.",Acknowledgements,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgements,[0],[0]
Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations.,abstractText,[0],[0]
"Most existing work utilizes some specific types of linguisticallyinspired tree structures, like constituency and dependency parse trees.",abstractText,[0],[0]
This is often done via a standard RNN decoder that operates on a linearized target tree structure.,abstractText,[0],[0]
"However, it is an open question of what specific linguistic formalism, if any, is the best structural representation for NMT.",abstractText,[0],[0]
"In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures.",abstractText,[0],[0]
"Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.1",abstractText,[0],[0]
A Tree-based Decoder for Neural Machine Translation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 184–188 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2029",text,[0],[0]
A typical document is usually organized in a coherent way that each text unit is relevant to its context and plays a role in the entire semantics.,1 Introduction,[0],[0]
"Text-level discourse analysis tries to identify such discourse structure of a document and its success can benefit many downstream tasks, such as sentiment analysis (Polanyi and van den Berg, 2011) and document summarization (Louis et al., 2010).
",1 Introduction,[0],[0]
"One most influential text-level discourse parsing theory is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), under which a text is parsed to a hierarchical discourse tree.",1 Introduction,[0],[0]
"The leaf nodes of this tree correspond to Elementary Discourse Units (EDUs, usually clauses) and then leaf nodes are recursively connected by rhetorical rela-
tions to form larger text spans until the final tree is built.",1 Introduction,[0],[0]
RST also depicts which part is more important in a relation by tagging Nucleus or Satellite.,1 Introduction,[0],[0]
"Generally, each relation at least includes a Nucleus and there are three nuclearity types: NucleusSatellite (NS), Satellite-Nucleus (SN) and NucleusNucleus (NN).",1 Introduction,[0],[0]
"Therefore, the performance of RST discourse parsing can be evaluated from three aspects: span, nuclearity and relation.
",1 Introduction,[0],[0]
"To parse discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis.",1 Introduction,[0],[0]
Marcu (1999) first employed a transition system to derive a discourse parse tree.,1 Introduction,[0],[0]
"In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions.",1 Introduction,[0],[0]
"More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015).",1 Introduction,[0],[0]
"Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014).
",1 Introduction,[0],[0]
"In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very few labelled text-level discourse corpus available for training.",1 Introduction,[0],[0]
The limited training data will cause unbalanced actions and lead to the problems of data sparsity and overfitting.,1 Introduction,[0],[0]
"Thus, we propose to use the transition-based model to parse a naked dis-
184
course tree (i.e., identifying span and nuclearity) in the first stage.",1 Introduction,[0],[0]
The benefits are three-fold.,1 Introduction,[0],[0]
"First, we can still use the transition based model which is a good tree construction tool.",1 Introduction,[0],[0]
"Second, much fewer actions need to be identified in the tree construction process.",1 Introduction,[0],[0]
"Third, we could separately label relations, which needs careful consideration.
",1 Introduction,[0],[0]
"In the second stage, relation labels for each span are determined independently.",1 Introduction,[0],[0]
"Prior studies (Joty et al., 2013; Feng and Hirst, 2014) have found that rhetorical relations distribute differently intra-sententially vs. multi-sententially.",1 Introduction,[0],[0]
They discriminate the two levels by training two models with different feature sets.,1 Introduction,[0],[0]
We take a further step and argue that relations between paragraphs are usually more loosely connected than those between sentences within the same paragraph.,1 Introduction,[0],[0]
"Therefore we train three separate classifiers for labeling relations at three levels: withinsentence, across-sentence and across-paragraph.",1 Introduction,[0],[0]
Different features are used for each classifier and the naked tree structure generated in the first stage is also leveraged as features.,1 Introduction,[0],[0]
Experiments on the RST-DT corpus demonstrate the effectiveness of our pipelined two-stage discourse parsing model.,1 Introduction,[0],[0]
Our discourse parsing process is composed of two stages: tree structure construction and relation labeling.,2 Our Method,[0],[0]
"In this work, we follow the convention to use the gold standard EDU segmentations and focus on building a tree with nuclearity and relation labels assigned for each inner node.",2 Our Method,[0],[0]
"In a typical transition-based system for discourse parsing, the parsing process is modeled as a sequence of shift and reduce actions, which are applied to a stack and a queue.",2.1 Tree Structure Construction,[0],[0]
The stack is initialized to be empty and the queue contains all EDUs in the document.,2.1 Tree Structure Construction,[0],[0]
"At each step, the parser performs either shift or reduce.",2.1 Tree Structure Construction,[0],[0]
"Shift pushes the first EDU in the queue to the top of the stack, while reduce pops and merges the top elements in the stack to get a new subtree, which is then pushed back to the top of the stack.",2.1 Tree Structure Construction,[0],[0]
A parse tree can be finally constructed until the queue is empty and the stack only contains the complete tree.,2.1 Tree Structure Construction,[0],[0]
"Only one classifier is learned to judge the actions at each step.
",2.1 Tree Structure Construction,[0],[0]
"To derive a discourse tree in a unified framework, prior systems design multiple reduce actions
with consideration of both nuclearity and relation types.",2.1 Tree Structure Construction,[0],[0]
"With 3 nuclearity types and 18 relation types, the number of reduce actions exceeds 40, leading to the data sparsity problem.
",2.1 Tree Structure Construction,[0],[0]
"In our parsing model, a transition-based system is responsible for building the naked tree without relation labels.",2.1 Tree Structure Construction,[0],[0]
"We only design four types of actions, including: Shift, Reduce-NN, Reduce-NS, Reduce-SN.",2.1 Tree Structure Construction,[0],[0]
"We identify span and nuclearity simultaneously in the transition-based tree construction, since nuclearity is actually closely related to the tree structure, just as the left-arc and rightarc action in dependency parsing.",2.1 Tree Structure Construction,[0],[0]
The number of the four actions on the training set of RST-DT corpus is shown in Table 1.,2.1 Tree Structure Construction,[0],[0]
"Though the four actions still have an unbalanced distribution, the relatively large number of occurrences assures that the classifier in our system can be trained more sufficiently.",2.1 Tree Structure Construction,[0],[0]
The most challenging subtask of discourse parsing is relation labeling.,2.2 Relation Labeling,[0],[0]
"In a binarized RST discourse tree, a relation label can be determined for each internal node, describing the relation between its left and right subtrees1.
",2.2 Relation Labeling,[0],[0]
We conduct relation labeling after the naked tree structure has been constructed.,2.2 Relation Labeling,[0],[0]
"On one hand, the naked tree structure can provide more information to support relation classification, verified in (Feng and Hirst, 2014).",2.2 Relation Labeling,[0],[0]
"For example, some relations tend to appear around the tree root while other relaitons would like to keep away from the root.",2.2 Relation Labeling,[0],[0]
"On the other hand, we can elaborately distinguish relations at different levels, including within-sentence, across-sentence, acrossparagraph.",2.2 Relation Labeling,[0],[0]
"We add across-paragraph level because some relations, like textual-organization and topic-change are observed to mainly occur between paragraphs.
",2.2 Relation Labeling,[0],[0]
"Therefore, we adopt three classifiers for labeling relations at different levels.",2.2 Relation Labeling,[0],[0]
"We first traverse the naked tree in post order and ignore leaf nodes, since we only need to judge relations for internal nodes.",2.2 Relation Labeling,[0],[0]
"Next, for each internal node, we determine
1Relation label is actually assigned to the satellite subtree and a “Span” label is assigned to the nucleus substree.
whether its left and right subtrees are in different paragraphs, or the same paragraph, or the same sentence.",2.2 Relation Labeling,[0],[0]
"For each level, we predict a relation label using the corresponding classifier.",2.2 Relation Labeling,[0],[0]
We use SVM classifiers for the four classification tasks (one action classifier and three relation classifiers).,2.3 Training,[0],[0]
We take the linear kernel for fast training and use squared hinge loss with L1 penalty on the error term.,2.3 Training,[0],[0]
"The penalty coefficient C is set to 1.
",2.3 Training,[0],[0]
The four classifiers are learned with offline training.,2.3 Training,[0],[0]
Training instances for the action classifier are generated by converting gold parse trees into a sequence of actions.,2.3 Training,[0],[0]
Then we extract features for each action before it is performed.,2.3 Training,[0],[0]
Training instances for relation classifiers are prepared by traversing the gold parse trees and extracting features for the relation of each internal node.,2.3 Training,[0],[0]
"This section details the features used in our model, which are a key to the four classifiers in discourse parsing.
",3 Features,[0],[0]
"For the action classifier, features are extracted from the top 2 elements S1, S2 in the stack and the first EDU Q1 in the queue.",3 Features,[0],[0]
"We design the feature sets for the action classifier as follows: • Status features: the previous action; number of
elements in the stack and queue.
",3 Features,[0],[0]
"• Position features: whether S1, S2 or S1, Q1 are in the same sentence or paragraph; whether they are start or end of a sentence, paragraph or document; distance from S1, S2, Q1 to the start and end of document.
",3 Features,[0],[0]
"• Structural features: nuclearity type (NN, NS or SN) of S1, S2; number of EDUs and sentences in S1, S2; length comparison of S1, S2 with respect to EDUs and sentences.
",3 Features,[0],[0]
"• Dependency features: whether dependency relations exist between S1, S2 or between S1, Q1; the dependency direction and relation type.
",3 Features,[0],[0]
"• N-gram features: the first and the last n words and their POS tags in the text of S1, S2, Q1, where n ∈ {1, 2}.
",3 Features,[0],[0]
"• Nucleus features: the dependency heads of the nucleus EDUs2 for S1, S2, Q1 and their POS tags; brown clusters (Brown et al., 1992; 2Nucleus EDU is defined by recursively selecting the Nucleus in the binary tree until an EDU (leaf node) is reached.
",3 Features,[0],[0]
"Turian et al., 2010) of all the words in the nucleus EDUs of S1, S2, Q1.",3 Features,[0],[0]
"Next, we list all the features used for the three relation classifiers.",3 Features,[0],[0]
"Given an internal node P in the naked tree, we aim to predict the relation between its left child Cleft and right child",3 Features,[0],[0]
Cright.,3 Features,[0],[0]
"Dependency features, N-gram features and nucleus features discussed above are also needed, the only difference is that these features are applied to the left and right children.",3 Features,[0],[0]
"Other features include: • Refined Structural features: nuclearity type
of node P ; distance from P , Cleft, Cright to the start and end of the document / paragraph / sentence with respect to paragraphs / sentences / EDUs; number of paragraphs / sentences / EDUs in Cleft and Cright; length comparison of Cleft and Cright with respect to paragraphs / sentences / EDUs.
",3 Features,[0],[0]
"• Tree features: depth and height of the node P in the tree; nuclearity type of P and P ’s grandparent node, if they exist.",3 Features,[0],[0]
This feature type benefits from our stagewise parsing method.,3 Features,[0],[0]
Relation labeling classifiers at different levels pick somewhat different features from all the features.,3 Features,[0],[0]
N-gram and structural features work for the three classifiers.,3 Features,[0],[0]
Dependency features are only used for within-sentence classifier.,3 Features,[0],[0]
Nucleus features and tree features are only used for acrosssentence and across-paragraph classifiers.,3 Features,[0],[0]
"We evaluate our parser on RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and thoroughly analyze different components of our method.",4 Experiments,[0],[0]
Results show our parsing model achieves state-of-the-art performance on the text-level discourse parsing task.,4 Experiments,[0],[0]
"RST-DT annotates 385 documents (347 for training and 38 for testing) from the Wall Street Journal using Rhetorical Structure Theory (Mann and Thompson, 1988).",4.1 Setup,[0],[0]
"Conventionally, we use 18 coarse-grained relations and binarize non-binary relations with right-branching (Sagae and Lavie, 2005).",4.1 Setup,[0],[0]
"For preprocessing, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to lemmatize words, get POS tags, segment sentences and syntactically parse them.
",4.1 Setup,[0],[0]
"To directly compare with other discourse parsing systems, we employ the same evaluation met-
rics, i.e. the precision, recall and F-score 3 with respect to span (S), nuclearity (N) and relation (R), as defined by Marcu (2000).",4.1 Setup,[0],[0]
"We compare our system against other stateof-the-art discourse parsers, shown in Table 2.",4.2 Results and Analysis,[0],[0]
"Among them, Joty et al. (2013), Li et al. (2014) and Li et al. (2016) all employ CKY-like algorithms to search global optimal parsing result.",4.2 Results and Analysis,[0],[0]
Ji and Eisenstein (2014) and Heilman and Sagae (2015) use transition-based parsing systems with improvements on the feature representation.,4.2 Results and Analysis,[0],[0]
"Feng and Hirst (2014) adopts a greedy approach that merges two adjacent spans at each step and two CRFs are used to predict the structure and the relation separately.
",4.2 Results and Analysis,[0],[0]
"From Table 2, we can see that our method outperforms all the others with respect to span and nuclearity, and exceeds most systems on relation labeling.",4.2 Results and Analysis,[0],[0]
"Especially, our method significantly outperforms other transition-based models (Ji and Eisenstein, 2014; Heilman and Sagae, 2015) on building the naked tree structure (span and nuclearity).",4.2 Results and Analysis,[0],[0]
This is mainly due to the proper design of actions in our transition-based system.,4.2 Results and Analysis,[0],[0]
"The reason that Ji and Eisenstein (2014) achieve a high score of relation labeling may be that their latent representations are more advantageous in capturing semantics, which will inspire us to refine our features in future work.
",4.2 Results and Analysis,[0],[0]
"To further explore the influence of different components in our model, we implement three simplified versions (i.e., Simp-1/2/3), as is shown in Table 3.",4.2 Results and Analysis,[0],[0]
"Stage means whether two-stage strat-
3Precision, recall and F-score are the same when manual segmentation is used.
",4.2 Results and Analysis,[0],[0]
"4The human agreement on the annotations of RST corpus
egy is adopted, Level denotes whether three kinds of relations (i.e., within-sentence, across-sentence, and across-paragraph) are differently classified, and Tree represents whether relation labeling uses tree features generated in the first stage.
",4.2 Results and Analysis,[0],[0]
"The simplest model Simp-1 is almost the same as (Heilman and Sagae, 2015) except that we employ more features.",4.2 Results and Analysis,[0],[0]
That Simp-1 has a high performance also means that transition-based method has potentials for constructing discourse trees.,4.2 Results and Analysis,[0],[0]
"Simp-2 adopts the two-stage strategy, but uses only one classifier to classify all the relations.",4.2 Results and Analysis,[0],[0]
"We can observe that the pipelined two stages bring a significant improvement with respect to all the aspects, compared to Simp-1.",4.2 Results and Analysis,[0],[0]
The difference between Simp-3 and Ours is that Simp-3 does not exploit the tree structure features generated in the first stage.,4.2 Results and Analysis,[0],[0]
We can see that the three-level relation classification and tree features together bring an improvement of about 1 percent on relation labeling.,4.2 Results and Analysis,[0],[0]
"Compared with prior work, this slight improvement is also valuable and more efficacious features need to be explored.
",4.2 Results and Analysis,[0],[0]
"Though the three-level relation labeling does not achieve prominent improvement, we get some interesting results via analyzing the performance on each relation.",4.2 Results and Analysis,[0],[0]
The Attribution and Same-Unit relations are the top 2 relations that we successfully classify with F-score as 0.87 and 0.83 respectively and over 90 percent of these two relations occur within sentences.,4.2 Results and Analysis,[0],[0]
This means that within-sentence relations are relatively easy to cope with.,4.2 Results and Analysis,[0],[0]
We also compare our final model with Simp-1 and results show that the TextualOrganization and Topic-Comment relaitons gain an increase by 20% and 8% respectively.,4.2 Results and Analysis,[0],[0]
"Most of the Textual-Organization and Topic-Comment relations are loosely across paragraphs and their numbers (i.e., 148 and 130 instances in training data) are also relatively small.",4.2 Results and Analysis,[0],[0]
We can see that our method can improve on predicting infrequent relations and partly solve the data sparsity problem.,4.2 Results and Analysis,[0],[0]
"At the same time, we infer that relations indeed belong to different levels and deserve fine treatment.",4.2 Results and Analysis,[0],[0]
"In this paper, we design a novel two-stage method for text-level discourse analysis.",5 Conclusion,[0],[0]
The first stage adopts the transition-based algorithm to construct naked trees with consideration of span and nuclearity.,5 Conclusion,[0],[0]
The second stage categorizes relations into three levels and uses three classifiers for relation labeling.,5 Conclusion,[0],[0]
"This pipelined design can mitigate the data sparsity problem in tree construction, and provide a new view of elaborately treating relations.",5 Conclusion,[0],[0]
Comprehensive experiments show the effectiveness of our proposed method.,5 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments on this paper.,Acknowledgments,[0],[0]
This work was partially supported by National Natural Science Foundation of China (61572049 and 61333018).,Acknowledgments,[0],[0]
The correspondence author of this paper is Sujian Li.,Acknowledgments,[0],[0]
"Previous work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and relation), but did not achieve satisfactory performance.",abstractText,[0],[0]
"In this paper, we propose that transition-based model is more appropriate for parsing the naked discourse tree (i.e., identifying span and nuclearity) due to data sparsity.",abstractText,[0],[0]
"At the same time, we argue that relation labeling can benefit from naked tree structure and should be treated elaborately with consideration of three kinds of relations including within-sentence, across-sentence and across-paragraph relations.",abstractText,[0],[0]
"Thus, we design a pipelined two-stage parsing method for generating an RST tree from text.",abstractText,[0],[0]
"Experimental results show that our method achieves state-of-the-art performance, especially on span and nuclearity identification.",abstractText,[0],[0]
A Two-Stage Parsing Method for Text-Level Discourse Analysis,title,[0],[0]
"Let def= {(p 1 , . . .",1.1. Symmetric distribution properties,[0],[0]
",p k ) :",1.1. Symmetric distribution properties,[0],[0]
"p i 0, Pk i=1",1.1. Symmetric distribution properties,[0],[0]
p,1.1. Symmetric distribution properties,[0],[0]
"i =1, 1  k  1} denote the collection of all discrete distributions over finite or infinite support.",1.1. Symmetric distribution properties,[0],[0]
A distribution property is a mapping f : !,1.1. Symmetric distribution properties,[0],[0]
"R. It is symmetric if it remains unchanged under relabeling of domain symbols, namely if it is determined by just the probability multiset {p
1 , p 2 , . . .",1.1. Symmetric distribution properties,[0],[0]
",p k }.",1.1. Symmetric distribution properties,[0],[0]
Many important properties are symmetric.,1.1. Symmetric distribution properties,[0],[0]
"For example:
Support size S(p) = |{x : p(x) > 0}|, plays an important role in population and vocabulary estimation.
",1.1. Symmetric distribution properties,[0],[0]
Support coverage S m (p),1.1. Symmetric distribution properties,[0],[0]
"= P
x (1 (1 p(x))m), the expected number of elements observed in m samples, arises in ecological and biological studies, e.g., (Colwell et al., 2012).
",1.1. Symmetric distribution properties,[0],[0]
"Shannon entropy H(p) = P
x p(x) log 1 p(x) , central to information theory (Cover & Thomas, 2006), has numerous
*Equal contribution 1Cornell University, Ithaca, NY 2Yahoo Inc!, Sunnyvale, CA 3University of California, San Diego 4Google Research.",1.1. Symmetric distribution properties,[0],[0]
Correspondence to:,1.1. Symmetric distribution properties,[0],[0]
"Jayadev Acharya <acharya@cornell.edu>, Hirakendu Das <hdas@yahooinc.com>, Alon Orlitsky <alon@ucsd.edu>, Ananda Theertha Suresh <theertha@google.com>.
",1.1. Symmetric distribution properties,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1.1. Symmetric distribution properties,[0],[0]
"Copyright 2017 by the author(s).
applications.
",1.1. Symmetric distribution properties,[0],[0]
Distance to uniform kp,1.1. Symmetric distribution properties,[0],[0]
"uk 1 = P
x |p(x) 1/|X ||, where u is the uniform distribution over the domain X of p.",1.1. Symmetric distribution properties,[0],[0]
"This distance measure appears in the error of hypothesis testing, and the uniform distribution is arguably one of the commonest discrete distributions.",1.1. Symmetric distribution properties,[0],[0]
"Considerable research, over many years, has focused on estimating distribution properties.",1.2. Distribution estimation,[0],[0]
"In the common setting, an unknown underlying distribution p 2 generates n independent samples",1.2. Distribution estimation,[0],[0]
"Xn def= X
1 , , . . .",1.2. Distribution estimation,[0],[0]
",X n , and the objective is to estimate a given property f(p) as accurately as possible.
",1.2. Distribution estimation,[0],[0]
"Specifically, an estimator for a distribution p over X is a function ˆf :",1.2. Distribution estimation,[0],[0]
Xn !,1.2. Distribution estimation,[0],[0]
R mapping observed samples to a property estimate.,1.2. Distribution estimation,[0],[0]
"The sample complexity of ˆf is the smallest number of samples it requires to estimate a property f with accuracy "" and confidence probability , for all distributions in a collection P ✓ ,
C ˆ",1.2. Distribution estimation,[0],[0]
f,1.2. Distribution estimation,[0],[0]
"(f,P, , "") def=",1.2. Distribution estimation,[0],[0]
"min n n : p(|f(p) ˆf(Xn)| "")  8p 2 P o .
",1.2. Distribution estimation,[0],[0]
"The sample complexity of estimating f is the lowest sample complexity of any estimator,
C⇤(f,P, , "") =",1.2. Distribution estimation,[0],[0]
"min ˆ
f
C ˆ",1.2. Distribution estimation,[0],[0]
f,1.2. Distribution estimation,[0],[0]
"(f,P, , "").
",1.2. Distribution estimation,[0],[0]
"By taking the median of about log 1 independent estimators, the error rate can be driven down from a constant to .",1.2. Distribution estimation,[0],[0]
"Therefore, the sample complexity depends on only through a factor of at most log 1
.",1.2. Distribution estimation,[0],[0]
"For simplicity, we therefore abbreviate C ˆf (f,P, 1/3, "") by C ˆf (f,P, "").",1.2. Distribution estimation,[0],[0]
"Recent research has shown that while simple estimators for the aforementioned properties require sample size n proportional to the support size k, more sophisticated techniques need only a sub-linear sample size n = ⇥(k/ log k).
",1.3. Result summary,[0],[0]
"However, each of the problems was approximated via different estimators and analysis techniques, that for some properties were rather complex.
",1.3. Result summary,[0],[0]
"Motivated by the principle of maximum likelihood, we show that a single, simple, plug-in estimator—profile maximum likelihood (PML) (Orlitsky et al., 2004b)— is competitive for estimating any symmetric property.",1.3. Result summary,[0],[0]
"Its sample complexity is at most quadratically worse than that of any estimator.
",1.3. Result summary,[0],[0]
"Specifically, we show that if a symmetric property can be estimated using n samples with confidence , then the PML plug-in estimator can estimate it using as many samples with confidence ·epn.",1.3. Result summary,[0],[0]
"While this increase may seem high, note that it is sub-exponential.",1.3. Result summary,[0],[0]
"We show that if a property has an estimator that has a small bounded difference constant (how much the estimator changes when we change one sample), then the error probability reduces exponentially with n",1.3. Result summary,[0],[0]
(Please see Section 7.1).,1.3. Result summary,[0],[0]
"Combined, these two facts imply that for properties with locally-smooth estimators, the PML plug-in estimator is optimal up to a constant: CPML = ⇥(C⇤).",1.3. Result summary,[0],[0]
"We then show that all the above properties have locally-smooth estimators, hence they can be estimated by the PML plug-in estimator with up to a constant factor more than the optimal number of samples.",1.3. Result summary,[0],[0]
The rest of the paper is organized as follows.,1.4. Outline,[0],[0]
In Section 2 we describe existing results and those shown in this paper.,1.4. Outline,[0],[0]
In Section 3 we formally define the quantities involved and state the results.,1.4. Outline,[0],[0]
In Section 4 we define profiles and PML.,1.4. Outline,[0],[0]
"In Section 5, we outline the new approach.",1.4. Outline,[0],[0]
"In Section 6, we demonstrate auxiliary results for maximum likelihood estimators.",1.4. Outline,[0],[0]
"In Section 7, we outline how we apply maximum likelihood to support, support coverage, entropy, and uniformity.",1.4. Outline,[0],[0]
"In Section 8, we provide the details for support, and support coverage and in the appendix we outline results for distance to uniformity and entropy.",1.4. Outline,[0],[0]
Plug-in estimation is a general approach for estimating distribution properties.,2.1. Previous Results,[0],[0]
"It uses the samples Xn to find an approximation p̂ of p, and lets f(p̂) estimate f(p).
",2.1. Previous Results,[0],[0]
"One of the most common distribution estimators, dating back to Fisher is maximum likelihood, that for clarity we call sequence maximum likelihood (SML) (Aldrich, 1997).",2.1. Previous Results,[0],[0]
To any sample xn,2.1. Previous Results,[0],[0]
it assigns the distribution p that maximizes p(xn).,2.1. Previous Results,[0],[0]
The SML estimate is exceedingly simple to derive.,2.1. Previous Results,[0],[0]
"The multiplicity N
x
def = N
x (xn) of symbol x is the number of times it appears in the sequence xn.",2.1. Previous Results,[0],[0]
"The empiri-
cal frequency estimator assigns to each symbol x, the fraction p̂(x) def=",2.1. Previous Results,[0],[0]
"N
x /n of times it appears in the sample xn.",2.1. Previous Results,[0],[0]
"For example, if x7 =bananas, empirical frequency would assign p̂(a) = 3/7, p̂(n) = 2/7, and p̂(b) = p̂(s) = 1/7.",2.1. Previous Results,[0],[0]
"It can be readily shown that SML is exactly the empirical frequency estimator.
",2.1. Previous Results,[0],[0]
"While the SML plug-in estimator performs well in the limit of many samples, sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties.
",2.1. Previous Results,[0],[0]
Support size.,2.1. Previous Results,[0],[0]
"With finitely many samples, S(p) cannot be estimated to any accuracy as many symbols with arbitrarily small probability may not be observed.",2.1. Previous Results,[0],[0]
"Motivated by databases, where each entry appears at least once, (Raskhodnikova et al., 2009) considered distributions whose non-zero probabilities are at least 1
k
,
1 k
def = {p 2 : p(x) 2 {0} [ [1/k, 1]} ,
and estimated the normalized support ˜S(p) def=",2.1. Previous Results,[0],[0]
S(p)/k.,2.1. Previous Results,[0],[0]
"It can be shown that CSML( ˜S(p), 1
k
, "") = ⇥(k log 1 "" ).",2.1. Previous Results,[0],[0]
"Yet (Valiant & Valiant, 2011a; Wu & Yang, 2015) showed that C⇤( ˜S(p), 1
k
, "") = ⇥ ⇣ k
log k · log2 1 ""
⌘
.
Support coverage.",2.1. Previous Results,[0],[0]
"Here too we consider the normalized coverage ˜S
m
(p) def = S
m (p)/m.",2.1. Previous Results,[0],[0]
"(Good & Toulmin, 1956) proposed the Good Toulmin (GT) estimator that achieves CGT( ˜S
m (p), , "") = m/2.",2.1. Previous Results,[0],[0]
"Recently, (Orlitsky et al., 2016) derived a simple estimator showing that C⇤( ˜S
m (p), , "") = ⇥( m logm · log 1 ""
).",2.1. Previous Results,[0],[0]
"(Zou et al., 2016) derived a more complex estimator with similar dependence on m but worse dependence on "".
Shannon entropy.",2.1. Previous Results,[0],[0]
"Since elements with arbitrarily small probability can contribute to an arbitrarily high entropy, H(p) cannot be estimated over aribtrary support with finitely many samples.",2.1. Previous Results,[0],[0]
"Therefore researchers are mostly interested in estimating entropy of distributions with support size at most k.
k
def = {p 2 : S(p)  k}.
",2.1. Previous Results,[0],[0]
"It can be shown that CSML(H(p), k
, "") = ⇥(k "" ) (Paninski, 2003).",2.1. Previous Results,[0],[0]
"Moreover, (Paninski, 2003) showed that C⇤(H(p),
k , "") is sublinear in k, (Valiant & Valiant, 2011a) showed that the optimal dependence on k is k/ log k and (Wu & Yang, 2016; Jiao et al., 2015) obtained the optimal dependence on both k, and "", and showed that C⇤(H(p),
k
, "") = ⇥ ⇣ k
log k · 1",2.1. Previous Results,[0],[0]
"""
⌘
.
Distance to uniform.",2.1. Previous Results,[0],[0]
"(Valiant & Valiant, 2011b) showed that C⇤(kp uk
1 , k
, "") =",2.1. Previous Results,[0],[0]
"O ⇣ k
log k · 1 "" 2
⌘
, and (Jiao et al., 2016) showed that this bound is tight.
",2.1. Previous Results,[0],[0]
"These results are summarized in Table 1.
",2.1. Previous Results,[0],[0]
Other properties were considered as well.,2.1. Previous Results,[0],[0]
"(Bar-Yossef et al., 2001; Acharya et al., 2015; Caferov et al., 2015; Obremski & Skorski, 2017) estimated Rényi entropy and (Bu et al., 2016) estimated KL divergence.",2.1. Previous Results,[0],[0]
"(Canonne, 2015) surveyed testing whether distributions have certain properties, and (Jiao et al., 2014) studied the performance of SML estimators for several properties.",2.1. Previous Results,[0],[0]
"Closest to this work in terms of approach and techniques are (Acharya et al., 2011; 2012; 2013a;b; Valiant & Valiant, 2013; Orlitsky & Suresh, 2015) that design algorithms whose sample complexity is provably close to the best possible regardless of the domain size.",2.1. Previous Results,[0],[0]
Symmetric distribution properties do not depend on the symbol labels.,2.2. Profile Maximum Likelihood,[0],[0]
They are determined by a simple sufficient statistic: the number of elements appearing any given number of times.,2.2. Profile Maximum Likelihood,[0],[0]
"The profile of a sequence Xn, denoted '(Xn) is the multiset of the multiplicities of all the symbols appearing in Xn.",2.2. Profile Maximum Likelihood,[0],[0]
"For example, '(a b r a c a d a b r a) = {1, 1, 2, 2, 5}, as two symbols appearing once, two appearing twice, and one symbol appearing five times, removing the association of the individual symbols with the multiplicities.",2.2. Profile Maximum Likelihood,[0],[0]
"Profiles are also referred to as histograms of histograms (Batu et al., 2000), histogram order statistics (Paninski, 2003), and fingerprints (Valiant & Valiant, 2011a).
",2.2. Profile Maximum Likelihood,[0],[0]
"Motivated by the principle of maximum likelihood, (Orlitsky et al., 2004b; 2017b) discarded the symbol labels, and considered the profile maximum likelihood (PML) distribution that maximizes the probability of the observed profile.
",2.2. Profile Maximum Likelihood,[0],[0]
A number of PML properties were established.,2.2. Profile Maximum Likelihood,[0],[0]
"(Orlitsky et al., 2004b; 2005) proved PML’s existence, consistency, and some of its properties.",2.2. Profile Maximum Likelihood,[0],[0]
"(Orlitsky et al., 2004d; 2005; Orlitsky & Pan, 2009; Pan et al., 2009) described additional properties and derived the PML distributions of several short and simple profiles.",2.2. Profile Maximum Likelihood,[0],[0]
"(Orlitsky et al., 2017b;c) provide a unified review of several of these results.",2.2. Profile Maximum Likelihood,[0],[0]
"(Anevski et al., 2013) contains a combination of previously-known and new results.",2.2. Profile Maximum Likelihood,[0],[0]
"A related distribution-estimation approach is described in (Orlitsky et al., 2004c; 2003).
",2.2. Profile Maximum Likelihood,[0],[0]
Several approaches were taken to computing the PML distribution.,2.2. Profile Maximum Likelihood,[0],[0]
"Algebraic computation was considered in (Acharya et al., 2010).",2.2. Profile Maximum Likelihood,[0],[0]
"A combination of the EM and MCMC algorithms have shown excellent results for calculating the PML distribution and applying it to support-size estimation (Orlitsky et al., 2004a; 2006; Pan, 2012) and a summary of some of the results appears in (Orlitsky et al., 2017a).",2.2. Profile Maximum Likelihood,[0],[0]
"(Vontobel, 2012; 2014) derived the Bethe approximation of these algorithms.
",2.2. Profile Maximum Likelihood,[0],[0]
"Following the first draft of this work, (Vatedka & Vontobel, 2016) showed that both theoretically and empirically plug-in estimators obtained from the PML estimate yield good estimates for symmetric functionals of Markov distributions.",2.2. Profile Maximum Likelihood,[0],[0]
"We show that replacing the SML plug-in estimator by PML yields a unified estimator that, like the best results shown via specialized techniques developed, is optimal.
",2.3. New Results,[0],[0]
Theorem 1.,2.3. New Results,[0],[0]
"There is a unified approach based on PML distribution that achieves the optimal sample complexity for the problems of estimating the entropy, support, support coverage, and distance to uniformity.
",2.3. New Results,[0],[0]
We prove in Corollary 1 that the PML approach is competitive with respect to any symmetric property.,2.3. New Results,[0],[0]
"For symmetric properties, these results are perhaps a justification of Fisher’s thoughts on Maximum Likelihood:
“Of course nobody has been able to prove that maximum likelihood estimates are best under all circumstances.",2.3. New Results,[0],[0]
Maximum likelihood estimates computed with all the information available may turn out to be inconsistent.,2.3. New Results,[0],[0]
"Throwing away a substantial part of the information may render them consistent.”
R. A. Fisher’s thoughts on Maximum Likelihood (Le Cam, 1979).
",2.3. New Results,[0],[0]
"To prove these PML guarantees, we establish two results that are of interest on their own right.
",2.3. New Results,[0],[0]
"• With n samples, PML estimates any symmetric property of p with essentially the same accuracy, and at most e3 p n times the error, of any other estimator.",2.3. New Results,[0],[0]
"This
follows by combining Theorem 3 with Lemma 1.
",2.3. New Results,[0],[0]
"• For a large class of symmetric properties, including all those mentioned above, if there is an estimator that uses n samples, and has an error probability 1/3, we design an estimator using O(n) samples, whose error probability is nearly exponential in n.",2.3. New Results,[0],[0]
We remark that this decay is much faster than applying the median trick.,2.3. New Results,[0],[0]
"This result follows by combining McDiarmid’s inequality with Lemma 2.
",2.3. New Results,[0],[0]
"Combined, these results prove that PML plug-in estimators are sample-optimal.
",2.3. New Results,[0],[0]
"We also introduce the notion of -approximate ML distributions, described in Definition 1.",2.3. New Results,[0],[0]
"These distributions are more relaxed version of PML, hence may be more easily computed, yet they provide essentially the same performance guarantees.",2.3. New Results,[0],[0]
"In the past, different sophisticated estimators were used for every property in Table 1.",3. Formal Definitions and Results,[0],[0]
"We show that the simple plug-in estimator that uses any PML approximation p̃, has optimal performance guarantees for all these properties.
",3. Formal Definitions and Results,[0],[0]
"In the next theorem, assume n is at least the optimal sample complexity of estimating entropy, support, support coverage, and distance to uniformity (given in Table 1) respectively.",3. Formal Definitions and Results,[0],[0]
Theorem 2.,3. Formal Definitions and Results,[0],[0]
"For all "" > c/n0.2, any plug-in exp ( pn)approximate PML p̃ satisfies,
Entropy
C p̃(H(p), k , "") ⇣ C⇤(H(p), k , ""),
Support size
C p̃(S(p)/k, 1 k , "") ⇣ C⇤(S(p)/k, 1 k , ""),
Support coverage
C p̃(S m (p)/m, , "") ⇣ C⇤(S m (p)/m, , ""),
Distance to uniformity
C p̃(kp uk 1 , X , "") ⇣ C⇤(kp uk1, k, "").",3. Formal Definitions and Results,[0],[0]
"For a sequence Xn, recall that the multilplicity N x is the number of times x appears in Xn.",4.1. Preliminaries,[0],[0]
"Discarding, the labels, profile of a sequence (Orlitsky et al., 2004b) is defined below.",4.1. Preliminaries,[0],[0]
Let n be all profiles of length-n sequences.,4.1. Preliminaries,[0],[0]
"Then,
4 = {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}.",4.1. Preliminaries,[0],[0]
"In particular, a profile of a length-n sequence is an unordered partition of n. Therefore, | n|, the number of profiles of length-n sequences is equal to the partition number of n. Then, by the Hardy-Ramanujan bounds on the partition number,
For a, b > 0, denote a . b or b & a if for some universal constant c, a/b  c. Denote a ⇣ b if both a . b and a & b.
Lemma 1 ((Hardy & Ramanujan, 1918)).",4.1. Preliminaries,[0],[0]
"| n|  exp(3 p n).
",4.1. Preliminaries,[0],[0]
"For a distribution p, the probability of a profile ' is defined as
p(') def =
X
X
n
:'(X
n
)='
p(Xn),
the probability of observing a sequence with profile '.",4.1. Preliminaries,[0],[0]
"Under i.i.d. sampling, p(')",4.1. Preliminaries,[0],[0]
"= P
X
n
:'(X
n
)='
Q
n
i=1
p(X i ).",4.1. Preliminaries,[0],[0]
"For example, the probability of observing a sequence with profile ' = {1, 2} is the probability of observing a sequence with one symbol appearing once, and one symbol appearing twice.",4.1. Preliminaries,[0],[0]
"A sequence with a symbol x appearing twice and y appearing once (e.g., x y x) has probability p(x)2p(y).",4.1. Preliminaries,[0],[0]
"Appropriately normalized, for any p, the probability of the profile {1, 2} is
p({1, 2})= X
'(X
n )",4.1. Preliminaries,[0],[0]
"={1,2}
n
Y
i=1
p(X i ) =
✓
3
1
◆
X
a 6=b2X p(a)2p(b),
(1)
where the normalization factor is independent of p.",4.1. Preliminaries,[0],[0]
The summation is a monomial symmetric polynomial in the probability values.,4.1. Preliminaries,[0],[0]
"See (Pan, 2012) for more examples.",4.1. Preliminaries,[0],[0]
Recall that p X n is the distribution maximizing the probability of Xn.,4.2. PML Estimation Scheme,[0],[0]
"Similarly, define (Orlitsky et al., 2004b):
p '
def = max
p2P p(')
as the distribution in P that maximizes the probability of observing a sequence with profile '.
",4.2. PML Estimation Scheme,[0],[0]
"For example, for ' = {1, 2}.",4.2. PML Estimation Scheme,[0],[0]
"For P = k , from (1),
p ' = arg max
p2 k
X a 6=b p(a)2p(b).
",4.2. PML Estimation Scheme,[0],[0]
"Note that in contrast, SML only maximizes one term of this expression.
",4.2. PML Estimation Scheme,[0],[0]
"We give two examples from the table in (Orlitsky et al., 2004b) to distinguish between SML and PML distributions,
and also show an instance where PML outputs distributions with a larger support than those appearing in the sample.",4.2. PML Estimation Scheme,[0],[0]
Example 1.,4.2. PML Estimation Scheme,[0],[0]
"Let X = {a, b, . . .",4.2. PML Estimation Scheme,[0],[0]
", z}.",4.2. PML Estimation Scheme,[0],[0]
"Suppose Xn = x y x, then the SML distribution is (2/3, 1/3).",4.2. PML Estimation Scheme,[0],[0]
"However, the distribution in that maximizes the probability of the profile '(x y x) = {1, 2} is (1/2, 1/2).",4.2. PML Estimation Scheme,[0],[0]
"Another example, illustrating the power of PML to predict new symbols is Xn = a b a c, with profile '(a b a c) = {1, 1, 2}.",4.2. PML Estimation Scheme,[0],[0]
"The SML distribution is (1/2, 1/4, 1/4), but the PML is a uniform distribution over 5 elements, namely (1/5, 1/5, 1/5, 1/5, 1/5).
",4.2. PML Estimation Scheme,[0],[0]
Suppose we want to estimate a symmetric property f(p) of an unknown distribution p 2 P given n independent samples.,4.2. PML Estimation Scheme,[0],[0]
"Our high level approach using PML is described below.
",4.2. PML Estimation Scheme,[0],[0]
"Input: Class of distributions P , symmetric function f(·), sample Xn
1.",4.2. PML Estimation Scheme,[0],[0]
"Compute p ' : argmax p2P p('(Xn)).
",4.2. PML Estimation Scheme,[0],[0]
2.,4.2. PML Estimation Scheme,[0],[0]
"Output f(p ' ).
",4.2. PML Estimation Scheme,[0],[0]
"There are a few advantages of this approach (as is true with any plug-in approach): (i) the computation of PML is agnostic to the function f at hand, (ii) there are no parameters to be tuned, (iii) techniques such as Poisson sampling or median tricks are not necessary, (iv) well motivated by the maximum-likelihood principle.
",4.2. PML Estimation Scheme,[0],[0]
"Comparison to the linear-programming plug-in estimator (Valiant & Valiant, 2011a).",4.2. PML Estimation Scheme,[0],[0]
"Our approach is perhaps closest in flavor to the plug-in estimator of (Valiant & Valiant, 2011a).",4.2. PML Estimation Scheme,[0],[0]
"Indeed, as mentioned in (Valiant, 2012), their linear-programming estimator is motivated by the question of estimating the PML.",4.2. PML Estimation Scheme,[0],[0]
"Their result was the first estimator to provide sample complexity bounds in terms of the alphabet size, and accuracy the problems of entropy and support estimation.",4.2. PML Estimation Scheme,[0],[0]
"Before we explain the differences of the two approaches, we briefly explain their approach.
",4.2. PML Estimation Scheme,[0],[0]
"Define, ' µ (Xn) to be the number of elements that appear µ times.",4.2. PML Estimation Scheme,[0],[0]
"For example, when Xn = a b r a c a d a b r a, ' 1 = 2,' 2 = 2, and ' 5
= 1. (Valiant & Valiant, 2011a) design a linear program that uses SML for high values of µ, and formulate a linear program to find a distribution for which E['
µ ]’s are close to the observed ' µ",4.2. PML Estimation Scheme,[0],[0]
’s. They then plug-in this estimate to estimate the property.,4.2. PML Estimation Scheme,[0],[0]
"On the other hand, our approach, by the nature of ML principle, tries to find the distribution that best explains the entire profile of the observed data, not just some partial characteristics.",4.2. PML Estimation Scheme,[0],[0]
"It therefore has the potential to estimate any symmetric property and estimate the distribution closely in any distance measures, competitive with the best possible.",4.2. PML Estimation Scheme,[0],[0]
"For example, the guarantees of the linear program approach are suboptimal in terms of the desired accuracy "".",4.2. PML Estimation Scheme,[0],[0]
"For entropy
estimation the optimal dependence is 1 "" , whereas (Valiant & Valiant, 2011a) yields 1
"" 2 .",4.2. PML Estimation Scheme,[0],[0]
"This is more prominent for support size and support coverage, which have optimal dependence of polylog( 1
"" ), whereas (Valiant & Valiant, 2011a) gives a 1
"" 2 dependence.",4.2. PML Estimation Scheme,[0],[0]
"Besides, we analyze the first method proposed for estimating symmetric properties, designed from the first principles, and show that in fact it is competitive with the optimal estimators for various problems.",4.2. PML Estimation Scheme,[0],[0]
Our arguments have two components.,5. Proof Outline,[0],[0]
"In Section 6 we prove a general result for the performance of plug-in estimation via maximum likelihood approaches.
",5. Proof Outline,[0],[0]
"Let P be a class of distributions over Z , and f : P !",5. Proof Outline,[0],[0]
R be a function.,5. Proof Outline,[0],[0]
"For z 2 Z , let
p z
def = argmax
p2P p(z)
be the maximum-likelihood estimator of z in P .",5. Proof Outline,[0],[0]
"Upon observing z, f(p
z ) is the ML estimator of f .",5. Proof Outline,[0],[0]
"In Theorem 4, we show that if there is an estimator that achieves error probability , then the ML estimator has an error probability at most |Z|.",5. Proof Outline,[0],[0]
"We note that variations of this result in the asymptotic statistics were studied before (see (Lehmann & Casella, 1998)).",5. Proof Outline,[0],[0]
"Our contribution is to use these results in the context of symmetric properties and show sample complexity bounds in the non-asymptotic regime.
",5. Proof Outline,[0],[0]
"We emphasize that, throughout this paper Z will be the set of profiles of length n, and P will be distributions induced over profiles by length-n i.i.d. samples.",5. Proof Outline,[0],[0]
"Therefore, we have |Z| = | n|.",5. Proof Outline,[0],[0]
"By Lemma 1, if there is a profile based estimator with error probability , then the PML approach will have error probability at most exp(3 p n).",5. Proof Outline,[0],[0]
"Such arguments were used in hypothesis testing to show the existence of competitive testing algorithms for fundamental statistical problems (Acharya et al., 2011; 2012).
",5. Proof Outline,[0],[0]
At its face value this seems like a weak result.,5. Proof Outline,[0],[0]
"Our second key step is to prove that for the properties we are interested, it is possible to obtain very sharp guarantees.",5. Proof Outline,[0],[0]
"For example, we show that if we can estimate the entropy to an accuracy ±"" with error probability 1/3 using n samples, then we can estimate the entropy to accuracy ±2"" with error probability exp( n0.9) using only 2n samples.",5. Proof Outline,[0],[0]
"Using this sharp concentration, the new error probability term dominates | n|, and we obtain our results.",5. Proof Outline,[0],[0]
The arguments for sharp concentration are based on modifications to existing estimators and a new analysis.,5. Proof Outline,[0],[0]
Most of these results are technical and are in the appendix.,5. Proof Outline,[0],[0]
We establish performance guarantees of ML property estimation in a general set-up.,6. Maximum Likelihood Property Estimation,[0],[0]
"Recall that P is a collection of distributions over Z , and f : P !",6. Maximum Likelihood Property Estimation,[0],[0]
R.,6. Maximum Likelihood Property Estimation,[0],[0]
"Given a sample Z from an unknown p 2 P , we want to estimate f(p).",6. Maximum Likelihood Property Estimation,[0],[0]
"The maximum likelihood approach is the following twostep procedure.
",6. Maximum Likelihood Property Estimation,[0],[0]
1.,6. Maximum Likelihood Property Estimation,[0],[0]
"Find p Z = argmax p2P p(Z).
2.",6. Maximum Likelihood Property Estimation,[0],[0]
"Output f(p Z ).
",6. Maximum Likelihood Property Estimation,[0],[0]
We bound the performance of this approach in the following theorem.,6. Maximum Likelihood Property Estimation,[0],[0]
Theorem 3.,6. Maximum Likelihood Property Estimation,[0],[0]
Suppose there is an estimator ˆf : Z !,6. Maximum Likelihood Property Estimation,[0],[0]
"R, such that for any p, and Z ⇠ p,
Pr
⇣
f(p) ˆf(Z)",6. Maximum Likelihood Property Estimation,[0],[0]
"> "" ⌘ < , (2)
then
Pr (|f(p) f(p",6. Maximum Likelihood Property Estimation,[0],[0]
Z ),6. Maximum Likelihood Property Estimation,[0],[0]
| > 2,6. Maximum Likelihood Property Estimation,[0],[0]
""")  · |Z| .",6. Maximum Likelihood Property Estimation,[0],[0]
"(3)
Proof.",6. Maximum Likelihood Property Estimation,[0],[0]
Consider symbols with p(z) and p(z) < separately.,6. Maximum Likelihood Property Estimation,[0],[0]
A distribution p with p(z) outputs z with probability at least .,6. Maximum Likelihood Property Estimation,[0],[0]
"For (2) to hold, we must have,
f(p) ˆf(z) <",6. Maximum Likelihood Property Estimation,[0],[0]
""".",6. Maximum Likelihood Property Estimation,[0],[0]
"By the definition of ML, p z (z) p(z) , and for (2) to hold for p
z
, f(p z ) ˆf(z) < "".",6. Maximum Likelihood Property Estimation,[0],[0]
"By the triangle inequality, for all such z,
|f(p) f(p z )|  f(p) ˆf(z) + f(p z ) ˆf(z)  2"".",6. Maximum Likelihood Property Estimation,[0],[0]
"Thus if p(z) , then PML satisfies the required guarantee with zero probability of error, and any error occurs only when p(z) < .",6. Maximum Likelihood Property Estimation,[0],[0]
We bound this probability as follows.,6. Maximum Likelihood Property Estimation,[0],[0]
"When Z ⇠ p,
Pr (p(Z) < )  ",6. Maximum Likelihood Property Estimation,[0],[0]
"X
z2Z:p(z)<
p(z) < · |Z| .
",6. Maximum Likelihood Property Estimation,[0],[0]
"For some problems, it might be easier to just approximate the ML, instead of finding it exactly.",6. Maximum Likelihood Property Estimation,[0],[0]
We define an approximation ML as follows: Definition 1 ( -approximate ML).,6. Maximum Likelihood Property Estimation,[0],[0]
Let  1.,6. Maximum Likelihood Property Estimation,[0],[0]
"For Z 2 Z , p̃ Z 2 P is a -approximate ML distribution if p̃ z
(z) ·",6. Maximum Likelihood Property Estimation,[0],[0]
"p
z (z).",6. Maximum Likelihood Property Estimation,[0],[0]
"When Z is profiles of length-n, a -approximate PML is a distribution p̃
' such that p̃ ' (') ·",6. Maximum Likelihood Property Estimation,[0],[0]
"p ' (').
",6. Maximum Likelihood Property Estimation,[0],[0]
The next result proves guarantees for any -approximate ML estimator.,6. Maximum Likelihood Property Estimation,[0],[0]
Theorem 4.,6. Maximum Likelihood Property Estimation,[0],[0]
Suppose there exists an estimator satisfying (2).,6. Maximum Likelihood Property Estimation,[0],[0]
"For any p 2 P and Z ⇠ p, any -approximate ML p̃
Z
satisfies:
Pr (|f(p) f(p̃ Z )",6. Maximum Likelihood Property Estimation,[0],[0]
| > 2,6. Maximum Likelihood Property Estimation,[0],[0]
""")  · |Z|/ .
",6. Maximum Likelihood Property Estimation,[0],[0]
The proof is very similar to the previous theorem and is presented in the Appendix B.,6. Maximum Likelihood Property Estimation,[0],[0]
"Suppose for a property f(p), there is an estimator with sample complexity n that achieves an accuracy ±"" with probability of error at most 1/3.",6.1. Competitiveness of ML via Median Trick,[0],[0]
The standard method to boost the error probability is the median trick: (i) Obtain O(log(1/ )),6.1. Competitiveness of ML via Median Trick,[0],[0]
independent estimates using O(n log(1/ )) independent samples.,6.1. Competitiveness of ML via Median Trick,[0],[0]
(ii) Output the median of these estimates.,6.1. Competitiveness of ML via Median Trick,[0],[0]
"This is an ""-accurate estimator of f(p) with error probability at most .",6.1. Competitiveness of ML via Median Trick,[0],[0]
"By definition, estimators are a mapping from the samples to R. However, in many applications the estimators map from a much smaller (some sufficient statistic) of the samples.",6.1. Competitiveness of ML via Median Trick,[0],[0]
"Denote by Z
n the space consisting of all sufficient statistics that the estimator uses.",6.1. Competitiveness of ML via Median Trick,[0],[0]
"For example, estimators for symmetric properties, such as entropy typically use the profile of the sequence, and hence Z n =
n. Using the median-trick, we get the following result.
",6.1. Competitiveness of ML via Median Trick,[0],[0]
Corollary 1.,6.1. Competitiveness of ML via Median Trick,[0],[0]
Let ˆf : Z n !,6.1. Competitiveness of ML via Median Trick,[0],[0]
"R be an estimator of f(p) with accuracy "" and error-probability 1/3.",6.1. Competitiveness of ML via Median Trick,[0],[0]
"The ML estimator achieves accuracy 2"" with probability at least 2/3 using
min
⇢ n0 :",6.1. Competitiveness of ML via Median Trick,[0],[0]
"n0
20 log(3Z n 0 )
> n samples.
",6.1. Competitiveness of ML via Median Trick,[0],[0]
Proof.,6.1. Competitiveness of ML via Median Trick,[0],[0]
"Since n is the number of samples to get error probability 1/3, by the Chernoff bound, the error after n0 samples is at most exp( (n0/(20n))).",6.1. Competitiveness of ML via Median Trick,[0],[0]
"Therefore, the error probability of the ML estimator for accuracy 2"" is at most exp( (n0/(20n)))Z
n 0 , which we desire to be at most 1/3.
",6.1. Competitiveness of ML via Median Trick,[0],[0]
"For estimators that use the profile of sequences, | n| < exp(3 p n).",6.1. Competitiveness of ML via Median Trick,[0],[0]
Plugging this in the previous result shows that the PML based approach has a sample complexity of at most O(n2).,6.1. Competitiveness of ML via Median Trick,[0],[0]
"This result holds for all symmetric properties, independent of "", and the alphabet size k.",6.1. Competitiveness of ML via Median Trick,[0],[0]
"For the problems mentioned earlier, something much better in possible, namely the PML approach is optimal up to constant factors.",6.1. Competitiveness of ML via Median Trick,[0],[0]
"To obtain sample-optimality guarantees for PML, we need to drive the error probability down much faster than the median trick.",7.1. Sharp Concentration for Some Properties,[0],[0]
We achieve this by using McDiarmid’s inequality stated below.,7.1. Sharp Concentration for Some Properties,[0],[0]
Let ˆf : X ⇤ !,7.1. Sharp Concentration for Some Properties,[0],[0]
R. Suppose ˆf gets n independent samples Xn from an unknown distribution.,7.1. Sharp Concentration for Some Properties,[0],[0]
"Moreover, changing one of the X
j to any X 0 j changed ˆf by
at most c⇤.",7.1. Sharp Concentration for Some Properties,[0],[0]
"Then McDiarmid’s inequality (bounded difference inequality, (Boucheron et al., 2013))",7.1. Sharp Concentration for Some Properties,[0],[0]
"states that,
Pr
⇣
ˆf(Xn) E[ ˆf(Xn)]",7.1. Sharp Concentration for Some Properties,[0],[0]
"> t ⌘  2 exp ✓ 2t 2
nc2⇤
◆
.",7.1. Sharp Concentration for Some Properties,[0],[0]
"(4)
",7.1. Sharp Concentration for Some Properties,[0],[0]
This inequality can be used to show strong error probability bounds for many problems.,7.1. Sharp Concentration for Some Properties,[0],[0]
"We mention a simple application for estimating discrete distributions.
",7.1. Sharp Concentration for Some Properties,[0],[0]
Example 2.,7.1. Sharp Concentration for Some Properties,[0],[0]
"It is well known (Devroye & Lugosi, 2001) that SML requires ⇥(k/""2) samples to estimate p in `
1 distance with probability at least 2/3.",7.1. Sharp Concentration for Some Properties,[0],[0]
"In this case, ˆf(Xn) = P
x
N
x
n p(x) , and therefore c⇤ is at most 2/n. Using McDiarmid’s inequality, it follows that SML has an error probability of = 2 exp( k/2), while still using ⇥(k/""2) samples.
",7.1. Sharp Concentration for Some Properties,[0],[0]
"Let B n be the bias of an estimator ˆf(Xn) of f(p), namely B
n
def = f(p) E[ ˆf(Xn)] .",7.1. Sharp Concentration for Some Properties,[0],[0]
"By the triangle inequality,
f(p) ˆf(Xn)
 f(p) E[ ˆf(Xn)]",7.1. Sharp Concentration for Some Properties,[0],[0]
+ ˆf(Xn),7.1. Sharp Concentration for Some Properties,[0],[0]
"E[ ˆf(Xn)]
= B n +
ˆf(Xn) E[ ˆf(Xn)] .
",7.1. Sharp Concentration for Some Properties,[0],[0]
"Plugging this in (4),
Pr
⇣
f(p) ˆf(Xn)]",7.1. Sharp Concentration for Some Properties,[0],[0]
"> t+B n
⌘ 2 exp ✓ 2t 2
nc2⇤
◆
.",7.1. Sharp Concentration for Some Properties,[0],[0]
"(5)
With this in hand, we need to show that c⇤ can be bounded for estimators for the properties we consider.",7.1. Sharp Concentration for Some Properties,[0],[0]
"In particular, we will show that
Lemma 2.",7.1. Sharp Concentration for Some Properties,[0],[0]
Let ↵ > 0 be a fixed constant.,7.1. Sharp Concentration for Some Properties,[0],[0]
"For entropy, support, support coverage, and distance to uniformity there exist profile based estimators that use the optimal number of samples (given in Table 1), have bias "" and if we change any of the samples, changes by at most c · n↵
n , where c is a positive constant.
",7.1. Sharp Concentration for Some Properties,[0],[0]
We prove this lemma by proposing several modifications to the existing sample-optimal estimators.,7.1. Sharp Concentration for Some Properties,[0],[0]
The modified estimators will preserve the sample complexity up to constant factors and also have a small c⇤.,7.1. Sharp Concentration for Some Properties,[0],[0]
"The proof details are given in the appendix.
",7.1. Sharp Concentration for Some Properties,[0],[0]
"Using (5) with Lemma 2,
Theorem 5.",7.1. Sharp Concentration for Some Properties,[0],[0]
"Let n be the optimal sample complexity of estimating entropy, support, support coverage and distance to uniformity (given in table 1) and c be a large positive constant.",7.1. Sharp Concentration for Some Properties,[0],[0]
"Let "" c/n0.2, then any for any > exp ( pn), the -PML estimator estimates entropy, support, support
coverage, and distance to uniformity to an accuracy of 4"" with probability at least 1 exp( pn).
",7.1. Sharp Concentration for Some Properties,[0],[0]
Proof.,7.1. Sharp Concentration for Some Properties,[0],[0]
Let ↵ = 0.05.,7.1. Sharp Concentration for Some Properties,[0],[0]
"By Lemma 2, for each property of interest, there are estimators based on the profiles of the samples such that using near-optimal number of samples, they have bias "" and maximum change if we change any of the samples is at most c0n↵/n for some constant c0.",7.1. Sharp Concentration for Some Properties,[0],[0]
"Hence, by McDiarmid’s inequality, an accuracy of 2"" is achieved with probability at least 1 2 exp ⇣ 2""2n1 2↵/c02 ⌘
.",7.1. Sharp Concentration for Some Properties,[0],[0]
Now suppose p̃ is any -approximate PML distribution.,7.1. Sharp Concentration for Some Properties,[0],[0]
"Then by Theorem 4
Pr (|f(p) f(p̃)| > 4"") < ·",7.1. Sharp Concentration for Some Properties,[0],[0]
"| n|
2 exp( 2"" 2n1 2↵/c02 + 3
p n)
 exp( pn), where in the last step we used ""2n1 2↵ & c0pn, and > exp( pn).",7.1. Sharp Concentration for Some Properties,[0],[0]
We analyze both support coverage and the support estimation via a single approach.,8. Support and Support Coverage,[0],[0]
We first start with support coverage.,8. Support and Support Coverage,[0],[0]
"Recall that the goal is to estimate S
m (p), the expected number of distinct symbols that we see after observing m samples from p.",8. Support and Support Coverage,[0],[0]
"By the linearity of expectation,
S m
(p) = X
x2X E[I N",8. Support and Support Coverage,[0],[0]
x (X m ),8. Support and Support Coverage,[0],[0]
>0,8. Support and Support Coverage,[0],[0]
"] =
X x2X (1 (1 p(x))m) .
",8. Support and Support Coverage,[0],[0]
"The problem is closely related to the support coverage problem (Orlitsky et al., 2016), where the goal is to estimate U
t (Xn), the number of new distinct symbols that we observe in n · t additional samples.",8. Support and Support Coverage,[0],[0]
"Hence
S m
(p) = E "" n X
i=1
' i
#
+ E[U t ],
where t = (m n)/n.",8. Support and Support Coverage,[0],[0]
"We show that the modification of an estimator in (Orlitsky et al., 2016) is also near-optimal and satisfies conditions in Lemma 2.",8. Support and Support Coverage,[0],[0]
"We propose to use the following estimator
ˆS m
(p) = n X
i=1
' i + USGT t (Xn),
where USGT t (Xn) = P n
i=1
'",8. Support and Support Coverage,[0],[0]
"i ( t)i Pr(Z i) and Z is a Poisson random variable with mean r and t = (m n)/n.
",8. Support and Support Coverage,[0],[0]
"The above theorem also works for any "" & 1/n0.25 ⌘ for any ⌘ > 0
We remark that the proof also holds for Binomial smoothed random variables as discussed in (Orlitsky et al., 2016).
",8. Support and Support Coverage,[0],[0]
We need to bound the maximum coefficient and the bias to apply Lemma 2.,8. Support and Support Coverage,[0],[0]
"We first bound the maximum coefficient of this estimator.
",8. Support and Support Coverage,[0],[0]
Lemma 3.,8. Support and Support Coverage,[0],[0]
"For all n  m/2, the maximum coefficient of ˆS m (p) is at most 1 + er(t 1).
",8. Support and Support Coverage,[0],[0]
Proof.,8. Support and Support Coverage,[0],[0]
"For any i, the coefficient of ' i is 1+ ( t)i Pr(Z i).",8. Support and Support Coverage,[0],[0]
"It can be upper bounded as 1 + P t
i=0
e r (rt) i
i!
",8. Support and Support Coverage,[0],[0]
"= 1 +
er(t 1).
",8. Support and Support Coverage,[0],[0]
"The next lemma bounds the bias of the estimator.
",8. Support and Support Coverage,[0],[0]
Lemma 4.,8. Support and Support Coverage,[0],[0]
"For all n  m/2, the bias of the estimator is bounded by
|E[ ˆS m (p)]",8. Support and Support Coverage,[0],[0]
S m (p)|  2 + 2er(t 1),8. Support and Support Coverage,[0],[0]
"+min(m,S(p))e r.
Proof.",8. Support and Support Coverage,[0],[0]
"As before let t = (m n)/n.
",8. Support and Support Coverage,[0],[0]
E[ ˆS m (p)],8. Support and Support Coverage,[0],[0]
"S m (p)
=
n
X
i=1
",8. Support and Support Coverage,[0],[0]
E,8. Support and Support Coverage,[0],[0]
[' i ] + E[USGT t (Xn)],8. Support and Support Coverage,[0],[0]
"X
x2X (1 (1 p(x))m)
= E[USGT t (Xn)]",8. Support and Support Coverage,[0],[0]
"X
x2X ((1 p(x))n (1 p(x))m) .
",8. Support and Support Coverage,[0],[0]
"Hence by Lemma 8 and Corollary 2, in (Orlitsky et al., 2016), we get
|E[ ˆS m (p)]",8. Support and Support Coverage,[0],[0]
"S m (p)|2+2er(t 1) +min(m,S(p))e r.
Using the above two lemmas we prove results for both the observed support coverage and support estimator.",8. Support and Support Coverage,[0],[0]
"Recall that the quantity of interest in support coverage estimation is S
m (p)/m, which we wish to estimate to an accuracy of "".
",8.1. Support Coverage Estimator,[0],[0]
Proof of Lemma 2 for observed.,8.1. Support Coverage Estimator,[0],[0]
"If we choose r = log 3 "" , then by Lemma 3, the maximum coefficient of ˆS
m
(p)/m
is at most 2 m e m n log
3 "" , which for m  ↵n log(n/21/↵)
log(3/"") is at most n↵/m < n↵/n.",8.1. Support Coverage Estimator,[0],[0]
"Similarly, by Lemma 4,
1 m |E[ ˆS m (p)]",8.1. Support Coverage Estimator,[0],[0]
"S m (p)|  1 m (2 + 2er(t 1) +me r)  "",
for all "" > 6n↵/n.",8.1. Support Coverage Estimator,[0],[0]
"Recall that the quantity of interest in support estimation is ˜S(p), which we wish to estimate to an accuracy of "".
",8.2. Support Estimator,[0],[0]
Proof of Lemma 2 for support.,8.2. Support Estimator,[0],[0]
Note that we are interested in distributions with all the non zero probabilities are at least 1,8.2. Support Estimator,[0],[0]
/k.,8.2. Support Estimator,[0],[0]
"We propose to estimate ˜S(p) using ˆS
m (p)/k, for m = k log 3
"" .",8.2. Support Estimator,[0],[0]
"If we choose r = log 3 "" , then by Lemma 3, the maximum coefficient of ˆS
m (p)/k is at most 2
k
e m n log 3 "" , which for n k
↵ log(k/2 1/↵ ) log
2
3
"" is at most k↵/k < n↵/n.
",8.2. Support Estimator,[0],[0]
"To bound the bias, note that for this choice of m
0  S(p)",8.2. Support Estimator,[0],[0]
"S m (p) = X
x
(1 p(x))m
 X
x
e mp(x)  ",8.2. Support Estimator,[0],[0]
"ke log 3"" = k"" 3 .
",8.2. Support Estimator,[0],[0]
"Similarly, by Lemma 4,
1 k |E[ ˆS m (p)]",8.2. Support Estimator,[0],[0]
"S(p)|
 1 k |E[ ˆS m (p)]",8.2. Support Estimator,[0],[0]
S m (p)|+ 1 k |S(p) S m,8.2. Support Estimator,[0],[0]
"(p)|
 1 k (2 + 2er(t 1) + ke r) +
""
3
 "",
for all "" > 12n↵/n.",8.2. Support Estimator,[0],[0]
"We studied estimation of symmetric properties of discrete distributions using the principle of maximum likelihood, and proved optimality of this approach for a number of problems.",9. Discussion and Future Directions,[0],[0]
A number of directions are of interest.,9. Discussion and Future Directions,[0],[0]
"We believe that the lower bound requirement on "" is perhaps an artifact of our proof technique, and that the PML based approach is indeed optimal for all ranges of "".",9. Discussion and Future Directions,[0],[0]
Approximation algorithms for estimating the PML distributions would be a fruitful direction to pursue.,9. Discussion and Future Directions,[0],[0]
"Given our results, approximations stronger than exp( ""2n) would be very interesting.",9. Discussion and Future Directions,[0],[0]
"In the particular case when the desired accuracy is a constant, even an exponential approximation would be sufficient for many properties.",9. Discussion and Future Directions,[0],[0]
"We plan to apply the heuristics proposed by (Vontobel, 2012) for various problems we consider, and compare with the state of the art provable methods.",9. Discussion and Future Directions,[0],[0]
"The authors thank the reviewers for valuable feedback and the NSF for support through grants CIF-1564355, CIF1619448, CRII-CIF-1657471, and a Cornell University start-up grant.",Acknowledgements,[0],[0]
"Jayadev Acharya thanks Clement Canonne, Jiantao Jiao, and Pascal Vontobel for interesting discussions.",Acknowledgements,[0],[0]
"Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications.",abstractText,[0],[0]
"Recently, researchers applied different estimators and analysis tools to derive asymptotically sample-optimal approximations for each of these properties.",abstractText,[0],[0]
"We show that a single, simple, plug-in estimator—profile maximum likelihood (PML)– is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.",abstractText,[0],[0]
A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2401
A Unified Syntax-aware Framework for Semantic Role Labeling Zuchao Li1,2,∗, Shexia He1,2,∗, Jiaxun Cai1,2, Zhuosheng Zhang1,2, Hai Zhao1,2,†, Gongshen Liu3, Linlin Li4, Luo Si4 1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China
3School of Cyber Security, Shanghai Jiao Tong University, China 4Alibaba Group, Hangzhou, China
{charlee,heshexia,caijiaxun,zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, lgshen@sjtu.edu.cn,
{linyan.lll,luo.si}@alibaba-inc.com Abstract
Semantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models.",text,[0],[0]
The purpose of semantic role labeling (SRL) is to derive the predicate-argument structure of each predicate in a sentence.,1 Introduction,[0],[0]
"A popular formalism to represent the semantic predicate-argument structure is based on dependencies, namely dependency SRL, which annotates the heads of arguments rather than phrasal arguments.",1 Introduction,[0],[0]
"Given a sentence (in Figure 1), SRL is generally decomposed
∗ These authors made equal contribution.† Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15- ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04) and the joint research project with Youtu Lab of Tencent.
",1 Introduction,[0],[0]
"A2 AM-TMP
A0
Someone always makes you happy make.02
A1
Figure 1: An example of dependency-based SRL.
into multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes).",1 Introduction,[0],[0]
"SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016).
",1 Introduction,[0],[0]
"Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically.",1 Introduction,[0],[0]
"In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002).",1 Introduction,[0],[0]
"Despite the success of these models, the main reasons for putting syntax aside are two-fold.",1 Introduction,[0],[0]
"First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation.",1 Introduction,[0],[0]
"Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance.
",1 Introduction,[0],[0]
"However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008).",1 Introduction,[0],[0]
"Recently, Marcheggiani and Titov (2017)
proposed a syntactic graph convolutional networks (GCNs) based SRL model and further improved the SRL performance with relatively better syntactic parser as input.",1 Introduction,[0],[0]
"Since syntax can provide rich structure and information for SRL, we seek to effectively model complex syntactic tree structure for incorporating syntax into neural SRL.
",1 Introduction,[0],[0]
"In this paper, we present a general framework1 for SRL, which enables us to integrate syntax into SRL in diverse ways.",1 Introduction,[0],[0]
"Following Marcheggiani and Titov (2017), we focus on argument labeling and formulate SRL as sequence labeling problem.",1 Introduction,[0],[0]
"However, we differ by (1) leveraging enhanced word representation, (2) applying recent advances in recurrent neural networks (RNNs), such as highway connections (Srivastava et al., 2015), (3) using deep encoder with residual connections (He et al., 2016), (4) further extending Syntax Aware Long Short-Term Memory (SA-LSTM)",1 Introduction,[0],[0]
"(Qian et al., 2017) for SRL, and (5) introducing the Tree-Structured Long Short-Term Memory (Tree-LSTM) (Tai et al., 2015) to model syntactic information for SRL.
",1 Introduction,[0],[0]
"In addition, as pointed out by He et al. (2017) for span SRL, the worse syntactic input will hurt performance if the syntactically-driven SRL model trusts syntactic information too much, and high-quality syntax can still make a large impact on SRL, which motivates us to investigate the effect of syntactic quality on dependency SRL.",1 Introduction,[0],[0]
"In summary, our major contributions are as follows:",1 Introduction,[0],[0]
• We propose a unified neural framework for dependency SRL to more effectively integrate syntactic information with multiple methods.,1 Introduction,[0],[0]
• Our SRL framework incorporated with syntax achieves the new state-of-the-art results on both English and Chinese CoNLL-2009 benchmarks.,1 Introduction,[0],[0]
"• We explore the impact of different quality of syntactic input on SRL performance, showing that high quality syntactic parse may indeed improve syntax-aware SRL.",1 Introduction,[0],[0]
"In order to explore the effectiveness of the syntactic feature from various perspectives, we propose a unified neural framework that is capable of optionally accommodating various types of syntactic encoders for syntax-based SRL.
",2 A Unified SRL Framework,[0],[0]
"Since the CoNLL-2009 shared task (Hajič et al.,
1Our code is available here: https://github.com/ bcmi220/unified_syn_srl.
2009) have beforehand indicated the predicate positions, we need to identify and label all arguments for each predicate, which is a typical sequence tagging problem.",2 A Unified SRL Framework,[0],[0]
"In this work, we construct a general SRL framework for argument labeling.",2 A Unified SRL Framework,[0],[0]
"As shown in Figure 2, our SRL framework includes three main modules, (1) BiLSTM encoder that directly takes sequential inputs, (2) MLP with highway connections for softmax output layer, and (3) an optional syntactic encoder that receives the outputs of the BiLSTM encoder and then let its own outputs integrate with the BiLSTM outputs through residual connections.
",2 A Unified SRL Framework,[0],[0]
"Note that when the syntactic encoder is completely removed, MLP only takes inputs directly from the BiLSTM encoder, which let our framework become a syntax-agnostic labeler.",2 A Unified SRL Framework,[0],[0]
"Word representation Given a sentence and known predicate, we consider predicate-specific word representation, following previous work (Marcheggiani and Titov, 2017).",2.1 Sentence Encoder,[0],[0]
"Specifically, each word embedding representation ei of input sentence is the concatenation of several features, a randomly initialized word embedding eri , a pretrained word embedding epi , a randomly initialized lemma embedding eli, a randomly initialized POS tag embedding eposi , and a predicate-specific feature efi , which is a binary flag set 0 or 1 indicating whether the current word is the given predicate.
",2.1 Sentence Encoder,[0],[0]
"To further enhance the word representation, we leverage an external embedding ELMo (Embeddings from Language Models) proposed by Peters et al. (2018).",2.1 Sentence Encoder,[0],[0]
"ELMo is obtained by deep bidirectional language model that takes characters as input, enriching subword information and contextual information, which has expressive representation power.",2.1 Sentence Encoder,[0],[0]
"Eventually, the resulting word representation is concatenated as ei =",2.1 Sentence Encoder,[0],[0]
"[eri , e p i , e",2.1 Sentence Encoder,[0],[0]
"l i, e pos",2.1 Sentence Encoder,[0],[0]
"i , e f",2.1 Sentence Encoder,[0],[0]
"i ,ELMoi].
",2.1 Sentence Encoder,[0],[0]
"BiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs.",2.1 Sentence Encoder,[0],[0]
"Given an input sequence (e1, . . .",2.1 Sentence Encoder,[0],[0]
", en), the BiLSTM processes these embedding vectors sequentially from both directions to obtain two separated hidden states, −→ h i and ←−",2.1 Sentence Encoder,[0],[0]
h i respectively.,2.1 Sentence Encoder,[0],[0]
"By concatenating the two states, we get a contextual representation hi =",2.1 Sentence Encoder,[0],[0]
"[ −→ h i, ←−",2.1 Sentence Encoder,[0],[0]
h,2.1 Sentence Encoder,[0],[0]
"i], which will be taken by the next
Word representation
cats love hats
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
+ + +",2.1 Sentence Encoder,[0],[0]
"+
BiLSTM encoder
The
The cats love hats
NMOD SBJ
OBJ
h3
h2
h1
h4
The cats love
hats
ReLU(∑∙)",2.1 Sentence Encoder,[0],[0]
ReLU(∑∙),2.1 Sentence Encoder,[0],[0]
ReLU(∑∙),2.1 Sentence Encoder,[0],[0]
"ReLU(∑∙)
",2.1 Sentence Encoder,[0],[0]
"NMOD SBJ OBJ
W se
lf
W se
lf
W se
lf
W se
lf
The
cats
love
hats
Syntactic Encoder
GCNs
Tree-LSTM
SA-LSTM +
Hidden Layer
Softmax
Highway
residual connections
Figure 2: The unified syntax-based SRL framework
BiLSTM layer as input.",2.1 Sentence Encoder,[0],[0]
"In this work, we stack four layers of BiLSTM.",2.1 Sentence Encoder,[0],[0]
"We adopt a Multi-Layer Perceptron (MLP) with highway connections (Srivastava et al., 2015) on the top of our deep encoder, which takesthe concatenated representation as input.",2.2 Role Labeler,[0],[0]
The MLP consists of 10 layers and we employ ReLU activations for the hidden layer.,2.2 Role Labeler,[0],[0]
"To get the final predicted semantic roles, we use a softmax layer over the outputs to maximize the likelihood of labels.",2.2 Role Labeler,[0],[0]
"The MLP part takes inputs from both the BiLSTM encoder and syntactic encoder, which are joint through a residual connection (He et al., 2016) as shown in Figure 2.",2.2 Role Labeler,[0],[0]
"It is worth noting that our deep encoder is different from the one of Marcheggiani and Titov (2017), which directly applies a softmax transformation over the syntactic representation and predicts the role label for each word.",2.2 Role Labeler,[0],[0]
"That is, their syntactic encoder outputs are directly taken as the input of hidden layer.",2.2 Role Labeler,[0],[0]
"To integrate the syntactic information into sequential neural networks, we employ a syntactic encoder on top of the BiLSTM encoder.
",3 Syntactic Encoder,[0],[0]
"Specifically, given a syntactic dependency tree
T , for each node nk in T , let C(k) denote the syntactic children set of nk,H(k) denote the syntactic head of nk, and L(k, ·) be the dependency relation between node nk and those have a direct arc from or to nk.",3 Syntactic Encoder,[0],[0]
"Then we formulate the syntactic encoder as a transformation f τ over the node nk, which may take some of C(k), H(k), or L(k, ·) as input, and compute a syntactic representation vk for node nk, namely, vk = f τ (C(k), H(k), L(k, ·), xk).",3 Syntactic Encoder,[0],[0]
"When not otherwise specified, xk denotes the input feature representation of nk which may be either the word representation ek or the output of BiLSTM hk, σ denotes the logistic sigmoid function, and denotes the element-wise multiplication.
",3 Syntactic Encoder,[0],[0]
"In practice, the transformation f τ can be any syntax encoding method.",3 Syntactic Encoder,[0],[0]
"In this paper, we will consider three types of syntactic encoders, syntactic graph convolutional network (Syntactic GCN) (in Section 3.1), syntax aware LSTM (SA-LSTM) (in Section 3.2), tree-structured LSTM (TreeLSTM) (in Section 3.3).",3 Syntactic Encoder,[0],[0]
"Then, we will provide a brief introduction in subsequent subsections.",3 Syntactic Encoder,[0],[0]
"GCN (Kipf and Welling, 2017) is proposed to induce the representations of nodes in a graph based on the properties of their neighbors.",3.1 Syntactic GCN,[0],[0]
"Given its effectiveness, Marcheggiani and Titov (2017) in-
troduce a generalized version for the SRL task, namely syntactic GCN, and shows that syntactic GCN is effective in incorporating syntactic information into neural models.
",3.1 Syntactic GCN,[0],[0]
"Syntactic GCN captures syntactic information flows in two directions, one from heads to dependents (along), the other from dependents to heads (opposite).",3.1 Syntactic GCN,[0],[0]
"Besides, it also models the information flows from a node to itself, namely, it assumes that a syntactic graph contains self-loop for each node.",3.1 Syntactic GCN,[0],[0]
"Thus, the syntactic GCN transformation of a node nk is defined on its neighborhood N(k) = C(k)∪H(k)∪{nk}.",3.1 Syntactic GCN,[0],[0]
"For each edge connects nk and its neighbor nj , we can compute a vector representation for it,
uk,j = W dir(k,j)xj + b L(k,j),
where dir(k, j) denotes the direction type (along, opposite or self-loop) of the edge from nk to nj , W dir(k,j) is direction type specific parameter, bL(k,j) is label specific parameter.",3.1 Syntactic GCN,[0],[0]
"Considering that syntactic information from all the neighboring nodes may make different contribution to semantic role labeling, syntactic GCN introduces an additional edge-wise gating for each node pair (nk, nj) as
gk,j = σ(W dir(k,j) g xk + b L(k,j) g ).
",3.1 Syntactic GCN,[0],[0]
"The syntactic representation vk for a node nk can be then computed as:
vk = ReLU( ∑
j∈N(k)
gk,j uk,j).",3.1 Syntactic GCN,[0],[0]
"SA-LSTM (Qian et al., 2017) is an extension of the standard BiLSTM architecture, which aims to simultaneously encode the syntactic and contextual information for a given word as shown in Figure 2.",3.2 SA-LSTM,[0],[0]
"On one hand, the SA-LSTM calculates the hidden state in sequence timestep order like the standard LSTM,
ig = σ(W (i)xk + U (i)hk−1 + b",3.2 SA-LSTM,[0],[0]
"(i)),
fg = σ(W (f)xk + U (f)hk−1 + b (f)),
og = σ(W (o)xk + U (o)hk−1 + b (o)),
u = f(W (u)xk + U (u)hk−1 + b (u)),
ck = ig u+ fg ck−1.
",3.2 SA-LSTM,[0],[0]
"On the other hand, it further incorporates the syntactic information into the representation of each word by introducing an additional gate,
sg = σ(W (s)xk + U (s)hk−1 + b",3.2 SA-LSTM,[0],[0]
"(s)),
hk = og f(ck) + sg h̃k.
where h̃k = f",3.2 SA-LSTM,[0],[0]
"( ∑
tj<tk αj × hj) is the weighted
sum of all hidden state vectors hj which come from previous node (word) nj , the weight factor αj is actually a trainable weight related to the dependency relation L(k, ·) when there exists a directed edge from nj to nk.
Note that h̃k is always the hidden state vector of the syntactic head of nk according to the definition of αj .",3.2 SA-LSTM,[0],[0]
"Since a word will be assigned a single syntactic head, such a strict constraint prevents the SA-LSTM from incorporating complex syntactic structures.",3.2 SA-LSTM,[0],[0]
"Inspire by the idea of GCN, we relax the directed constraint of αj , whenever there is an edge between nj and nk.
",3.2 SA-LSTM,[0],[0]
"After the SA-LSTM transformation, the outputs of the SA-LSTM layer from both directions are concatenated and taken as the syntactic representation of each word nk, i.e., vk = [ −→ hk, ←− hk].",3.2 SA-LSTM,[0],[0]
"Different from the syntactic GCN, SA-LSTM encoding both syntactic and contextual information in a single vector vk.",3.2 SA-LSTM,[0],[0]
"Tree-LSTM (Tai et al., 2015) can be considered as an extension of the standard LSTM, which aims to model the tree-structured topologies.",3.3 Tree-LSTM,[0],[0]
"At each timestep, it composes an input vector and the hidden states from arbitrarily many child units.",3.3 Tree-LSTM,[0],[0]
"Specifically, the main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units.",3.3 Tree-LSTM,[0],[0]
A TreeLSTM unit can be connected to arbitrary number of child units and assigns a single forget gate for each child unit.,3.3 Tree-LSTM,[0],[0]
"This provides Tree-LSTM the flexibility to incorporate or drop the information from each child unit.
",3.3 Tree-LSTM,[0],[0]
"Given a syntactic tree, the Tree-LSTM transformation is defined on node nk and its children set C(k), which can be formulated as follows (Tai
et al., 2015):
h̃k = ∑
j∈C(k)
hk, (1)
ig = σ(W (i)xk + U (i)h̃k + b (i)),
fk,jg = σ(W",3.3 Tree-LSTM,[0],[0]
"(f)xk + U (f)hj + b (f)), (2)
og = σ(W (o)xk + U (o)h̃k + b (o)),
u = tanh(W",3.3 Tree-LSTM,[0],[0]
"(u)xk + U (u)h̃k + b (u)), ck = ig u+ ∑
j∈C(k)
fk,jg cj ,
hk = og tanh(ck).
where j ∈ C(k), hj is the hidden state of the j-th child node, ck is the memory cell of the head node k, and hk is the hidden state of node k. Note that in Eq.(2), a single forget gate fk,jg is computed for each hidden state hj .
",3.3 Tree-LSTM,[0],[0]
"However, the primitive form of Tree-LSTM does not take the dependency relations into consideration.",3.3 Tree-LSTM,[0],[0]
"Given the importance of dependency relations in SRL task, we further extend the TreeLSTM by adding an additional gate rg and reformulate the Eq.",3.3 Tree-LSTM,[0],[0]
"(1),
rk,jg = σ(W (r)xk + U (r)hj + b L(k,j)), h̃k = ∑
j∈C(k)
rk,jg hj .
where bL(k,j) is a relation label specific bias term.",3.3 Tree-LSTM,[0],[0]
"After the Tree-LSTM transformation, the hidden state of each node in dependency tree is taken as its syntactic representation, i.e., vk = hk.",3.3 Tree-LSTM,[0],[0]
"We evaluate our models performance of syntactic GCN (henceforth Syn-GCN), SA-LSTM and Tree-LSTM on CoNLL-2009 datasets both for English and Chinese with standard training, development and test splits.",4 Experiments,[0],[0]
"For predicate disambiguation, we follow previous work (Marcheggiani and Titov, 2017), using the off-the-shelf disambiguator from Roth and Lapata (2016).",4 Experiments,[0],[0]
"For syntactic dependency tree, we parse the corpus with Biaffine Parser (Dozat and Manning, 2017).",4 Experiments,[0],[0]
"In our experiments, the pre-trained word embeddings for English are 100-dimensional GloVe vectors (Pennington et al., 2014).",4.1 Experimental Settings,[0],[0]
"For Chinese, we
exploit Wikipedia documents to train the same dimensional Word2Vec embeddings (Mikolov et al., 2013).",4.1 Experimental Settings,[0],[0]
"All other vectors are randomly initialized, the dimension of lemma embeddings is 100, and the dimension of POS tag embedding is 32.",4.1 Experimental Settings,[0],[0]
"In addition, we use 300-dimensional ELMo embedding for English2.
",4.1 Experimental Settings,[0],[0]
"During training, we use the categorical crossentropy as objective, with Adam optimizer (Kingma and Ba, 2015) the learning rate 0.001, and the batch size is set to 64.",4.1 Experimental Settings,[0],[0]
The BiLSTM encoder consists of 4-layer BiLSTM with 512- dimensional hidden units.,4.1 Experimental Settings,[0],[0]
We apply dropout for BiLSTM with a 90% keeping probability between time-steps and layers.,4.1 Experimental Settings,[0],[0]
We train models for a maximum of 20 epochs and obtain the nearly best model based on English development results.,4.1 Experimental Settings,[0],[0]
"We compare our models of Syn-GCN, SA-LSTM and Tree-LSTM with previous approaches for dependency SRL on both English and Chinese.",4.2 Results,[0],[0]
"Noteworthily, our model is local (argument identification and classification decisions are conditionally independent) and single without reranking, which neither includes global inference nor com-
2For Chinese, we do not use pre-trained ELMo whose weights are only available for English.
bines multiple models.",4.2 Results,[0],[0]
"The experimental results on the in-domain English and Chinese test sets are summarized in Tables 1 and 2, respectively.
",4.2 Results,[0],[0]
"For English, our models of Syn-GCN, SALSTM and Tree-LSTM overwhelmingly surpass most previously published single models, achieving state-of-the-art results of 89.8%, 89.7% and 89.4% in F1 scores respectively.",4.2 Results,[0],[0]
"In comparison to ensemble models, our Syn-GCN even performs better than the previous model (Marcheggiani and Titov, 2017) with a margin of 0.7% F1.
",4.2 Results,[0],[0]
"From Table 1, we also see that our Syn-GCN model provides the best recall and F1 score, while our SA-LSTM model yields the competitive performance with higher precision at the expense of recall, which shows that SA-LSTM is better at classifying arguments.",4.2 Results,[0],[0]
"Overall, the Tree-LSTM gives slightly weaker performance, which may be attributed to tree-structured network topology.",4.2 Results,[0],[0]
"More specifically, Tree-LSTM only considers information from arbitrary child units so that each node lacks of the information from parent.",4.2 Results,[0],[0]
"However, our Syn-GCN and SA-LSTM combine bidirectional information, both head-to-dependent and dependent-to-head.
",4.2 Results,[0],[0]
"For Chinese (Table 2), even though we use the same parameters as for English, our models are still comparable with the best reported results.
",4.2 Results,[0],[0]
Table 3 presents the results on English out-ofdomain test set.,4.2 Results,[0],[0]
"Our models outperform the highest records achieved by He et al. (2018), with absolute improvements of 0.2-0.5% in F1 scores.",4.2 Results,[0],[0]
These favorable results on both in-domain and outof-domain data demonstrate the effectiveness and robustness of our proposed unified framework.,4.2 Results,[0],[0]
"To investigate the contributions of word representation and deep encoder in our method, we conduct a series of ablation studies on the English development set, unless otherwise stated.
",4.3 Ablation and Analysis,[0],[0]
"Effect of word representation In order to better understand how the enhanced word representation influences our model performance, we train our Syn-GCN model with different settings in input word embeddings.",4.3 Ablation and Analysis,[0],[0]
Table 4 shows results for our system when we remove POS tag and ELMo embedding respectively.,4.3 Ablation and Analysis,[0],[0]
"Interestingly, the impact of POS tag embedding (about 0.4% F1) is less compared to the previous works, which allows us to build an accuracy model even when the POS tag is unavailable.",4.3 Ablation and Analysis,[0],[0]
We also observe that effect of ELMo embedding is somewhat surprising (1.2% F1 performance degradation).,4.3 Ablation and Analysis,[0],[0]
"Experimental results indicate that a combination of these features could enhance the word representation, leading to SRL performance improvement.
",4.3 Ablation and Analysis,[0],[0]
"Effect of deep encoder Table 5 reports F1 scores of our Syn-GCN model, Marcheggiani and Titov (2017), He et al. (2018) and Cai et al. (2018) on English test set in both syntax-agnostic and syntax-aware settings.",4.3 Ablation and Analysis,[0],[0]
"The comparison shows that our framework is more effective for incorporating syntactic information by giving more performance improvement through introducing syntax over syntax-agnostic SRL than previous state-ofthe-art systems did.
",4.3 Ablation and Analysis,[0],[0]
"To further investigate the impact of deep encoder, we perform our Syn-GCN, SA-LSTM and Tree-LSTM models with another alternative configuration, using the same encoder as (Marcheggiani and Titov, 2017)",4.3 Ablation and Analysis,[0],[0]
"(M&T encoder for short), which removes the residual connections from our framework.",4.3 Ablation and Analysis,[0],[0]
The corresponding results of our models are also summarized in Table 6 for comparison.,4.3 Ablation and Analysis,[0],[0]
Note that the first row is the results of our syntax-agnostic model.,4.3 Ablation and Analysis,[0],[0]
"Surprisingly, we observe a dramatical performance decline of 1.2% F1 for our Syn-GCN model with M&T encoder.",4.3 Ablation and Analysis,[0],[0]
A less significant performance loss for our SALSTM (−0.4%) and Tree-LSTM (−0.5%) models shows that the Syn-GCN is more sensitive to contextual information.,4.3 Ablation and Analysis,[0],[0]
"Nevertheless, the overall results show that applying deep encoder could receive higher gains.",4.3 Ablation and Analysis,[0],[0]
"As mentioned before, syntactic parsers are unreliable due to the risk of erroneous syntactic input, especially on out-of-domain data.",4.4 Syntactic Role,[0],[0]
This section thus attempts to explore the impact of different quality of syntactic input on SRL performance.,4.4 Syntactic Role,[0],[0]
"To this end, we further carry out experiments on English test data with different syntactic inputs based on our Syn-GCN model.
",4.4 Syntactic Role,[0],[0]
"Syntactic Input Four types of syntactic inputs are used to explore the role of syntax in our unified framework, (1) the automatically predicted parse provided by CoNLL-2009 shared task, (2) the parsing results of the CoNLL-2009 data by state-of-theart syntactic parser, the Biaffine Parser (used in our previous experiments), (3) corresponding results from another parser, the BIST Parser (Kiperwasser and Goldberg, 2016), which is also adopted by Marcheggiani and Titov (2017), (4) the gold syntax available from the official data set.
",4.4 Syntactic Role,[0],[0]
Evaluation Metric,4.4 Syntactic Role,[0],[0]
"It is worth noting that for SRL task, the standard evaluation metric is the semantic labeled F1 score (Sem-F1), and we use the labeled attachment score (LAS) to quantify the quality of syntactic input.",4.4 Syntactic Role,[0],[0]
"In addition, the ratio between labeled F1 score for semantic dependencies and the LAS for syntactic dependencies (Sem-F1/LAS) proposed by CoNLL2008 shared task3 (Surdeanu et al., 2008), are also given for reference.",4.4 Syntactic Role,[0],[0]
"To a certain extent, the ratio Sem-F1/LAS could normalize the semantic score relative to syntactic parse, impartially estimating the true performance of SRL, independent of the performance of the input syntactic parser.
",4.4 Syntactic Role,[0],[0]
Comparison and Discussion Table 7 presents the comprehensive results of our Syn-GCN model on the four syntactic inputs aforementioned of different quality together with previous SRL models.,4.4 Syntactic Role,[0],[0]
A number of observations can be made from these results.,4.4 Syntactic Role,[0],[0]
"First, our model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, ob-
3CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one.",4.4 Syntactic Role,[0],[0]
"Their main difference is that predicates have been pre-identified for the latter.
",4.4 Syntactic Role,[0],[0]
taining overall higher scores compared to previous state-of-the-arts.,4.4 Syntactic Role,[0],[0]
"Second, It is interesting to note that the Sem-F1/LAS score of our model becomes relatively smaller as the syntactic input becomes better.",4.4 Syntactic Role,[0],[0]
"Though not so surprised, these results show that our SRL component is even relatively stronger.",4.4 Syntactic Role,[0],[0]
"Third, when we adopt a syntactic parser with higher parsing accuracy, our SRL system will achieve a better performance.",4.4 Syntactic Role,[0],[0]
"Notably, our model yields a Sem-F1 of 90.5% taking gold syntax as input.",4.4 Syntactic Role,[0],[0]
"It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017).",4.4 Syntactic Role,[0],[0]
"Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing.",5 Related Work,[0],[0]
"In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Björkelund et al., 2009; Zhao et al., 2013).",5 Related Work,[0],[0]
"Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed.",5 Related Work,[0],[0]
"For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks.",5 Related Work,[0],[0]
"FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation
applying tensor-based approach.
",5 Related Work,[0],[0]
"Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018).",5 Related Work,[0],[0]
"Similarly, Marcheggiani et al. (2017) also proposed a syntax-agnostic model for dependency SRL and obtained favorable results.",5 Related Work,[0],[0]
"Despite the success of syntax-agnostic models, there are several works focus on leveraging the advantages of syntax.",5 Related Work,[0],[0]
Roth and Lapata (2016) employed dependency path embedding to model syntactic information and exhibited a notable success.,5 Related Work,[0],[0]
Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into a neural SRL model.,5 Related Work,[0],[0]
"Qian et al. (2017) proposed SALSTM to model the whole tree structure of dependency relation in an architecture engineering way.
",5 Related Work,[0],[0]
"Besides, syntax encoding has also successfully promoted other NLP tasks.",5 Related Work,[0],[0]
"Tree-LSTM (Tai et al., 2015) is a variant of the standard LSTM that can encode a dependency tree with arbitrary branching factors, which has shown effectiveness on semantic relatedness and the sentiment classification tasks.",5 Related Work,[0],[0]
"In this work, we extend the Tree-LSTM with a relation specific gate and employ it to recursively encode the syntactic dependency tree for SRL. RCNN (Zhu et al., 2015) is an extension of the recursive neural network (Socher et al., 2010) which has been popularly used to encode trees with fixed branching factors.",5 Related Work,[0],[0]
"The RCNN is able to encode a tree structure with arbitrary number of factors and is useful in a re-ranking model for dependency parsing (Zhu et al., 2015).
",5 Related Work,[0],[0]
"In our experiments, we simplify and reformulate the RCNN model.",5 Related Work,[0],[0]
"However, the simplified model performs poorly on the development and the test sets.",5 Related Work,[0],[0]
The reason might be that the RCNN model with a single global composition parameter is too simple to cover all types of syntactic relation in a dependency tree.,5 Related Work,[0],[0]
"Because of the poor performance of the modified RCNN, we do not include it in this work.",5 Related Work,[0],[0]
"Considering there might be other approach to incorporate the recursive network in SRL model, we leave it as our future work and just provide a brief discussion here.
",5 Related Work,[0],[0]
"In this work, we extend existing methods and introduce Tree-LSTM for incorporating syntax into SRL.",5 Related Work,[0],[0]
"Rather than proposing completely new model, we synthesize these techniques and present a unified framework to take genuine superiority of syntactic information.",5 Related Work,[0],[0]
"This paper presents a unified neural framework for dependency-based SRL, effectively incorporating syntactic information by directly modeling syntax based on syntactic parse tree.",6 Conclusion,[0],[0]
"Rather than proposing completely new model, we extend existing models and apply tree-structured LSTM for SRL.",6 Conclusion,[0],[0]
"Our approach significantly outperforms all previous models, achieving state-of-the-art results on the CoNLL-2009 benchmarks for both English and Chinese.
",6 Conclusion,[0],[0]
"Our experiments specially show that giving an enlarged performance gap from syntax-agnostic to syntax-aware setting, SRL can be further promoted with the help of deep enhanced representation and effective methods of integrating syntax.",6 Conclusion,[0],[0]
"Furthermore, we explore the impact of the quality of syntactic input.",6 Conclusion,[0],[0]
The relevant results indicate that high-quality syntactic parse is more favorable to semantic role labeling.,6 Conclusion,[0],[0]
"Brussels, Belgium, October 31 November 4, 2018.",abstractText,[0],[0]
c,abstractText,[0],[0]
©2018 Association for Computational Linguistics 2401,abstractText,[0],[0]
"A Unified Syntax-aware Framework for Semantic Role Labeling Zuchao Li1,2,∗, Shexia He1,2,∗, Jiaxun Cai, Zhuosheng Zhang, Hai Zhao1,2,†, Gongshen Liu, Linlin Li, Luo Si Department of Computer Science and Engineering, Shanghai Jiao Tong University Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China School of Cyber Security, Shanghai Jiao Tong University, China Alibaba Group, Hangzhou, China {charlee,heshexia,caijiaxun,zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, lgshen@sjtu.edu.cn, {linyan.lll,luo.si}@alibaba-inc.com",abstractText,[0],[0]
Abstract,abstractText,[0],[0]
A Unified Syntax-aware Framework for Semantic Role Labeling,title,[0],[0]
"Low-rank matrix recovery problem has been extensively studied during the past decades, due to its wide range of applications, such as collaborative filtering (Srebro et al., 2004; Rennie & Srebro, 2005) and multi-label learning (Cabral et al., 2011; Xu et al., 2013).",1. Introduction,[0],[0]
"The objective of low-rank matrix recovery is to estimate the unknown lowrank matrix X⇤ 2 Rd1⇥d2 from partial observations, such as a set of linear measurements in matrix sensing or a subset of its entries in matrix completion.",1. Introduction,[0],[0]
"Significant efforts have been made to estimate low-rank matrices, among which one of the most prevalent approaches is nuclear norm re-
*Equal contribution 1Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA.",1. Introduction,[0],[0]
"Correspondence to: Quanquan Gu <qg5w@virginia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"laxation based optimization (Srebro et al., 2004; Candès & Tao, 2010; Rohde et al., 2011; Recht et al., 2010; Negahban & Wainwright, 2011; 2012; Gui & Gu, 2015).",1. Introduction,[0],[0]
"While such convex relaxation based methods enjoy a rigorous theoretical guarantee to recover the unknown low-rank matrix, due to the nuclear norm regularization/minimization, these algorithms involve a singular value decomposition at each iteration, whose time complexity is O(d3) to recover a d⇥d matrix.",1. Introduction,[0],[0]
"Hence, they are computationally very expensive.
",1. Introduction,[0],[0]
"In order to address the aforementioned computational issue, recent studies (Keshavan et al., 2009; 2010; Jain et al., 2013a; Jain & Netrapalli, 2014; Hardt, 2014; Hardt & Wootters, 2014; Hardt et al., 2014; Zhao et al., 2015; Chen & Wainwright, 2015; Sun & Luo, 2015; Zheng & Lafferty, 2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015; Park et al., 2016b; Wang et al., 2016) have been carried out to perform factorization on the matrix space, which naturally ensures the low-rankness of the produced estimator.",1. Introduction,[0],[0]
"Although this matrix factorization technique converts the previous optimization problem into a nonconvex one, which is more difficult to analyze, it significantly improves the computational efficiency.
",1. Introduction,[0],[0]
"However, for large-scale matrix recovery, such nonconvex optimization approaches are still computationally expensive, because they are based on gradient descent or alternating minimization, which involve the time-consuming calculation of full gradient at each iteration.",1. Introduction,[0],[0]
"De Sa et al. (2014) developed a stochastic gradient descent approach for Gaussian ensembles, but the sample complexity (i.e., number of measurements or observations required for exact recovery) of their algorithm is not optimal.",1. Introduction,[0],[0]
"Recently, Jin et al. (2016) and Zhang et al. (2017b) proposed stochastic gradient descent algorithms for noiseless matrix completion and matrix sensing, respectively.",1. Introduction,[0],[0]
"Although these algorithms achieve linear rate of convergence and improved computational complexity over aforementioned deterministic optimization based approaches, they are limited to specific low-rank matrix recovery problems, and unable to be extended to more general problems and settings.
",1. Introduction,[0],[0]
"In this paper, inspired by the idea of variance reduction for stochastic gradient (Schmidt et al., 2013; Konečnỳ & Richtárik, 2013; Johnson & Zhang, 2013; Defazio et al.,
2014a;b; Mairal, 2014; Xiao & Zhang, 2014; Konečnỳ et al., 2014; Reddi et al., 2016; Allen-Zhu & Hazan, 2016; Chen & Gu, 2016; Zhang & Gu, 2016), we propose a unified stochastic gradient descent framework with variance reduction for low-rank matrix recovery, which integrates both optimization-theoretic and statistical analyses.",1. Introduction,[0],[0]
"To the best of our knowledge, this is the first unified accelerated stochastic gradient descent framework for low-rank matrix recovery with strong convergence guarantees.",1. Introduction,[0],[0]
"With a desired initial estimator given by a general initialization algorithm, we show that our algorithm achieves linear convergence rate and better computational complexity against the state-of-the-art algorithms.",1. Introduction,[0],[0]
"The contributions of our work are further highlighted as follows:
1.",1. Introduction,[0],[0]
"We develop a generic stochastic variance-reduced gradient descent algorithm for low-rank matrix recovery, which can be applied to various low rank-matrix estimation problems, including matrix sensing, noisy matrix completion and one-bit matrix completion.",1. Introduction,[0],[0]
"In particular, for noisy matrix sensing, it is guaranteed to linearly converge to the unknown low-rank matrix up to the minimax statistical precision (Negahban & Wainwright, 2011; Wang et al., 2016); while for noiseless matrix sensing, our algorithm achieves the optimal sample complexity (Recht et al., 2010; Tu et al., 2015; Wang et al., 2016), and attains a linear rate of convergence.",1. Introduction,[0],[0]
"Besides, for noisy matrix completion, it achieves the best-known sample complexity required by nonconvex matrix factorization (Zheng & Lafferty, 2016).
2.",1. Introduction,[0],[0]
"At the core of our algorithm, we construct a novel semi-stochastic gradient term, which is substantially different from the one if following the original stochastic variance-reduced gradient using chain rule (Johnson & Zhang, 2013).",1. Introduction,[0],[0]
"This uniquely constructed semi-stochastic gradient has not appeared in the literature, and is essential for deriving the minimax optimal statistical rate.
3.",1. Introduction,[0],[0]
"Our unified framework is built upon the mild restricted strong convexity and smoothness conditions (Negahban et al., 2009; Negahban & Wainwright, 2011) regarding the objective function.",1. Introduction,[0],[0]
"Based on the above mentioned conditions, we derive an innovative projected notion of the restricted Lipschitz continuous gradient property, which we believe is of independent interest for other nonconvex problems to prove sharp statistical rates.",1. Introduction,[0],[0]
We further establish the linear convergence rate of our generic algorithm.,1. Introduction,[0],[0]
"Besides, for each specific examples, we verify that the conditions required in the generic setting are satisfied with high probability, which demonstrates the applicability of our framework.
4.",1. Introduction,[0],[0]
"Our algorithm has a lower computational complexity compared with existing approaches (Jain et al., 2013a; Zhao et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015;
Park et al., 2016b; Wang et al., 2016).",1. Introduction,[0],[0]
"More specifically, to achieve ✏ precision, the gradient complexity1 of our algorithm is O (N + 2b)",1. Introduction,[0],[0]
"log(1/✏)
.",1. Introduction,[0],[0]
"Here N denotes the total number of observations, d denotes the dimensionality of the unknown low-rank matrix X⇤, b denotes the batch size, and  denotes the condition number of X⇤ (see Section 2 for a detailed definition).",1. Introduction,[0],[0]
"In particular, if the condition number satisfies   N/b, our algorithm is computationally more efficient than the state-of-the-art generic algorithm in Wang et al. (2016).
Notation.",1. Introduction,[0],[0]
"We use [d] and I d to denote {1, 2, . . .",1. Introduction,[0],[0]
", d} and d⇥d identity matrix respectively.",1. Introduction,[0],[0]
"We write A>A = I
d2 , if A 2 Rd1⇥d2 is orthonormal.",1. Introduction,[0],[0]
"For any matrix A 2 Rd1⇥d2 , we use A
i,⇤ and A⇤,j to denote the i-th row and j-th column of A, respectively.",1. Introduction,[0],[0]
"In addition, we use A
ij to denote the (i, j)-th element of A. Denote the row space and column space of A by row(A) and col(A) respectively.",1. Introduction,[0],[0]
"Let d = max{d
1 , d 2 }, and ` (A) be the `-th largest singular value of A. For vector x 2 Rd, we use kxk
q
=
(⌃
d
i=1
|x i |q)1/q to denote its ` q vector norm for 0 < q",1. Introduction,[0],[0]
< 1.,1. Introduction,[0],[0]
"Denote the spectral and Frobenius norm of A by kAk
2 and kAk
F respectively.",1. Introduction,[0],[0]
"We use kAk1,1 = maxi,j |Aij | to denote the element-wise infinity norm of A, and we use kAk
2,1 to represent the largest `2-norm of its rows.",1. Introduction,[0],[0]
"Given two sequences {a
n } and {b n }, we write a n = O(b n )",1. Introduction,[0],[0]
"if there exists a constant 0 < C
1 < 1 such that a n  C 1 b n .",1. Introduction,[0],[0]
Note that other notations are defined throughout the paper.,1. Introduction,[0],[0]
"In this section, we present our generic stochastic gradient descent algorithm with variance reduction as well as several illustrative examples.",2. Methodology,[0],[0]
"First, we briefly introduce the general problem setup for low-rank matrix recovery.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
Suppose X⇤ 2 Rd1⇥d2 is an unknown rank-r matrix.,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Let the singular value decomposition (SVD) of X⇤ be X⇤ = U ⇤ ⌃⇤V ⇤> , where U
⇤ 2 Rd1⇥r, V
⇤ 2 Rd2⇥r are orthonormal matrices, and ⌃⇤ 2 Rr⇥r is a diagonal matrix.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Let
1
2
· · · r 0 be the sorted nonzero singular values of X⇤, and denote the condition number of X⇤ by , i.e.,  =
1 / r .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Besides, let U⇤ = U ⇤ (⌃⇤)1/2 and V⇤ = V ⇤ (⌃⇤)1/2.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
Recall that we aim to recover X⇤ through a collection of N observations or measurements.,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Let L
N : Rd1⇥d2 !",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"R be the sample loss function, which evaluates the fitness of any matrix X associated with the total N observations.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Then the low-rank
1Gradient complexity is defined as the number of gradients calculated in total.
matrix recovery problem can be formulated as follows:
minX2Rd1⇥d2 LN (X) :",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"= 1 N
P
N
i=1
` i (X),
subject to X 2 C, rank(X)  r, (2.1)
where ` i (X) measures the fitness of X associated with the i-th observation.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Here C ✓ Rd1⇥d2 is a feasible set, such that X⇤ 2 C.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"In order to more efficiently estimate the unknown low-rank matrix, following Jain et al. (2013a); Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016a); Wang et al. (2016), we decompose X as UV> and consider the following nonconvex optimization problem via matrix factorization:
minU2C1,V2C2 LN (UV>) :",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
= 1 N P N i=1,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"` i (UV>), (2.2)
where C 1 ✓ Rd1⇥r, C 2 ✓ Rd2⇥r are the rotation-invariant sets induced by C. Recall X⇤ can be factorized as X⇤ = U⇤V⇤>, then we need to make sure that U⇤ 2 C
1 and V⇤ 2 C
2 .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Besides, it can be seen from (2.2) that the optimal solution is not unique in terms of rotation.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"In order to deal with such identifiability issue, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016b), we consider the following regularized optimization problem:
minU2C1,V2C2 FN (U,V) := LN (UV>)",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"+R(U,V),
where the regularization term is defined as R(U,V) = kU>U V>Vk2
F /8.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"We further decompose the objective function F
N (U,V) into n components to apply stochastic variance-reduced gradient descent:
F N (U,V) := 1 n
P
n
i=1
F i (U,V), (2.3)
where we assume N = nb, and b denotes batch size, i.e., the number of observations associated with each F
i .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"More specifically, we have
F i (U,V) =",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"L i (UV>) +R(U,V),
L i (UV>)",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
= 1 b P b j=1 ` ij (UV > ).,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"(2.4)
Therefore, based on (2.3) and (2.4), we are able to apply the stochastic variance-reduced gradient, which is displayed as Algorithm 1.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"As will be seen in later theoretical analysis, the variance of the proposed stochastic gradient indeed decreases as the iteration number increases, which leads to a faster convergence rate.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Let PCi be the projection operator onto the feasible set C
i
in Algorithm 1, where i 2 {1, 2}.
",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Note that our proposed Algorithm 1 is different from the standard stochastic variance-reduced gradient algorithm (Johnson & Zhang, 2013) in several aspects.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"First, instead of conducting gradient descent directly on X, our algorithm performs alternating stochastic gradient descent on the factorized matrices U and V, which leads to a better computational complexity but a more challenging analysis.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Second,
we construct a novel semi-stochastic gradient term for U (resp. V) as rUFit(U,V) rLit(eX)V +rLN (eX)V, which is different from rUFit(U,V) rUFit(eU, eV) + rUFN (eU, eV) if following the original stochastic variance reduced gradient descent (Johnson & Zhang, 2013).",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
This uniquely devised semi-stochastic gradient is essential for deriving the minimax optimal statistical rate.,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Last but not least, we introduce a projection step to ensure that the estimator produced at each iteration belongs to a feasible set, which is necessary for various low-rank matrix recovery problems.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
We also note that Reddi et al. (2016); Allen-Zhu & Hazan (2016) recently developed SVRG algorithms for general nonconvex finite-sum optimization problem.,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"However, their algorithms only guarantee a sublinear rate of convergence to a stationary point, and cannot exploit the special structure of low-rank matrix factorization.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"In stark contrast, our algorithm is able to leverage the structure of the problem and guaranteed to linearly converge to the unknown low-rank matrix instead of a stationary point.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
Algorithm 1 Low-Rank Stochastic Variance-Reduced Gradient Descent (,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"LRSVRG) Input: loss function L
N ; step size ⌘; number of iterations S,m; initial solution (eU0, eV0).",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"for: s = 1, 2, . . .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
", S do
eU = eUs 1, eV = eVs 1, eX = eUeV> U0 = eU, V0 = eV for: t = 0, 1, 2, . . .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
",m 1 do
Randomly pick i t 2 {1, 2, . . .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
", n} Ut+1 = PC1
Ut ⌘(rUFit(Ut,Vt) rL
it( eX)Vt +rL N",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"( eX)Vt)
Vt+1",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"= PC2 Vt ⌘(rVFit(Ut,Vt) rL
it( eX)>Ut +rL N ( eX)>Ut)
end for (
eUs, eVs) =",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"(Ut,Vt), random t 2 {0, . . .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
",m 1} end for
Output: (eUS , eVS).
",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"Algorithm 2 Initialization Input: loss function L
N ; step size ⌧ ; iteration number T .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"initialize: X
0 = 0 for: t = 1, 2, 3, . . .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
", T do X
t = P r X t 1 ⌧rLN (Xt 1)
end for [U 0 ,⌃0,V 0 ]",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"= SVD r (X T )
",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"eU0 = U 0 (⌃0)1/2, eV0 = V 0 (⌃0)1/2
Output: (eU0, eV0)
",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"As will be seen in later analysis, Algorithm 1 requires a good initial solution to guarantee the linear convergence rate.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"To obtain such an initial solution, we employ the initialization algorithm in Algorithm 2, which is originally proposed in Wang et al. (2016).",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"For any rank-r matrix X 2 Rd1⇥d2 , we use SVD
r
(X) to denote its singular
value decomposition.",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
If SVD r (X) =,2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"[U,⌃,V], we use P r
(X) =",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"U⌃V> to denote the best rank-r approximation of X, or in other words, P
r denotes the projection operator such that P
r (X) = argmin rank(Y)r kX YkF .",2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery,[0],[0]
"In this subsection, we introduce two examples, which include matrix sensing and matrix completion, to illustrate the applicability of our proposed algorithm (Algorithm 1).",2.2. Applications to Specific Models,[0],[0]
"The application of our algorithm to one-bit matrix completion can be found in Appendix A. To apply the proposed method, we only need to specify the form of F
N (U,V) for each specific model, as defined in (2.3).",2.2. Applications to Specific Models,[0],[0]
"In matrix sensing (Recht et al., 2010; Negahban & Wainwright, 2011), we intend to recover the unknown matrix X⇤ 2 Rd1⇥d2 with rank-r from a set of noisy linear measurements such that y = A(X⇤) +",2.2.1. MATRIX SENSING,[0],[0]
"✏, where the linear measurement operator A : Rd1⇥d2 !",2.2.1. MATRIX SENSING,[0],[0]
RN is defined as A(X) =,2.2.1. MATRIX SENSING,[0],[0]
"(hA
1 ,Xi, hA 2 ,Xi, . . .",2.2.1. MATRIX SENSING,[0],[0]
", hA N ,Xi)>, for any X 2 Rd1⇥d2 .",2.2.1. MATRIX SENSING,[0],[0]
"Here N denotes the number of observations, and ✏ represents a sub-Gaussian noise vector with i.i.d. elements and parameter ⌫.",2.2.1. MATRIX SENSING,[0],[0]
"In addition, for each sensing matrix A
i 2 Rd1⇥d2 , it has i.i.d. standard Gaussian entries.",2.2.1. MATRIX SENSING,[0],[0]
"Therefore, we formulate F
N (U,V) for matrix sensing as follows F
N
(U,V) = n 1 P n
i=1 FSi(U,V), where for each component function, we have FSi(U,V) = kySi ASi(UV>)k22/(2b) + R(U,V).",2.2.1. MATRIX SENSING,[0],[0]
"Note that R(U,V) denotes the regularizer, which is defined in Section 2.1.",2.2.1. MATRIX SENSING,[0],[0]
"In addition, {S
i }n i=1 denote the mutually disjoint subsets such that [n
i=1
S i =",2.2.1. MATRIX SENSING,[0],[0]
[,2.2.1. MATRIX SENSING,[0],[0]
"N ], and ASi is defined as a linear measurement operator ASi : Rd1⇥d2 !",2.2.1. MATRIX SENSING,[0],[0]
"Rb, satisfying ASi(X) = (hA
i1 ,Xi, hAi2 ,Xi, . . .",2.2.1. MATRIX SENSING,[0],[0]
", hAib ,Xi)>, with corresponding observations ySi = (yi1 , yi2 , . . .",2.2.1. MATRIX SENSING,[0],[0]
", yib)>.",2.2.1. MATRIX SENSING,[0],[0]
"For matrix completion with noisy observations (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012), our primary goal is to recover the unknown low-rank matrix X⇤ 2 Rd1⇥d2 from a set of randomly observed noisy elements.",2.2.2. MATRIX COMPLETION,[0],[0]
"For example, one commonly-used model is the uniform observation model, which is defined as follows:
",2.2.2. MATRIX COMPLETION,[0],[0]
Y jk,2.2.2. MATRIX COMPLETION,[0],[0]
":=
⇢ X⇤ jk",2.2.2. MATRIX COMPLETION,[0],[0]
"+ Z jk , with probability p, ⇤, otherwise,
where Z 2 Rd1⇥d2 is a noise matrix such that each element Z
jk follows i.i.d.",2.2.2. MATRIX COMPLETION,[0],[0]
"Gaussian distribution with variance ⌫2/(d
1 d 2 ), and we call Y 2 Rd1⇥d2 the observation matrix.",2.2.2. MATRIX COMPLETION,[0],[0]
"In particular, we observe each elements independently with probability p 2 (0, 1).",2.2.2. MATRIX COMPLETION,[0],[0]
"We
denote ⌦ ✓ [d 1 ] ⇥",2.2.2. MATRIX COMPLETION,[0],[0]
"[d 2 ] by the index set of the observed entries, then F
⌦ (U,V) for matrix completion is formulated as F
⌦
(U,V) = n 1 P n
i=1",2.2.2. MATRIX COMPLETION,[0],[0]
"F ⌦Si (U,V), where each component function is defined as F
⌦Si (U,V) =
P
(j,k)2⌦Si (U
j⇤V> k⇤ Yjk)2/(2b)",2.2.2. MATRIX COMPLETION,[0],[0]
"+R(U,V).",2.2.2. MATRIX COMPLETION,[0],[0]
"Note that
{⌦Si}n i=1",2.2.2. MATRIX COMPLETION,[0],[0]
"denote the mutually disjoint subsets such that [n i=1
⌦Si = ⌦.",2.2.2. MATRIX COMPLETION,[0],[0]
"In addition, we have |⌦Si | = b for i = 1, . . .",2.2.2. MATRIX COMPLETION,[0],[0]
", n such that |⌦| = nb.",2.2.2. MATRIX COMPLETION,[0],[0]
"In this section, we present our main theoretical results for Algorithms 1 and 2.",3. Main Theory,[0],[0]
We first introduce several definitions for simplicity.,3. Main Theory,[0],[0]
"Recall that the singular value decomposition of X⇤ is X⇤ = U ⇤ ⌃⇤V ⇤> , then following Tu et al. (2015); Zheng & Lafferty (2016), we define Y⇤ 2 R(d1+d2)⇥(d1+d2) as the corresponding lifted positive semidefinite matrix of X⇤ 2 Rd1⇥d2 in higher dimension
Y⇤ =
 U⇤U⇤> U⇤V⇤>
V⇤U⇤> V⇤V⇤>
= Z⇤Z⇤>,
where U⇤ = U ⇤ (⌃⇤)1/2, V⇤ = V ⇤ (⌃⇤)1/2, and Z⇤ is defined as Z⇤ =",3. Main Theory,[0],[0]
"[U⇤;V⇤] 2 R(d1+d2)⇥r. Besides, we define the solution set in terms of the true parameter Z⇤ as follows:
Z = n Z 2 R(d1+d2)⇥r Z = Z⇤R for some R 2 Q r o ,
where Q r denotes the set of r ⇥ r orthonormal matrices.",3. Main Theory,[0],[0]
"According to this definition, for any Z 2 Z , we can obtain X⇤ = Z
U Z> V , where Z U and Z V denote the top d 1 ⇥ r and bottom d
2 ⇥ r matrices of Z 2 R(d1+d2)⇥r respectively.",3. Main Theory,[0],[0]
Definition 3.1.,3. Main Theory,[0],[0]
"Define the distance between Z and Z⇤ in terms of the optimal rotation as d(Z,Z⇤) such that
d(Z,Z⇤) =",3. Main Theory,[0],[0]
min eZ2Z kZ,3. Main Theory,[0],[0]
eZk F = min,3. Main Theory,[0],[0]
R2Qr kZ,3. Main Theory,[0],[0]
"Z⇤Rk F .
Note that if d(Z,Z⇤)  ",3. Main Theory,[0],[0]
"p 1 , we have kX X⇤k F
 c p 1
d(Z,Z⇤), where c is a constant (Yi et al., 2016).",3. Main Theory,[0],[0]
Definition 3.2.,3. Main Theory,[0],[0]
"Define the neighbourhood of Z⇤ with radius R as
B(R) = n Z 2 R(d1+d2)⇥r d(Z,Z⇤)  R o .
",3. Main Theory,[0],[0]
"Next, we lay out several conditions, which are essential for proving our main theory.",3. Main Theory,[0],[0]
"We impose restricted strong convexity (RSC) and smoothness (RSS) conditions (Negahban et al., 2009; Loh & Wainwright, 2013) on the sample loss function L
N .",3. Main Theory,[0],[0]
Condition 3.3 (Restricted Strong Convexity).,3. Main Theory,[0],[0]
"Assume L
N
is restricted strongly convex with parameter µ, such that for all matrices X,Y 2 Rd1⇥d2 with rank at most 3r
L N (Y) L N (X) +",3. Main Theory,[0],[0]
hrL N,3. Main Theory,[0],[0]
"(X),Y Xi+ µ 2 kY Xk2 F .
",3. Main Theory,[0],[0]
Condition 3.4 (Restricted Strong Smoothness).,3. Main Theory,[0],[0]
"Assume L N
is restricted strongly smooth with parameter L, such that for all matrices X,Y 2 Rd1⇥d2 with rank at most 3r
L N (Y)  L N (X) +",3. Main Theory,[0],[0]
"hrL N (X),Y Xi+ L 2 kY Xk2 F .
",3. Main Theory,[0],[0]
"Based on Conditions 3.3 and 3.4, we prove that the sample loss function L
N satisfies a projected notion of the restricted Lipschitz continuous gradient property as displayed in the following lemma.",3. Main Theory,[0],[0]
Lemma 3.5.,3. Main Theory,[0],[0]
"Suppose the sample loss function L
N satisfies Conditions 3.3 and 3.4.",3. Main Theory,[0],[0]
"For any rank-r matrices X,Y 2 Rd1⇥d2 , let the singular value decomposition of X be U
1 ⌃ 1 V > 1 , then we have
L N (X) L N (Y) +",3. Main Theory,[0],[0]
"hrL N (Y),X Yi
+
1 4L keU>(rL N (X) rL",3. Main Theory,[0],[0]
"N (Y))k2 F
+
1 4L k(rL N (X) rL",3. Main Theory,[0],[0]
"N (Y))eVk2 F ,
where eU 2 Rd1⇥r1 is an orthonormal matrix with r 1  3r which satisfies col(U
1 ) ✓ col(eU), and eV 2 Rd2⇥r2 is an orthonormal matrix with r
2  3r that satisfies col(V 1 ) ✓ col(eV), and L is the RSS parameter.
",3. Main Theory,[0],[0]
Lemma 3.5 is essential to analyze the nonconvex optimization for low-rank matrix recovery and derive a linear convergence rate.,3. Main Theory,[0],[0]
"Since the RSC and RSS conditions can only be verified over the subspace of low-rank matrices, the standard Lipschitz continuous gradient property could not be derived.",3. Main Theory,[0],[0]
That is why we need such a restricted version of Lipschitz continuous gradient property.,3. Main Theory,[0],[0]
"To the best of our knowledge, this new notion of Lipschitz continuous gradient has never been proposed in the literature before.",3. Main Theory,[0],[0]
"We believe it can be of broader interests for other nonconvex optimization problems to prove tight bounds.
",3. Main Theory,[0],[0]
"Moreover, we assume that the gradient of the sample loss function rL
N at X⇤ is upper bounded.",3. Main Theory,[0],[0]
Condition 3.6.,3. Main Theory,[0],[0]
Recall the unknown rank-r matrix X⇤ 2 Rd1⇥d2 .,3. Main Theory,[0],[0]
"Given a fixed sample size N and tolerance parameter 2 (0, 1), we let ✏(N, ) be the smallest scalar such that with probability at least 1 , we have
krL N (X⇤)k 2  ✏(N, ),
where ✏(N, ) depends on sample size N and .
",3. Main Theory,[0],[0]
"Finally, we assume that each component loss function L i in (2.4) satisfies the restricted strong smoothness condition.",3. Main Theory,[0],[0]
Condition 3.7 (Restricted Strong Smoothness for each Component).,3. Main Theory,[0],[0]
"Given a fixed batch size b, assume L
i is restricted strongly smooth with parameter L0, such that for
all matrices X,Y 2 Rd1⇥d2 with rank at most 3r
L i (Y)  ",3. Main Theory,[0],[0]
L i (X) + hrL,3. Main Theory,[0],[0]
"i (X),Y Xi+ L 0
2
kY Xk2 F .
",3. Main Theory,[0],[0]
"In latter analysis for generic setting, we assume that Conditions 3.3-3.7 hold, while for each specific model, we will verify these conditions respectively in the appendix.",3. Main Theory,[0],[0]
"The following theorem shows that, in general, Algorithm 1 converges linearly to the unknown low-rank matrix X⇤ up to a statistical precision.",3.1. Results for the Generic Setting,[0],[0]
Theorem 3.8 (LRSVRG).,3.1. Results for the Generic Setting,[0],[0]
"Suppose that Conditions 3.3, 3.4, 3.6, and 3.7 are satisfied.",3.1. Results for the Generic Setting,[0],[0]
"There exist constants c 1 , c 2 , c 3 and c 4
such that for any eZ0 =",3.1. Results for the Generic Setting,[0],[0]
"[eU0; eV0] 2 B(c
2 p r ) with c 2  min{1/4, p
2µ0/(5(3L+ 1))}, if the sample size N is large enough such that ✏2(N, )  c2 2 (1 ⇢)µ0 2 r /(c 3
r), where µ0 = min{µ, 1}, and the contraction parameter ⇢ is defined as follows:
⇢ = 10
µ0
✓
1
⌘m 1
+ c 4 ⌘ 1
L02 ◆ ,
then with the step size ⌘ = c 1 / 1 and the number of iterations m properly chosen, the estimator eZS =",3.1. Results for the Generic Setting,[0],[0]
"[eUS ; eVS ] outputed from Algorithm 1 satisfies
E ⇥ d2(eZS ,Z⇤) ⇤  ⇢Sd2(eZ0,Z⇤)",3.1. Results for the Generic Setting,[0],[0]
+,3.1. Results for the Generic Setting,[0],[0]
"c3r✏ 2 (N, )
(1 ⇢)µ0",3.1. Results for the Generic Setting,[0],[0]
"r
, (3.1)
with probability at least 1 .",3.1. Results for the Generic Setting,[0],[0]
Remark 3.9.,3.1. Results for the Generic Setting,[0],[0]
"Theorem 3.8 implies that to achieve linear rate of convergence, it is necessary to set the step size ⌘ to be small enough and the inner loop iterations m to be large enough such that ⇢",3.1. Results for the Generic Setting,[0],[0]
< 1.,3.1. Results for the Generic Setting,[0],[0]
Here we present a specific example to demonstrate such ⇢ is attainable.,3.1. Results for the Generic Setting,[0],[0]
"As stated in Theorem 3.8, if we set the step size ⌘ = c0
1
/ 1 , where c0 1 = µ0/ 15c 4 L02
, then the contraction parameter ⇢ is calculated as follows:
⇢ = 10
mµ0⌘ 1 +
2
3
.
",3.1. Results for the Generic Setting,[0],[0]
"Therefore, under the condition that m c 5 2, we obtain ⇢  5/6 < 1, which leads to the linear convergence rate of Algorithm 1.",3.1. Results for the Generic Setting,[0],[0]
"Besides, our algorithm also achieves the linear convergence in terms of reconstruction error, since the reconstruction error keXs X⇤k2
F can be upper bounded by C
1 · d2(eZs,Z⇤), where C is a constant.",3.1. Results for the Generic Setting,[0],[0]
Remark 3.10.,3.1. Results for the Generic Setting,[0],[0]
"The right hand side of (3.1) consists of two parts, where the first one represents the optimization error and the second one denotes the statistical error.",3.1. Results for the Generic Setting,[0],[0]
"Note that in the noiseless case, since ✏(N, ) = 0, the statistical error becomes zero.",3.1. Results for the Generic Setting,[0],[0]
"As stated in Remark 3.9, with
appropriate ⌘ and m, we are able to achieve the linear rate of convergence.",3.1. Results for the Generic Setting,[0],[0]
"Therefore, in order to make sure the optimization error satisfies ⇢Sd2(eZ0,Z⇤)  ✏, it suffices to perform S = O log(1/✏)
outer loop iterations.",3.1. Results for the Generic Setting,[0],[0]
Recall that from Remark 3.9 we have m = O(2).,3.1. Results for the Generic Setting,[0],[0]
"Since for each outer loop iteration, it is required to calculate m mixed stochastic variance-reduced gradients and one full gradient, the overall gradient complexity for our algorithm to achieve ✏ precision is
O ⇣ (N + 2b)",3.1. Results for the Generic Setting,[0],[0]
"log ⇣ 1
✏
⌘⌘
.
",3.1. Results for the Generic Setting,[0],[0]
"However, the gradient complexity of the state-of-the-art gradient descent based algorithm (Wang et al., 2016) to achieve ✏ precision is O N log(1/✏)
.",3.1. Results for the Generic Setting,[0],[0]
"Therefore, provided that   n, our method is computationally more efficient than the state-of-the-art gradient descent approach.",3.1. Results for the Generic Setting,[0],[0]
"The detailed comparison of the overall computational complexity among different methods for each specific model can be found in next subsection.
",3.1. Results for the Generic Setting,[0],[0]
"To satisfy the initial condition eZ0 2 B(c 2 p r
) in Theorem 3.8, according to Lemma 5.14 in Tu et al. (2015), it suffices to guarantee that eX0 is close enough to the unknown rankr matrix X⇤ such that keX0 X⇤k
F  c r , where c  min{1/2, 2c
2 }.",3.1. Results for the Generic Setting,[0],[0]
The following theorem shows the output of Algorithm 2 can satisfy this condition.,3.1. Results for the Generic Setting,[0],[0]
Theorem 3.11.,3.1. Results for the Generic Setting,[0],[0]
"(Wang et al., 2016) Suppose the sample loss function L
N satisfies Conditions 3.3, 3.4 and 3.6.",3.1. Results for the Generic Setting,[0],[0]
"Let eX0 = eU0 eV0>, where (eU0, eV0) is the produced initial solution in Algorithm 2.",3.1. Results for the Generic Setting,[0],[0]
"If L/µ 2 (1, 4/3), then with step size ⌧ = 1/L, we have with probability at least 1 that
keX0 X⇤k F  ⇢T kX⇤k F + 2
p 3r✏(N, )
L(1 ⇢) ,
where ⇢ = 2 p 1 µ/L is the contraction parameter.
",3.1. Results for the Generic Setting,[0],[0]
"Theorem 3.11 suggests that, in order to guarantee keX0 X⇤k
F  c r , we need to perform at least T = log(c0
r /kX",3.1. Results for the Generic Setting,[0],[0]
"⇤k F
)/ log(⇢) number of iterations to ensure the optimization error is small enough, and it is also necessary to make sure the sample size N is large enough such that ✏(N, )  c0L(1 ⇢)
",3.1. Results for the Generic Setting,[0],[0]
"r
/ 2 p 3r
, which corresponds to a sufficiently small statistical error.",3.1. Results for the Generic Setting,[0],[0]
"In this subsection, we demonstrate the implications of our generic theory to specific models.",3.2. Implications for Specific Models,[0],[0]
"For each specific model, we only need to verify Conditions 3.3-3.7.",3.2. Implications for Specific Models,[0],[0]
"We denote d = max{d
1 , d 2
} in the following discussions.",3.2. Implications for Specific Models,[0],[0]
"We provide the theoretical guarantee of our algorithm for matrix sensing.
",3.2.1. MATRIX SENSING,[0],[0]
Corollary 3.12.,3.2.1. MATRIX SENSING,[0],[0]
"Consider matrix sensing with standard normal linear operator A and noise vector ✏, whose entries follow i.i.d. sub-Gaussian distribution with parameter ⌫.",3.2.1. MATRIX SENSING,[0],[0]
"There exist constants {c
i }8 i=1 such that if the number of observations satisfies N c
1 rd and we choose the parameters ⌘ = c
2 / 1 , where c 2 = µ0/ c 3  , m c 4
2, then for any initial solution satisfies eZ0 2 B(c
5 p r
), with probability at least 1 c
6
exp
c 7 d , the output of Algorithm 1 satisfies
E ⇥ d2(eZS ,Z⇤) ⇤  ⇢Sd2(eZ0,Z⇤)",3.2.1. MATRIX SENSING,[0],[0]
+,3.2.1. MATRIX SENSING,[0],[0]
"c 8 ⌫2 rd
N , (3.2)
where the contraction parameter ⇢ < 1.",3.2.1. MATRIX SENSING,[0],[0]
Remark 3.13.,3.2.1. MATRIX SENSING,[0],[0]
"According to (3.2), in the noisy setting, the output of our algorithm achieves O p rd/N
statistical error after O log(N/(rd))
number of outer loop iterations.",3.2.1. MATRIX SENSING,[0],[0]
"This statistical error matches the minimax lower bound for matrix sensing (Negahban & Wainwright, 2011).",3.2.1. MATRIX SENSING,[0],[0]
"In the noiseless case, to ensure the restricted strong convexity and smoothness conditions of our objective function, we require sample size N = O(rd), which attains the optimal sample complexity for matrix sensing (Recht et al., 2010; Tu et al., 2015; Wang et al., 2016).",3.2.1. MATRIX SENSING,[0],[0]
"Most importantly, from Remark 3.10 we know that for the output eZS of our algorithm, the overall computational complexity of our algorithm to achieve ✏ precision for matrix sensing is O (Nd2 + 2bd2) log(1/✏)
.",3.2.1. MATRIX SENSING,[0],[0]
"Nevertheless, the overall computational complexity for the state-of-the-art gradient descent algorithms in both noiseless (Tu et al., 2015) and noisy (Wang et al., 2016) cases to obtain ✏ precision is O Nd2 log(1/✏)
.",3.2.1. MATRIX SENSING,[0],[0]
"Therefore, our algorithm is more efficient provided that   n, which is consistent with the result obtained by (Zhang et al., 2017b).",3.2.1. MATRIX SENSING,[0],[0]
"In their work, they proposed an accelerated stochastic gradient descent method for matrix sensing based on the restricted isometry property.",3.2.1. MATRIX SENSING,[0],[0]
"However, since the restricted isometry property is more restrictive than the restricted strong convex and smoothness conditions, their results cannot be applied to more general low-rank matrix recovery problems.",3.2.1. MATRIX SENSING,[0],[0]
We provide the theoretical guarantee of our algorithm for matrix completion.,3.2.2. MATRIX COMPLETION,[0],[0]
"In particular, we consider a partial observation model, which means only the elements over a subset X ✓",3.2.2. MATRIX COMPLETION,[0],[0]
"[d
1 ] ⇥",3.2.2. MATRIX COMPLETION,[0],[0]
[d 2 ] are observed.,3.2.2. MATRIX COMPLETION,[0],[0]
"In addition, we assume a uniform sampling model for X , which is defined as 8(j, k) 2 X , j ⇠ uniform([d
1 ]), k ⇠ uniform([d 2 ]).",3.2.2. MATRIX COMPLETION,[0],[0]
"To avoid overly sparse matrices (Gross, 2011; Negahban & Wainwright, 2012), we impose the following incoherence condition (Candès & Recht, 2009).",3.2.2. MATRIX COMPLETION,[0],[0]
"More specifically, suppose the singular value decomposition of X⇤ is X⇤ = U ⇤ ⌃⇤V ⇤> , we assume the following conditions hold kU⇤k 2,1  p r/d 1 and kV⇤k 2,1 
",3.2.2. MATRIX COMPLETION,[0],[0]
"A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery p
r/d 2 , where r denotes the rank of X⇤, and denotes the incoherence parameter for X⇤.
",3.2.2. MATRIX COMPLETION,[0],[0]
"In order to make sure our produced estimator satisfies incoherence constraint, we need a projection step, which is displayed in Algorithm 1.",3.2.2. MATRIX COMPLETION,[0],[0]
"Therefore, we construct two feasible sets C
i
= {A 2 Rdi⇥r kAk 2,1  ↵i}, where
↵ i =
p
r 1 /d",3.2.2. MATRIX COMPLETION,[0],[0]
"i , and i 2 {1, 2}.",3.2.2. MATRIX COMPLETION,[0],[0]
"Thus for any U 2 C 1
and V 2 C 2 , we have X = UV> 2 C = {A 2 Rd1⇥d2 kAk1,1  ↵}, where ↵ = r 1/ p",3.2.2. MATRIX COMPLETION,[0],[0]
"d 1 d 2 .
",3.2.2. MATRIX COMPLETION,[0],[0]
We have the following convergence result of our algorithm for matrix completion.,3.2.2. MATRIX COMPLETION,[0],[0]
Corollary 3.14.,3.2.2. MATRIX COMPLETION,[0],[0]
Consider noisy matrix completion under uniform sampling model.,3.2.2. MATRIX COMPLETION,[0],[0]
Suppose X⇤ satisfies incoherence condition.,3.2.2. MATRIX COMPLETION,[0],[0]
"There exist constants {c
i }7 i=1 such that if we choose parameters ⌘ = c
1 / 1 , where c 1 = µ0/ c 2 
, m c
3 2, and the number of observations satisfies N c
4 r2d log d, then for any initial solution satisfies eZ0 2 B(c
5 p r ), then with probability at least 1 c 6
/d, the output of Algorithm 1 satisfies
E[d2(eZS ,Z⇤) ⇤  ⇢Sd2(eZ0,Z⇤) +",3.2.2. MATRIX COMPLETION,[0],[0]
"c 7
rd log d
N , (3.3)
where = max{⌫2, r 2 2 1 }, the contraction parameter ⇢ <",3.2.2. MATRIX COMPLETION,[0],[0]
1.,3.2.2. MATRIX COMPLETION,[0],[0]
Remark 3.15.,3.2.2. MATRIX COMPLETION,[0],[0]
"Corollary 3.14 implies that after O log(N/(r2d log d))
number of outer loops, our algorithm achieves O r p d log d/N
statistical error, which is near optimal compared with the minimax lower bound O( p
rd log d/N) for matrix completion proved in Negahban & Wainwright (2012); Koltchinskii et al. (2011).",3.2.2. MATRIX COMPLETION,[0],[0]
"And its sample complexity is O(r2d log d), which matches the best-known sample complexity of matrix completion using nonconvex matrix factorization (Zheng & Lafferty, 2016).",3.2.2. MATRIX COMPLETION,[0],[0]
"Recall that from Remark 3.10, the overall computational complexity of our algorithm to reach ✏ accuracy for matrix completion is O (N + 2b)r3d log(1/✏)
.",3.2.2. MATRIX COMPLETION,[0],[0]
"However, for the state-of-the-art gradient descent based algorithms, the computational complexity for both noiseless (Zheng & Lafferty, 2016) and noisy (Wang et al., 2016) cases to obtain ✏ accuracy is O Nr3d log(1/✏)
.",3.2.2. MATRIX COMPLETION,[0],[0]
Thus the computational complexity of our algorithm is lower than the state-of-the-art gradient descent methods if we have   n.,3.2.2. MATRIX COMPLETION,[0],[0]
"In addition, for the online stochastic gradient descent algorithm (Jin et al., 2016), the overall computational complexity is O(r44d log(1/✏)).",3.2.2. MATRIX COMPLETION,[0],[0]
"Since their results has a fourth power dependency on both r and , our method can yield a significant improvement over the online method when r, is large.",3.2.2. MATRIX COMPLETION,[0],[0]
"In this section, we present the experimental performance of our proposed algorithm for different models based on
numerical simulations and real data experiments.",4. Experiments,[0],[0]
"We first investigate the effectiveness of our proposed algorithm compared with the state-of-the-art gradient descent algorithm (Wang et al., 2016; Zheng & Lafferty, 2016).",4.1. Numerical Simulations,[0],[0]
"Then, we evaluate the sample complexity required by both methods to achieve exact recovery in the noiseless case.",4.1. Numerical Simulations,[0],[0]
"Finally, we illustrate the statistical error of our method in the noisy case.",4.1. Numerical Simulations,[0],[0]
Note that both algorithms use the same initialization method (Algorithm 2) with optimal parameters selected by cross validation.,4.1. Numerical Simulations,[0],[0]
"Furthermore, all results are averaged over 30 trials.",4.1. Numerical Simulations,[0],[0]
"Note that due to the space limit, we only lay out simulation results for matrix completion, results for other models can be found in Appendix A.
For matrix completion, we consider the unknown low-rank matrix X⇤ in the following settings: (i) d
1 = 100, d 2 =
80, r = 2; (ii) d 1 = 120, d 2 = 100, r = 3; (iii) d 1 = 140, d 2
= 120, r = 4.",4.1. Numerical Simulations,[0],[0]
"First, we generate the unknown lowrank matrix X⇤ as X⇤ = U⇤V⇤>, where U⇤ 2 Rd1⇥r and V⇤ 2 Rd2⇥r are randomly generated.",4.1. Numerical Simulations,[0],[0]
"Next, we use uniform observation model to obtain data matrix Y.",4.1. Numerical Simulations,[0],[0]
"Finally, we consider two settings: (1) noisy case: the noise follows i.i.d. normal distribution with variance 2 = 0.25 and (2) noiseless case.
",4.1. Numerical Simulations,[0],[0]
"For the results of convergence rate, we show the mean squared error kbX X⇤k2
F
/(d 1 d 2 ) in log scale versus number of effective data passes.",4.1. Numerical Simulations,[0],[0]
Figures 1(a) and 1(c) illustrate the linear rate of convergence of our algorithm (LRSVRG) in the setting (i).,4.1. Numerical Simulations,[0],[0]
"The results imply that after the same number of effective data passes, our algorithm is more efficient than the state-of-the-art gradient descent algorithm in estimation error.",4.1. Numerical Simulations,[0],[0]
"For the results of sample complexity, we illustrate the empirical probability of exact recovery under rescaled sample size N/(rd log d).",4.1. Numerical Simulations,[0],[0]
"For the estimator bX given by different algorithms, it is considered to achieve exact recovery, if the relative error kbX X⇤k
F /kX⇤k F
is less than 10 3.",4.1. Numerical Simulations,[0],[0]
Figure 1(b) shows the empirical recovery probability of different methods in the setting (i).,4.1. Numerical Simulations,[0],[0]
"It implies a phase transition around N = 3rd log d. Although our theoretical results requires O(r2d log d) sample complexity, the simulation results suggest that our method achieves the optimal sample complexity N = O(rd log d).",4.1. Numerical Simulations,[0],[0]
Note that we leave out results in other settings to avoid redundancy since we get similar patterns for these results.,4.1. Numerical Simulations,[0],[0]
"The results of statistical error are displayed in Figure 1(d), which is consistent with our main result in Corollary 3.14.",4.1. Numerical Simulations,[0],[0]
"We apply our proposed stochastic variance-reduced gradient algorithm for matrix completion to collaborative filtering in recommendation system, and compare it with sev-
eral state-of-the-art matrix completion algorithms, including singular value projection (SVP) (Jain et al., 2010), trace norm constraint (TNC) (Jaggi et al., 2010), alternating minimization (AltMin) (Jain et al., 2013b), spectral regularization (SoftImpute) (Mazumder et al., 2010), rank-one matrix pursuit (Wang et al., 2014), nuclear norm penalty (Negahban & Wainwright, 2011), nonconvex SCAD penalty (Gui & Gu, 2015) and gradient descent (Zheng & Lafferty, 2016).",4.2. Real Data Experiments,[0],[0]
"In particular, we use three large recommendation datasets called Jester1, Jester2 and Jester3 (Goldberg et al., 2001), which contain anonymous ratings on 100 jokes from different users.",4.2. Real Data Experiments,[0],[0]
"The jester datasets consist of {24983, 23500, 24938} rows and 100 columns respectively, with {106, 106, 6 ⇥ 105} ratings correspondingly.",4.2. Real Data Experiments,[0],[0]
"Besides, the rating scales take value from [ 10, 10].",4.2. Real Data Experiments,[0],[0]
Our goal is to recover the whole rating matrix based on partial observations.,4.2. Real Data Experiments,[0],[0]
"Therefore, we randomly choose half of the ratings as our observed data, and predict the other half based on different matrix completion algorithms.",4.2. Real Data Experiments,[0],[0]
"We perform 10 different observed/unobserved entry splittings, and record the averaged root mean square error (RMSE) as well as CPU time for different algorithms.",4.2. Real Data Experiments,[0],[0]
"We summarize the comparisons in Table 1, which suggests that our proposed LRSVRG algorithm outperforms all the other baseline algorithms in terms of RMSE and CPU time, which aligns well with our theory.",4.2. Real Data Experiments,[0],[0]
We proposed a unified stochastic variance-reduced gradient descent framework for low-rank matrix recovery that integrates both optimization-theoretic and statistical analyses.,5. Conclusions and Future Work,[0],[0]
"Based on the mild restricted strong convexity and smoothness conditions, we derived a projected notion of the restricted Lipschitz continuous gradient property, and established the linear convergence rate of our proposed algorithm.",5. Conclusions and Future Work,[0],[0]
"With an appropriate initialization procedure, we proved that our algorithm enjoys improved computational complexity compared with existing approaches.",5. Conclusions and Future Work,[0],[0]
There are still many interesting problems along this line of research.,5. Conclusions and Future Work,[0],[0]
"For example, we will study accelerating the low-rank plus sparse matrix/tensor recovery (Gu et al., 2014; 2016; Yi et al., 2016; Zhang et al., 2017a) through variance reduction technique in the future.",5. Conclusions and Future Work,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgment,[0],[0]
This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539.,Acknowledgment,[0],[0]
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.,Acknowledgment,[0],[0]
We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery.,abstractText,[0],[0]
"Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery.",abstractText,[0],[0]
"Based upon the mild restricted strong convexity and smoothness conditions, we derive a projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity.",abstractText,[0],[0]
"Moreover, our algorithm can be employed to both noiseless and noisy observations, where the (near) optimal sample complexity and statistical rate can be attained respectively.",abstractText,[0],[0]
"We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally.",abstractText,[0],[0]
A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery,title,[0],[0]
"Multi-label classification aims to build classification models for objects assigned with multiple labels simultaneously, which is a common learning paradigm in real-world applications.",1. Introduction,[0],[0]
"In text categorization, a document may be associated with a range of topics, such as science, entertainment, and news (Schapire & Singer, 2000); in image classification, an image can have both field and mountain tags (Boutell et al., 2004); in music information retrieval, a piece of music can convey various messages such as classic, piano and passionate (Turnbull et al., 2008).
",1. Introduction,[0],[0]
"In traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal.",1. Introduction,[0],[0]
"In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as
1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China.",1. Introduction,[0],[0]
"Correspondence to: Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
each instance can be associated with multiple labels simultaneously.,1. Introduction,[0],[0]
"For example, it is difficult to tell which mistake of the following two cases is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label.",1. Introduction,[0],[0]
"Therefore, a number of performance measures focusing on different aspects have been proposed, such as Hamming loss, ranking loss, one-error, average precision, coverage (Schapire & Singer, 2000), micro-F1 and macro-F1 (Tsoumakas et al., 2011).
",1. Introduction,[0],[0]
"Multi-label learning algorithms usually perform differently on different measures; however, there are only a few studies about multi-label performance measures.",1. Introduction,[0],[0]
Dembczynski et al. (2010) showed that Hamming loss and subset 0/1 loss could not be optimized at the same time.,1. Introduction,[0],[0]
"Gao & Zhou (2013) proposed to study the Bayes consistency of surrogate losses for multi-label learning; they proved that none of convex surrogate loss is consistent with ranking loss, and gave a consistent surrogate loss function for Hamming loss in deterministic case.",1. Introduction,[0],[0]
"There are a number of studies about F-measure, mostly focusing on single-label tasks, including multi-label learning as application.",1. Introduction,[0],[0]
"For example, Ye et al. (2012) gave justifications and connections about F-measure optimization using decision theoretic approaches (DTA) and empirical utility maximization approaches (EUM).",1. Introduction,[0],[0]
"Later, Waegeman et al. (2014) studied the F-measure optimality of inference algorithms from the DTA perspective.",1. Introduction,[0],[0]
Koyejo et al. (2015) devoted to study of EUM optimal multi-label classifiers.,1. Introduction,[0],[0]
"These theoretical studies offer much insight, though lacking a unified understanding of relation among a variety of multi-label performance measures.",1. Introduction,[0],[0]
"Moreover, some performance measures which have been popularly used in evaluation (Zhang & Wu, 2015) have not been theoretically studied.
",1. Introduction,[0],[0]
"In this paper, we try to disclose some shared properties among different measures and establish a unified understanding for multi-label performance evaluation.",1. Introduction,[0],[0]
"We propose a margin view to revisit eleven commonly used multi-label performance measures, including Hamming loss, ranking loss, one-error, coverage, average precision, macro-, micro- and instance-averaging F-measures and AUCs.",1. Introduction,[0],[0]
"Specifically, we propose the concepts of labelwise margin and instance-wise margin, based on which the corresponding effectiveness of multi-label classifiers is defined and then used as bridge to connect different perfor-
mance measures.",1. Introduction,[0],[0]
"Our theoretical results show that by maximizing instance-wise margin, macro-AUC, macro-F1 and Hamming loss are to be optimized, whereas by maximizing label-wise margin, the other eight performance measures except micro-AUC are to be optimized.",1. Introduction,[0],[0]
"Inspired by the theoretical findings, we design the LIMO (Labelwise and Instance-wise Margins Optimization) approach to maximize both the two margins.",1. Introduction,[0],[0]
"Experiments validate our theoretical findings and demonstrate a flexible way to optimize different measures through one approach by different parameter settings.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
Section 2 introduces the notation and definitions of eleven multi-label performance measures.,1. Introduction,[0],[0]
"Section 3 proposes the label-wise and instance-wise margins, and presents our theoretical results.",1. Introduction,[0],[0]
Section 4 presents the LIMO approach.,1. Introduction,[0],[0]
Section 5 reports the results of experiments.,1. Introduction,[0],[0]
"Finally, Section 6 concludes and indicates several future issues.",1. Introduction,[0],[0]
"Assume that xi ∈ Rd×1 is a real value instance vector, yi ∈ {0, 1}l×1 is a label vector for xi. m denotes the number of training samples.",2.1. Notation,[0],[0]
"Therefore yij (i ∈ {1, . . .",2.1. Notation,[0],[0]
",m}, j ∈ {1, . . .",2.1. Notation,[0],[0]
", l}) means the jth label of the ith instance, and yij = 1 or 0 means the jth label is relevant or irrelevant.",2.1. Notation,[0],[0]
"The instance matrix is X ∈ Rm×d and the label matrix is Y ∈ {0, 1}m×l.",2.1. Notation,[0],[0]
H :,2.1. Notation,[0],[0]
"Rd → {0, 1}l is the multi-label classifier, which consists of l models, one for a label, so H = {h1, . . .",2.1. Notation,[0],[0]
", hl} and hj(xi) denotes the prediction of yij .",2.1. Notation,[0],[0]
"Moreover, F : Rd → Rl is the multi-label predictor and the predicted value can be regarded as the confidence of relevance.",2.1. Notation,[0],[0]
"Similarly, F can be decomposed as {f1, . . .",2.1. Notation,[0],[0]
", fl} where fj(xi) denotes the predicted value of yij .
",2.1. Notation,[0],[0]
H can be induced from F via thresholding functions.,2.1. Notation,[0],[0]
"For example, hj(xi) =",2.1. Notation,[0],[0]
[[fj(xi) > t(xi)]] uses a thresholding function based on the instance xi and outputs 1 if predicted value is higher than the threshold.,2.1. Notation,[0],[0]
"[[π]] returns 1 if predicate π holds, and 0 otherwise.
",2.1. Notation,[0],[0]
"For simplification, we use Y i· to denote the ith row vector and Y ·j to denote the jth column vector of the label matrix.",2.1. Notation,[0],[0]
"Furthermore, Y +i· (or Y − i· ) denotes the index set of relevant (or irrelevant) labels of Y i·.",2.1. Notation,[0],[0]
"Formally, Y +i· = {j |yij = 1} and Y −i· = {j |yij = 0}.",2.1. Notation,[0],[0]
"In terms of jth column of label matrix, Y +·j = {i |yij = 1} denotes the index set of positive instances of the jth label and Y −·j = {i |yij = 0} denotes the set of negative instances similarly.",2.1. Notation,[0],[0]
"We use | · | to denote the cardinality of a set, thus, the number of relevant labels of xi is |Y +i· |.",2.1. Notation,[0],[0]
Table 1 summarizes the eleven multi-label performance measures commonly used in previous studies.,2.2. Multi-label Performance Measures,[0],[0]
"The first five measures (Hamming loss, ranking loss, one-error, coverage, average precision) are considered in Schapire & Singer (2000) and a multitude of works, e.g., Huang et al. (2012) and Zhang & Wu (2015).",2.2. Multi-label Performance Measures,[0],[0]
The next six measures are extensions of F-measure and AUC (the Area Under the ROC Curve) in multi-label classification via different averaging strategies.,2.2. Multi-label Performance Measures,[0],[0]
"These F-measures are popluar both in algorithm evaluation (Liu & Tsang, 2015) and theoretical analysis (Koyejo et al., 2015).",2.2. Multi-label Performance Measures,[0],[0]
"AUCs are used for algorithm evaluation such as in Lampert (2011), Pham et al. (2015) and Zhang & Wu (2015).
",2.2. Multi-label Performance Measures,[0],[0]
"Some of these measures are defined on classifier H , and they care about the binary classification performance.",2.2. Multi-label Performance Measures,[0],[0]
"While some of these measures are defined on predictor F , and they usually measure the ranking performance of the predictor.",2.2. Multi-label Performance Measures,[0],[0]
We have noticed that some performance measures on ranking are ill-defined when F is a constant function.,2.2. Multi-label Performance Measures,[0],[0]
"For example, if F outputs 1 for all labels, then oneerror(F ) = 0, coverage(F ) = 0 and various AUCs will be 1, which are the optimal values respectively.",2.2. Multi-label Performance Measures,[0],[0]
"In multi-label learning community, there is often an underlying assumption that a total ranking can be induced from continuous real-value predictions, which is common in practical cases.",2.2. Multi-label Performance Measures,[0],[0]
"In this paper, we still stick to the convention in previous works and assume that no tie happens in continuous prediction to solve this definition flaw.",2.2. Multi-label Performance Measures,[0],[0]
"Here we define two new concepts: label-wise margin and instance-wise margin.
",3. Theoretical Results,[0],[0]
Definition 1.,3. Theoretical Results,[0],[0]
"Given a multi-label predictor F : Rd → Rl and F = {f1, . . .",3. Theoretical Results,[0],[0]
", fl}, a training set (X,Y ), the labelwise margin on instance xi is defined as:
γlabeli =",3. Theoretical Results,[0],[0]
"min u,v {fu(xi)− fv(xi)",3. Theoretical Results,[0],[0]
"| (u, v) ∈",3. Theoretical Results,[0],[0]
Y +i· ×,3. Theoretical Results,[0],[0]
"Y − i· }.
Y +i· × Y",3. Theoretical Results,[0],[0]
"− i· is the set of all the (relevant, irrelevant) label index pairs of instance i.
Definition 2.",3. Theoretical Results,[0],[0]
"Given a multi-label predictor F : Rd → Rl and F = {f1, . . .",3. Theoretical Results,[0],[0]
", fl}, a training set (X,Y ), the instancewise margin on label Y ·j is defined as:
γinstj = min a,b {fj(xa)− fj(xb)",3. Theoretical Results,[0],[0]
"| (a, b) ∈ Y +·j × Y",3. Theoretical Results,[0],[0]
"− ·j }.
",3. Theoretical Results,[0],[0]
Y +·j × Y,3. Theoretical Results,[0],[0]
"− ·j is the set of all the (positive, negative) instance index pairs of label j.
Label-wise margin and instance-wise margin describe the discriminative ability of F .",3. Theoretical Results,[0],[0]
"The larger the label-wise mar-
gin, the easier to distinguish relevant and irrelevant labels of an instance.",3. Theoretical Results,[0],[0]
"Meanwhile, the larger the instance-wise margin, the easier for F to distinguish positive and negative instances of a particular label.",3. Theoretical Results,[0],[0]
"Therefore, we want to maximize label-wise/instance-wise margin to get better performance.
",3. Theoretical Results,[0],[0]
"Although we prefer maximizing these two margins, with respect to performance measures, the objective can be relaxed.",3. Theoretical Results,[0],[0]
"We define three properties a predictor F can have: label-wise effective, instance-wise effective and double effective.
",3. Theoretical Results,[0],[0]
Definition 3.,3. Theoretical Results,[0],[0]
"If all the label-wise margins ofF on a dataset D = (X,Y ) are positive, this predictor F is label-wise effective on D.
Definition 4.",3. Theoretical Results,[0],[0]
"If all the instance-wise margins of F on a dataset D = (X,Y ) are positive, this predictor F is instance-wise effective on D.
Definition 5.",3. Theoretical Results,[0],[0]
"If all the label-wise margins and instancewise margins of F on a dataset D = (X,Y ) are positive, this predictor F is double effective on D.
Roughly speaking, label-wise effective means F can exactly distinguish relevant and irrelevant labels of each instance and instance-wise effective means F can exactly distinguish positive and negative instances of every label.",3. Theoretical Results,[0],[0]
"Not surprisingly, double effective F has the strongest ability in distinguishing.
",3. Theoretical Results,[0],[0]
"In the next two subsections, we use the effectiveness to an-
alyze different performance measures, and summarize the analysis results in Section 3.3.",3. Theoretical Results,[0],[0]
"Several multi-label performance measures can be empirically optimized according to the following theorems:
Theorem 1.",3.1. Performance Measures on Ranking,[0],[0]
"If a multi-label predictorF is label-wise effective on D, then ranking loss, one-error, coverage, average precision and instance-AUC are optimized on the dataset.
",3.1. Performance Measures on Ranking,[0],[0]
Proof.,3.1. Performance Measures on Ranking,[0],[0]
(a) Ranking loss:,3.1. Performance Measures on Ranking,[0],[0]
"From the definition of labelwise effective, for every pair (u, v) ∈ Y +i· × Y − i· , we have fu(xi) > fv(xi).",3.1. Performance Measures on Ranking,[0],[0]
"Therefore, the reversed set Sirank (in Table 1 ranking loss) is empty and the cardinality of the set is zero, which implies the cardinality sum of all reversed sets rloss(F ) = 0.",3.1. Performance Measures on Ranking,[0],[0]
"Ranking loss is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
"(b) One-error: For a label-wise effective F , because labelwise margin is positive on an instance xi, we have:
max u fu(xi) >",3.1. Performance Measures on Ranking,[0],[0]
"max v
fv(xi),∀u ∈",3.1. Performance Measures on Ranking,[0],[0]
Y,3.1. Performance Measures on Ranking,[0],[0]
"+i· ,∀v ∈",3.1. Performance Measures on Ranking,[0],[0]
Y,3.1. Performance Measures on Ranking,[0],[0]
"− i· .
",3.1. Performance Measures on Ranking,[0],[0]
"Then ∀xi, argmaxF (xi) ∈ Y",3.1. Performance Measures on Ranking,[0],[0]
"+i· .
",3.1. Performance Measures on Ranking,[0],[0]
"Thus, [[argmaxF (xi) /∈",3.1. Performance Measures on Ranking,[0],[0]
Y,3.1. Performance Measures on Ranking,[0],[0]
+i· ]],3.1. Performance Measures on Ranking,[0],[0]
"= 0 for every instance xi, and one-error(F ) = 0.",3.1. Performance Measures on Ranking,[0],[0]
"One-error is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
"(c) Coverage: When F is label-wise effective, the maximum rank of a relevant label is less than the minimum rank of an irrelevant label, which means:
max u∈Y +i· rankF (xi, u) < min",3.1. Performance Measures on Ranking,[0],[0]
"v∈Y −i· rankF (xi, v), (1)
max u∈Y +i·
rankF (xi, u) = |Y +i· |.
",3.1. Performance Measures on Ranking,[0],[0]
"Therefore, coverage can be calculated as:
coverage(F )",3.1. Performance Measures on Ranking,[0],[0]
"= 1
m ∑m i=1",3.1. Performance Measures on Ranking,[0],[0]
"[|Y +i· | − 1].
",3.1. Performance Measures on Ranking,[0],[0]
"Which is the optimal value of coverage.
",3.1. Performance Measures on Ranking,[0],[0]
(d) Average precision: Assume that j is a relevant label of instance,3.1. Performance Measures on Ranking,[0],[0]
"i, it follows from Equation (1) that:
rankF (xi, j) =",3.1. Performance Measures on Ranking,[0],[0]
|{k ∈ Y,3.1. Performance Measures on Ranking,[0],[0]
"+i· |rankF (xi, k) ≤",3.1. Performance Measures on Ranking,[0],[0]
"rankF (xi, j)}|
Since rankF (xi, j) is exactly the definition of Sijprecision, avgprec(F )",3.1. Performance Measures on Ranking,[0],[0]
"= 1, i.e, average precision is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
"(e) Instance-AUC: Because of label-wise effective, for an instance xi, we have:
fu(xi) > fv(xi),∀(u, v) ∈ Y",3.1. Performance Measures on Ranking,[0],[0]
"+i· × Y − i· .
",3.1. Performance Measures on Ranking,[0],[0]
"Therefore, the size of the correct ordered prediction value pair on instance i is:
|{(u, v) ∈",3.1. Performance Measures on Ranking,[0],[0]
Y +i· × Y − i· |fu(xi) ≥ fv(xi)}| = |Y + i· ||Y,3.1. Performance Measures on Ranking,[0],[0]
− i·,3.1. Performance Measures on Ranking,[0],[0]
"|.
",3.1. Performance Measures on Ranking,[0],[0]
So instance-AUC(F ),3.1. Performance Measures on Ranking,[0],[0]
"= 1 and instance-AUC is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
"Similar to the proof of instance-AUC, we can prove the result of macro-AUC:
Theorem 2.",3.1. Performance Measures on Ranking,[0],[0]
"If a multi-label predictor F is instance-wise effective on D, then macro-AUC is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
Proof.,3.1. Performance Measures on Ranking,[0],[0]
"Because of instance-wise effective, for a label vector Y ·j , we have:
fj(xa) >",3.1. Performance Measures on Ranking,[0],[0]
"fj(xb),∀(a, b) ∈ Y +·j × Y − ·j .
",3.1. Performance Measures on Ranking,[0],[0]
"Therefore, the size of the correct ordered prediction value pair on label j is:
{(a, b) ∈",3.1. Performance Measures on Ranking,[0],[0]
Y +·j × Y − ·j |fj(xa) ≥ fj(xb)},3.1. Performance Measures on Ranking,[0],[0]
"= |Y + ·j ||Y − ·j |.
",3.1. Performance Measures on Ranking,[0],[0]
So macro-AUC(F ),3.1. Performance Measures on Ranking,[0],[0]
"= 1 and macro-AUC is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
Micro-AUC sees the label matrix as a whole and cannot be optimized by instance-wise effective F or label-wise effective F .,3.1. Performance Measures on Ranking,[0],[0]
"However, the double effective F is much more powerful.",3.1. Performance Measures on Ranking,[0],[0]
"We now prove the following result of micro-AUC.
Theorem 3.",3.1. Performance Measures on Ranking,[0],[0]
"If a multi-label predictor F is double effective on D, then as the number of instances grows, micro-AUC is optimized.
",3.1. Performance Measures on Ranking,[0],[0]
Proof.,3.1. Performance Measures on Ranking,[0],[0]
"We first prove a result of random variables Ai, B,C. If n random variablesA1, A2, · · · , An are drawn from uniform distribution U(0, 1), for a random constant a, the event that at least one Ai is smaller than a is:
Pr[∃Ai, Ai ≤ a] = 1− (1− a)n.
",3.1. Performance Measures on Ranking,[0],[0]
"Another random variable B is uniformly distributed in (0,min{Ai}), and the probability that a random variable C ∼ U(0, 1) is bigger than B is:
Pr[C > B] ≥Pr[(C ≥ a) ∧ (∃Ai, Ai ≤ a)]
=(1− a 2 )",3.1. Performance Measures on Ranking,[0],[0]
[1− (1− a)n].,3.1. Performance Measures on Ranking,[0],[0]
"(2)
For any small a, we can choose a large enough n",3.1. Performance Measures on Ranking,[0],[0]
"to make Equation (2) close to 1.
",3.1. Performance Measures on Ranking,[0],[0]
"Given a label matrix Y ∈ {0, 1}m×l and the corresponding prediction matrix F ∈ (0, 1)m×l, because predictor F is double effective, the prediction matrix satisfies the following conditions:
",3.1. Performance Measures on Ranking,[0],[0]
"Fij > Fiu if Yij = 1 ∧ Yiu = 0, Fij > Fvj if Yij = 1 ∧ Yvj = 0.
",3.1. Performance Measures on Ranking,[0],[0]
"To force the value in F is in (0, 1), we further assume a uniform distribution Fij ∼ U(0, 1) when Yij = 1.
",3.1. Performance Measures on Ranking,[0],[0]
"If Yij = 0, then Fij should be less than Fiu if Yiu = 1 and Fvj if Yvj = 1.",3.1. Performance Measures on Ranking,[0],[0]
"Suppose that the minimum value b is defined as:
b = min { min v {Fvj |Yvj = 1},min u {Fiu|Yiu = 1} } .
",3.1. Performance Measures on Ranking,[0],[0]
"Then Fij is drawn from U(0, b).",3.1. Performance Measures on Ranking,[0],[0]
"And we can choose a small constant value a > b.
",3.1. Performance Measures on Ranking,[0],[0]
"According to Equation (2), the probability that a random pair (i, j, u, v) to be a correct micro pair is:
Pmicro = Pr[Fij > Fuv|Yij = 1, Yuv = 0]
≥ (1− a 2 )",3.1. Performance Measures on Ranking,[0],[0]
"[1− (1− a)n],
where n = k
ml (m+ l − 2)
",3.1. Performance Measures on Ranking,[0],[0]
"In the practical case, the number of labels is proportional to the number of instances: k ∝ m.",3.1. Performance Measures on Ranking,[0],[0]
"We assume k = pm where p is a constant smaller than l.
lim m→∞ n = lim m→∞
p l",3.1. Performance Measures on Ranking,[0],[0]
"(m+ l − 2) =∞,
lim m→∞ |Smicro| |( ∑m i=1 |Y +",3.1. Performance Measures on Ranking,[0],[0]
i· |) · ( ∑m i=1 |Y,3.1. Performance Measures on Ranking,[0],[0]
"− i· |)| = lim m→∞ Pmicro = 1.
",3.1. Performance Measures on Ranking,[0],[0]
"Therefore, micro-AUC is to be optimized as the number of instances grows.
",3.1. Performance Measures on Ranking,[0],[0]
"With the above analysis, we can conclude that a label-wise effective F can optimize ranking loss, one-error, coverage, average precision, instance-AUC, micro-AUC and an instance-wise effective F can optimize macro-AUC.",3.1. Performance Measures on Ranking,[0],[0]
"For micro-AUC, a double effective F can optimize it as the number of instances increases.",3.1. Performance Measures on Ranking,[0],[0]
"As mentioned in Section 2.2, there are some measures evaluating classifier H instead of predictor F .",3.2. Performance Measures on Classification,[0],[0]
"There are many thresholding or binarization strategies (Fan & Lin, 2007; Fürnkranz et al., 2008; Read et al., 2011).",3.2. Performance Measures on Classification,[0],[0]
"For simplicity, we focus on two main strategies: thresholding on each instance and thresholding on each label.
",3.2. Performance Measures on Classification,[0],[0]
A label-wise effective F can be equipped with a thresholding function based on each instance such as t(xi) and construct the H by hj(xi) =,3.2. Performance Measures on Classification,[0],[0]
[[fj(xi) > t(xi)],3.2. Performance Measures on Classification,[0],[0]
].,3.2. Performance Measures on Classification,[0],[0]
"However, using t(xi) on an instance-wise effective F is unreasonable since the predicted values on different labels may not be comparable.",3.2. Performance Measures on Classification,[0],[0]
"In a word, we should use suitable threshold function on different effective F s, i.e., t(xi) on each instance for label-wise effective F , and tj on each label
for instance-wise effective F .",3.2. Performance Measures on Classification,[0],[0]
"It is reasonable to use either t(xi) or tj for double effective F .
",3.2. Performance Measures on Classification,[0],[0]
"To formally analyze the performance measures on classification, we define the threshold error:
Definition 6.",3.2. Performance Measures on Classification,[0],[0]
"Given a descending ordered real-value sequence x1, x2, . . .",3.2. Performance Measures on Classification,[0],[0]
", xk with an optimal cut number c∗, where c∗ ∈ N and 1 ≤ c∗ ≤",3.2. Performance Measures on Classification,[0],[0]
k.,3.2. Performance Measures on Classification,[0],[0]
"For a real value threshold t ∈ (xk − 1, x1 + 1), the threshold error = | argmini(xi)− c∗| where xi > t.
Intuitively, the threshold error counts how many items are incorrectly classified on a descending ordered sequence where the correct answer is c∗.",3.2. Performance Measures on Classification,[0],[0]
"Based on the threshold error, we propose the following theorems about performance measures on classification.
",3.2. Performance Measures on Classification,[0],[0]
Theorem 4.,3.2. Performance Measures on Classification,[0],[0]
"For a label-wise effective F , if the thresholding function makes at most i error on each instance i, the micro-F1, instance-F1 and Hamming loss are bounded as follows:
micro-F1(H) = instance-F1(H)
",3.2. Performance Measures on Classification,[0],[0]
≥ 1 m m∑ i=1,3.2. Performance Measures on Classification,[0],[0]
min {2(|Y +i· | − i) 2|Y,3.2. Performance Measures on Classification,[0],[0]
"+i· | − i , 2|Y",3.2. Performance Measures on Classification,[0],[0]
"+i· | 2|Y +i· |+ i } ,
hloss(H) ≤ 1 ml ∑m i=1",3.2. Performance Measures on Classification,[0],[0]
"i.
",3.2. Performance Measures on Classification,[0],[0]
"The main idea of the above theorem is that, given the threshold error and the number of relevant labels, we can compute the gap between the worst possible and the perfect contingency table.",3.2. Performance Measures on Classification,[0],[0]
"Hence the F-measure is based on the contingency table, the lower bound can be deduced.",3.2. Performance Measures on Classification,[0],[0]
"The detailed proof of Theorem 4 is in Appendix A.1.
",3.2. Performance Measures on Classification,[0],[0]
"Similar to Theorem 4, we can prove the results for labelwise effective F :
Theorem 5.",3.2. Performance Measures on Classification,[0],[0]
"For an instance-wise effective F , if the thresholding function makes at most j error on each label j, then the macro-F1 and Hamming loss are bounded as follows:
macro-F1(H)",3.2. Performance Measures on Classification,[0],[0]
≥ 1,3.2. Performance Measures on Classification,[0],[0]
l l∑ j=1 min {2(|Y +·j | − j) 2|Y,3.2. Performance Measures on Classification,[0],[0]
"+·j | − j , 2|Y +·j | 2|Y +·j |+ j } ,
hloss(H) ≤ 1 ml ∑l j=1 j .
",3.2. Performance Measures on Classification,[0],[0]
"The detailed proof of Theorem 5 is in Appendix A.2.
",3.2. Performance Measures on Classification,[0],[0]
"With the above analysis, we can conclude that a labelwise effective F can optimize instance-F1 and micro-F1, an instance-wise effective F can optimize macro-F1.",3.2. Performance Measures on Classification,[0],[0]
Both the two effective F s can optimize Hamming loss.,3.2. Performance Measures on Classification,[0],[0]
"For a double effective F , because it enjoys both the properties, it can optimize all the above mentioned performance measures if proper thresholds are used.",3.2. Performance Measures on Classification,[0],[0]
Table 2 summarizes our theoretical results in Section 3.1 and 3.2.,3.3. Summary,[0],[0]
Each row shows the results of one multi-label performance measure.,3.3. Summary,[0],[0]
"Note that double effective is a special case of label-wise effective and instance-wise effective and thus, if one performance measure is optimized by either label-wise or instance-wise effective predictor, it will also be optimized by double effective predictor.
",3.3. Summary,[0],[0]
"In the light of the analysis, the performance on different performance measures through optimizing margins can be expected.",3.3. Summary,[0],[0]
"For example, if one maximizes instance-wise margin on each label, s/he will get good performance on macro-AUC but may suffer higher loss on ranking loss, coverage and some other measures where ‘7’ marked in the inst-wise column.",3.3. Summary,[0],[0]
"If one tries to maximize the label-wise margin but pay no attention to instance-wise margin, s/he may perform well on average precision but poor on macroF1 (e.g., Elisseeff & Weston (2002)).",3.3. Summary,[0],[0]
Maximzing both the label-wise margin and instance-wise margins to get a double effective F is expected to be the best choice.,3.3. Summary,[0],[0]
"The above analysis reveals that maximizing different margins will optimize different measures, and if possible, double effective F is prefered since it enjoys the benefits of maximizing both the label-wise margin and the instancewise margin.",4. The LIMO Approach,[0],[0]
"Therefore, we propose the LIMO approach.",4. The LIMO Approach,[0],[0]
"LIMO is a single approach which can optimize both the two margins, and it can also be degenerated to optimize either margin seperately via parameter setting.",4. The LIMO Approach,[0],[0]
"Suppose that F is a linear predictor, which means F (X) = W TX where W = [w1,w2, · · · ,wl].",4.1. Formulation,[0],[0]
"We propose the following formulation:
argmin W ,ξ l∑ i=1",4.1. Formulation,[0],[0]
"||wi||2 + λ1 m∑ i=1 ∑ (u,v) ξuvi +",4.1. Formulation,[0],[0]
"λ2 l∑ j=1 ∑ (a,b) ξjab
s.t.",4.1. Formulation,[0],[0]
"w>u xi −w>v xi > 1− ξuvi , ξuvi ≥ 0, for i = 1, · · · ,m and (u, v) ∈",4.1. Formulation,[0],[0]
"Y +i· × Y − i· ,
w>j xa",4.1. Formulation,[0],[0]
"−w>j xb > 1− ξ j ab, ξ j ab ≥ 0,
for j = 1, · · · , l and (a, b) ∈ Y +·j × Y",4.1. Formulation,[0],[0]
"− ·j .
(3)
",4.1. Formulation,[0],[0]
"Here ξuvi and ξ j ab are the slack variables, and λ1, λ2 are the trade-off parameters.",4.1. Formulation,[0],[0]
"When both λ1 and λ2 are positive, both label-wise and instance-wise margins are considered.",4.1. Formulation,[0],[0]
"If we set λ1 = 0 (or λ2 = 0), then only the instance-wise (or label-wise) margin is considered.",4.1. Formulation,[0],[0]
"In this paper, if the approach only considers instance-wise (or label-wise) margin , we call the approach as LIMO-inst (or LIMO-label).",4.1. Formulation,[0],[0]
"And LIMO considers both the two margins.
",4.1. Formulation,[0],[0]
"Algorithm 1 LIMO Input:
Data matrix X ∈ Rm×d, label matrix Y ∈ {0, 1}m×l, step size η, trade-off parameters λ1, λ2, and the maximium iteration number T .
",4.1. Formulation,[0],[0]
"Procedure: 1: Initialize W 0 with N(0, 1/ √ d) random values.
",4.1. Formulation,[0],[0]
"2: Compute the weight vector cinst of each instance, cinsti = |Y + i· ||Y",4.1. Formulation,[0],[0]
− i· |/ ∑m i=1 |Y,4.1. Formulation,[0],[0]
+,4.1. Formulation,[0],[0]
i· ||Y,4.1. Formulation,[0],[0]
− i· |.,4.1. Formulation,[0],[0]
"3: Compute the weight vector clabel of each label, clabelj = |Y + ·j ||Y − ·j |/ ∑l j=1 |Y + ·j ||Y − ·j |. 4: for t = 1, 2, · · · , T do 5:",4.1. Formulation,[0],[0]
"Random sample an instance xti using weight c
inst, 6: Random sample a positive label yiu and a negative label yiv of instance xti.",4.1. Formulation,[0],[0]
7:,4.1. Formulation,[0],[0]
if 1−w>u xti +w>v xti > 0,4.1. Formulation,[0],[0]
then 8: wtu = w t−1 u − η(−λ1xti +wt−1u ).,4.1. Formulation,[0],[0]
9: wtv = w t−1 v,4.1. Formulation,[0],[0]
−,4.1. Formulation,[0],[0]
η(λ1xti +wt−1v ).,4.1. Formulation,[0],[0]
10: end if 11: Random sample index j of label using weight clabel.,4.1. Formulation,[0],[0]
12:,4.1. Formulation,[0],[0]
Random sample a positive instance xta and a negative instance xtb on label j. 13: if 1−w>j xta +w>j xtb > 0,4.1. Formulation,[0],[0]
then 14: wtj = w t−1 j − η(λ2(xtb − xta) +w t−1 j ).,4.1. Formulation,[0],[0]
15: end if 16: end for 17: W = 1T ∑T t=1,4.1. Formulation,[0],[0]
"W
t. Output:
Multi-label linear model W .",4.1. Formulation,[0],[0]
The objective Equation (3) is difficult to solve directly because of the large number of constraints and slack variables.,4.2. Algorithm,[0],[0]
"For a training set with m instances and l labels, the number of constraints will be O(m2l + ml2), which may exceed memory limit in real-world applicaitons.
",4.2. Algorithm,[0],[0]
"In order to deal with the computational problem, we solve Equation (3) by stochastic gradient descent (SGD) with fixed step size and the default averaging technique in Shalev-Shwartz & Ben-David (2014, Chapter 14.3).",4.2. Algorithm,[0],[0]
"The key point of SGD is to find out a random vector, whose expected value at each iteration equals the gradient direction.",4.2. Algorithm,[0],[0]
We randomly sample two kinds of triplets and use them to compute the correct direction.,4.2. Algorithm,[0],[0]
"At each iteration t, we sample a triplet (xti, yiu, yiv) where yiu is relevant and yiv is irrelevant, and a triplet (j,xta,x t b) where x t a is a positive instance and xtb is a negative instance both on label j.",4.2. Algorithm,[0],[0]
Then we use the two triplets to compute the random gradient vector for SGD.,4.2. Algorithm,[0],[0]
"The detailed algorithm is presented in Algorithm 1 and the proof that the random vector is an unbiased estimation of the gradient direction is available in Appendix A.3.
",4.2. Algorithm,[0],[0]
"After the training procedure, we can use the linear model to predict continuous confidence values on the training data, then choose the best threshold value by optimizing a specific classification measure.",4.2. Algorithm,[0],[0]
We conduct experiments with LIMO on both synthetic and benchmark data.,5. Experiments,[0],[0]
"Note that the main purpose of our work is to study multi-label performance measures from the aspect of margin optimization, and thus, the goal of our experiments is to validate our theoretical findings rather than claim that LIMO is superior, although its performance is really highly competitive.",5. Experiments,[0],[0]
We conduct experiments on synthetic data with 4 labels.,5.1. Synthetic Data,[0],[0]
"2000 data points are randomly generated from a (−1,+1)2 square, and the labels are assigned as in Figure 1.",5.1. Synthetic Data,[0],[0]
50% data are held out for testing.,5.1. Synthetic Data,[0],[0]
The synthetic data is designed to simulate a typical real-world circumstance.,5.1. Synthetic Data,[0],[0]
"The number of co-occurrent labels varies, the regions of each label are different and the data cannot be perfectly seperated by a linear learner.
",5.1. Synthetic Data,[0],[0]
"To demonstrate the relationship between margins and performance measures, we degenerate LIMO to only consider either margin by setting the trade-off parameter λ1 or λ2 to zero.",5.1. Synthetic Data,[0],[0]
LIMO-inst sets λ1 = 0 and LIMO-label sets λ2 = 0.,5.1. Synthetic Data,[0],[0]
"The other parameter is set to 100 and LIMO sets
Figure 1.",5.1. Synthetic Data,[0],[0]
"Input space consists of four regions with different assignments of the label set {A,B,C,D}.",5.1. Synthetic Data,[0],[0]
"The center point is with coordinate (0, 0).
",5.1. Synthetic Data,[0],[0]
"ABCD
ABD
BCA
-1 0 1 -1
0
1
λ1 = λ2 = 100.",5.1. Synthetic Data,[0],[0]
Ten replications of the experiment are conducted and the average results are reported.,5.1. Synthetic Data,[0],[0]
"Because the range of performance measure coverage is not [0, 1], while some performance measures are better when higher, and some are better when lower, we rescale all the performance values into relative values for clearer visualization.",5.1. Synthetic Data,[0],[0]
The best one is rescaled to 1 and the worst one is rescaled to 0.,5.1. Synthetic Data,[0],[0]
"Figure 2 shows the relative results, where the originally worst performance value is given on the right.",5.1. Synthetic Data,[0],[0]
"instance F1 macro F1 micro F1
Hamming loss
1.0 0.8 0.6 0.4 0.2 0.0
LIMO-inst-t LIMO-inst-t(x) LIMO-label-t LIMO-label-t(x) LIMO-t
LIMO-t(x) 0.804
0.852
0.837
0.188
relative value
a b s o lu te w o rs",5.1. Synthetic Data,[0],[0]
"t v a lu e
* *
*
*
*
The results shown in Figure 2 support our theoretical findings in Table 2.",5.1. Synthetic Data,[0],[0]
"For example, micro-AUC is considered to be optimized by double effective F but not the other two, therefore LIMO (the red circle) gets the best relative value.",5.1. Synthetic Data,[0],[0]
"For some measures proved to be optimized by label-wise margin such as ranking loss, average precision, coverage and instance-AUC, LIMO-label beats LIMO-inst.",5.1. Synthetic Data,[0],[0]
"While for macro-AUC, LIMO-inst wins.",5.1. Synthetic Data,[0],[0]
"For one-error, all three versions of LIMO do extremely well and get less than 0.001 absolute value, which may be the reason why the relative values are unexpected.
micro-AUC
instance-AUC
macro-AUC
coverage
one-error
avg.",5.1. Synthetic Data,[0],[0]
"precision
ranking loss
1.0 0.8 0.6 0.4 0.2 0.0
LIMO-inst LIMO-label LIMO
0.854
0.973
0.828
1.575
0.001
0.992
0.027
relative value
a b s o lu te w o rs",5.1. Synthetic Data,[0],[0]
"t v a lu e
Figure 3 shows the relative performance on classification.",5.1. Synthetic Data,[0],[0]
"We use two types of thresholding discussed in Section 3.2:
threshold function based on each instance or each label (denoted by -t(x) or -t in the legend).",5.1. Synthetic Data,[0],[0]
The thresholds are estimated on training data.,5.1. Synthetic Data,[0],[0]
This figure exactly shows our theoretical results: LIMO-label equipped with t(x) can optimize instance-F1 and micro-F1; LIMO-inst equipped with t can optimize macro-F1.,5.1. Synthetic Data,[0],[0]
"By considering both label-wise margin and instance-wise margin, LIMO works well on all four classificaiton measures.",5.1. Synthetic Data,[0],[0]
"We conduct experiments on eleven multi-label performance measures to further show that optimizing the labelwise or the instance-wise margin can lead to different results, as revealed in our theoretical analysis.
",5.2. Benchmark Data,[0],[0]
Five benchmark multi-label datasets1 are used in our experiments.,5.2. Benchmark Data,[0],[0]
We choose them because they denote different domains: (i),5.2. Benchmark Data,[0],[0]
"A music dataset CAL500, (ii) an email dataset enron, (iii) a clinical text dataset medical, (iv) an image dataset corel5k, (v) a tagging dataset bibtex.",5.2. Benchmark Data,[0],[0]
"We randomly split each dataset into two parts, i.e., 70% for training and 30% for testing.",5.2. Benchmark Data,[0],[0]
"The experiments are repeated ten times, and the averaged results are reported.
",5.2. Benchmark Data,[0],[0]
"Because our algorithm optimizes a linear model, three linear methods called Binary Relevance (BR) (Zhang & Zhou, 2014), ML-kNN (Zhang & Zhou, 2007) and GFM (Waegeman et al., 2014) are provided for fair comparison.",5.2. Benchmark Data,[0],[0]
"As in experiments on synthetic data, we degenerate LIMO (λ1 = λ2 = 1) to LIMO-inst (λ1 = 0, λ2 = 1) and LIMOlabel (λ1 = 1, λ2 = 0).",5.2. Benchmark Data,[0],[0]
The step size of SGD is set to 0.01.,5.2. Benchmark Data,[0],[0]
"For BR, L2-regularized SVM (Chang & Lin, 2011) with C=1 is used as base learner.",5.2. Benchmark Data,[0],[0]
"For ML-kNN and GFM, the number of nearest neighbors is 10.",5.2. Benchmark Data,[0],[0]
Suitable thresholds discussed in Section 3.2 are used for classification measures.,5.2. Benchmark Data,[0],[0]
We take the default parameter settings recommended by authors of the compared methods respectively.,5.2. Benchmark Data,[0],[0]
"Because on one hand, we believe the parameter settings recommended by their authors are meaningful, on the other hand, it is hard to say which parameter setting is better in terms of eleven performance measures.
",5.2. Benchmark Data,[0],[0]
"Because some measures are better when higher, and some measures are better when lower, to demonstrate the results more clearly, we compute the average rank of each approach over all datasets on a specific measure.",5.2. Benchmark Data,[0],[0]
"For example, when we want to examine how LIMO performs on ranking loss, we first compute the ranks on each dataset: LIMO ranks 1st on CAL500, enron, bibtex and ranks 2nd on medical, corel5k.",5.2. Benchmark Data,[0],[0]
Then the average rank of LIMO on ranking loss is (1+1+1+2+2)/5=1.4.,5.2. Benchmark Data,[0],[0]
Figure 4 shows the average ranks.,5.2. Benchmark Data,[0],[0]
"Due to the space limit, the detailed results used to compute the ranks are provided in Appendix B.2.
1http://mulan.sourceforge.net/datasets-mlc.html
The results in Figure 4 are consistent with our theoretical findings.",5.2. Benchmark Data,[0],[0]
"LIMO-inst (the square) performs well on marcoF1 and macro-AUC, while LIMO-label (the triangle) performs well on other performance measures.",5.2. Benchmark Data,[0],[0]
"LIMO (the circle) almost ranks top on every performance measure.
",5.2. Benchmark Data,[0],[0]
The experiments on synthetic and benchmark data support our theoretical analysis.,5.2. Benchmark Data,[0],[0]
"Although different performance measures focus on different aspects, they share the common property which is formalized in our work as labelwise margin and instance-wise margin.",5.2. Benchmark Data,[0],[0]
"In practice, it is recommended to use higher weight (λ1/λ2) on specific margin to optimize the required performance measure.",5.2. Benchmark Data,[0],[0]
"LIMO with nonlinear predictors may perform better, which needs a novel optimization algorithm.",5.2. Benchmark Data,[0],[0]
"In this paper, we establish a unified view for a variety of multi-label performance measures.",6. Conclusion,[0],[0]
"Based on the proposed concepts of label-wise/instance-wise margins, we prove that some performance measures are to be optimized by label-wise effective classifiers, whereas some by instancewise effective classifiers.",6. Conclusion,[0],[0]
"Inspired by the theoretical findings, we design the LIMO approach which can be adjusted to label-wise/instance-wise effective via different parameter settings.
",6. Conclusion,[0],[0]
Our work discloses that there are some shared properties among different subsets of multi-label performance measures.,6. Conclusion,[0],[0]
"This explains why some measures seem to be redundant in experiments, and suggests that in future empirical studies, rather than randomly grasp a set of measures for evaluation, it is more informative to evaluate using measures with different properties, such as some measures optimized by label-wise effective predictors and some optimized by instance-wise effective predictors.",6. Conclusion,[0],[0]
"In the future, it is encouraging to study the asymptotic properties of these performance measures when the two margins are suboptimal.",6. Conclusion,[0],[0]
The margin view also sheds a light for the design of novel multi-label algorithms.,6. Conclusion,[0],[0]
"This research was supported by the NSFC (61333014), 973 Program (2014CB340501), and the Collaborative Innovation Center of Novel Software Technology and Industrialization.",Acknowledgements,[0],[0]
"Authors want to thank reviewers for helpful comments, and thank Sheng-Jun Huang, Xiu-Shen Wei, Miao Xu for reading a draft.",Acknowledgements,[0],[0]
Multi-label classification deals with the problem where each instance is associated with multiple class labels.,abstractText,[0],[0]
"Because evaluation in multi-label classification is more complicated than singlelabel setting, a number of performance measures have been proposed.",abstractText,[0],[0]
It is noticed that an algorithm usually performs differently on different measures.,abstractText,[0],[0]
"Therefore, it is important to understand which algorithms perform well on which measure(s) and why.",abstractText,[0],[0]
"In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification.",abstractText,[0],[0]
"In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures are to be optimized.",abstractText,[0],[0]
"Based on the defined margins, a max-margin approach called LIMO is designed and empirical results validate our theoretical findings.",abstractText,[0],[0]
A Unified View of Multi-Label Performance Measures,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3643–3653 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3643",text,[0],[0]
"Multimodal machine translation is the problem of translating sentences paired with images into a different target language (Elliott et al., 2016).",1 Introduction,[0],[0]
"In this setting, translation is expected to be more accurate compared to purely text-based translation, as the visual context could help resolve ambiguous multi-sense words.",1 Introduction,[0],[0]
"Examples of real-world applications of multimodal (vision plus text) translation include translating multimedia news, web product information, and movie subtitles.
",1 Introduction,[0],[0]
"Several previous endeavours (Huang et al., 2016; Calixto et al., 2017a; Elliott and Kádár, 2017) have demonstrated improved translation quality when utilizing images.",1 Introduction,[0],[0]
"However, how to effectively integrate the visual information still remains a challenging problem.",1 Introduction,[0],[0]
"For instance, in the WMT 2017 multimodal machine translation challenge (Elliott et al., 2017), methods that incorporated visual information did not outperform pure text-based approaches with a big margin.
",1 Introduction,[0],[0]
"In this paper, we propose a new model called Visual Attention Grounding Neural Ma-
chine Translation (VAG-NMT) to leverage visual information more effectively.",1 Introduction,[0],[0]
"We train VAG-NMT with a multitask learning mechanism that simultaneously optimizes two objectives: (1) learning a translation model, and (2) constructing a vision-language joint semantic embedding.",1 Introduction,[0],[0]
"In this model, we develop a visual attention mechanism to learn an attention vector that values the words that have closer semantic relatedness with the visual context.",1 Introduction,[0],[0]
The attention vector is then projected to the shared embedding space to initialize the translation decoder such that the source sentence words that are more related to the visual semantics have more influence during the decoding stage.,1 Introduction,[0],[0]
"When evaluated on the benchmark Multi30K and the Ambiguous COCO datasets, our VAG-NMT model demonstrates competitive performance compared to existing state-of-the-art multimodal machine translation systems.
",1 Introduction,[0],[0]
"Another important challenge for multimodal machine translation is the lack of a large-scale, realistic dataset.",1 Introduction,[0],[0]
"To our knowledge, the only existing benchmark is Multi30K (Elliott et al., 2016), which is based on an image captioning dataset, Flickr30K (Young et al., 2014) with manual German and French translations.",1 Introduction,[0],[0]
"There are roughly 30K parallel sentences, which is relatively small compared to text-only machine translation datasets that have millions of sentences such as WMT14 EN→DE.",1 Introduction,[0],[0]
"Therefore, we propose a new multimodal machine translation dataset called IKEA to simulate the real-world problem of translating product descriptions from one language to another.",1 Introduction,[0],[0]
"Our IKEA dataset is a collection of parallel English, French, and German product descriptions and images from IKEA’s and UNIQLO’s websites.",1 Introduction,[0],[0]
"We have included a total of 3,600 products so far and will include more in the future.",1 Introduction,[0],[0]
"In the machine translation literature, there are two major streams for integrating visual information: approaches that (1) employ separate attention for different (text and vision) modalities, and (2) fuse visual information into the NMT model as part of the input.",2 Related Work,[0],[0]
"The first line of work learns independent context vectors from a sequence of text encoder hidden states and a set of location-preserving visual features extracted from a pre-trained convnet, and both sets of attentions affect the decoder’s translation (Calixto et al., 2017a; Helcl and Libovický, 2017).",2 Related Work,[0],[0]
"The second line of work instead extracts a global semantic feature and initializes either the NMT encoder or decoder to fuse the visual context (Calixto et al., 2017b; Ma et al., 2017).",2 Related Work,[0],[0]
"While both approaches demonstrate significant improvement over their Text-Only NMT baselines, they still perform worse than the best monomodal machine translation system from the WMT 2017 shared task (Zhang et al., 2017).
",2 Related Work,[0],[0]
The model that performs best in the multimodal machine translation task employed image context in a different way.,2 Related Work,[0],[0]
"(Huang et al., 2016) combine region features extracted from a region-proposal network (Ren et al., 2015) with the word sequence feature as the input to the encoder, which leads to significant improvement over their NMT baseline.",2 Related Work,[0],[0]
"The best multimodal machine translation system in WMT 2017 (Caglayan et al., 2017) performs element-wise multiplication of the target language embedding with an affine transformation of the convnet image feature vector as the mixed input to the decoder.",2 Related Work,[0],[0]
"While this method outperforms all other methods in WMT 2017 shared task workshop, the advantage over the monomodal translation system is still minor.
",2 Related Work,[0],[0]
The proposed visual context grounding process in our model is closely related to the literature on multimodal shared space learning.,2 Related Work,[0],[0]
"(Kiros et al., 2014) propose a neural language model to learn a visual-semantic embedding space by optimizing a ranking objective, where the distributed representation helps generate image captions.",2 Related Work,[0],[0]
"(Karpathy and Li, 2014) densely align different objects in the image with their corresponding text captions in the shared space, which further improves the quality of the generated caption.",2 Related Work,[0],[0]
"In later work, multimodal shared space learning was extended to multimodal multilingual shared space learning.",2 Related Work,[0],[0]
"(Calixto et al., 2017c) learn a multi-modal multilin-
gual shared space through optimization of a modified pairwise contrastive function, where the extra multilingual signal in the shared space leads to improvements in image-sentence ranking and semantic textual similarity task.",2 Related Work,[0],[0]
"(Gella et al., 2017) extend the work from (Calixto et al., 2017c) by using the image as the pivot point to learn the multilingual multimodal shared space, which does not require large parallel corpora during training.",2 Related Work,[0],[0]
"Finally, (Elliott and Kádár, 2017) is the first to integrate the idea of multimodal shared space learning to help multimodal machine translation.",2 Related Work,[0],[0]
"Their multi-task architecture called “imagination” shares an encoder between a primary task of the classical encoder-decoder NMT and an auxiliary task of visual feature reconstruction.
",2 Related Work,[0],[0]
"Our VAG-NMT mechanism is inspired by (Elliott and Kádár, 2017), but has important differences.",2 Related Work,[0],[0]
"First, we modify the auxiliary task as a visual-text shared space learning objective instead of the simple image reconstruction objective.",2 Related Work,[0],[0]
"Second, we create a visual-text attention mechanism that captures the words that share a strong semantic meaning with the image, where the grounded visual-context has more impact on the translation.",2 Related Work,[0],[0]
"We show that these enhancements lead to improvement in multimodal translation quality over (Elliott and Kádár, 2017).",2 Related Work,[0],[0]
"Given a set of parallel sentences in language X and Y , and a set of corresponding images V paired with each sentence pair, the model aims to translate sentences {xi}Ni=1 ∈ X in language X to sentences {yi}Ni=1 ∈ Y in language Y with the assistance of images {vi}Ni=1 ∈ V .
",3 Visual Attention Grounding NMT,[0],[0]
"We treat the problem of multimodal machine
translation as a joint optimization of two tasks: (1) learning a robust translation model and (2) constructing a visual-language shared embedding that grounds the visual semantics with text.",3 Visual Attention Grounding NMT,[0],[0]
Figure 1 shows an overview of our VAG-NMT model.,3 Visual Attention Grounding NMT,[0],[0]
"We adopt a state-of-the-art attention-based sequenceto-sequence structure (Bahdanau et al., 2014) for translation.",3 Visual Attention Grounding NMT,[0],[0]
"For the joint embedding, we obtain the text representation using a weighted sum of hidden states from the encoder of the sequenceto-sequence model and we obtain the image representation from a pre-trained convnet.",3 Visual Attention Grounding NMT,[0],[0]
"We learn the weights using a visual attention mechanism, which represents the semantic relatedness between the image and each word in the encoded text.",3 Visual Attention Grounding NMT,[0],[0]
"We learn the shared space with a ranking loss and the translation model with a cross entropy loss.
",3 Visual Attention Grounding NMT,[0],[0]
"The joint objective function is defined as:
J(θT , φV )",3 Visual Attention Grounding NMT,[0],[0]
= αJT (θT ),3 Visual Attention Grounding NMT,[0],[0]
"+ (1− α)JV (φV ), (1) where JT is the objective function for the sequence-to-sequence model, JV is the objective function for joint embedding learning, θT are the parameters in the translation model, and φV are the parameters for the shared vision-language embedding learning, and α determines the contribution of the machine translation loss versus the visual grounding loss.",3 Visual Attention Grounding NMT,[0],[0]
Both JT and JV share the parameters of the encoder from the neural machine translation model.,3 Visual Attention Grounding NMT,[0],[0]
We describe details of the two objective functions in Section 3.2.,3 Visual Attention Grounding NMT,[0],[0]
"We first encode an n-length source sentence {x}, as a sequence of tokens x = {x1, x2, . . .",3.1 Encoder,[0],[0]
", xn}, with a bidirectional GRU (Schuster and Paliwal, 1997; Cho et al., 2014).",3.1 Encoder,[0],[0]
"Each token is represented by a one-hot vector, which is then mapped into an embedding ei through a pre-trained embedding matrix.",3.1 Encoder,[0],[0]
The bidirectional GRU processes the embedding tokens in two directions: left-toright (forward) and right-to-left (backward).,3.1 Encoder,[0],[0]
"At every time step, the encoder’s GRU cell generates two corresponding hidden state vectors:
−→ hi =−−−−−−−−−−→
GRU(hi−1, ei) and ←− hi =",3.1 Encoder,[0],[0]
"←−−−−−−−−−− GRU(hi−1, ei).",3.1 Encoder,[0],[0]
The two hidden state vectors are then concatenated together to serve as the encoder hidden state vector of the source token at step i:,3.1 Encoder,[0],[0]
hi =,3.1 Encoder,[0],[0]
"[ ←− hi , −→ hi ].",3.1 Encoder,[0],[0]
"After encoding the source sentence, we project both the image and text into the shared space to find a good distributed representation that can capture the semantic meaning across the two modalities.",3.2 Shared embedding objective,[0],[0]
"Previous work has shown that learning a multimodal representation is effective for grounding knowledge between two modalities (Kiros et al., 2014; Chrupala et al., 2015).",3.2 Shared embedding objective,[0],[0]
"Therefore, we expect the shared encoder between the two objectives to facilitate the integration of the two modalities and positively influence translation during decoding.
",3.2 Shared embedding objective,[0],[0]
"To project the image and the source sentence to a shared space, we obtain the visual embedding (v) from the pool5 layer of ResNet50 (He et al., 2015a) pre-trained on ImageNet classification (Russakovsky et al., 2015), and the source sentence embedding using the weighted sum of encoder hidden state vectors ({hi}) to represent the entire source sentence (t).",3.2 Shared embedding objective,[0],[0]
We project each {hi} to the shared space through an embedding layer.,3.2 Shared embedding objective,[0],[0]
"As different words in the source sentence will have different importance, we employ a visual-language attention mechanism—inspired by the attention mechanism applied in sequenceto-sequence models (Bahdanau et al., 2014)—to emphasize words that have the stronger semantic connection with the image.",3.2 Shared embedding objective,[0],[0]
"For example, the highlighted word “cat"" in the source sentence in Fig. 1 has the more semantic connection with the image.
",3.2 Shared embedding objective,[0],[0]
"Specifically, we produce a set of weights β = {β1, β2, . . .",3.2 Shared embedding objective,[0],[0]
", βn} with our visual-attention mechanism, where the attention weight βi for the i’th word is computed as:
βi = exp(zi)∑N l=1 exp(zl) , (2)
and zi = tanh(Wvv) · tanh(Whhi) is computed by taking the dot product between the transformed encoder hidden state vector hi and the transformed image feature vector v, and Wv and Wh are the association transformation parameters.
",3.2 Shared embedding objective,[0],[0]
"Then, we can get a weighted sum of the encoder hidden state vectors t = ∑n i=1 βihi to represent the semantic meaning of the entire source sentence.",3.2 Shared embedding objective,[0],[0]
The next step is to project the source sentence feature vector t and the image feature vector v into the same shared space.,3.2 Shared embedding objective,[0],[0]
"The projected vector for text is: temb = tanh(Wtembt + btemb) and the projected vector for image is: vemb = tanh(Wvembv + bvemb).
",3.2 Shared embedding objective,[0],[0]
"We follow previous work on visual semantic embedding (Kiros et al., 2014) to minimize a pairwise ranking loss to learn the shared space:
JV (φV )",3.2 Shared embedding objective,[0],[0]
=,3.2 Shared embedding objective,[0],[0]
"∑ p ∑ k max{0, γ − s(vp, tp) + s(vp, tk 6=p)}
+ ∑ k ∑ p max{0, γ − s(tk, vk) + s(tk, vp 6=k)},
(3)
where γ is a margin, and s is the cosine distance between two vectors in the shared space.",3.2 Shared embedding objective,[0],[0]
"k and p are the indexes of the images and text sentences, respectively.",3.2 Shared embedding objective,[0],[0]
"tk 6=p and vp 6=k are the contrastive examples with respect to the selected image and the selected source text, respectively.",3.2 Shared embedding objective,[0],[0]
"When the loss decreases, the distance between a paired image and sentence will drop while the distance between an unpaired image and sentence will increase.
",3.2 Shared embedding objective,[0],[0]
"In addition to grounding the visual context into the shared encoder through the multimodal shared space learning, we also initialize the decoder with the learned attention vector t such that the words that have more relatedness with the visual semantics will have more impact during the decoding (translation) stage.",3.2 Shared embedding objective,[0],[0]
"However, we may not want to solely rely on only a few most important words.",3.2 Shared embedding objective,[0],[0]
"Thus, to produce the initial hidden state of the decoder, we take a weighted average of the attention vector t and the mean of encoder hidden states:
s0 = tanh(Winit(λt+ (1− λ) 1
N",3.2 Shared embedding objective,[0],[0]
"N∑ i hi)), (4)
where λ determines the contribution from each vector.",3.2 Shared embedding objective,[0],[0]
"Through our experiments, we find the best value for λ is 0.5.",3.2 Shared embedding objective,[0],[0]
"During the decoding stage, at each time step j, the decoder generates a decoder hidden state sj from a conditional GRU cell (Sennrich et al., 2017) whose input is the previously generated translation token yj−1, the previous decoder hidden state sj−1, and the context vector cj at the current time step:
sj = cGRU(sj−1, yj−1, cj) (5)
",3.3 Translation objective,[0],[0]
"The context vector cj is a weighted sum of the encoder hidden state vectors, and captures the relevant source words that the decoder should focus on when generating the current translated token yj .",3.3 Translation objective,[0],[0]
"The weight associated with each encoder hidden state is determined by a feed-forward network.
",3.3 Translation objective,[0],[0]
"From the hidden state sj we can predict the conditional distribution of the next token yj with a fullyconnected layerWo given the previous token’s language embedding ej−1, the current hidden state dj and the context vector for current step cj :
p(yj |yj−1, x) = softmax(Woot), (6)
where ot = tanh(Weej−1 +Wddj +Wccj).",3.3 Translation objective,[0],[0]
"The three inputs are transformed with We, Wd, and Wc, respectively and then summed before being fed into the output layer.
",3.3 Translation objective,[0],[0]
"We train the translation objective by optimizing a cross entropy loss function:
JT (θT ) =",3.3 Translation objective,[0],[0]
"− ∑ j log p(yj |yj−1, x) (7)
By optimizing the objective of the translation and the multimodal shared space learning tasks jointly along with the visual-language attention mechanism, we can simultaneously learn a general mapping between the linguistic signals in two languages and grounding of relevant visual content in the text to improve the translation.",3.3 Translation objective,[0],[0]
"Previous available multimodal machine translation models are only tested on image caption datasets, we, therefore, propose a new dataset, IKEA, that has the real-world application of international online shopping.",4 IKEA Dataset,[0],[0]
We create the dataset by crawling commercial products’ descriptions and images from IKEA and UNIQLO websites.,4 IKEA Dataset,[0],[0]
"There are 3,600 products and we plan to expand the data in the future.",4 IKEA Dataset,[0],[0]
"Each sample is composed of the web-crawled English description of a product, an image of the product, and the web-crawled German or French description of the product.
",4 IKEA Dataset,[0],[0]
"Different than the image caption datasets, the German or French sentences in the IKEA dataset is not an exact parallel translation of its English sentence.",4 IKEA Dataset,[0],[0]
Commercial descriptions in different languages can be vastly different in surface form but still keep the core semantic meaning.,4 IKEA Dataset,[0],[0]
We think IKEA is a good data set to simulate real-world multimodal translation problems.,4 IKEA Dataset,[0],[0]
"The sentence in the IKEA dataset contains 60-70 words on average, which is five times longer than the average text length in Multi30K (Elliott et al., 2016).",4 IKEA Dataset,[0],[0]
"These sentences not only describe the visual appearance of the product, but also the product usage.",4 IKEA Dataset,[0],[0]
"Therefore, capturing the connection between
visual semantics and the text is more challenging on this dataset.",4 IKEA Dataset,[0],[0]
The dataset statistics and an example of the IKEA dataset is in Appendix A.,4 IKEA Dataset,[0],[0]
"We evaluate our proposed model on three datasets: Multi30K (Elliott et al., 2016), Ambiguous COCO (Elliott et al., 2017), and our newly-collected IKEA dataset.",5.1 Datasets,[0],[0]
The Multi30K dataset is the largest existing human-labeled dataset for multimodal machine translation.,5.1 Datasets,[0],[0]
"It consists of 31,014 images, where each image is annotated with an English caption and manual translations of image captions in German and French.",5.1 Datasets,[0],[0]
"There are 29,000 instances for training, 1,014 instances for validation, and 1,000 for testing.",5.1 Datasets,[0],[0]
"Additionally, we also evaluate our model on the Ambiguous COCO Dataset collected in the WMT2017 multimodal machine translation challenge (Elliott et al., 2017).",5.1 Datasets,[0],[0]
"It contains 461 images from the MSCOCO dataset (Lin et al., 2014), whose captions contain verbs with ambiguous meanings.",5.1 Datasets,[0],[0]
"We pre-process all English, French, and German sentences by normalizing the punctuation, lower casing, and tokenizing with the Moses toolkit.",5.2 Setting,[0],[0]
"A Byte-Pair-Encoding (BPE) (Sennrich et al., 2015) operation with 10K merge operations is learned from the pre-processed data and then applied to segment words.",5.2 Setting,[0],[0]
"We restore the original words by concatenating the subwords segmented by BPE in
post-processing.",5.2 Setting,[0],[0]
"During training, we apply early stopping if there is no improvement in BLEU score on validation data for 10 validation steps.",5.2 Setting,[0],[0]
We apply beam search decoding to generate translation with beam size equal to 12.,5.2 Setting,[0],[0]
"We evaluate the performance of all models using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).",5.2 Setting,[0],[0]
"The setting used in IKEA dataset is the same as Multi30K, except that we lower the default batch size from 32 to 12; since IKEA dataset has long sentences and large variance in sentence length, we use smaller batches to make the training more stable.",5.2 Setting,[0],[0]
Full details of our hyperparameter choices can be found in Appendix B. We run all models five times with different random seeds and report the mean and standard deviation.,5.2 Setting,[0],[0]
We compare the performance of our model against the state-of-the-art multimodal machine translation approaches and the text-only baseline.,5.3 Results,[0],[0]
"The idea of our model is inspired by the ""Imagination"" model (Elliott and Kádár, 2017), which unlike our models, simply averages the encoder hidden states for visual grounding learning.",5.3 Results,[0],[0]
"As ""Imagination"" does not report its performance on Multi30K 2017 and Ambiguous COCO in its original paper, we directly use their result reported in the WMT 2017 shared task as a comparison.",5.3 Results,[0],[0]
LIUMCVC is the best multimodal machine translation model in WMT 2017 multimodal machine translation challenge and exploits visual information with several different methods.,5.3 Results,[0],[0]
"We always compare our VAGNMT with the method that has been reported to
have the best performance on each dataset.",5.3 Results,[0],[0]
"Our VAG-NMT surpasses the results of the “Imagination"" model and the LIUMCVC’s model by a noticeable margin in terms of BLEU score on both the Multi30K dataset (Table 1) and the Ambiguous COCO dataset (Table 2).",5.3 Results,[0],[0]
"The METEOR score of our VAG-NMT is slightly worse than that of ""Imagination"" for English -> German on Ambiguous COCO Dataset.",5.3 Results,[0],[0]
"This is likely because the “Imagination"" result was produced by ensembling the result of multiple runs, which typically leads to 1-2 higher BLEU and METEOR points compared to a single run.",5.3 Results,[0],[0]
"Thus, we expect our VAG-NMT to outperform the “Imagination"" baseline if we also use an ensemble.
",5.3 Results,[0],[0]
We observe that our multimodal VAG-NMT model has equal or slightly better result compared to the text-only neural machine translation model on the Multi30K dataset.,5.3 Results,[0],[0]
"On the Ambiguous COCO dataset, our VAG-NMT demonstrates clearer improvement over this text-only baseline.",5.3 Results,[0],[0]
"We suspect this is because Multi30K does not have many cases where images can help improve translation quality, as most of the image captions are short and simple.",5.3 Results,[0],[0]
"In contrast, Ambiguous COCO was purposely curated such that the verbs in the captions can have ambiguous meaning.",5.3 Results,[0],[0]
"Thus, visual context will play a more important role in Ambiguous COCO; namely, to help clarify the sense of the source text and guide the translation to select the correct word in the target language.
",5.3 Results,[0],[0]
We then evaluate all models on the IKEA dataset.,5.3 Results,[0],[0]
Table 3 shows the results.,5.3 Results,[0],[0]
Our VAGNMT has a higher value in BLEU and a comparable value in METEOR compared to the Text-only NMT baseline.,5.3 Results,[0],[0]
"Our VAG-NMT outperforms LIUMCVC’s multimodal system by a large margin, which shows that the LIUMCVC’s multimodal’s good performance on Multi30K does not generalize to this real-world product dataset.",5.3 Results,[0],[0]
The good performance may come from the visual attention mechanism that learns to focus on text segments that are related to the images.,5.3 Results,[0],[0]
"Such attention there-
fore teaches the decoder to apply the visual context to translate those words.",5.3 Results,[0],[0]
This learned attention is especially useful for datasets with long sentences that have irrelevant text information with respect to the image.,5.3 Results,[0],[0]
"To assess the learned joint embedding, we perform an image retrieval task evaluated using the Recall@K metric (Kiros et al., 2014) on the Multi30K dataset.
",5.4 Multimodal embedding results,[0],[0]
"We project the image feature vectors V = {v1, v2, . . .",5.4 Multimodal embedding results,[0],[0]
", vn} and their corresponding captions S = {s1, s2, . . .",5.4 Multimodal embedding results,[0],[0]
", sn} into a shared space.",5.4 Multimodal embedding results,[0],[0]
"We follow the experiments conducted by the previous visual semantic embedding work (Kiros et al., 2014), where for each embedded text vector, we find the closest image vectors around it based on the cosine similarity.",5.4 Multimodal embedding results,[0],[0]
"Then we can compute the recall rate of the paired image in the top K nearest neighbors, which is also known as R@K score.",5.4 Multimodal embedding results,[0],[0]
"The shared space learned with VAG-NMT achieves 64% R@1, 88.6% R@5, and 93.8% R@10 on Multi30K, which demonstrates the good quality of the learned shared semantics.",5.4 Multimodal embedding results,[0],[0]
"We also achieved 58.13% R@1, 87.38% R@5 and 93.74% R@10 on IKEA dataset; 41.35% R@1, 85.48% R@5 and 92.56% R@10 on COCO dataset.",5.4 Multimodal embedding results,[0],[0]
"Besides the quantitative results, we also show several qualitative results in Figure 2.",5.4 Multimodal embedding results,[0],[0]
"We show the top five images retrieved by the example captions.
",5.4 Multimodal embedding results,[0],[0]
"The images share ""cyclist"", ""helmet"", and ""dog"" mentioned in the caption.",5.4 Multimodal embedding results,[0],[0]
We use Facebook to hire raters that speak both German and English to evaluate German translation quality.,5.5 Human evaluation,[0],[0]
"As Text-Only NMT has the highest BLEU results among all baseline models, we compare the translation quality between the Text-Only and the VAG-NMT on all three datasets.",5.5 Human evaluation,[0],[0]
We randomly selected 100 examples for evaluation for each dataset.,5.5 Human evaluation,[0],[0]
The raters are informed to focus more on semantic meaning than grammatical correctness when indicating the preference of the two translations.,5.5 Human evaluation,[0],[0]
They are also given the option to choose a tie if they cannot decide.,5.5 Human evaluation,[0],[0]
"We hired two raters and the inter-annotator agreement is 0.82 in Cohen’s Kappa.
",5.5 Human evaluation,[0],[0]
"We summarize the results in Table 4, where we list the number of times that raters prefer one translation over another or think they are the same quality.",5.5 Human evaluation,[0],[0]
"Our VAG-NMT performs better than Text-Only NMT on MSCOCO and IKEA dataset, which correlates with the automatic evaluation metrics.",5.5 Human evaluation,[0],[0]
"However, the result of VAG-NMT is slightly worse than the Text-Only NMT on the Multi30K test dataset.",5.5 Human evaluation,[0],[0]
"This also correlates with the result of automatic evaluation metrics.
",5.5 Human evaluation,[0],[0]
"Finally, we also compare the translation quality between LIUMCVC multimodal and VAGNMT on 100 randomly selected examples from the IKEA dataset.",5.5 Human evaluation,[0],[0]
"VAG-NMT performs better
than LIUMCVC multimodal.",5.5 Human evaluation,[0],[0]
"The raters prefer our VAG-NMT in 91 cases, LIUMCVC multimodal in 68 cases, and cannot tell in 47 cases.",5.5 Human evaluation,[0],[0]
"To demonstrate the effectiveness of our visual attention mechanism, we show some qualitative examples in Figure 3.",6 Discussion,[0],[0]
"Each row contains three images that share similar semantic meaning, which are retrieved by querying the image caption using our learned shared space.",6 Discussion,[0],[0]
The original caption of each image is shown below each image.,6 Discussion,[0],[0]
"We highlight the three words in each caption that have the highest weights assigned by the visual attention mechanism.
",6 Discussion,[0],[0]
"In the first row of Figure 3, the attention mechanism assigns high weights to the words “skiing"", “snowboarding"", and “snow"".",6 Discussion,[0],[0]
"In the second row, it assigns high attention to “rafting"" or “raft"" for every caption of the three images.",6 Discussion,[0],[0]
"These examples demonstrate evidence that our attention mechanism learns to assign high weights to words that have corresponding visual semantics in the image.
",6 Discussion,[0],[0]
"We also find that our visual grounding attention captures the dependency between the words that
have strong visual semantic relatedness.",6 Discussion,[0],[0]
"For example, in Figure 3, words, such as “raft"",“river"", and “water"", with high attention appear in the image together.",6 Discussion,[0],[0]
This shows that the visual dependence information is encoded into the weighted sum of attention vectors which is applied to initialize the translation decoder.,6 Discussion,[0],[0]
"When we apply the sequence-to-sequence model to translate a long sentence, the encoded visual dependence information strengthens the connection between the words with visual semantic relatedness.",6 Discussion,[0],[0]
Such connections mitigate the problem of standard sequenceto-sequence models tending to forget distant history.,6 Discussion,[0],[0]
"This hypothesis is supported by the fact that our VAG-NMT outperforms all the other methods on the IKEA dataset which has long sentences.
",6 Discussion,[0],[0]
"Lastly, in Figure 4 we provide some qualitative comparisons between the translations from VAG-NMT and Text-Only NMT.",6 Discussion,[0],[0]
"In the first example, our VAG-NMT properly translates the word ""racquet"" to “den schläger"", while the Text-Only NMT mistranslated it to “den boden"" which means “ground"" in English.",6 Discussion,[0],[0]
"We suspect the attention mechanism and visual shared space capture the visual dependence between the word “tennis"" and “racquet"".",6 Discussion,[0],[0]
"In the second example, our VAGNMT model correctly translates the preposition “up"" to “hinauf"" but Text-Only NMT mistranslates it to “hinunter"" which means “down"" in English.",6 Discussion,[0],[0]
We consistently observe that VAG-NMT translates prepositions better than Text-Only NMT.,6 Discussion,[0],[0]
"We think
it is because the pre-trained convnet features captured the relative object position that leads to a better preposition choice.",6 Discussion,[0],[0]
"Finally, in the third example, we show a failure case where Text-Only NMT generates a better translation.",6 Discussion,[0],[0]
"Our VAGNMT mistranslates the verb phrase “sticking out"" to “springt aus"" which means “jump out"" in German, while Text-Only NMT translates to “streckt aus"", which is correct.",6 Discussion,[0],[0]
We find that VAG-NMT often makes mistakes when translating verbs.,6 Discussion,[0],[0]
"We think it is because the image vectors are pretrained on an object classification task, which does not have any human action information.",6 Discussion,[0],[0]
We proposed a visual grounding attention structure to take advantage of the visual information to perform machine translation.,7 Conclusion and Future Work,[0],[0]
"The visual attention mechanism and visual context grounding module help to integrate the visual content into the sequence-to-sequence model, which leads to better translation quality compared to the model with only text information.",7 Conclusion and Future Work,[0],[0]
We achieved stateof-the-art results on the Multi30K and Ambiguous COCO dataset.,7 Conclusion and Future Work,[0],[0]
"We also proposed a new product dataset, IKEA, to simulate a real-world online product description translation challenge.
",7 Conclusion and Future Work,[0],[0]
"In the future, we will continue exploring different methods to ground the visual context into the translation model, such as learning a multimodal shared space across image, source language
text, as well as target language text.",7 Conclusion and Future Work,[0],[0]
"We also want to change the visual pre-training model from an image classification dataset to other datasets that have both objects and actions, to further improve translation performance.
",7 Conclusion and Future Work,[0],[0]
"Acknowledge
We would like to thank Daniel Boyla for providing insightful discussions to help with this research.",7 Conclusion and Future Work,[0],[0]
We also want to thank Chunting Zhou and Ozan Caglayan for suggestions on machine translation model implementation.,7 Conclusion and Future Work,[0],[0]
This work was supported in part by NSF CAREER IIS-1751206.,7 Conclusion and Future Work,[0],[0]
"In this Appendix, we share details on the hyperparameter settings for our model and the training process.",B Hyperparameter Settings,[0],[0]
The word embedding size for both encoder and decoder are 256.,B Hyperparameter Settings,[0],[0]
"The Encoder is a onelayer bidirectional recurrent neural network with Gated Recurrent Unit (GRU), which has a hidden size of 512.",B Hyperparameter Settings,[0],[0]
The decoder is a recurrent neural network with conditional GRU of the same hidden size.,B Hyperparameter Settings,[0],[0]
The visual representation is a 2048- dim vector extracted from the pool5 layer of a pre-trained ResNet-50 network.,B Hyperparameter Settings,[0],[0]
The dimension of the shared visual-text semantic embedding space is 512.,B Hyperparameter Settings,[0],[0]
"We set the decoder initialization weight
value λ to 0.5.",B Hyperparameter Settings,[0],[0]
"During training, we use Adam (Kingma and Ba, 2014) to optimize our model with a learning rate of 4e",B Hyperparameter Settings,[0],[0]
− 4 for German Dataset and 1e,B Hyperparameter Settings,[0],[0]
− 3 for French dataset.,B Hyperparameter Settings,[0],[0]
The batch size is 32.,B Hyperparameter Settings,[0],[0]
"The total gradient norm is clipped to 1 (Pascanu et al., 2012).",B Hyperparameter Settings,[0],[0]
"Dropout is applied at the embedding layer in the encoder, context vectors extracted from the encoder and the output layer of the decoder.",B Hyperparameter Settings,[0],[0]
"For Multi30K German dataset the dropout probabilities are (0.3, 0.5, 0.5) and for Multi30K French dataset the dropout probabilities are (0.2, 0.4, 0.4).",B Hyperparameter Settings,[0],[0]
"For the Multimodal shared space learning objective function, the margin size γ is set to 0.1.",B Hyperparameter Settings,[0],[0]
The objective split weight α is set to 0.99.,B Hyperparameter Settings,[0],[0]
"We initialize the weights of all the parameters with the method introduced in (He et al., 2015b).",B Hyperparameter Settings,[0],[0]
We conducted an ablation test to further evaluate the effectiveness of our visual-text attention mechanism.,C Ablation Analysis on Visual-Text Attention,[0],[0]
We created two comparison experiments where we reduced the impact of visual-text attention with different design options.,C Ablation Analysis on Visual-Text Attention,[0],[0]
"In the first experiment, we remove the visual-attention mechanism in our pipeline and simply use the mean of the encoder hidden states to learn the shared embedding space.",C Ablation Analysis on Visual-Text Attention,[0],[0]
"In the second experiment, we initialize the decoder with just the mean of encoder hidden states without the weighted sum of encoder states using the learned visual-text attention scores.
",C Ablation Analysis on Visual-Text Attention,[0],[0]
We run both experiments on Multi30K German Dataset five times and demonstrate the results in table 5.,C Ablation Analysis on Visual-Text Attention,[0],[0]
"As can be seen, the performance of the changed translation model is obviously worse than the full VAG-NMT in both experiments.",C Ablation Analysis on Visual-Text Attention,[0],[0]
This observation suggests that the visual-attention mechanism is critical in improving the translation performance.,C Ablation Analysis on Visual-Text Attention,[0],[0]
The model improvement comes from the attention mechanism influencing the model’s objective function and decoder’s initialization.,C Ablation Analysis on Visual-Text Attention,[0],[0]
We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information.,abstractText,[0],[0]
Our model jointly optimizes the learning of a shared visuallanguage embedding and a translator.,abstractText,[0],[0]
The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics.,abstractText,[0],[0]
Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets.,abstractText,[0],[0]
We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario.,abstractText,[0],[0]
"On this dataset, our visual attention grounding model outperforms other methods by a large margin.",abstractText,[0],[0]
A Visual Attention Grounding Neural Model for Multimodal Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3749–3760 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3749",text,[0],[0]
"Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases.",1 Introduction,[0],[0]
"There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substitution Generation (Glavaš and Štajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity.",1 Introduction,[0],[0]
"Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers
1The code and data are publicly available on the authors’ homepages and GitHub: https://github.com/ mounicam/lexical_simplification.
(Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010), or people with reading disabilities (Rello et al., 2013).
",1 Introduction,[0],[0]
"Most current approaches to lexical simplification heavily rely on corpus statistics and surface level features, such as word length and corpusbased word frequencies (read more in §5).",1 Introduction,[0],[0]
Two of the most commonly used assumptions are that simple words are associated with shorter lengths and higher frequencies in a corpus.,1 Introduction,[0],[0]
"However, these assumptions are not always accurate and are often the major source of errors in the simplification pipeline (Shardlow, 2014).",1 Introduction,[0],[0]
"For instance, the word foolishness is simpler than its meaningpreserving substitution folly even though foolishness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006).",1 Introduction,[0],[0]
"In fact, we found that 21% of the 2272 meaningequivalent word pairs randomly sampled from PPDB2 (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent.
",1 Introduction,[0],[0]
"To alleviate these inevitable shortcomings of corpus and surface-based methods, we explore a simple but surprisingly unexplored idea – creating an English lexicon of 15,000 words with wordcomplexity ratings by humans.",1 Introduction,[0],[0]
"We also propose a new neural readability ranking model with a Gaussian-based feature vectorization layer, which can effectively exploit these human ratings as well as other numerical features to measure the complexity of any given word or phrase (including those outside the lexicon and/or with sentential context).",1 Introduction,[0],[0]
"Our model significantly outperforms the state-of-the-art on the benchmark SemEval-2012 evaluation for Substitution Ranking (Specia et al.,
2PPDB is a large paraphrase database derived from static bilingual translation data available at: http:// paraphrase.org
2012; Paetzold and Specia, 2017), with or without using the manually created word-complexity lexicon, achieving a Pearson correlation of 0.714 and 0.702 respectively.",1 Introduction,[0],[0]
"We also apply the new ranking model to identify lexical simplifications (e.g., commemorate→ celebrate) among the large number of paraphrase rules in PPDB with improved accuracy compared to previous work for Substitution Generation.",1 Introduction,[0],[0]
"At last, by utilizing the wordcomplexity lexicon, we establish a new state-ofthe-art on two common test sets for Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017).",1 Introduction,[0],[0]
"We make our code, the wordcomplexity lexicon, and a lexical resource of over 10 million paraphrase rules with improved readability scores (namely SimplePPDB++) all publicly available.",1 Introduction,[0],[0]
"We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators.3 Despite the actual larger English vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus4 was effective for simplification purposes (see experiments in §4) as our neural ranking model (§3) can estimate the complexity of any word or phrase even out-of-vocabulary.
",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
We asked 11 non-native but fluent English speakers to rate words on a 6-point Likert scale.,2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"We found that an even number 6-point scale worked better than a 5-point scale in a pilot experiment with two annotators, as the 6-point scheme allowed annotators to take a natural two-step approach: first determine whether a word is simple or complex; then decide whether it is ‘very simple’ (or ‘very complex’), ‘simple’ (or ‘complex’), or ‘moderately simple’ (or ‘moderately complex’).",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"For words with multiple capitalized versions (e.g., nature, Nature, NATURE), we displayed the most frequent form to the annotators.",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"We also asked the annotators to indicate the words for which they had trouble assessing their complexity due to ambiguity, lack of context or any other reason.",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"All the annotators reported little difficulty, and explained possible reasons such as that word bug is simple
3Download at https://github.com/mounicam/",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"lexical_simplification
4https://catalog.ldc.upenn.edu/ ldc2006t13
regardless of its meaning as an insect in biology or an error in computer software.5
With our hired annotators, we were able to have most annotators complete half or the full list of 15,000 words for better consistency, and collected between 5 and 7 ratings for each word.",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"It took most annotators about 2 to 2.5 hours to rate 1,000 words.",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"Table 1 shows few examples from the lexicon along with their human ratings.
",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"In order to assess the annotation quality, we computed the Pearson correlation between each annotator’s annotations and the average of the rest of the annotations (Agirre et al., 2014).",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"For our final word-complexity lexicon, we took an average of the human ratings for each word, discarding those (about 3%) that had a difference ≥ 2 from the mean of the rest of the ratings.",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
The overall inter-annotator agreement improved from 0.55 to 0.64 after discarding the outlying ratings.,2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"For the majority of the disagreements, the ratings of one annotator and the mean of the rest were fairly close: the difference is ≤ 0.5 for 47% of the annotations; ≤ 1.0 for 78% of the annotations; and ≤ 1.5 for 93% of the annotations on the 6-point scale.",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"We hired annotators of different native languages intentionally, which may have contributed to the variance in the judgments.6",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"We leave further investigation and possible crowdsourcing annotation to future work.
",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"5The word-happiness lexicon (Dodds et al., 2011) of 10,222 words was also similarly created by human rating on the most frequent words without context or word-sense disambiguation.
",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"6One recent work similarly observed lower interannotator agreement among non-native speakers than native speakers when asked to identify complex words in given text paragraphs (Yimam et al., 2017).",2 Constructing A Word-Complexity Lexicon with Human Judgments,[0],[0]
"In order to predict the complexity of any given word or phrase, within or outside the lexicon, we propose a Neural Readability Ranking model that can leverage the created word-complexity lexicon and take context (if available) into account to further improve performance.",3 Neural Readability Ranking Model for Words and Phrases,[0],[0]
Our model uses a Gaussian-based vectorization layer to exploit numerical features more effectively and can outperform the state-of-the-art approaches on multiple lexical simplification tasks with or without the word-complexity lexicon.,3 Neural Readability Ranking Model for Words and Phrases,[0],[0]
"We describe the general model framework in this section, and task-specific configurations in the experiment section (§4).",3 Neural Readability Ranking Model for Words and Phrases,[0],[0]
"Given a pair of words/phrases 〈wa, wb〉 as input, our model aims to output a real number that indicates the relative complexity P (y|〈wa, wb〉) of wa and wb.",3.1 Neural Readability Ranker (NRR),[0],[0]
"If the output value is negative, then wa is simpler thanwb and vice versa.",3.1 Neural Readability Ranker (NRR),[0],[0]
"Figure 1 shows the general architecture of our ranking model highlighting the three main components:
1.",3.1 Neural Readability Ranker (NRR),[0],[0]
"An input feature extraction layer (§3.2) that creates lexical and corpus-based features for each input f(wa) and f(wb), and pairwise features f(〈wa, wb〉).",3.1 Neural Readability Ranker (NRR),[0],[0]
"We also inject the word-complexity lexicon into the model as a numerical feature plus a binary indicator.
2.",3.1 Neural Readability Ranker (NRR),[0],[0]
"A Gaussian-based feature vectorization layer (§3.3) that converts each numerical feature, such as the lexicon scores and n-gram probabilities, into a vector representation by a series of Gaussian radial basis functions.
3.",3.1 Neural Readability Ranker (NRR),[0],[0]
"A feedforward neural network performing regression with one task-specific output node that adapts the model to different lexical simplification tasks (§4).
",3.1 Neural Readability Ranker (NRR),[0],[0]
"Our model first processes each input word or phrase in parallel, producing vectorized features.",3.1 Neural Readability Ranker (NRR),[0],[0]
All the features are then fed into a joint feedforward neural network.,3.1 Neural Readability Ranker (NRR),[0],[0]
"We use a combination of rating scores from the word-complexity lexicon, lexical and corpus features (Pavlick and Callison-Burch, 2016) and collocational features (Paetzold and Specia, 2017).
",3.2 Features,[0],[0]
"We inject the word-complexity lexicon into the NRR model by adding two features for each input word or phrase: a 0-1 binary feature representing the presence of a word (the longest word in a multi-word phrase) in the lexicon, and the corresponding word complexity score.",3.2 Features,[0],[0]
"For out-ofvocabulary words, both features have the value 0.",3.2 Features,[0],[0]
We back-off to the complexity score of the lemmatized word if applicable.,3.2 Features,[0],[0]
"We also extract the following features: phrase length in terms of words and characters, number of syllables, frequency with respect to Google Ngram corpus (Brants and Franz, 2006), the relative frequency in Simple Wikipedia with respect to normal Wikipedia (Pavlick and Nenkova, 2015) and ngram probabilities from a 5-gram language model trained on the SubIMDB corpus (Paetzold and Specia, 2016c), which has been shown to work well for lexical simplification.",3.2 Features,[0],[0]
"For a word w, we take language model probabilities of all the possible n-grams within the context window of 2 to the left and right of w.",3.2 Features,[0],[0]
"When w is a multi-word phrase, we break w into possible n-grams and average the probabilities for a specific context window.
",3.2 Features,[0],[0]
"For an input pair of words/phrases 〈wa, wb〉, we include individual features f(w1), f(w2) and the differences f(wa)−f(wb).",3.2 Features,[0],[0]
"We also use pairwise features f(〈wa, wb〉) including cosine similarity cos(−→w a,−→w b) and the difference −→w a−−→w b between the word2vec (Mikolov et al., 2013) embedding of the input words.",3.2 Features,[0],[0]
The embeddings for a mutli-word phrase are obtained by averaging the embeddings of all the words in the phrase.,3.2 Features,[0],[0]
"We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package.7",3.2 Features,[0],[0]
Our model relies primarily on numerical features as many previous approaches for lexical simplification.,3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"Although these continuous features can be directly fed into the network, it is helpful to exploit fully the nuanced relatedness between different intervals of feature values.
",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
We adopt a smooth binning approach and project each numerical feature into a vector representation by applying multiple Gaussian radial basis functions.,3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"For each feature f , we divide its
7https://code.google.com/archive/p/ word2vec/
value range",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"[fmin, fmax] evenly into k bins and place a Gaussian function for each bin with the mean µj",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"(j ∈ {1, 2, . . .",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
", k}) at the center of the bin and standard deviation σ.",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"We specify σ as a fraction γ of bin width:
σ = 1
k",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
(fmax − fmin) ·,3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"γ (1)
where γ is a tunable hyperparameter in the model.",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"For a given feature value f(·), we then compute the distance to each bin as follows:
dj(f(·))",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"= e− (f(·)−µj)
2
2σ2 (2)
and normalize to project into a k-dimensional vector −−→ f(·) =",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"(d1, d2, . . .",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
", dk).
",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"We vectorize all the features except word2vec vectors, −−−→ f(wa), −−−→ f(wb),
−−−−−−−−−→ f(wa)−f(wb), and−−−−−−−→ f(〈wa, wb〉), then concatenate them as inputs.",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"Figure 2 presents a motivating t-SNE visualization of the word-complexity scores from the lexicon after the vectorization in our NRR model, where different feature value ranges are gathered together with some distances in between.",3.3 Vectorizing Numerical Features via Gaussian Binning,[0],[0]
"We use PyTorch framework to implement the NRR model, which consists of an input layer, three hidden layers with eight nodes in each layer and the tanh activation function, and a single node linear output layer.",3.4 Training and Implementation Details,[0],[0]
"The training objective is to minimize the Mean Squared Error (MSE):
L(θ) = 1
m m∑ i=1",3.4 Training and Implementation Details,[0],[0]
"(yi − ŷi)2 (3)
where yi and ŷi are the true and predicted relative complexity scores of 〈wa, wb〉 which can be configured accordingly for different lexical simplification tasks and datasets, m is the number of training examples, and θ is the set of parameters of the NRR model.",3.4 Training and Implementation Details,[0],[0]
"We use Adam algorithm (Kingma and Ba, 2014) for optimization and also apply a dropout of 0.2 to prevent overfitting.",3.4 Training and Implementation Details,[0],[0]
We set the rate to 0.0005 and 0.001 for experiments in (§4.1) and (§4.2) respectively.,3.4 Training and Implementation Details,[0],[0]
"For Gaussian binning layer, we set the number of bins k to 10 and γ to 0.2 without extensive parameter tuning.",3.4 Training and Implementation Details,[0],[0]
"For each experiment,we report results with 100 epochs.",3.4 Training and Implementation Details,[0],[0]
"As the lexical simplification research field traditionally studies multiple sub-tasks and datasets, we present a series of experiments to demonstrate the effectiveness of our newly created lexicon and neural readability ranking (NRR) model.",4 Lexical Simplification Applications,[0],[0]
"Given an instance consisting of a target complex word in a sentence and a set of candidate substitutions, the goal of the Substitution Ranking task is to rank the candidates in the order of their simplicity.",4.1 Substitution Ranking,[0],[0]
"In this section, we show that our proposed NRR model outperforms the state-of-the-art neural model on this task, with or without using the word-complexity lexicon.
Data.",4.1 Substitution Ranking,[0],[0]
"We use the dataset from the English Lexical Simplification shared-task at SemEval 2012 (Specia et al., 2012) for evaluation.",4.1 Substitution Ranking,[0],[0]
"The training and test sets consist of 300 and 1,710 instances, respectively, with a total of 201 target words (all single word, mostly polysemous) and each in 10 different sentences.",4.1 Substitution Ranking,[0],[0]
"One example of such instance contains a target complex word in context:
When you think about it, that’s pretty terrible.
and a set of candidate substitutions {bad, awful, deplorable}.",4.1 Substitution Ranking,[0],[0]
Each instance contains at least 2 and an average of 5 candidates to be ranked.,4.1 Substitution Ranking,[0],[0]
"There are a total of 10034 candidates in the dataset, 88.5% of which are covered by our word-complexity lexicon and 9.9% are multi-word phrases (3438 unique candidates with 81.8% in-vocabulary and 20.2% multi-word).
",4.1 Substitution Ranking,[0],[0]
Task-specific setup of the NRR model.,4.1 Substitution Ranking,[0],[0]
"We train the NRR model with every pair of candidates 〈ca, cb〉 in a candidate set as the input, and the difference of their ranks ra−rb as the groundtruth label.",4.1 Substitution Ranking,[0],[0]
"For each such pair, we also include another training instance with 〈cb, ca〉 as the input and rb",4.1 Substitution Ranking,[0],[0]
− ra as the label.,4.1 Substitution Ranking,[0],[0]
"Given a test instance with candidate set C, we rank the candidates as follows: for every pair of candidates 〈ca, cb〉, the model predicts the relative complexity score S(ca, cb); we then compute a single score R(ca) = ∑ ca 6=cb∈C S(ca, cb) for each candidate by aggregating pairwise scores and rank the candidates in the increasing order of these scores.
",4.1 Substitution Ranking,[0],[0]
Comparison to existing methods.,4.1 Substitution Ranking,[0],[0]
"We compare with the state-of-the-art neural model (Paetzold
and Specia, 2017) for substitution ranking with the best reported results on the SemEval 2012 dataset.",4.1 Substitution Ranking,[0],[0]
"Our baselines also include several other existing methods: Biran et al. (2011), Kajiwara et al. (2013), and Glavaš & Štajner (2015), which use carefully designed heuristic scoring functions to combine various information such as corpus statistics and semantic similarity measures from WordNet; Horn et al. (2014) and the Boundary Ranker (Paetzold and Specia, 2015), which respectively use a supervised SVM ranking model and pairwise linear classification model with various features.",4.1 Substitution Ranking,[0],[0]
"All of these methods have been implemented as part of the LEXenstein toolkit (Paetzold and Specia, 2015), which we use for the experimental comparisons here.",4.1 Substitution Ranking,[0],[0]
"In addition, we also compare to the best system (Jauhar and Specia, 2012) among participants at SemEval 2012, which used SVMbased ranking.
Results.",4.1 Substitution Ranking,[0],[0]
Table 2 compares the performances of our NRR model to the state-of-the-art results reported by Paetzold and Specia (2017).,4.1 Substitution Ranking,[0],[0]
We use precision of the simplest candidate (P@1) and Pearson correlation to measure performance.,4.1 Substitution Ranking,[0],[0]
"P@1 is equivalent to TRank (Specia et al., 2012), the official metric for the SemEval 2012 English Lexical Simplification task.",4.1 Substitution Ranking,[0],[0]
"While P@1 captures the practical utility of an approach, Pearson correlation indicates how well the system’s rankings correlate with human judgment.",4.1 Substitution Ranking,[0],[0]
We train our NRR model with all the features (NRRall) mentioned in §3.2 except the word2vec embedding features to avoid overfitting on the small training set.,4.1 Substitution Ranking,[0],[0]
"Our full model (NRRall+binning+WC) exhibits a statistically significant improvement over the state-of-
paraphrases of ‘modification’ ranked by simplicity SimplePPDB tweak, modify, process, variable, layout SimplePPDB++ change, adjustment, amendment, shift,
difference
paraphrases of ‘aggregation’ SimplePPDB pod, swarm, node, clump, pool SimplePPDB++ cluster, pool, collection, addition, grouping
the-art for both measures.",4.1 Substitution Ranking,[0],[0]
"We use paired bootstrap test (Berg-Kirkpatrick et al., 2012; Efron and Tibshirani, 1993) as it can be applied to any performance metric.",4.1 Substitution Ranking,[0],[0]
We also conducted ablation experiments to show the effectiveness of the Gaussianbased feature vectorization layer (+binning) and the word-complexity lexicon (+WC).,4.1 Substitution Ranking,[0],[0]
"We also can apply our NRR model to rank the lexical and phrasal paraphrase rules in the Paraphrase Database (PPDB) (Pavlick et al., 2015), and identify good simplifications (see examples in Table 3).",4.2 SimplePPDB++,[0],[0]
"The resulting lexical resource, SimplePPDB++, contains all 13.1 million lexical and phrasal paraphrase rules in the XL version of PPDB 2.0 with readability scores in ‘simplifying’, ‘complicating’, or ‘nonsense/no-difference’ categories, allowing flexible trade-off between highquality and high-coverage paraphrases.",4.2 SimplePPDB++,[0],[0]
"In this section, we show the effectiveness of the NRR model we used to create SimplePPDB++ by comparing with the previous version of SimplePPDB (Pavlick and Callison-Burch, 2016) which used a three-way logistic regression classifier.",4.2 SimplePPDB++,[0],[0]
"In next section, we demonstrate the utility of SimplePPDB++ for the Substitution Generation task.
",4.2 SimplePPDB++,[0],[0]
Task-specific setup of NRR model.,4.2 SimplePPDB++,[0],[0]
"We use the same manually labeled data of 11,829 paraphrase rules as SimplePPDB for training and testing, of which 26.5% labeled as ‘simplifying’, 26.5%
as ‘complicating’, and 47% as ‘nonsense/nodifference’.",4.2 SimplePPDB++,[0],[0]
We adapt our NRR model to perform the three-way classification by treating it as a regression problem.,4.2 SimplePPDB++,[0],[0]
"During training, we specify the ground truth label as follows: y = -1",4.2 SimplePPDB++,[0],[0]
"if the paraphrase rule belongs to the ‘complicating’ class, y = +1 if the rule belongs to the ‘simplifying’class, and y",4.2 SimplePPDB++,[0],[0]
= 0 otherwise.,4.2 SimplePPDB++,[0],[0]
"For predicting, the network produces a single real-value output ŷ ∈",4.2 SimplePPDB++,[0],[0]
"[−1, 1] which is then mapped to three-class labels based on the value ranges for evaluation.",4.2 SimplePPDB++,[0],[0]
"The thresholds for the value ranges are -0.4 and 0.4 chosen by cross-validation.
",4.2 SimplePPDB++,[0],[0]
Comparison to existing methods.,4.2 SimplePPDB++,[0],[0]
"We compare our neural readability ranking (NRR) model used to create the SimplePPDB++ against SimplePPDB, which uses a multi-class logistic regression model.",4.2 SimplePPDB++,[0],[0]
"We also use several other baselines, including W2V which uses logistic regression with only word2vec embedding features.
Results.",4.2 SimplePPDB++,[0],[0]
"Following the evaluation setup in previous work (Pavlick and Callison-Burch, 2016), we compare accuracy and precision by 10-fold cross-validation.",4.2 SimplePPDB++,[0],[0]
Folds are constructed in such a way that the training and test vocabularies are disjoint.,4.2 SimplePPDB++,[0],[0]
Table 4 shows the performance of our model compared to SimplePPDB and other baselines.,4.2 SimplePPDB++,[0],[0]
We use all the features (NRRall) in §3.2 except for the context features as we are classifying paraphrase rules in PPDB that come with no context.,4.2 SimplePPDB++,[0],[0]
"SimplePPDB used the same features plus additional discrete features, such as POS tags, character unigrams and bigrams.",4.2 SimplePPDB++,[0],[0]
Our neural readability ranking model alone with Gaussian binning (NRRall+binning) achieves better accuracy and precision while using less features.,4.2 SimplePPDB++,[0],[0]
"Leverag-
ing the lexicon (NRRall+binning+WC) shows statistically significant improvements over SimplePPDB rankings based on the paired bootstrap test.",4.2 SimplePPDB++,[0],[0]
"The accuracy increases by 3.2 points, the precision for ‘simplifying’ class improves by 7.4 points and the precision for ‘complicating’ class improves by 4.0 points.",4.2 SimplePPDB++,[0],[0]
"Substitution Generation is arguably the most challenging research problem in lexical simplification, which involves producing candidate substitutions for each target complex word/phrase, followed by the substitution ranking.",4.3 Substitution Generation,[0],[0]
"The key focus is to not only have better rankings, but more importantly, to have a larger number of simplifying substitutions generated.",4.3 Substitution Generation,[0],[0]
"This is a more realistic evaluation to demonstrate the utility of SimplePPDB++ and the effectiveness of the NRR ranking model we used to create it, and how likely such lexical resources can benefit developing end-to-end sentence simplification system (Narayan and Gardent, 2016; Zhang and Lapata, 2017) in future work.
",4.3 Substitution Generation,[0],[0]
Data.,4.3 Substitution Generation,[0],[0]
"We use the dataset from (Pavlick and Callison-Burch, 2016), which contains 100 unique target words/phrases sampled from the Newsela Simplification Corpus (Xu et al., 2015) of news articles, and follow the same evaluation procedure.",4.3 Substitution Generation,[0],[0]
"We ask two annotators to evaluate whether the generated substitutions are good simplifications.
",4.3 Substitution Generation,[0],[0]
Comparison to existing methods.,4.3 Substitution Generation,[0],[0]
"We evaluate the correctness of the substitutions generated by SimplePPDB++ in comparison to several existing methods: Glavaš (Glavaš and Štajner, 2015), Kauchak (Coster and Kauchak, 2011), WordNet Generator (Devlin and Tait, 1998; Carroll et al., 1999), and SimplePPDB (Pavlick and CallisonBurch, 2016).",4.3 Substitution Generation,[0],[0]
"Glavaš obtains candidates with the highest similarity scores in the GloVe (Pennington et al., 2014) word vector space.",4.3 Substitution Generation,[0],[0]
Kauchak’s generator is based on Simple Wikipedia and normal Wikipedia parallel corpus and automatic word alignment.,4.3 Substitution Generation,[0],[0]
"WordNet-based generator simply uses the synonyms of word in WordNet (Miller, 1995).",4.3 Substitution Generation,[0],[0]
"For all the existing methods, we report the results based on the implementations in (Pavlick and Callison-Burch, 2016), which used SVMbased ranking.",4.3 Substitution Generation,[0],[0]
"For both SimplePPDB and SimplePPDB++, extracted candidates are high quality paraphrase rules (quality score≥3.5 for words and
≥4.0 for phrases) belonging to the same syntactic category as target word according to PPDB 2.0 (Pavlick et al., 2015).
",4.3 Substitution Generation,[0],[0]
Results.,4.3 Substitution Generation,[0],[0]
"Table 5 shows the comparison of SimplePPDB and SimplePPDB++ on the number of substitutions generated for each target, the mean average precision and precision@1 for the final ranked list of candidate substitutions.",4.3 Substitution Generation,[0],[0]
"This is a fair and direct comparison between SimplePPDB++ and SimplePPDB, as both methods have access to the same paraphrase rules in PPDB as potential candidates.",4.3 Substitution Generation,[0],[0]
The better NRR model we used in creating SimplePPDB++ allows improved selections and rankings of simplifying paraphrase rules than the previous version of SimplePPDB.,4.3 Substitution Generation,[0],[0]
"As an additional reference, we also include the measurements for the other existing methods based on (Pavlick and Callison-Burch, 2016), which, by evaluation design, are focused on the comparison of precision while PPDB has full coverage.",4.3 Substitution Generation,[0],[0]
Complex Word Identification (CWI) identifies the difficult words in a sentence that need to be simplified.,4.4 Complex Word Identification,[0],[0]
"According to Shardlow (2014), this step can improve the simplification system by avoiding mistakes such as overlooking challenging words or oversimplifying simple words.",4.4 Complex Word Identification,[0],[0]
"In this section, we demonstrate how our word-complexity lexicon helps with the CWI task by injecting human ratings into the state-of-the-art systems.
",4.4 Complex Word Identification,[0],[0]
Data.,4.4 Complex Word Identification,[0],[0]
"The task is to predict whether a target word/phrase in a sentence is ‘simple’ or ‘complex’, and an example instance is as follows:
Nine people were killed in the bombardment.
",4.4 Complex Word Identification,[0],[0]
"We conduct experiments on two datasets: (i) Semeval 2016 CWI shared-task dataset (Paetzold
and Specia, 2016a), which has been widely used for evaluating CWI systems and contains 2,237 training and 88,221 test instances from Wikipedia; and (ii) CWIG3G2 dataset (Yimam et al., 2017), which is also known as English monolingual CWI 2018 shared-task dataset (Yimam et al., 2018) and comprises of 27,299 training, 3,328 development and 4,252 test instances from Wikipedia and news articles.",4.4 Complex Word Identification,[0],[0]
"Table 7 shows the coverage of our wordcomplexity lexicon over the two CWI datasets.
",4.4 Complex Word Identification,[0],[0]
Comparison to existing methods.,4.4 Complex Word Identification,[0],[0]
"We consider two state-of-the-art CWI systems: (i) the nearest centroid classifier proposed in (Yimam et al., 2017), which uses phrase length, number of senses, POS tags, word2vec cosine similarities, ngram frequency in Simple Wikipedia corpus and Google 1T corpus as features; and (ii) SV000gg (Paetzold and Specia, 2016b) which is an ensemble of binary classifiers trained with a combination of lexical, morphological, collocational, and semantic features.",4.4 Complex Word Identification,[0],[0]
The latter is the best performing system on the Semeval 2016 CWI dataset.,4.4 Complex Word Identification,[0],[0]
"We also compare to threshold-based baselines that use word length, number of word senses and frequency in the Simple Wikipedia.
",4.4 Complex Word Identification,[0],[0]
Utilizing the word-complexity lexicon.,4.4 Complex Word Identification,[0],[0]
"We enhance the SV000gg and the nearest centroid classifier by incorporating the word-complexity lexicon as additional features as described in §3.2.
",4.4 Complex Word Identification,[0],[0]
"We added our modifications to the implementation of SV000gg in the LEXenstein toolkit, and used our own implementation for the nearest centroid classifier.",4.4 Complex Word Identification,[0],[0]
"Additionally, to evaluate the wordcomplexity lexicon in isolation, we train a decision tree classifier with only human ratings as input (WC-only), which is equivalent to learning a threshold over the human ratings.
Results.",4.4 Complex Word Identification,[0],[0]
"We compare our enhanced approaches (SV000gg+WC and NC+WC) and lexicon only approach (WC-only), with the state-of-the-art and baseline threshold-based methods.",4.4 Complex Word Identification,[0],[0]
"For measuring performance, we use F-score and accuracy as well as G-score, the harmonic mean of accuracy and recall.",4.4 Complex Word Identification,[0],[0]
G-score is the official metric of the CWI task of Semeval 2016.,4.4 Complex Word Identification,[0],[0]
Table 6 shows that the wordcomplexity lexicon improves the performance of SV000gg and the nearest centroid classifier in all the three metrics.,4.4 Complex Word Identification,[0],[0]
The improvements are statistically significant according to the paired bootstrap test with p < 0.01.,4.4 Complex Word Identification,[0],[0]
"The word-complexity lexicon alone (WC-only) performs satisfactorily on the CWIG3G2 dataset, which effectively is a simple table look-up approach with extreme time and space efficiency.",4.4 Complex Word Identification,[0],[0]
"For CWI SemEval 2016 dataset, WC-only approach gives the best accuracy and Fscore, though this can be attributed to the skewed distribution of dataset (only 5% of the test instances are ‘complex’).",4.4 Complex Word Identification,[0],[0]
Lexical simplification: Prior work on lexical simplification depends on lexical and corpusbased features to assess word complexity.,5 Related Work,[0],[0]
"For complex word identification, there are broadly two lines of research: learning a frequency-based threshold over a large corpus (Shardlow, 2013b) or training an ensemble of classifiers over a combination of lexical and language model features
(Shardlow, 2013a; Paetzold and Specia, 2016a; Yimam et al., 2017; Kriz et al., 2018).",5 Related Work,[0],[0]
Substitution ranking also follows similar trend.,5 Related Work,[0],[0]
Biran et al. (2011) and Bott et al. (2012) employed simplicity measures based on word length and word frequencies from Wikipedia and Simple Wikipedia.,5 Related Work,[0],[0]
Kajiwara et al. (2013) combined WordNet similarity measures with Simple Wikipedia frequencies.,5 Related Work,[0],[0]
"Glavaš and Štajner (2015) averaged the rankings produced by a collection of frequency, language model and semantic similarity features.",5 Related Work,[0],[0]
"Horn et al. (2014) trained an SVM classifier over corpusbased features.
",5 Related Work,[0],[0]
"Only recently, researchers started to apply neural networks to simplification tasks.",5 Related Work,[0],[0]
"To the best of our knowledge, the work by Paetzold and Specia (2017) is the first neural model for lexical simplification which uses a feedforward network with language model probability features.",5 Related Work,[0],[0]
"Our NRR model is the first pairwise neural ranking model to vectorize numeric features and to embed human judgments using a word-complexity lexicon of 15,000 English words.
",5 Related Work,[0],[0]
"Besides lexical simplification, another line of relevant research is sentence simplification that uses statistical or neural machine translation (MT) approaches (Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018).",5 Related Work,[0],[0]
"It has shown possible to integrate paraphrase rules in PPDB into statistical MT for sentence simplification (Xu et al., 2016) and bilingual translation (Mehdizadeh Seraj et al., 2015), while how to inject SimplePPDB++ into neural MT remains an open research question.
",5 Related Work,[0],[0]
Lexica for simplification: There have been previous attempts to use manually created lexica for simplification.,5 Related Work,[0],[0]
"For example, Elhadad and Sutaria (2007) used UMLS lexicon (Bodenreider, 2007), a repository of technical medical terms; Ehara et al. (2010) asked non-native speakers to answer multiple-choice questions corresponding to 12,000 English words to study each user’s familiarity of vocabulary; Kaji et al. (2012) and Kajiwara et al. (2013) used a dictionary of 5,404 Japanese words based on the elementary school textbooks; Xu et al. (2016) used a list of 3,000 most common English words; Lee and Yeung (2018) used an ensemble of vocabulary lists of different complexity levels.",5 Related Work,[0],[0]
"However, to the best of our knowledge, there is no previous study on manually building a large word-complexity lexi-
con with human judgments that has shown substantial improvements on automatic simplification systems.",5 Related Work,[0],[0]
"We were encouraged by the success of the word-emotion lexicon (Mohammad and Turney, 2013) and the word-happiness lexicon (Dodds et al., 2011, 2015).
",5 Related Work,[0],[0]
Vectorizing features:,5 Related Work,[0],[0]
"Feature binning is a standard feature engineering and data processing method to discretize continuous values, more commonly used in non-neural machine learning models.",5 Related Work,[0],[0]
"Our work is largely inspired by recent works on entity linking that discussed feature quantization for neural models (Sil et al., 2017; Liu et al., 2016) and neural dependency parsing with embeddings of POS tags as features (Chen and Manning, 2014).",5 Related Work,[0],[0]
We proposed a new neural readability ranking model and showed significant performance improvement over the state-of-the-art on various lexical simplification tasks.,6 Conclusion,[0],[0]
"We release a manually constructed word-complexity lexicon of 15,000 English words and an automatically constructed lexical resource, SimplePPDB++, of over 10 million paraphrase rules with quality and simplicity ratings.",6 Conclusion,[0],[0]
"For future work, we would like to extend our lexicon to cover specific domains, different target users and languages.",6 Conclusion,[0],[0]
We thank anonymous reviewers for their thoughtful comments.,Acknowledgments,[0],[0]
"We thank Avirup Sil and Anastasios Sidiropoulos for valuable discussions, Sanja Štajner and Seid Muhie Yimam for sharing their code and data.",Acknowledgments,[0],[0]
"We also thank the annotators: Jeniya Tabassum, Ashutosh Baheti, Wuwei Lan, Fan Bai, Alexander Konovalov, Chaitanya Kulkarni, Shuaichen Chang, Jayavardhan Reddy, Abhishek Kumar and Shreejit Gangadharan.
",Acknowledgments,[0],[0]
This material is based on research sponsored by the NSF under grants IIS-1822754 and IIS1755898.,Acknowledgments,[0],[0]
The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of the NSF or the U.S. Government.,Acknowledgments,[0],[0]
Current lexical simplification approaches rely heavily on heuristics and corpus level features that do not always align with human judgment.,abstractText,[0],[0]
"We create a human-rated wordcomplexity lexicon of 15,000 English words and propose a novel neural readability ranking model with a Gaussian-based feature vectorization layer that utilizes these human ratings to measure the complexity of any given word or phrase.",abstractText,[0],[0]
Our model performs better than the state-of-the-art systems for different lexical simplification tasks and evaluation datasets.,abstractText,[0],[0]
"Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).1",abstractText,[0],[0]
A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 463–472 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1043
Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks
William R. Foland Jr. Department of Computer Science
University of Colorado Boulder, CO 80309
William.Foland@colorado.edu
James H. Martin Department of Computer Science and
Institute of Cognitive Science University of Colorado
Boulder, CO 80309 James.Martin@colorado.edu
Abstract
We present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%. AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more. The AMR parser does not rely on a syntactic preparse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.",text,[0],[0]
"Semantic analysis is the process of extracting meaning from text, revealing key ideas such as ”who did what to whom, when, how, and where?”, and is considered to be one of the most complex tasks in natural language processing.",1 Introduction,[0],[0]
"Historically, an important consideration has been the definition of the output of the task - how can the concepts in a sentence be captured in a general, consistent and expressive manner that facilitates downstream semantic processing?",1 Introduction,[0],[0]
"Over the years many formalisms have been proposed as suitable target representations including variants of first order logic, semantic networks, and frame-based slot-filler notations.",1 Introduction,[0],[0]
Such representations have found a place in many semantic applications but there is no clear consensus as to the best representation.,1 Introduction,[0],[0]
"However, with the rise of supervised machine learning techniques, a new requirement has come to the fore: the ability of human annotators to quickly and reliably generate semantic representations as training data.
",1 Introduction,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2012)1 was developed to provide
1http://amr.isi.edu/language.html
a computationally useful and expressive representation that could be reliably generated by human annotators.",1 Introduction,[0],[0]
Sentence meanings in AMR are represented in the form of graphs consisting of concepts (nodes) connected by labeled relations (edges).,1 Introduction,[0],[0]
"AMR graphs include a number of traditional NLP representations including named entities (Nadeau and Sekine, 2007), word senses (Banerjee and Pedersen, 2002), coreference relations, and predicate-argument structures (Kingsbury and Palmer, 2002; Palmer et al., 2005).",1 Introduction,[0],[0]
"More recent innovations include wikification of named entities and normalization of temporal expressions (Verhagen et al., 2010; Strötgen and Gertz, 2010).",1 Introduction,[0],[0]
"(2016) provides an insightful discussion of the relationship between AMR and other formal representations including first order logic.
",1 Introduction,[0],[0]
"The process of creating AMR’s for sentences is called AMR Parsing and was first introduced in (Flanigan et al., 2014).",1 Introduction,[0],[0]
A key factor driving the development of AMR systems has been the increasing availability of training resources in the form of corpora where each sentence is paired with a corresponding AMR representation 2.,1 Introduction,[0],[0]
A consistent framework for evaluating AMR parsers was defined by the Semeval-2016 Meaning Representation Parsing Task3.,1 Introduction,[0],[0]
"Standard training, development and test splits for the AMR Annotation Release 1 corpus are provided, as well as an additional out-of-domain test dataset, for system comparisons.",1 Introduction,[0],[0]
"4
Viewed as a structured prediction task, AMR parsing poses some difficult challenges not faced by other related language processing tasks including part of speech tagging, syntactic parsing or se-
2See amr.isi.edu for information on currently available resources
3http://alt.qcri.org/semeval2016/task8/# 4Available from LDC as LDC2015E86 DEFT Phase 2 AMR Annotation R1 dataset.
463
mantic role labeling.",1 Introduction,[0],[0]
"The prediction task in these settings can be cast as per-token labeling tasks (i.e. IOB tags) or as a sequence of discrete parser actions, as in transition-based (shift-reduce) approaches to dependency parsing.
",1 Introduction,[0],[0]
The first challenge is that AMR representations are by design abstracted away from their associated surface forms.,1 Introduction,[0],[0]
"AMR corpora pair sentences with their corresponding representations, without providing an explicit annotation, or alignment, that links the parts of the representation to their corresponding elements of the sentence.",1 Introduction,[0],[0]
"Not surprisingly, this complicates training, decoding and evaluation.
",1 Introduction,[0],[0]
"The second challenge is the fact that, as noted earlier, the AMR parsing task is an amalgam of predicate identification and classification, entity recognition, co-reference, word sense disambigua-
tion and semantic role labeling — each of which relies on the others for successful analysis.",1 Introduction,[0],[0]
The architecture and system presented in the following sections is largely motivated by these two challenges.,1 Introduction,[0],[0]
Most current AMR parsers are constructed using some form of supervised machine learning that exploits existing AMR corpora.,2.1 AMR Parsers,[0],[0]
"In general, these systems make use of features derived from various forms of syntactic analysis, ranging from partof-speech tagging to more complex dependency or phrase-structure analysis.",2.1 AMR Parsers,[0],[0]
"Currently, most systems fall into two classes: (1) systems that incrementally transform a dependency parse into an AMR
graph using transition-based systems (Wang et al., 2015, 2016), and (2) graph-oriented approaches that use syntactic features to score edges between all concept pairs, and then use a maximum spanning connected subgraph (MSCG) algorithm to select edges that will constitute the graph (Flanigan et al., 2014; Werling et al., 2015).
",2.1 AMR Parsers,[0],[0]
"As expected, there are exceptions to these general approaches.",2.1 AMR Parsers,[0],[0]
The largely rule-based approach of (2015) converts logical forms from an existing semantic analyzer into AMR graphs.,2.1 AMR Parsers,[0],[0]
"They demonstrate the ability to use their existing system to generate AMRs in German, French, Spanish and Japanese without the need for a native AMR corpus.
(2015) proposes a synchronous hyperedge replacement grammar solution, (2015) uses syntaxbased machine translation techniques to create tree structures similar to AMR, while (2015) creates logical form representations of sentences and then converts these to AMR.
",2.1 AMR Parsers,[0],[0]
"An exception to the use of heavily engineered features is the deep learning approach of (2016), which, following (Collobert et al., 2011), relies on word embeddings and recurrent neural networks to generate AMR graphs.",2.1 AMR Parsers,[0],[0]
"Unlike relatively simple sequence processing tasks like part-of-speech tagging and NER, semantic analysis requires the ability to keep track of relevant information that may be arbitrarily far away from the words currently under consideration.",2.2 Bidirectional LSTM Neural Networks,[0],[0]
Recurrent neural networks (RNNs) are a class of neural architecture that use a form of short-term memory in order to solve this semantic distance problem.,2.2 Bidirectional LSTM Neural Networks,[0],[0]
"Basic RNN systems have been enhanced with the use of special memory cell units, referred to as Long Short-Term Memory neural networks, or LSTM’s (Hochreiter and Schmidhuber, 1997).",2.2 Bidirectional LSTM Neural Networks,[0],[0]
"Such systems can effectively process information dispersed over hundreds of words (Schmidhuber et al., 2002; Gers et al., 2001).
",2.2 Bidirectional LSTM Neural Networks,[0],[0]
Bidirectional LSTMs (B-LSTM) networks are LSTMs that are connected so that both future and past sequence context can be examined.,2.2 Bidirectional LSTM Neural Networks,[0],[0]
"(2015), successfully used a bidirectional LSTM network for semantic role labelling.",2.2 Bidirectional LSTM Neural Networks,[0],[0]
"We use the LSTM cell as described in (Graves et al., 2013), configured in a B-LSTM shown in Figure 2, as the core network architecture in the system.",2.2 Bidirectional LSTM Neural Networks,[0],[0]
"Five B-LSTM Neural
Networks comprise the parser.",2.2 Bidirectional LSTM Neural Networks,[0],[0]
"Our parser5 will be explained using this example sentence: France plans further nuclear cooperation with numerous countries .
",3 Parser Overview,[0],[0]
"A graphical depiction of an AMR for this sentence is shown in Figure 1a.
",3 Parser Overview,[0],[0]
"Given an input sentence, the approach taken in our AMR parser is similar to (Flanigan et al., 2014) in that it consists of two subtasks: (1) discover the concepts (nodes and sub-graphs) present in the sentence, and (2) determine the relations (arcs) that connect the concepts (relations capture both traditional predicate-argument structures (ARGs), as well as additional modifier relations that capture notions including quantification, polarity, and cardinality.)",3 Parser Overview,[0],[0]
Neither of these tasks is straightforward in the AMR context.,3 Parser Overview,[0],[0]
"Among the complications are the fact that individual words may contribute to more than one node (as in the case of France), parts of the graph may be “reentrant”, participating in relations with multiple concepts, and predicate-argument and modifier relations can be introduced by arbitrary parts of the input.
",3 Parser Overview,[0],[0]
"At a high level, our system takes an input sentence in form of a vector of word embeddings
5source at https://github.com/BillFoland/daisyluAMR
and uses a series of recurrent neural networks to (1) discover the basic set of nodes and subgraphs that comprise the AMR, (2) discover the set of predicate-argument relations among those concepts, and (3) identifying any relevant modifier relations that are present.
",3 Parser Overview,[0],[0]
A high level block diagram of the parser is shown in Figure 1b.,3 Parser Overview,[0],[0]
"The parser extracts features from the sentence which are processed by a bidirectional LSTM network (B-LSTM) to create a set of AMR subgraphs, which contain one or two concepts as well as their internal relations to each other.",3 Parser Overview,[0],[0]
Features based on the sentence and these subgraphs are then processed by a pair of B-LSTM networks to compute the probabilities of relations between all subgraphs.,3 Parser Overview,[0],[0]
"All subgraphs are then connected using an iterative, greedy algorithm to compute a single component graph, with all subgraphs connected by relations.",3 Parser Overview,[0],[0]
"Separately, another two B-LSTM networks compute attribute and name categories, which are then appended to the graph.",3 Parser Overview,[0],[0]
"Finally, the subgraphs are expanded into the most probable AMR concept and relation primitives to create the final AMR.",3 Parser Overview,[0],[0]
"Mapping the words in a sentence to AMR concepts is a critical first step in the parsing process, and can influence the performance of all subsequent processing.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"Although the most common mapping is one word to one concept, a series of consecutive words, or span, can also be associated with an AMR concept.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"Likewise, a span of words can be mapped to a small connected subgraph, such as the single word span France which is mapped to a subgraph composed of two concepts connected by a name relation.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"(see the shaded section of Figure 1a).
","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"Training corpora provide sentences which are annotated by humans with AMR graphs, not necessarily including a reference span to subgraph mapping.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
An automatic AMR aligner can be used to predict relationships between words and gold AMR’s.,"4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"We use the alignments produced by the aligner of (2014), along with the words and reference AMR graphs, to identify a subgraph type to associate with each span.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
Each word in the sentence is then associated with an IOBES subgraph type tag.,"4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"We call the algorithm which defines span
to subgraph mapping the Expert Span Identifier, and use it to train the SG Network.
","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"A convenient development detail stems from the fact that during the AMR creation process, the identified subgraphs must be expanded into individual concepts and relations.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"For example, the subgraph type ”Named”, along with the span France, must be expanded to create the concepts, relations, and attributes shown in Figure 1a.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"A Subgraph Expander algorithm implements this task, which is essentially the inverse of the Expert Span Identifier.","4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
The Expert Span Identifier and Subgraph Expander were developed by cascading the two in a test configuration as shown in Figure 3a.,"4.1 AMR Spans, Subgraphs, and Subgraph Decoding",[0],[0]
"All input features for the five networks correspond to the sequence of words in the input sentence, and are presented to the networks as indices into lookup tables.",4.2 Features,[0],[0]
"With the exception of pre-trained word embeddings, these lookup tables are randomly initialized prior to training and representations are created during the training process.",4.2 Features,[0],[0]
The use of distributed word representations generated from large text corpora is pervasive in modern NLP.,4.2.1 Word Embeddings,[0],[0]
"We start with 300 dimension GloVe representations (Pennington et al., 2014) trained on the 840 billion word common crawl (Smith et al., 2013).",4.2.1 Word Embeddings,[0],[0]
"We added two binary dimensions: one for out of vocabulary words, and one for padding, resulting in vectors with a width of 302.",4.2.1 Word Embeddings,[0],[0]
"These embeddings are mapped from the words in the sentence, and are then trained using back propagation just like other parameters in the network.",4.2.1 Word Embeddings,[0],[0]
"The AMR standard was expanded to include the annotation of named entities with a canonical form, using Wikipedia as the standard (see France in Figure 1a).",4.2.2 Wikifier,[0],[0]
"The wiki link associated with this ”wikification” is expressed using the :wiki attribute, which requires some kind of global external knowledge of the Wikipedia ontology.",4.2.2 Wikifier,[0],[0]
"We use the University of Illinois Wikifier (Ratinov et al., 2011; Cheng and Roth, 2013) to identify the :link directly, and use the possible categories output from the wikifier as feature inputs to the NCat Network.
",4.2.2 Wikifier,[0],[0]
"Named Entity Recognition can be valuable input to a parser, and state-of-the-art NER systems can be created using convolutional neural networks (Collobert et al., 2011) or LSTM (Chiu and Nichols, 2015) aided by information from gazetteers.",4.2.2 Wikifier,[0],[0]
"These gazetteers are large dictionaries containing well known named entities (e.g., (Florian et al., 2003)).
",4.2.2 Wikifier,[0],[0]
"Rather than add gazetteer features to our system, we make use of the NER information already calculated and provided by the Univ. of Illinois Wikifier.",4.2.2 Wikifier,[0],[0]
"We then encode the classified named entities output from the wikifier as feature embeddings, which are used by the SG Network.",4.2.2 Wikifier,[0],[0]
"The features used as input to the SG network are:
• word: 45Kx302, the word embeddings • suffix: 430x5, embeddings based on the final
two letters of each word.",4.2.3 AMR Subgraph (SG) Network,[0],[0]
"• caps: 5x5, embeddings based on the capital-
ization pattern of the word.",4.2.3 AMR Subgraph (SG) Network,[0],[0]
• NER:,4.2.3 AMR Subgraph (SG) Network,[0],[0]
"5x5, embeddings indexed by NER
from the Wikifier, ’O’, ’LOC’, ’ORG’, ’PER’ or ’MISC’.
",4.2.3 AMR Subgraph (SG) Network,[0],[0]
"The SG Network produces probabilities for 46 BIOES tagged subgraph types, and the highest probability tag is chosen for each word, as shown for the example sentence in Table 1.",4.2.3 AMR Subgraph (SG) Network,[0],[0]
The AMR concepts (nodes) are connected by relations (arcs).,4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"We found it convenient to distinguish predicate argument relations, or ”Args” from other relations, which we call ”Nargs”.",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"For example, see ARG0 and ARG1 relations in Figure 1a are ”Args”, compared with the name, degree, mod, or quant relations which are ”Nargs”.
",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"The Args Network is run once for each predicate subgraph, and produces a matrix Pargs which defines the probability (prior to the identification of any relations6) of a type of predicate argument relation from a predicate subgraph to any other SG identified subgraph.",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"(For example, see ARG0 and ARG1 relations in Figure 1a.)",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"The matrix has dimensions 5 by s, where 5 is the number of predicate arg relations identified by the network, and s is the total number of subgraphs identified by the SG Network for the sentence.
",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"The Args features, calculated for each source predicate subgraph, are:
• Word, Suffix and Caps as in the SG network.",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"• SG: 46x5, indexed by the SG network identi-
fied subgraph.",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"• PredWords[5], 45Kx302:",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"The word embed-
dings of the word and surrounding 2 words associated with the source predicate subgraph.
",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"6relation probabilities change as hard decisions are made, see section 4.3
Distance[4] 5
Table 2: Args Network Features for the word France while evaluating outgoing args for the word cooperation, associated with predicate cooperate-01
• PredSG[5], 46x10:",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
The SG embedding of the word and surrounding 2 words associated with the source predicate subgraph.,4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
"• regionMark: 21x5, indexed by the distance in words between the word and the word associated with the source predicate subgraph.
",4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
Table 2 shows an example feature set for one subgraph while evaluating a predicate subgraph.,4.2.4 Predicate Argument Relations (Args) Network,[0],[0]
The Nargs Network uses features similar to the Args network.,4.2.5 Non-Predicate Relations (Nargs) Network,[0],[0]
"It is run once for each subgraph, and produces a matrix Pnargs which defines the probability of a type of relation from a subgraph to any other subgraph, prior to the identification of any relations.7",4.2.5 Non-Predicate Relations (Nargs) Network,[0],[0]
"The matrix has dimensions 43 by s, where 43 is the number of non-arg relations identified by the network, and s is the total number of subgraphs identified by the SG Network for the sentence.",4.2.5 Non-Predicate Relations (Nargs) Network,[0],[0]
"The Attr Network determines a primary attribute for each subgraph, if any.8 This network is simplified to detect only one attribute (there could be
7Degree, mod, or quant are examples of Narg relations in Figure 1a.
8(TOP: plan-01) and (op1: france) are attribute examples shown in Figure 1a.
many) per subgraph, and only computes probabilities for the two most common attributes: TOP and polarity.",4.2.6 Attributes (Attr) Network,[0],[0]
"Note that subgraph expansion also identifies many attributes, for example the words associated with named entities, or the normalized quantity and date representations.",4.2.6 Attributes (Attr) Network,[0],[0]
"A known shortcoming of this network is that the TOP and polarity attributes are not mutually exclusive, but noting that the cooccurrence of the two does not occur in the training data, we chose to avoid adding a separate network to allow the prediction of both attributes for a single subgraph.",4.2.6 Attributes (Attr) Network,[0],[0]
"The NCat Network uses features similar to the SG Network, along with the suggested categories (up to eight) from the Wikifier, and produces probabilities for each of 68 :instance roles, or categories, for named entities identified in the training set AMR’s.
• Word, Suffix and Caps as in the SG network.",4.2.7 Named Category (NCat) Network,[0],[0]
"• WikiCat[8]: 108 x 5, indexed by suggested
categories from the Wikifier.",4.2.7 Named Category (NCat) Network,[0],[0]
"The generated Pargs and Pnargs for each SG identified subgraph are processed to determine the most likely relation connections, using the constraints:
1.",4.3 Relation Resolution,[0],[0]
AMR’s are single component graphs without cycles.,4.3 Relation Resolution,[0],[0]
2.,4.3 Relation Resolution,[0],[0]
"AMR’s are simple directed graphs, a max of one relation between any two subgraphs is allowed.",4.3 Relation Resolution,[0],[0]
3.,4.3 Relation Resolution,[0],[0]
"Outgoing predicate relations are limited to one of each kind (i.e. can’t have two ARG0’s)
",4.3 Relation Resolution,[0],[0]
We initialize a graph description with all the subgraphs identified by the SG network.,4.3 Relation Resolution,[0],[0]
Probabilities for all possible edges are represented in the Pargs and Pnargs matrices.,4.3 Relation Resolution,[0],[0]
"The Subgraphs are connected to one another by applying a greedy algorithm, which repeatedly selects the most probable edge from the Pargs and Pnargs matrices and adds the edge to the graph description.",4.3 Relation Resolution,[0],[0]
"After an edge is selected to be added to the graph, we adjust Pargs and Pnargs based on the constraints (hard decisions change the probabilities), and repeat adding edges until all remaining edge probabilities are below a threshold.",4.3 Relation Resolution,[0],[0]
"(The optimum value of this threshold, 0.55, was found by experimenting with the development data set).",4.3 Relation Resolution,[0],[0]
"From then on, only the most probable edges which span graph components are chosen, until the graph contains a single component.
",4.3 Relation Resolution,[0],[0]
"Expressed as a step by step procedure, we first define pconnect as the probability threshold at which to require graph component spanning, and we repeat the following, until any two subgraphs in the graph are connected by at least one path.
",4.3 Relation Resolution,[0],[0]
1.,4.3 Relation Resolution,[0],[0]
Select the most probable outgoing relation from any of the identified subgraph probability matrices.,4.3 Relation Resolution,[0],[0]
Denote this probability as pr. 2.,4.3 Relation Resolution,[0],[0]
"If pr < pconnect, keep selecting most probable relations until a component spanning connection is found.",4.3 Relation Resolution,[0],[0]
3. Add the selected relation to the graph.,4.3 Relation Resolution,[0],[0]
"If a cycle is created, reverse the relation direction and label.",4.3 Relation Resolution,[0],[0]
4. Eliminate impossible relations based on the constraints and re-normalize the affected Pargs and Pnargs matrices.,4.3 Relation Resolution,[0],[0]
"AMR Construction converts the connected subgraph AMR into the final AMR graph form, with proper concepts, relations, and root, as follows:
1.",4.4 AMR Construction,[0],[0]
"The TOP attribute occurs exactly once in each AMR, so the subgraph with highest TOP probability produced by the Attr network is
identified.",4.4 AMR Construction,[0],[0]
The AMR graph is adjusted so that it is rooted with the most probable TOP subgraph.,4.4 AMR Construction,[0],[0]
"After graph adjustment, new cycles are sometimes created, which are removed by using -of relation reversal.",4.4 AMR Construction,[0],[0]
2.,4.4 AMR Construction,[0],[0]
"The subgraphs identified by the SG network, which were considered to be single nodes during relation resolution, are expanded to basic AMR concepts and relations to form a concept/relation AMR graph representation, using the Subgraph Expander component developed as shown in Figure 3b.",4.4 AMR Construction,[0],[0]
"When a subgraph contains two concepts, the choice of connecting to parent or child within the subgraph is made based on training data statistics of each relation type (Arg or Narg) for each subgraph type.",4.4 AMR Construction,[0],[0]
3.,4.4 AMR Construction,[0],[0]
Nationalities are normalized (e.g. French to France).,4.4 AMR Construction,[0],[0]
4.,4.4 AMR Construction,[0],[0]
A very basic coreference resolution is performed by merging all concepts representing ”I” into a single concept.,4.4 AMR Construction,[0],[0]
Coreference resolution was otherwise ignored due to development time constraints.,4.4 AMR Construction,[0],[0]
Semantic graph comparison can be tricky because direct graph alignment fails in the presence of just a few miscompares.,5 Experimental Setup,[0],[0]
"A practical graph comparison program called Smatch (Cai and Knight, 2013) is used to consistently evaluate AMR parsers.",5 Experimental Setup,[0],[0]
"The smatch python script provides an F1 evaluation metric for whole-sentence semantic graph analysis by comparing sets of triples which describe portions of the graphs, and uses a hill climbing algorithm for efficiency.
",5 Experimental Setup,[0],[0]
"All networks, including SG, were trained using stochastic gradient descent (SGD) with a fixed learning rate.",5 Experimental Setup,[0],[0]
"We tried sentence level loglikelihood, which trains a viterbi decoder, as a training objective, but found no improvement over word-level likelihood (cross entropy).",5 Experimental Setup,[0],[0]
"After all LSTM and linear layers, we added dropout to minimize overfitting (Hinton et al., 2012) and batch normalization to reduce sensitivity to learning rates and initialization (Ioffe and Szegedy, 2015).
",5 Experimental Setup,[0],[0]
"For each of the five networks, we used the LDC2015E86 training split to train parameters, and periodically interrupted training to run the dev split (forward) in order to monitor performance.
",5 Experimental Setup,[0],[0]
The model parameters which resulted in best dev performance were saved as the final model.,5 Experimental Setup,[0],[0]
The test split was used as the ”in domain” data set to assess the fully assembled parser.,5 Experimental Setup,[0],[0]
"The inferred AMR’s were then evaluated using the smatch program to produce an F1 score.
",5 Experimental Setup,[0],[0]
"An evaluation dataset was provided for Semeval 2016 task 8, which is significantly different from the LDC2015E86 split dataset.",5 Experimental Setup,[0],[0]
"((2016) describes the eval dataset as ”quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion”).",5 Experimental Setup,[0],[0]
We report the statistics for smatch results of the ”test” and ”eval” datasets for 12 trained systems in Table 3.,6 Results,[0],[0]
"The top five scores for Semeval 2016 task 8, representing the previous state-of-the-art, are shown for context.",6 Results,[0],[0]
"With a smatch score of between 0.651 and 0.654, and a mean of 0.652, our system improves the state-of-the-art AMR parser performance by between 5.07% and 5.55%, and by a mean of 5.22%.",6 Results,[0],[0]
"The best performing systems for in-domain (dev and test) data correlated well with the best ones for the out-of-domain (eval) data, although the scores for the eval dataset were lower overall.",6 Results,[0],[0]
The word spans tagged by the SG network are used to determine the features for the other networks.,6.1 Individual Network Results,[0],[0]
"In particular, every span identified as a predicate will trigger the system to evaluate the Args network in order to determine the probabilities of outgoing predicate ARG relations.",6.1 Individual Network Results,[0],[0]
"Likewise, all spans identified as subgraphs (other than named subgraphs) will lead to a Nargs network evaluation to determine outgoing non-Arg relations.",6.1 Individual Network Results,[0],[0]
"The SG network identifies predicates with 0.93 F1, named subgraphs with 0.91 F1, and all other subgraphs with 0.94 F1.
",6.1 Individual Network Results,[0],[0]
"The Args network identifies ARG0 and ARG1 relations with 0.73 F1, but identification of ARG2, ARG3, and ARG4 drops down to (0.53, 0.20, and 0.43).",6.1 Individual Network Results,[0],[0]
It is difficult for the system to generalize among these relation tags because they differ significantly between predicates.,6.1 Individual Network Results,[0],[0]
"We have shown that B-LSTM neural networks can be used as the basis for a graph based semantic
parser.",7 Conclusion and Future Work,[0],[0]
"Our AMR parser effectively exploits the ability of B-LSTM networks to learn to selectively extract information from words separated by long distances in a sentence, and to build up higher level representations by rejecting or remembering important information during sequence processing.",7 Conclusion and Future Work,[0],[0]
"There are changes which could be made to eliminate all pre-processing and to further improve parser performance.
",7 Conclusion and Future Work,[0],[0]
"Eliminating the need for syntactic pre-parsing is valuable since a syntactic parser takes up significant time and computational resources, and errors in the generated syntax will propagate into an AMR parser.",7 Conclusion and Future Work,[0],[0]
"Our approach avoids both of these problems, while generating high quality results.
",7 Conclusion and Future Work,[0],[0]
"Wikification tasks are generally independent from parsing, but wiki links are a requirement for the latest AMR specification.",7 Conclusion and Future Work,[0],[0]
"Since our preferred wikifier application generates NER information, we used the generated NER tags as input to the SG network.",7 Conclusion and Future Work,[0],[0]
But it would also be fairly easy to add gazetteer information to the network features in order to remove the need for NER preprocessing.,7 Conclusion and Future Work,[0],[0]
"Therefore, the wikification subtask is the only portion of the parser which requires any pre-processing at all.",7 Conclusion and Future Work,[0],[0]
"Incorporating wikification gazetteers as B-LSTM features might allow a performant, fully self contained parser to be created.
",7 Conclusion and Future Work,[0],[0]
"Sense disambiguation is not a very generalizable task, senses other than 01 and 02 for different predicates may differ from each other in ways which are very difficult to discern.",7 Conclusion and Future Work,[0],[0]
"A better approach to disambiguation is to consider predicates separately, solving for a set of coefficients for each verb found in the training set.",7 Conclusion and Future Work,[0],[0]
A general set of model parameters could then be used to handle unseen examples.,7 Conclusion and Future Work,[0],[0]
"Likewise, high level ARGs like ARG2 and ARG3 don’t generalize very well among different predicates, and ARG inference accuracy could be improved with predicatespecific network parameters for the most common cases.
",7 Conclusion and Future Work,[0],[0]
"The alignment between concepts and words is not a reliable, direct mapping: some concepts cannot be grounded to words, some are ambiguous, and automatic aligners tend to have high error rates relative to human aligning judgements.",7 Conclusion and Future Work,[0],[0]
Improvements in the quality of the alignment in training data would improve parsing results.,7 Conclusion and Future Work,[0],[0]
"We present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%.",abstractText,[0],[0]
"AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more.",abstractText,[0],[0]
"The AMR parser does not rely on a syntactic preparse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.",abstractText,[0],[0]
Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1139–1149 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1105
Abstract Syntax Networks for Code Generation and Semantic Parsing
Maxim Rabinovich∗ Mitchell Stern∗ Dan Klein Computer Science Division
University of California, Berkeley {rabinovich,mitchell,klein}@cs.berkeley.edu
Abstract",text,[0],[0]
"Tasks like semantic parsing and code generation are challenging in part because they are structured (the output must be well-formed) but not synchronous (the output structure diverges from the input structure).
",1 Introduction,[0],[0]
"Sequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side.",1 Introduction,[0],[0]
"Yet these approaches do not account for much richer structural constraints on outputs—including well-formedness, well-typedness, and executability.",1 Introduction,[0],[0]
"The wellformedness case is of particular interest, since it can readily be enforced by representing outputs as abstract syntax trees (ASTs) (Aho et al., 2006), an approach that can be seen as a much lighter weight
∗Equal contribution.
",1 Introduction,[0],[0]
"name: [ ’D’, ’i’, ’r’, ’e’, ’ ’, ’W’, ’o’, ’l’, ’f’, ’ ’, ’A’, ’l’, ’p’, ’h’, ’a’] cost: [’2’] type: [’Minion’] rarity: [’Common’] race: [’Beast’] class: [’Neutral’] description: [ ’Adjacent’, ’minions’, ’have’, ’+’, ’1’, ’Attack’, ’.’]",1 Introduction,[0],[0]
health:,1 Introduction,[0],[0]
"[’2’] attack: [’2’] durability: [’-1’]
class DireWolfAlpha(MinionCard): def __init__(self): super().__init__",1 Introduction,[0],[0]
"( ""Dire Wolf Alpha"", 2, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, minion_type=MINION_TYPE.BEAST)
",1 Introduction,[0],[0]
"def create_minion(self, player): return Minion(2, 2, auras=[ Aura(ChangeAttack(1), MinionSelector(Adjacent()))
",1 Introduction,[0],[0]
"])
",1 Introduction,[0],[0]
"version of CCG-based semantic parsing (Zettlemoyer and Collins, 2005).
",1 Introduction,[0],[0]
"In this work, we introduce abstract syntax networks (ASNs), an extension of the standard encoder-decoder framework utilizing a modular decoder whose submodels are composed to natively generate ASTs in a top-down manner.",1 Introduction,[0],[0]
"The decoding process for any given input follows a dy-
1139
namically chosen mutual recursion between the modules, where the structure of the tree being produced mirrors the call graph of the recursion.",1 Introduction,[0],[0]
"We implement this process using a decoder model built of many submodels, each associated with a specific construct in the AST grammar and invoked when that construct is needed in the output tree.",1 Introduction,[0],[0]
"As is common with neural approaches to structured prediction (Chen and Manning, 2014; Vinyals et al., 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al., 2014).
",1 Introduction,[0],[0]
Our model significantly outperforms previous architectures for code generation and obtains competitive or state-of-the-art results on a suite of semantic parsing benchmarks.,1 Introduction,[0],[0]
"On the HEARTHSTONE dataset for code generation, we achieve a token BLEU score of 79.2 and an exact match accuracy of 22.7%, greatly improving over the previous best results of 67.1 BLEU and 6.1% exact match (Ling et al., 2016).
",1 Introduction,[0],[0]
The flexibility of ASNs makes them readily applicable to other tasks with minimal adaptation.,1 Introduction,[0],[0]
We illustrate this point with a suite of semantic parsing experiments.,1 Introduction,[0],[0]
"On the JOBS dataset, we improve on previous state-of-the-art, achieving 92.9% exact match accuracy as compared to the previous record of 90.7%.",1 Introduction,[0],[0]
"Likewise, we perform competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by Dong and Lapata (2016), though not quite reaching the records held by the best previous semantic parsing approaches (Wang et al., 2014).",1 Introduction,[0],[0]
"Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).",1.1 Related work,[0],[0]
"In the latter case, work has focused on making the task look like sequence-tosequence prediction, either by flattening the output tree (Vinyals et al., 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al., 2016).",1.1 Related work,[0],[0]
"Our work differs from both in its use of a recursive top-down generation procedure.
",1.1 Related work,[0],[0]
"Dong and Lapata (2016) introduced a sequenceto-sequence approach to semantic parsing, includ-
ing a limited form of top-down recursion, but without the modularity or tight coupling between output grammar and model characteristic of our approach.
",1.1 Related work,[0],[0]
"Neural (and probabilistic) modeling of code, including for prediction problems, has a longer history.",1.1 Related work,[0],[0]
"Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval.",1.1 Related work,[0],[0]
"More recently, Shin et al. (2017) attacked the same problem using a grammar-based variational autoencoder with top-down generation similar to ours instead.",1.1 Related work,[0],[0]
"Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).
",1.1 Related work,[0],[0]
"The prediction framework most similar in spirit to ours is the doubly-recurrent decoder network introduced by Alvarez-Melis and Jaakkola (2017), which propagates information down the tree using a vertical LSTM and between siblings using a horizontal LSTM.",1.1 Related work,[0],[0]
"Our model differs from theirs in using a separate module for each grammar construct and learning separate vertical updates for siblings when the AST labels require all siblings to be jointly present; we do, however, use a horizontal LSTM for nodes with variable numbers of children.",1.1 Related work,[0],[0]
"The differences between our models reflect not only design decisions, but also differences in data—since ASTs have labeled nodes and labeled edges, they come with additional structure that our model exploits.
",1.1 Related work,[0],[0]
"Apart from ours, the best results on the codegeneration task associated with the HEARTHSTONE dataset are based on a sequence-tosequence approach to the problem (Ling et al., 2016).",1.1 Related work,[0],[0]
"Abstract syntax networks greatly improve on those results.
",1.1 Related work,[0],[0]
"Previously, Andreas et al. (2016) introduced neural module networks (NMNs) for visual question answering, with modules corresponding to linguistic substructures within the input query.",1.1 Related work,[0],[0]
The primary purpose of the modules in NMNs is to compute deep features of images in the style of convolutional neural networks (CNN).,1.1 Related work,[0],[0]
These features are then fed into a final decision layer.,1.1 Related work,[0],[0]
"In contrast to the modules we describe here, NMN modules do not make decisions about what to generate or which modules to call next, nor do they
maintain recurrent state.",1.1 Related work,[0],[0]
"Our model makes use of the Abstract Syntax Description Language (ASDL) framework (Wang et al., 1997), which represents code fragments as trees with typed nodes.",2.1 Abstract Syntax Trees,[0],[0]
"Primitive types correspond to atomic values, like integers or identifiers.",2.1 Abstract Syntax Trees,[0],[0]
"Accordingly, primitive nodes are annotated with a primitive type and a value of that type—for instance, in Figure 3a, the identifier node storing ""create minion"" represents a function of the same name.
",2.1 Abstract Syntax Trees,[0],[0]
"Composite types correspond to language constructs, like expressions or statements.",2.1 Abstract Syntax Trees,[0],[0]
"Each type has a collection of constructors, each of which specifies the particular language construct a node of that type represents.",2.1 Abstract Syntax Trees,[0],[0]
Figure 4 shows constructors for the statement (stmt) and expression (expr) types.,2.1 Abstract Syntax Trees,[0],[0]
"The associated language constructs include function and class definitions, return statements, binary operations, and function calls.
",2.1 Abstract Syntax Trees,[0],[0]
"Composite types enter syntax trees via composite nodes, annotated with a composite type and a choice of constructor specifying how the node expands.",2.1 Abstract Syntax Trees,[0],[0]
"The root node in Figure 3a, for example, is
1The full grammar can be found online on the documentation page for the Python ast module: https://docs.python.org/3/library/ast. html#abstract-grammar
a composite node of type stmt that represents a class definition and therefore uses the ClassDef constructor.",2.1 Abstract Syntax Trees,[0],[0]
"In Figure 3b, on the other hand, the root uses the Call constructor because it represents a function call.
",2.1 Abstract Syntax Trees,[0],[0]
"Children are specified by named and typed fields of the constructor, which have cardinalities of singular, optional, or sequential.",2.1 Abstract Syntax Trees,[0],[0]
"By default, fields have singular cardinality, meaning they correspond to exactly one child.",2.1 Abstract Syntax Trees,[0],[0]
"For instance, the ClassDef constructor has a singular name field of type identifier.",2.1 Abstract Syntax Trees,[0],[0]
"Fields of optional cardinality are associ-
ated with zero or one children, while fields of sequential cardinality are associated with zero or more children—these are designated using ? and * suffixes in the grammar, respectively.",2.1 Abstract Syntax Trees,[0],[0]
"Fields of sequential cardinality are often used to represent statement blocks, as in the body field of the ClassDef and FunctionDef constructors.
",2.1 Abstract Syntax Trees,[0],[0]
"The grammars needed for semantic parsing can easily be given ASDL specifications as well, using primitive types to represent variables, predicates, and atoms and composite types for standard logical building blocks like lambdas and counting (among others).",2.1 Abstract Syntax Trees,[0],[0]
Figure 2 shows what the resulting λ-calculus trees look like.,2.1 Abstract Syntax Trees,[0],[0]
"The ASDL grammars for both λ-calculus and Prolog-style logical forms are quite compact, as Figures 9 and 10 in the appendix show.",2.1 Abstract Syntax Trees,[0],[0]
"We represent inputs as collections of named components, each of which consists of a sequence of tokens.",2.2 Input Representation,[0],[0]
"In the case of semantic parsing, inputs have a single component containing the query sentence.",2.2 Input Representation,[0],[0]
"In the case of HEARTHSTONE, the card’s name and description are represented as sequences of characters and tokens, respectively, while categorical attributes are represented as single-token sequences.",2.2 Input Representation,[0],[0]
"For HEARTHSTONE, we restrict our input and output vocabularies to values that occur more than once in the training set.",2.2 Input Representation,[0],[0]
Our model uses an encoder-decoder architecture with hierarchical attention.,3 Model Architecture,[0],[0]
The key idea behind our approach is to structure the decoder as a collection of mutually recursive modules.,3 Model Architecture,[0],[0]
The modules correspond to elements of the AST grammar and are composed together in a manner that mirrors the structure of the tree being generated.,3 Model Architecture,[0],[0]
"A vertical LSTM state is passed from module to module to propagate information during the decoding process.
",3 Model Architecture,[0],[0]
The encoder uses bidirectional LSTMs to embed each component and a feedforward network to combine them. Component-,3 Model Architecture,[0],[0]
"and token-level attention is applied over the input at each step of the decoding process.
",3 Model Architecture,[0],[0]
We train our model using negative log likelihood as the loss function.,3 Model Architecture,[0],[0]
"The likelihood encompasses terms for all generation decisions made by
the decoder.",3 Model Architecture,[0],[0]
Each component c of the input is encoded using a component-specific bidirectional LSTM.,3.1 Encoder,[0],[0]
"This results in forward and backward token encodings ( −→ hc, ←− hc) that are later used by the attention mechanism.",3.1 Encoder,[0],[0]
"To obtain an encoding of the input as a whole for decoder initialization, we concatenate the final forward and backward encodings of each component into a single vector and apply a linear projection.",3.1 Encoder,[0],[0]
"The decoder decomposes into several classes of modules, one per construct in the grammar, which we discuss in turn.",3.2 Decoder Modules,[0],[0]
"Throughout, we let v denote the current vertical LSTM state, and use f to represent a generic feedforward neural network.",3.2 Decoder Modules,[0],[0]
"LSTM updates with hidden state h and input x are notated as LSTM(h,x).
",3.2 Decoder Modules,[0],[0]
Composite type modules Each composite type T has a corresponding module whose role is to select among the constructors C for that type.,3.2 Decoder Modules,[0],[0]
"As Figure 5a exhibits, a composite type module receives a vertical LSTM state v as input and applies a feedforward network fT and a softmax output layer to choose a constructor:
p (C | T,v) =",3.2 Decoder Modules,[0],[0]
[ softmax (fT (v)) ],3.2 Decoder Modules,[0],[0]
"C .
",3.2 Decoder Modules,[0],[0]
"Control is then passed to the module associated with constructor C.
Constructor modules Each constructor C has a corresponding module whose role is to compute an intermediate vertical LSTM state vu,F for each of its fields F whenever C is chosen at a composite node u.
For each field F of the constructor, an embedding eF is concatenated with an attention-based context vector c and fed through a feedforward neural network fC to obtain a context-dependent field embedding:
ẽF = fC (eF, c) .
",3.2 Decoder Modules,[0],[0]
"An intermediate vertical state for the field F at composite node u is then computed as
vu,F = LSTM v (vu, ẽF) .
",3.2 Decoder Modules,[0],[0]
"Figure 5b illustrates the process, starting with a single vertical LSTM state and ending with one updated state per field.
",3.2 Decoder Modules,[0],[0]
Constructor field modules Each field F of a constructor has a corresponding module whose role is to determine the number of children associated with that field and to propagate an updated vertical LSTM state to them.,3.2 Decoder Modules,[0],[0]
"In the case of fields with singular cardinality, the decision and update are both vacuous, as exactly one child is always generated.",3.2 Decoder Modules,[0],[0]
"Hence these modules forward the field vertical LSTM state vu,F unchanged to the child w corresponding to F:
vw = vu,F. (1)
Fields with optional cardinality can have either zero or one children; this choice is made using a feedforward network applied to the vertical LSTM state:
p(zF = 1 | vu,F) = sigmoid (fgenF (vu,F)) .",3.2 Decoder Modules,[0],[0]
"(2)
If a child is to be generated, then as in (1), the state is propagated forward without modification.
",3.2 Decoder Modules,[0],[0]
"In the case of sequential fields, a horizontal LSTM is employed for both child decisions and state updates.",3.2 Decoder Modules,[0],[0]
We refer to Figure 5c for an illustration of the recurrent process.,3.2 Decoder Modules,[0],[0]
"After being initialized with a transformation of the vertical state, sF,0 = WFvu,F, the horizontal LSTM iteratively
decides whether to generate another child by applying a modified form of (2):
p (zF,i = 1",3.2 Decoder Modules,[0],[0]
"| sF,i−1, vu,F) = sigmoid (fgenF (sF,i−1, vu,F)) .
",3.2 Decoder Modules,[0],[0]
"If zF,i = 0, generation stops and the process terminates, as represented by the solid black circle in Figure 5c.",3.2 Decoder Modules,[0],[0]
"Otherwise, the process continues as represented by the white circle in Figure 5c.",3.2 Decoder Modules,[0],[0]
"In that case, the horizontal state su,i−1 is combined with the vertical state vu,F and an attention-based context vector cF,i using a feedforward network fupdateF to obtain a joint context-dependent encoding of the field F and the position i:
ẽF,i = f update F (vu,F, su,i−1, cF,i).
",3.2 Decoder Modules,[0],[0]
"The result is used to perform a vertical LSTM update for the corresponding child wi:
vwi = LSTM v(vu,F, ẽF,i).
",3.2 Decoder Modules,[0],[0]
"Finally, the horizontal LSTM state is updated using the same field-position encoding, and the process continues:
su,i = LSTM h(su,i−1, ẽF,i).
",3.2 Decoder Modules,[0],[0]
Primitive type modules Each primitive type T has a corresponding module whose role is to select among the values y within the domain of that type.,3.2 Decoder Modules,[0],[0]
"Figure 5d presents an example of the simplest form of this selection process, where the value y is obtained from a closed list via a softmax layer applied to an incoming vertical LSTM state:
p (y | T,v) =",3.2 Decoder Modules,[0],[0]
[ softmax (fT (v)),3.2 Decoder Modules,[0],[0]
"] y .
",3.2 Decoder Modules,[0],[0]
"Some string-valued types are open class, however.",3.2 Decoder Modules,[0],[0]
"To deal with these, we allow generation both from a closed list of previously seen values, as in Figure 5d, and synthesis of new values.",3.2 Decoder Modules,[0],[0]
"Synthesis is delegated to a character-level LSTM language model (Bengio et al., 2003), and part of the role of the primitive module for open class types is to choose whether to synthesize a new value or not.",3.2 Decoder Modules,[0],[0]
"During training, we allow the model to use the character LSTM only for unknown strings but include the log probability of that binary decision in the loss in order to ensure the model learns when to generate from the character LSTM.",3.2 Decoder Modules,[0],[0]
"The decoding process proceeds through mutual recursion between the constituting modules, where the syntactic structure of the output tree mirrors the call graph of the generation procedure.",3.3 Decoding Process,[0],[0]
"At each step, the active decoder module either makes a generation decision, propagates state down the tree, or both.
",3.3 Decoding Process,[0],[0]
"To construct a composite node of a given type, the decoder calls the appropriate composite type module to obtain a constructor and its associated module.",3.3 Decoding Process,[0],[0]
"That module is then invoked to obtain updated vertical LSTM states for each of the constructor’s fields, and the corresponding constructor field modules are invoked to advance the process to those children.
",3.3 Decoding Process,[0],[0]
"This process continues downward, stopping at each primitive node, where a value is generated but no further recursion is carried out.",3.3 Decoding Process,[0],[0]
"Following standard practice for sequence-tosequence models, we compute a raw bilinear attention score qrawt for each token t in the input using the decoder’s current state x and the token’s encoding et:
qrawt = e > t Wx.
",3.4 Attention,[0],[0]
The current state x can be either the vertical LSTM state in isolation or a concatentation of the vertical LSTM state and either a horizontal LSTM state or a character LSTM state (for string generation).,3.4 Attention,[0],[0]
"Each submodule that computes attention does so using a separate matrix W.
A separate attention score qcompc is computed for each component of the input, independent of its content:
qcompc = w > c x.
The final token-level attention scores are the sums of the raw token-level scores and the corresponding component-level scores:
qt = q raw t + q comp c(t) ,
where c(t) denotes the component in which token t occurs.",3.4 Attention,[0],[0]
"The attention weight vector a is then computed using a softmax:
a = softmax (q) .
",3.4 Attention,[0],[0]
"Given the weights, the attention-based context is given by:
c = ∑
t
atet.
",3.4 Attention,[0],[0]
"Certain decision points that require attention have been highlighted in the description above; however, in our final implementation we made attention available to the decoder at all decision points.
",3.4 Attention,[0],[0]
Supervised Attention,3.4 Attention,[0],[0]
"In the datasets we consider, partial or total copying of input tokens into primitive nodes is quite common.",3.4 Attention,[0],[0]
"Rather than providing an explicit copying mechanism (Ling et al., 2016), we instead generate alignments where possible to define a set of tokens on which the attention at a given primitive node should be concentrated.2 If no matches are found, the corresponding set of tokens is taken to be the whole input.
",3.4 Attention,[0],[0]
The attention supervision enters the loss through a term that encourages the final attention weights to be concentrated on the specified subset.,3.4 Attention,[0],[0]
"Formally, if the matched subset of componenttoken pairs is S, the loss term associated with the supervision would be
log ∑
t
exp (at)− log ∑
t∈S exp (at), (3)
2Alignments are generated using an exact string match heuristic that also included some limited normalization, primarily splitting of special characters, undoing camel case, and lemmatization for the semantic parsing datasets.
where at is the attention weight associated with token t, and the sum in the first term ranges over all tokens in the input.",3.4 Attention,[0],[0]
The loss in (3) can be interpreted as the negative log probability of attending to some token in S.,3.4 Attention,[0],[0]
"Data We use three semantic parsing datasets: JOBS, GEO, and ATIS.",4.1 Semantic parsing,[0],[0]
All three consist of natural language queries paired with a logical representation of their denotations.,4.1 Semantic parsing,[0],[0]
"JOBS consists of 640 such pairs, with Prolog-style logical representations, while GEO and ATIS consist of 880 and 5,410 such pairs, respectively, with λ-calculus logical forms.",4.1 Semantic parsing,[0],[0]
"We use the same training-test split as Zettlemoyer and Collins (2005) for JOBS and GEO, and the standard training-development-test split for ATIS.",4.1 Semantic parsing,[0],[0]
"We use the preprocessed versions of these datasets made available by Dong and Lapata (2016), where text in the input has been lowercased and stemmed using NLTK (Bird et al., 2009), and matching entities appearing in the same input-output pair have been replaced by numbered abstract identifiers of the same type.
",4.1 Semantic parsing,[0],[0]
Evaluation We compute accuracies using tree exact match for evaluation.,4.1 Semantic parsing,[0],[0]
"Following the publicly released code of Dong and Lapata (2016), we canonicalize the order of the children within conjunction and disjunction nodes to avoid spurious errors, but otherwise perform no transformations before comparison.",4.1 Semantic parsing,[0],[0]
"Data We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine.3",4.2 Code generation,[0],[0]
"Our trainingdevelopment-test split is identical to that of Ling et al. (2016), with split sizes of 533, 66, and 66, respectively.
",4.2 Code generation,[0],[0]
"Cards contain two kinds of components: textual components that contain the card’s name and a description of its function, and categorical ones that contain numerical attributes (attack, health, cost, and durability) or enumerated attributes (rarity, type, race, and class).",4.2 Code generation,[0],[0]
"The name of the card is represented as a sequence of characters, while
3Available online at https://github.com/ danielyule/hearthbreaker.
",4.2 Code generation,[0],[0]
its description consists of a sequence of tokens split on whitespace and punctuation.,4.2 Code generation,[0],[0]
"All categorical components are represented as single-token sequences.
",4.2 Code generation,[0],[0]
"Evaluation For direct comparison to the results of Ling et al. (2016), we evaluate our predicted code based on exact match and token-level BLEU relative to the reference implementations from the library.",4.2 Code generation,[0],[0]
"We additionally compute node-based precision, recall, and F1 scores for our predicted trees compared to the reference code ASTs.",4.2 Code generation,[0],[0]
"Formally, these scores are obtained by defining the intersection of the predicted and gold trees as their largest common tree prefix.",4.2 Code generation,[0],[0]
"For each experiment, all feedforward and LSTM hidden dimensions are set to the same value.",4.3 Settings,[0],[0]
"We select the dimension from {30, 40, 50, 60, 70} for the smaller JOBS and GEO datasets, or from {50, 75, 100, 125, 150} for the larger ATIS and HEARTHSTONE datasets.",4.3 Settings,[0],[0]
The dimensionality used for the inputs to the encoder is set to 100 in all cases.,4.3 Settings,[0],[0]
"We apply dropout to the non-recurrent connections of the vertical and horizontal LSTMs, selecting the noise ratio from {0.2, 0.3, 0.4, 0.5}.",4.3 Settings,[0],[0]
"All parameters are randomly initialized using Glorot initialization (Glorot and Bengio, 2010).
",4.3 Settings,[0],[0]
"We perform 200 passes over the data for the JOBS and GEO experiments, or 400 passes for the ATIS and HEARTHSTONE experiments.",4.3 Settings,[0],[0]
"Early stopping based on exact match is used for the semantic parsing experiments, where performance is evaluated on the training set for JOBS and GEO or on the development set for ATIS.",4.3 Settings,[0],[0]
Parameters for the HEARTHSTONE experiments are selected based on development BLEU scores.,4.3 Settings,[0],[0]
"In order to promote generalization, ties are broken in all cases with a preference toward higher dropout ratios and lower dimensionalities, in that order.
",4.3 Settings,[0],[0]
"Our system is implemented in Python using the DyNet neural network library (Neubig et al., 2017).",4.3 Settings,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 20 for the semantic parsing experiments, or a batch size of 10 for the HEARTHSTONE experiments.",4.3 Settings,[0],[0]
Our results on the semantic parsing datasets are presented in Table 1.,4.4 Results,[0],[0]
"Our basic system achieves
a new state-of-the-art accuracy of 91.4% on the JOBS dataset, and this number improves to 92.9% when supervised attention is added.",4.4 Results,[0],[0]
"On the ATIS and GEO datasets, we respectively exceed and match the results of Dong and Lapata (2016).",4.4 Results,[0],[0]
"However, these fall short of the previous best results of 91.3% and 90.4%, respectively, obtained by Wang et al. (2014).",4.4 Results,[0],[0]
"This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).
",4.4 Results,[0],[0]
"On the HEARTHSTONE dataset, we improve significantly over the initial results of Ling et al. (2016) across all evaluation metrics, as shown in Table 2.",4.4 Results,[0],[0]
"On the more stringent exact match metric, we improve from 6.1% to 18.2%, and on tokenlevel BLEU, we improve from 67.1 to 77.6.",4.4 Results,[0],[0]
"When supervised attention is added, we obtain an additional increase of several points on each scale, achieving peak results of 22.7% accuracy and 79.2 BLEU.",4.4 Results,[0],[0]
"As the examples in Figures 6-8 show, classes in the HEARTHSTONE dataset share a great deal of common structure.",4.5 Error Analysis and Discussion,[0],[0]
"As a result, in the simplest cases, such as in Figure 6, generating the code is simply a matter of matching the overall structure and plugging in the correct values in the initializer and a few other places.",4.5 Error Analysis and Discussion,[0],[0]
"In such cases, our system generally predicts the correct code, with the
exception of instances in which strings are incorrectly transduced.",4.5 Error Analysis and Discussion,[0],[0]
"Introducing a dedicated copying mechanism like the one used by Ling et al. (2016) or more specialized machinery for string transduction may alleviate this latter problem.
",4.5 Error Analysis and Discussion,[0],[0]
The next simplest category of card-code pairs consists of those in which the card’s logic is mostly implemented via nested function calls.,4.5 Error Analysis and Discussion,[0],[0]
"Figure 7 illustrates a typical case, in which the card’s effect is triggered by a game event (a spell being cast) and both the trigger and the effect are described by arguments to an Effect constructor.",4.5 Error Analysis and Discussion,[0],[0]
"Our system usually also performs well on instances like these, apart from idiosyncratic errors that can take the form of under- or overgeneration or simply substitution of incorrect predicates.
",4.5 Error Analysis and Discussion,[0],[0]
"Cards whose code includes complex logic expressed in an imperative style, as in Figure 8, pose the greatest challenge for our system.",4.5 Error Analysis and Discussion,[0],[0]
"Factors like variable naming, nontrivial control flow, and interleaving of code predictable from the description with code required due to the conventions of the library combine to make the code for these cards difficult to generate.",4.5 Error Analysis and Discussion,[0],[0]
"In some instances (as in the figure), our system is nonetheless able to synthesize a close approximation.",4.5 Error Analysis and Discussion,[0],[0]
"However, in the most complex cases, the predictions deviate significantly from the correct implementation.
",4.5 Error Analysis and Discussion,[0],[0]
"In addition to the specific errors our system makes, some larger issues remain unresolved.",4.5 Error Analysis and Discussion,[0],[0]
Existing evaluation metrics only approximate the actual metric of interest: functional equivalence.,4.5 Error Analysis and Discussion,[0],[0]
"Modifications of BLEU, tree F1, and exact
match that canonicalize the code—for example, by anonymizing all variables—may prove more meaningful.",4.5 Error Analysis and Discussion,[0],[0]
"Direct evaluation of functional equivalence is of course impossible in general (Sipser, 2006), and practically challenging even for the HEARTHSTONE dataset because it requires integrating with the game engine.
",4.5 Error Analysis and Discussion,[0],[0]
Existing work also does not attempt to enforce semantic coherence in the output.,4.5 Error Analysis and Discussion,[0],[0]
"Long-distance semantic dependencies, between occurrences of a single variable for example, in particular are not modeled.",4.5 Error Analysis and Discussion,[0],[0]
Nor is well-typedness or executability.,4.5 Error Analysis and Discussion,[0],[0]
Overcoming these evaluation and modeling issues remains an important open problem.,4.5 Error Analysis and Discussion,[0],[0]
ASNs provide a modular encoder-decoder architecture that can readily accommodate a variety of tasks with structured output spaces.,5 Conclusion,[0],[0]
"They are particularly applicable in the presence of recursive decompositions, where they can provide a simple decoding process that closely parallels the inherent structure of the outputs.",5 Conclusion,[0],[0]
"Our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work.",5 Conclusion,[0],[0]
MR is supported by an NSF Graduate Research Fellowship and a Fannie and John Hertz Foundation Google Fellowship.,Acknowledgments,[0],[0]
MS is supported by an NSF Graduate Research Fellowship.,Acknowledgments,[0],[0]
"Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs.",abstractText,[0],[0]
"We introduce abstract syntax networks, a modeling framework for these problems.",abstractText,[0],[0]
The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree.,abstractText,[0],[0]
"On the benchmark HEARTHSTONE dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-ofthe-art values of 67.1 and 6.1%.",abstractText,[0],[0]
"Furthermore, we perform competitively on the ATIS, JOBS, and GEO semantic parsing datasets with no task-specific engineering.",abstractText,[0],[0]
Abstract Syntax Networks for Code Generation and Semantic Parsing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1171–1181 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1108",text,[0],[0]
"Document summarization is a task to generate a fluent, condensed summary for a document, and keep important information.",1 Introduction,[0],[0]
"As a useful technique to alleviate the information overload people are facing today, document summarization has been extensively investigated.",1 Introduction,[0],[0]
Efforts on document summarization can be categorized to extractive and abstractive methods.,1 Introduction,[0],[0]
Extractive methods produce the summary of a document by extracting sentences from the original document.,1 Introduction,[0],[0]
"They have the advantage of producing fluent sentences and
preserving the meaning of original documents, but also inevitably face the drawbacks of information redundancy and incoherence between sentences.",1 Introduction,[0],[0]
"Moreover, extraction is far from the way humans write summaries.
",1 Introduction,[0],[0]
"On the contrary, abstractive methods are able to generate better summaries with the use of arbitrary words and expressions, but generating abstractive summaries is much more difficult in practice.",1 Introduction,[0],[0]
"Abstractive summarization involves sophisticated techniques including meaning representation, content organization, and surface realization.",1 Introduction,[0],[0]
"Each of these techniques has large space to be improved (Yao et al., 2017).",1 Introduction,[0],[0]
"Due to the immaturity of natural language generation techniques, fully abstractive approaches are still at the beginning and cannot always ensure grammatical abstracts.
",1 Introduction,[0],[0]
Recent neural networks enable an end-to-end framework for natural language generation.,1 Introduction,[0],[0]
"Success has been witnessed on tasks like machine translation and image captioning, together with the abstractive sentence summarization (Rush et al., 2015).",1 Introduction,[0],[0]
"Unfortunately, the extension of sentence abstractive methods to the document summarization task is not straightforward.",1 Introduction,[0],[0]
"Encoding and decoding for a long sequence of multiple sentences, currently still lack satisfactory solutions (Yao et al., 2017).",1 Introduction,[0],[0]
"Recent abstractive document summarization models are yet not able to achieve convincing performance, with a considerable gap from extractive methods.
",1 Introduction,[0],[0]
"In this paper, we review the key factors of document summarization, i.e., the saliency, fluency, coherence, and novelty requirements of the generated summary.",1 Introduction,[0],[0]
"Fluency is what neural generation models are naturally good at, but the other factors are less considered in previous neural abstractive models.",1 Introduction,[0],[0]
"A recent study (Chen et al., 2016) starts to consider the factor of novelty, using a distraction mechanism to avoid redundancy.",1 Introduction,[0],[0]
"As far as we
1171
know, however, saliency has not been addressed by existing neural abstractive models, despite its importance for summary generation.
",1 Introduction,[0],[0]
"In this work, we study how neural summarization models can discover the salient information of a document.",1 Introduction,[0],[0]
"Inspired by the graph-based extractive summarization methods, we introduce a novel graph-based attention mechanism in the encoderdecoder framework.",1 Introduction,[0],[0]
"Moreover, we investigate the challenges of accepting and generating long sequences for sequence-to-sequence (seq2seq) models, and propose a new hierarchical decoding algorithm with a reference mechanism to generate the abstractive summaries.",1 Introduction,[0],[0]
"The proposed method is able to tackle the constraints of saliency, nonredundancy, information correctness, and fluency under a unified framework.
",1 Introduction,[0],[0]
We conduct experiments on two large-scale corpora with human generated summaries.,1 Introduction,[0],[0]
"Experimental results demonstrate that our approach consistently outperforms previous neural abstractive summarization models, and is also competitive with state-of-the-art extractive methods.
",1 Introduction,[0],[0]
We organize the paper as follows.,1 Introduction,[0],[0]
Section 2 introduces related work.,1 Introduction,[0],[0]
Section 3 describes our method.,1 Introduction,[0],[0]
In Section 4 we present the experiments and have discussion.,1 Introduction,[0],[0]
Finally in Section 5 we conclude this paper.,1 Introduction,[0],[0]
Document summarization can be categorized to extractive methods and abstractive methods.,2.1 Extractive Summarization Methods,[0],[0]
Extractive methods extract sentences from the original document to form the summary.,2.1 Extractive Summarization Methods,[0],[0]
"Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007).",2.1 Extractive Summarization Methods,[0],[0]
"In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014).
",2.1 Extractive Summarization Methods,[0],[0]
Neural networks have also been widely investigated on the extractive summarization task.,2.1 Extractive Summarization Methods,[0],[0]
"Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b).",2.1 Extractive Summarization Methods,[0],[0]
More recent works predict the extraction of sentences in a more data-driven way.,2.1 Extractive Summarization Methods,[0],[0]
Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism.,2.1 Extractive Summarization Methods,[0],[0]
"Nal-
lapati et al. (2017) propose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents.",2.1 Extractive Summarization Methods,[0],[0]
Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods.,2.1 Extractive Summarization Methods,[0],[0]
Abstractive summarization aims at generating the summary based on understanding the input text.,2.2 Abstractive Summarization Methods,[0],[0]
"It involves multiple subproblems like simplification, paraphrasing, and fusion.",2.2 Abstractive Summarization Methods,[0],[0]
"Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015).
",2.2 Abstractive Summarization Methods,[0],[0]
"As for neural network models, success is achieved on sentence abstractive summarization.",2.2 Abstractive Summarization Methods,[0],[0]
"Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework.",2.2 Abstractive Summarization Methods,[0],[0]
Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework.,2.2 Abstractive Summarization Methods,[0],[0]
"These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents.
",2.2 Abstractive Summarization Methods,[0],[0]
Some recent works investigate neural abstractive models on the document summarization task.,2.2 Abstractive Summarization Methods,[0],[0]
"Cheng and Lapata (2016) also adopt a word extraction model, which is restricted to use the words of the source document to generate a summary, although the performance is much worse than the sentence extractive model.",2.2 Abstractive Summarization Methods,[0],[0]
Nallapati et al. (2016) extend the sentence summarization model by trying a hierarchical attention architecture and a limited vocabulary during the decoding phase.,2.2 Abstractive Summarization Methods,[0],[0]
However these models still investigate few properties of the document summarization task.,2.2 Abstractive Summarization Methods,[0],[0]
"Chen et al. (2016) first attempt to explore the novelty factor of summarization, and propose a distraction-based attentional model.",2.2 Abstractive Summarization Methods,[0],[0]
"Unfortunately these state-ofthe-art neural abstractive summarization models are still not competitive to extractive methods, and there are several problems remain to be solved.",2.2 Abstractive Summarization Methods,[0],[0]
In this section we introduce our method.,3.1 Overview,[0],[0]
"We adopt an encoder-decoder framework, which is
widely used in machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), etc.",3.1 Overview,[0],[0]
"In particular, we use a hierarchical encoderdecoder framework similar to (Li et al., 2015), as shown in Figure 1.",3.1 Overview,[0],[0]
"The main distinction of this work is that we introduce a graph-based attention mechanism which is illustrated in Figure 1b, and we propose a hierarchical decoding algorithm with a reference mechanism to tackle the difficulty of abstractive summary generation.",3.1 Overview,[0],[0]
"In the following parts, we will first introduce the encoder-decoder framework, and then describe the graph-based attention and the hierarchical decoding algorithm.",3.1 Overview,[0],[0]
The goal of the encoder is to map the input document to a vector representation.,3.2 Encoder,[0],[0]
"A document d is a sequence of sentences d = {si}, and a sentence si is a sequence of words si = {wi,k}.",3.2 Encoder,[0],[0]
"Each word wi,k is represented by its distributed representation ei,k, which is mapped by a word embedding matrix Ev.",3.2 Encoder,[0],[0]
"We adopt a hierarchical encoder framework, where we use a word encoder encword to encode the words of a sentence si into the sentence representation, and use a sentence encoder encsent to encode the sentences of a document d into the document representation.",3.2 Encoder,[0],[0]
"The input to the word encoder is the word sequence of a sentence, appended with an “<eos>” token indicating the end of a sentence.",3.2 Encoder,[0],[0]
"The word encoder sequentially updates its hidden state after receiving each word, as hi,k = encword(hi,k−1, ei,k).",3.2 Encoder,[0],[0]
"The last hidden state (after the word encoder receives “<eos>”) is denoted as hi,−1, and used as the embedding representation of the sentence si, denoted as xi.",3.2 Encoder,[0],[0]
"A sentence encoder is used to sequentially receive the embeddings of the sentences, given by hi = encsent(hi−1,xi).",3.2 Encoder,[0],[0]
A pseudo sentence of an “<eod>” token is appended at the end of the document to indicate the end of the whole document.,3.2 Encoder,[0],[0]
"The hidden state after the sentence encoder receives “<eod>” is treated as the representation of the input document c = h−1.
",3.2 Encoder,[0],[0]
"We use the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as both the word encoder encword and sentence encoder encsent.",3.2 Encoder,[0],[0]
"In particular, we adopt the variant of LSTM structure in (Graves, 2013).",3.2 Encoder,[0],[0]
"The decoder is used to generate output sentences {s′j} according to the representation of the input
sentences.",3.3 Decoder with Attention,[0],[0]
"We also use an LSTM-based hierarchical decoder framework to generate the summary, because the summary typically comprises several sentences.",3.3 Decoder with Attention,[0],[0]
The sentence decoder decsent receives the document representation c as the initial state h ′ 0,3.3 Decoder with Attention,[0],[0]
"= c,",3.3 Decoder with Attention,[0],[0]
"and predicts the sentence representations sequentially, by h ′",3.3 Decoder with Attention,[0],[0]
j = decsent(h ′,3.3 Decoder with Attention,[0],[0]
"j−1,x ′ j−1), where x ′ j−1 is the encoded representation of the previously generated sentence s ′",3.3 Decoder with Attention,[0],[0]
j−1.,3.3 Decoder with Attention,[0],[0]
The word decoder decword receives a sentence representation h ′ j as the initial state h ′,3.3 Decoder with Attention,[0],[0]
"j,0 = h ′ j , and predicts the word representations sequentially, by h ′ j,k =",3.3 Decoder with Attention,[0],[0]
"decword(h ′ j,k−1, ej,k−1), where ej,k−1 is the embedding of the previously generated word.",3.3 Decoder with Attention,[0],[0]
"The predicted word representations are mapped to vectors of the vocabulary size dimension, and then normalized by a softmax layer as the probability distribution of generating the words in the vocabulary.",3.3 Decoder with Attention,[0],[0]
"A word decoder stops when it generates the “<eos>” token and similarly the sentence decoder stops when it generates the “<eod>” token.
",3.3 Decoder with Attention,[0],[0]
"In primitive decoder models, c is the same for generating all the output words, which requires c to be a sufficient representation for the whole input sequence.",3.3 Decoder with Attention,[0],[0]
"The attention mechanism (Bahdanau et al., 2014) is usually introduced to alleviate the burden of remembering the whole input sequence, and to allow the decoder to pay different attention to different parts of input at different generation states.",3.3 Decoder with Attention,[0],[0]
"The attention mechanism sets a different cj when generating sentence j, by cj = ∑",3.3 Decoder with Attention,[0],[0]
i α j ihi. αji indicates how much the i-th original sentence si contributes to generating the j-th sentence.,3.3 Decoder with Attention,[0],[0]
α j,3.3 Decoder with Attention,[0],[0]
"i is usually computed as:
αji = e η",3.3 Decoder with Attention,[0],[0]
"( hi,h ′ j )
",3.3 Decoder with Attention,[0],[0]
"∑ l e η(hl,h′j)
(1)
where η is the function modeling the relation between hi and h ′",3.3 Decoder with Attention,[0],[0]
j .,3.3 Decoder with Attention,[0],[0]
"η can be defined using various functions including η (a,b) = aTb, η (a,b) = aTMb, and even a non-linear function achieved by a multi-layer neural network.",3.3 Decoder with Attention,[0],[0]
"In this paper we use η (a,b) = aTMb where M is a parameter matrix.",3.3 Decoder with Attention,[0],[0]
"Traditional attention computes the importance score of a sentence si, when generating sentence s ′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , according to the relation between the hidden state hi and current decoding state h ′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , as shown
in Figure 1a.",3.4 Graph-based Attention Mechanism,[0],[0]
"This attention mechanism is useful in scenarios like machine translation and image captioning, because the model is able to learn a relevance mapping between the input and output.",3.4 Graph-based Attention Mechanism,[0],[0]
"However, for document summarization, it is not easy for the model to learn how to summarize the salient information of a document, i.e., which sentences are more important to a document.
",3.4 Graph-based Attention Mechanism,[0],[0]
"To tackle this challenge, we learn from graphbased extractive summarization models TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are based on the PageRank (Page et al., 1999) algorithm.",3.4 Graph-based Attention Mechanism,[0],[0]
These unsupervised graph-based models show good ability to identify important sentences in a document.,3.4 Graph-based Attention Mechanism,[0],[0]
"The underlying idea is that a sentence is important in a document if it is heavily linked with many important sentences (Wan, 2010).
",3.4 Graph-based Attention Mechanism,[0],[0]
"In graph-based extractive summarization, a graph G is constructed to rank the original sentences.",3.4 Graph-based Attention Mechanism,[0],[0]
"The vertices V are the set of n sentences to be considered, and the edges E are the relations between the sentences, which are typically modeled by the similarity of sentences.",3.4 Graph-based Attention Mechanism,[0],[0]
Let W ∈ Rn×n be the adjacent matrix.,3.4 Graph-based Attention Mechanism,[0],[0]
"Then the saliency scores of the sentences are determined by making use of the global information on the graph recursively, as:
f (t+ 1) = λWD−1f(t)",3.4 Graph-based Attention Mechanism,[0],[0]
"+ (1− λ)y (2)
",3.4 Graph-based Attention Mechanism,[0],[0]
where f =,3.4 Graph-based Attention Mechanism,[0],[0]
"[f1, . . .",3.4 Graph-based Attention Mechanism,[0],[0]
", fn] ∈",3.4 Graph-based Attention Mechanism,[0],[0]
Rn denotes the rank scores of the n sentences.,3.4 Graph-based Attention Mechanism,[0],[0]
f(t) denotes the rank scores at the t-th iteration.,3.4 Graph-based Attention Mechanism,[0],[0]
"D is a diagonal matrix with its (i, i)-element equal to the sum of the i-th column of W .",3.4 Graph-based Attention Mechanism,[0],[0]
"Assume we use hi as the representation of si, and",3.4 Graph-based Attention Mechanism,[0],[0]
"W (i, j) = hTi Mhj , where M is a parameter matrix to be learned.",3.4 Graph-based Attention Mechanism,[0],[0]
"λ is a damping
factor.",3.4 Graph-based Attention Mechanism,[0],[0]
y ∈,3.4 Graph-based Attention Mechanism,[0],[0]
Rn with all elements equal to 1/n.,3.4 Graph-based Attention Mechanism,[0],[0]
"The solution of f can be calculated using the closedform:
f = (1− λ)(I",3.4 Graph-based Attention Mechanism,[0],[0]
"− λWD−1)−1y (3) In the graph model, the importance score of a sentence si is determined by the relation between hi and the {hl} of all other sentences.",3.4 Graph-based Attention Mechanism,[0],[0]
"Relatively, in traditional attention mechanisms, the importance (attention) score αji is determined by the relation between hi and h ′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , regardless of other original sentences.",3.4 Graph-based Attention Mechanism,[0],[0]
"In our model we hope to combine the two effects, and compute the rank scores of the original sentences regarding h
′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , so that the
importance scores of original sentences are different when decoding different state h
′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , denoted
by f j .",3.4 Graph-based Attention Mechanism,[0],[0]
In our model we use the scores f j to compute the attention.,3.4 Graph-based Attention Mechanism,[0],[0]
"Therefore, h
′ j should be
considered in the graph model.",3.4 Graph-based Attention Mechanism,[0],[0]
"Inspired by the query-focused graph-based extractive summarization model (Wan et al., 2007), we realize this by applying the idea of topic-sensitive PageRank (Haveliwala, 2002), which is to rank the sentences with the concern of their relevance to the topic.",3.4 Graph-based Attention Mechanism,[0],[0]
"We treat the current decoding state h
′",3.4 Graph-based Attention Mechanism,[0],[0]
"j as the topic and
add it into the graph as the 0-th pseudo-sentence.",3.4 Graph-based Attention Mechanism,[0],[0]
"Given a topic T , the topic-sensitive PageRank is similar to Eq. 3 except that y becomes:
yT =",3.4 Graph-based Attention Mechanism,[0],[0]
{ 1 |T | i ∈ T 0,3.4 Graph-based Attention Mechanism,[0],[0]
i /∈,3.4 Graph-based Attention Mechanism,[0],[0]
"T
(4)
",3.4 Graph-based Attention Mechanism,[0],[0]
"Therefore yT is always a one hot vector and only y0 = 1, indicating the 0-th sentence is s ′",3.4 Graph-based Attention Mechanism,[0],[0]
j .,3.4 Graph-based Attention Mechanism,[0],[0]
"Denote W j as the new adjacent matrix added with h ′ j , and D
j as the new diagonal matrix corresponding to W j .",3.4 Graph-based Attention Mechanism,[0],[0]
"Then the convergence score vector f j contains the importance scores for all the
input sentences when generating sentence s ′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , as:
f j = (1− λ)(I",3.4 Graph-based Attention Mechanism,[0],[0]
"− λW jDj−1)−1yT (5)
The new scores f j can be used to compute the graph-based attention when decoding h
′",3.4 Graph-based Attention Mechanism,[0],[0]
"j , to find
the sentences which are both globally important and relevant to current decoding state h
′",3.4 Graph-based Attention Mechanism,[0],[0]
"j . In-
spired by (Chen et al., 2016) we adopt a distraction mechanism to compute the final attention value αji , which subtracts the rank scores of the previous step, to penalize the model from attending to previously attended sentences, and also help to normalize the ranked scores f j .",3.4 Graph-based Attention Mechanism,[0],[0]
"The graph-based attention is finally computed as:
αji = max(f ji − f j−1 i , 0)∑
",3.4 Graph-based Attention Mechanism,[0],[0]
l ( max(f jl,3.4 Graph-based Attention Mechanism,[0],[0]
"− f j−1 l , 0) )",3.4 Graph-based Attention Mechanism,[0],[0]
"(6)
where f0 is initialized with all elements equal to 1/n.",3.4 Graph-based Attention Mechanism,[0],[0]
"The graph-based attention will only focus on those sentences ranked higher over the previous decoding step, so that it concentrates more on the sentences which are both salient and novel.",3.4 Graph-based Attention Mechanism,[0],[0]
"Both Eq. 5 and Eq. 6 are differentiable; thus we can use the graph-based attention function Eq. 6 to replace the traditional attention function Eq. 1, and the neural model using the graph-based attention can also be trained using traditional gradientbased methods.",3.4 Graph-based Attention Mechanism,[0],[0]
"The loss function L of the model is the negative log likelihood of generating summaries over the training set D:
L = ∑
(Y,X)∈D",3.5 Model Training,[0],[0]
"− log p(Y |X; θ) (7)
where X = { x1, . . .",3.5 Model Training,[0],[0]
", x|X| } and Y ={
y1, . . .",3.5 Model Training,[0],[0]
", y|Y | }
denote the word sequences of a document and its summary respectively, including the “<eos>” and “<eod>” tokens for structure information.",3.5 Model Training,[0],[0]
"Then
log p(Y |X; θ) = |Y |∑
τ=1
log p (yτ | {y1, . . .",3.5 Model Training,[0],[0]
", yτ−1} , c; θ)
(8) and log p (yτ | {y1, . . .",3.5 Model Training,[0],[0]
", yτ−1} , c; θ) is modeled by the LSTM encoder and decoder.",3.5 Model Training,[0],[0]
"We use the Adamax (Kingma and Ba, 2014) gradient-based
optimization method to optimize the model parameters θ.",3.5 Model Training,[0],[0]
"We find there are several problems during the generation of summary, including out-of-vocabulary (OOV) words, information incorrectness, error accumulation and repetition.",3.6 Decoding Algorithm,[0],[0]
These problems make the generated abstractive summaries far from satisfactory.,3.6 Decoding Algorithm,[0],[0]
"In this work, we propose a hierarchical decoding algorithm with a reference mechanism to tackle these difficulties, which effectively improves the quality of generated summaries.
",3.6 Decoding Algorithm,[0],[0]
"As OOV words frequently occur in name entities, we can first identify the entities of a document using NLP toolkit like Stanford CoreNLP1.",3.6 Decoding Algorithm,[0],[0]
Then we prefix every entity with an “@entity” token and a number indicating how many words the entity has.,3.6 Decoding Algorithm,[0],[0]
"We hope the entity prefixes can help better deal with entities which have more than one word, and help improve the accuracy of recovering OOV words in entities.",3.6 Decoding Algorithm,[0],[0]
"After decoding we recover the OOV words by matching entities in the original document according to the contexts.
",3.6 Decoding Algorithm,[0],[0]
"For the hierarchical decoder, a major challenge is that same sentences or phrases are often repeated in the output.",3.6 Decoding Algorithm,[0],[0]
"A beam search strategy may help to alleviate the repetition in a sentence, but the repetition in the whole generated summary is remained a problem.",3.6 Decoding Algorithm,[0],[0]
The word-level beam search is not easy to be extended to the sentence level.,3.6 Decoding Algorithm,[0],[0]
"The reason is that the K-best sentences generated by a word decoder will mostly be similar to each other, which is also noticed by Li et al. (2016).
",3.6 Decoding Algorithm,[0],[0]
In this paper we propose a hierarchical beam search algorithm with a reference mechanism.,3.6 Decoding Algorithm,[0],[0]
The hierarchical algorithm comprises K-best word-level beam search and N -best sentencelevel beam search.,3.6 Decoding Algorithm,[0],[0]
"At the word level, the only difference to vanilla beam search is that we add an additional term to the score p̃(yτ ) of generating word yτ , and now score(yτ ) = p̃(yτ ) + γ (ref(Yτ−1 + yτ , s∗)− ref(Yτ−1, s∗)), where Yτ−1 = {y1, . . .",3.6 Decoding Algorithm,[0],[0]
", yτ−1} and p̃(yτ ) = log p (yτ |Yτ−1, c; θ).",3.6 Decoding Algorithm,[0],[0]
s∗ is an original sentence to refer to.,3.6 Decoding Algorithm,[0],[0]
ref is a function which calculates the ratio of bigram overlap between two texts.,3.6 Decoding Algorithm,[0],[0]
"The added term aims to favor the generated word yτ with improving the bigram overlap between current generated summary Yτ−1 and the target orig-
1http://stanfordnlp.github.io/CoreNLP/
inal sentence s∗.",3.6 Decoding Algorithm,[0],[0]
"At the word decoder level, the reference mechanism helps to both improve the information correctness and avoid redundancy.",3.6 Decoding Algorithm,[0],[0]
"Because the reference score is based on the bigram overlap improvement to the whole generated summary Yτ−1, the awareness of previously generated sentences also helps alleviate sentence-level redundancy.",3.6 Decoding Algorithm,[0],[0]
A factor γ is introduced to control the influence of the reference mechanism.,3.6 Decoding Algorithm,[0],[0]
"Note that because of the non-optimal search, the generated sentence will still be different to the original sentence even with an extremely large γ.
",3.6 Decoding Algorithm,[0],[0]
"At the sentence level, N -best sentence beam is to keep the N generated sentences by referring to N different original sentences, which have the highest attention scores and have not been used as a reference.",3.6 Decoding Algorithm,[0],[0]
"With referring to N different sentences, the N candidate sentences are guaranteed diverse.",3.6 Decoding Algorithm,[0],[0]
Sentence-level beam search is realized by maximizing the accumulated score of all the sentences generated.,3.6 Decoding Algorithm,[0],[0]
"We conduct experiments on two large-scale corpora of CNN and DailyMail, which have been widely used in neural document summarization tasks.",4.1 Dataset,[0],[0]
"The corpora are originally constructed in (Hermann et al., 2015) by collecting human generated abstractive highlights from the news stories in the CNN and DailyMail website.",4.1 Dataset,[0],[0]
The statistics and split of the two datasets are listed in Table 1.,4.1 Dataset,[0],[0]
"We use the corpora which are already provided with labeled entities (Nallapati et al., 2016).",4.2 Implementation,[0],[0]
"The documents and summaries are first lowercased and tokenized, and all digit characters are replaced with the “#” symbol, similar to (Nallapati et al., 2016, 2017).",4.2 Implementation,[0],[0]
"We keep the 40,000 most frequently occurring words and other words are replaced with the “<OOV>” token.
",4.2 Implementation,[0],[0]
We use Theano2 for implementation.,4.2 Implementation,[0],[0]
"For the word encoder and decoder we use three layers of LSTM, and for the sentence encoder and decoder we use one layer of LSTM.",4.2 Implementation,[0],[0]
The dimension of hidden vectors are all 512.,4.2 Implementation,[0],[0]
"We use pre-trained GloVe (Pennington et al., 2014)",4.2 Implementation,[0],[0]
"vectors3 for the initialization of word vectors, which will be further trained in the model.",4.2 Implementation,[0],[0]
The dimension of word vectors is 100.,4.2 Implementation,[0],[0]
λ is set to 0.9.,4.2 Implementation,[0],[0]
"The parameters of Adamax are set to those provided in (Kingma and Ba, 2014).",4.2 Implementation,[0],[0]
"The batch size is set to 8 documents, and an epoch is set containing 10,000 randomly sampled documents.",4.2 Implementation,[0],[0]
Convergence is reached within 200 epochs on the DailyMail dataset and 120 epochs on the CNN dataset.,4.2 Implementation,[0],[0]
It takes about one day for every 30 epochs on a GTX-1080 GPU card.,4.2 Implementation,[0],[0]
γ is tuned on the validation set and the best choice is 300.,4.2 Implementation,[0],[0]
"The beam sizes for word decoder and sentence decoder are 15 and 2, respectively.",4.2 Implementation,[0],[0]
"We adopt the widely used ROUGE (Lin, 2004) toolkit for evaluation.",4.3 Evaluation,[0],[0]
"We first compare with the reported results in (Chen et al., 2016) including various traditional extractive methods and a state-of-the-art abstractive model (DistractionM3) on the CNN dataset, as shown in Table 2.",4.3 Evaluation,[0],[0]
Uni-GRU is a non-hierarchical seq2seq baseline model.,4.3 Evaluation,[0],[0]
In Table 3 we compare our method with the results of state-of-the-art neural summarization methods reported in recent papers.,4.3 Evaluation,[0],[0]
"Extractive models include NN-SE (Cheng and Lapata, 2016) and SummaRuNNer (Nallapati et al., 2017), while SummaRuNNer-abs is also an extractive model similar to SummaRuNNer but is trained directly on the abstractive summaries.",4.3 Evaluation,[0],[0]
"Moreover, we include several baselines for comparison, including the baselines reported in (Cheng and Lapata, 2016) although they are tested on 500 samples of the test set.",4.3 Evaluation,[0],[0]
LREG is a feature based method using linear regression.,4.3 Evaluation,[0],[0]
"NN-ABS is a neural abstractive baseline which is a simple hierarchical extension of (Rush et al., 2015).",4.3 Evaluation,[0],[0]
NN-WE is the abstractive model which restricts the generation of words from the original document.,4.3 Evaluation,[0],[0]
"Lead-3 is a strong extractive baseline that uses the lead three sentences as the summary.
",4.3 Evaluation,[0],[0]
"In Table 4 we compare our model with the abstractive attentional encoder-decoder models in
2https://github.com/Theano/Theano 3http://nlp.stanford.edu/projects/glove
(Nallapati et al., 2016), which leverage several effective techniques and achieve state-of-the-art performance on sentence abstractive summarization tasks.",4.3 Evaluation,[0],[0]
"The words-lvt2k and words-lvt2k-ptr are flat models and words-lvt2k-hieratt is a hierarchical extension.
",4.3 Evaluation,[0],[0]
Results in Table 2 show our abstractive method is able to outperform traditional extractive methods and the distraction-based abstractive model.,4.3 Evaluation,[0],[0]
"The results in Tables 3 and 4 show that our method has considerable improvement over neural abstractive baselines, and is able to outperform stateof-the-art neural extractive methods.",4.3 Evaluation,[0],[0]
"An interesting observation is the results of the hierarchical model in Table 4 are lower than the flat models, which may demonstrate the difficulty for a traditional attention model to identify the important information in a document.
",4.3 Evaluation,[0],[0]
"We also conducted human evaluation on 20 random samples from the DailyMail test set and compared the summaries generated by our method with the outputs of Lead-3, NN-SE (Cheng and
Lapata, 2016) and Distraction (Chen et al., 2016).",4.3 Evaluation,[0],[0]
"The output summaries of NN-SE are provided by the authors, and the output summaries of Distraction are achieved by running the code provided by the authors on the DailyMail dataset.",4.3 Evaluation,[0],[0]
"Three participants were asked to compare the generated summaries with the human summaries, and assess each summary from four independent perspectives: (1) How informative the summary is?",4.3 Evaluation,[0],[0]
(2) How concise the summary is?,4.3 Evaluation,[0],[0]
(3) How coherent (between sentences) the summary is?,4.3 Evaluation,[0],[0]
"(4) How fluent, grammatical the sentences of a summary are?",4.3 Evaluation,[0],[0]
Each property is assessed with a score from 1 (worst) to 5 (best).,4.3 Evaluation,[0],[0]
"The average results are presented in Table 5.
",4.3 Evaluation,[0],[0]
"As shown in Table 5, our method consistently outperforms the previous state-of-the-art abstractive method Distraction.",4.3 Evaluation,[0],[0]
"Compared with extractive methods, our method is able to generate more informative and concise summaries, which shows the advantage of abstractive methods.",4.3 Evaluation,[0],[0]
"The Distraction method in fact usually produces the shortest summaries, but the conciseness score is low mainly because sometimes it generates repeated sentences.",4.3 Evaluation,[0],[0]
The repetition also causes Distraction to achieve a low coherence score.,4.3 Evaluation,[0],[0]
"Concerning coherence and fluency, our abstractive method achieves slightly better scores than NN-SE, while not surprisingly Lead-3 gets the best scores.",4.3 Evaluation,[0],[0]
The fluency scores show the good ability of the abstractive model to generate fluent and grammatical sentences.,4.3 Evaluation,[0],[0]
We conduct experiments to see how the model’s performance is affected by the choice of the hyperparameters.,4.4 Model Validation,[0],[0]
For efficiency we test on 500 random samples from the DailyMail test set.,4.4 Model Validation,[0],[0]
Figure 2a shows the maximum average Rouge-2 F1-score achieved when the model is trained using different λ values within 200 and 300 epochs.,4.4 Model Validation,[0],[0]
"When using a larger λ, the performance is better and the convergence is faster.",4.4 Model Validation,[0],[0]
"When λ = 1.0 the model fails to train because of running into a singular matrix.
",4.4 Model Validation,[0],[0]
Figure 2b shows the results achieved when using different γ values in the hierarchical decoding algorithm.,4.4 Model Validation,[0],[0]
γ = 0 is the baseline of the traditional decoding algorithm which does not refer to the original document.,4.4 Model Validation,[0],[0]
"The poor results indicate that even the model is able to learn to identify the salient information in the original document, the performance is limited by the model’s ability of generating a long output sequence.",4.4 Model Validation,[0],[0]
That may be a reason why simple extensions of seq2seq models fail on the abstractive document summarization task.,4.4 Model Validation,[0],[0]
"The performance is significantly improved using a reasonable γ, and the optimal γ value is consistent with the one chosen on the validation set.",4.4 Model Validation,[0],[0]
"When using an extremely large γ, the permanence begins to decrease, because the model will copy too much from the original document, and at this time the generated text also becomes less fluent.",4.4 Model Validation,[0],[0]
Results show that introducing the reference mechanism in the hierarchical beam search is very effective.,4.4 Model Validation,[0],[0]
"The γ factor significantly affects the results, but the optimal value is easy to be decided on a validation set.
",4.4 Model Validation,[0],[0]
We also conduct ablation experiments on the CNN dataset to verify the effectiveness of the proposed model.,4.4 Model Validation,[0],[0]
Results on the CNN test set are shown in Table 6.,4.4 Model Validation,[0],[0]
"“w/o GraphAtt” is to replace
the graph-based attention by a traditional attention function.",4.4 Model Validation,[0],[0]
“w/o SentenceBeam” is to remove the sentence-level beam search.,4.4 Model Validation,[0],[0]
"“w/o BeamSearch” is to remove both the sentence-level and word-level beam search, and use a greedy decoding algorithm with the reference mechanism.",4.4 Model Validation,[0],[0]
"As seen from Table 6, the graph-based attention mechanism is significantly better than traditional attention mechanism for the document summarization task.",4.4 Model Validation,[0],[0]
Beam search helps significantly improve the generated summaries.,4.4 Model Validation,[0],[0]
"Our proposed decoding algorithm enables a sentence-level beam search, which helps improve the generated summaries with multiple sentences.",4.4 Model Validation,[0],[0]
We show the case study of a sample4 from the DailyMail test set in Figure 3.,4.5 Case Study,[0],[0]
We show the “@entity” and number here although they are removed in the evaluation.,4.5 Case Study,[0],[0]
We compare our result with the output by a model using traditional attention as Baseline Attention.,4.5 Case Study,[0],[0]
"We also show the output generated by a Baseline Decoder, which sets γ = 0 and does not use the sentence-level beam search, to study the difficulty for a traditional decoder to generate multiple sentences.",4.5 Case Study,[0],[0]
Many observations can be found in Figure 3.,4.5 Case Study,[0],[0]
The lead three sentences mainly focus on the money information and are not sufficient.,4.5 Case Study,[0],[0]
"As for the Baseline Decoder, first it usually ends the generation too early.",4.5 Case Study,[0],[0]
The “<eod>” token indicates where the original output stops.,4.5 Case Study,[0],[0]
"When we force the decoder not to end here, the model shows the ability to continue producing the important information.",4.5 Case Study,[0],[0]
"However, two flaws are presented.",4.5 Case Study,[0],[0]
"First is the repetition of “## - year -
4The original story and highlights can be found at http://www.dailymail.co.uk/news/article-3041766/Benefitscheat-pocketed-17-000-taxpayers-money.html
old”.",4.5 Case Study,[0],[0]
"Because the word decoder is unaware of the history generated sentences, it repeats generating the sequence as the subject all the time.",4.5 Case Study,[0],[0]
"Second, more importantly, is the information incorrectness.",4.5 Case Study,[0],[0]
"The “## - month - old” is not appropriate to describe the heroine, and the “six - month prison sentence” is in fact “three months”.",4.5 Case Study,[0],[0]
"Information incorrectness occurs because, for a decoder, it aims at generating a fluent sentence according to the input representation.",4.5 Case Study,[0],[0]
"However, no favor of consistent with the original input is concerned.",4.5 Case Study,[0],[0]
The proposed hierarchical decoding algorithm helps to alleviate the two problems.,4.5 Case Study,[0],[0]
The awareness of all the generated sentences helps prevent from always generating some important information.,4.5 Case Study,[0],[0]
The favor of bigram overlapping with the original sentences helps generate more correct sentences.,4.5 Case Study,[0],[0]
For example the model is able to correctly distinguish between the “three-month sentence” and the “##- month suspend”.,4.5 Case Study,[0],[0]
"In conclusion, our method is able to identify the most important information in the original document, and the decoding algorithm we propose is able to generate a more discourse-fluent and information-correct abstractive summary.
",4.5 Case Study,[0],[0]
"The visualization of the graph-based attention when our method generates the presented example
is shown in Figure 4.",4.5 Case Study,[0],[0]
"It seems that the graph-based attention mechanism is able to find the important sentences in the input document, and the distraction mechanism makes the decoder focus on different sentences during decoding.",4.5 Case Study,[0],[0]
Gradually the decoder attends to “<eod>” until it stops.,4.5 Case Study,[0],[0]
"In this paper we tackle the challenging task of abstractive document summarization, which is still less investigated to date.",5 Conclusion and Future Work,[0],[0]
"We study the difficulty of the abstractive document summarization task, and address the need of finding salient content from the original document, which is overlooked by previous studies.",5 Conclusion and Future Work,[0],[0]
"We propose a novel graph-based attention mechanism in a hierarchical encoderdecoder framework, and propose a hierarchical beam search algorithm to generate multi-sentence summary.",5 Conclusion and Future Work,[0],[0]
Extensive experiments verify the effectiveness of the proposed method.,5 Conclusion and Future Work,[0],[0]
Experimental results on two large-scale datasets demonstrate our method achieves state-of-the-art abstractive document summarization performance.,5 Conclusion and Future Work,[0],[0]
"It is also able to achieve competitive results with state-of-the-art neural extractive summarization models.
",5 Conclusion and Future Work,[0],[0]
There is lots of future work we can do.,5 Conclusion and Future Work,[0],[0]
"An appealing direction is to investigate the neural abstractive method on the multi-document summarization task, which is more challenging and lacks training data.",5 Conclusion and Future Work,[0],[0]
Further endeavor may be needed.,5 Conclusion and Future Work,[0],[0]
"This work was supported by 863 Program of China (2015AA015403), NSFC (61331011), and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology).",Acknowledgments,[0],[0]
"We thank the anonymous reviewers for helpful comments and Xinjie Zhou, Jianmin Zhang for doing human evaluation.",Acknowledgments,[0],[0]
Xiaojun Wan is the corresponding author.,Acknowledgments,[0],[0]
"Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques.",abstractText,[0],[0]
Recently impressive progress has been made to abstractive sentence summarization using neural models.,abstractText,[0],[0]
"Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets.",abstractText,[0],[0]
"In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework.",abstractText,[0],[0]
"The intuition is to address the saliency factor of summarization, which has been overlooked by prior works.",abstractText,[0],[0]
Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models.,abstractText,[0],[0]
"The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.ive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques.",abstractText,[0],[0]
Recently impressive progress has been made to abstractive sentence summarization using neural models.,abstractText,[0],[0]
"Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets.",abstractText,[0],[0]
"In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework.",abstractText,[0],[0]
"The intuition is to address the saliency factor of summarization, which has been overlooked by prior works.",abstractText,[0],[0]
Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models.,abstractText,[0],[0]
The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.,abstractText,[0],[0]
Abstractive Document Summarization with a Graph-Based Attentional Neural Model,title,[0],[0]
"Our accelerated algorithm is achieved by designing a random walk that has a faster mixing time than the random walks associated with previous algorithms. In addition to a faster algorithm, our results yield improved sample complexity bounds for recovery of the MNL/BTL parameters: to the best of our knowledge, we give the first general sample complexity bounds for recovering the parameters of the MNL model from multiway comparisons under any (connected) comparison graph (and improve significantly over previous bounds for the BTL model for pairwise comparisons). We also give a message-passing interpretation of our algorithm, which suggests a decentralized distributed implementation. Our experiments on several real world and synthetic datasets confirm that our new ASR algorithm is indeed orders of magnitude faster than existing algorithms.
1Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USA. Correspondence to: Arpit Agarwal <aarpit@seas.upenn.edu>, Shivani Agarwal <ashivani@seas.upenn.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",text,[0],[0]
"The problem of rank aggregation from pairwise or multiway comparisons is a fundamental one in machine learning with applications in recommendation systems, sports, social choice etc.",1. Introduction,[0],[0]
"In this problem, given pairwise or multiway comparisons among n items, the goal is to learn a score for each item.",1. Introduction,[0],[0]
These scores can further be used to produce a ranking over these items.,1. Introduction,[0],[0]
"For example, in recommendation systems, the goal might be to learn a ranking over items by observing the choices that users make when presented with different subsets of these items; in sports, the goal might be to rank teams/individuals at the end of a tournament based on pairwise or multiway games between these individuals/teams; in social choice, the goal might be to aggregate the choices of individuals when presented with different alternatives such as candidates in an election.
",1. Introduction,[0],[0]
"In the case of pairwise comparisons, a popular model is the Bradley-Terry-Luce (BTL) model (Bradley & Terry, 1952; Luce, 1959) which posits that given a set of n items, there is a positive weight wi associated with each item i, and the probability that i is preferred over j in a pairwise comparison between i and j is wiwi+wj .",1. Introduction,[0],[0]
"The BTL model is a special case of the multinomial logit (MNL)/Plackett-Luce model (Plackett, 1975; McFadden, 1974) which is defined for more general multiway comparisons.",1. Introduction,[0],[0]
"Under the MNL model, the probability that an item i is preferred amongst all items in a set S is wi∑
j∈S wj .
",1. Introduction,[0],[0]
"Rank aggregation under pairwise comparisons has been an active area of research, and several algorithms have been proposed that are consistent under the BTL model (Negahban et al., 2017; Rajkumar & Agarwal, 2014; Hunter, 2004; Chen & Suh, 2015; Jang et al., 2016; Guiver & Snelson, 2009; Soufiani et al., 2013).",1. Introduction,[0],[0]
"The case of multiway comparisons has also received some attention recently (Maystre & Grossglauser, 2015; Jang et al., 2017; Chen et al., 2017).",1. Introduction,[0],[0]
"Two popular algorithms are the rank centrality (RC) algorithm (Negahban et al., 2017) for the case of pairwise comparisons, and its generalization to the case of multiway comparisons, called the Luce spectral ranking (LSR) algorithm (Maystre & Grossglauser, 2015).",1. Introduction,[0],[0]
"The key idea behind these algorithms is to construct a random walk (equivalently a Markov chain) over the comparison graph on n items, where there is an edge between two items if they are com-
pared in a pairwise or multiway comparison.",1. Introduction,[0],[0]
"This random walk is constructed such that its stationary distribution corresponds to the weights of the MNL/BTL model.
",1. Introduction,[0],[0]
"Given the widespread application of these algorithms, understanding their computational aspects is of paramount importance.",1. Introduction,[0],[0]
For random walk based algorithms this amounts to analyzing the mixing/convergence time of their random walks to stationarity.,1. Introduction,[0],[0]
"In the case of rank centrality and Luce spectral ranking, ensuring that the stationary distribution of the random walk corresponds to the weights of the underlying model forces their construction to have self loops with large mass.",1. Introduction,[0],[0]
"These self loops can lead to a large mixing time of Ω ( ξ−1dmax ) , where dmax is the maximum number of unique comparisons that any item participates in; and ξ is the spectral gap of the graph Laplacian.",1. Introduction,[0],[0]
"In practical settings dmax can be very large, for example when the graph follows a power-law distribution, and can even be Ω(n) if one item is compared to a large fraction of the items.
",1. Introduction,[0],[0]
In this paper we show that it is possible to construct a faster mixing random walk whose mixing time is O ( ξ−1 ) .,1. Introduction,[0],[0]
"We are able to construct this random walk by relaxing the condition that its stationary distribution should exactly correspond to the weights of the MNL model, and instead imposing a weaker condition that the weights can be recovered through a linear transform of the stationary distribution.",1. Introduction,[0],[0]
"We call the resulting algorithm accelerated spectral ranking (ASR).
",1. Introduction,[0],[0]
"In addition to computational advantages, the faster mixing property of our random walk also comes with statistical advantages, as it is well understood that faster mixing Markov chains lend themselves to tighter perturbation error bounds (Mitrophanov, 2005).",1. Introduction,[0],[0]
"We are able to establish a sample complexity bound of O ( ξ−2 n poly(log n) ) , in terms of the total variation distance, for recovering the true weights under the MNL (and BTL) model for almost any comparison graph of practical interest.",1. Introduction,[0],[0]
"To our knowledge, these are the first sample complexity bounds for the general case of multiway comparisons under the MNL model.",1. Introduction,[0],[0]
Negahban et al. (2017) show similar results in terms of L2 error for the special case of BTL model.,1. Introduction,[0],[0]
"However, their bounds have an additional dependence on dmax, due to the large mixing time of their random walk.
",1. Introduction,[0],[0]
We also show that our algorithm can be viewed as a message passing algorithm.,1. Introduction,[0],[0]
"This connection provides a very attractive property to our algorithm – it can be implemented in a distributed manner with decentralized communication and comparison data being stored in different machines.
",1. Introduction,[0],[0]
We finally conduct several experiments on synthetic and real world datasets to compare the convergence time of our algorithm with the previous algorithms.,1. Introduction,[0],[0]
"These experiments confirm the behavior predicted by our theoretical analysis of mixing times– the convergence of our algorithm is in fact
orders of magnitude faster than existing algorithms.",1. Introduction,[0],[0]
"We summarize our contributions as follows:
1.",1.1. Our Contributions,[0],[0]
"Faster Algorithm: We present an algorithm for aggregating pairwise comparisons under the BTL model, and more general multiway comparisons under the MNL model, that is provably faster than the previous algorithms of Negahban et al. (2017); Maystre & Grossglauser (2015).",1.1. Our Contributions,[0],[0]
"We also give experimental evidence supporting this fact.
2.",1.1. Our Contributions,[0],[0]
New and Improved Error Bounds:,1.1. Our Contributions,[0],[0]
We present the first error bounds for parameter recovery by spectral ranking algorithms under the general MNL model for any general (connected) comparison graph.,1.1. Our Contributions,[0],[0]
"These bounds improve upon the existing bounds of Negahban et al. (2017) for the special case of the BTL model.
3.",1.1. Our Contributions,[0],[0]
Message Passing Interpretation: We provide an interpretation of our algorithm as a message passing/belief propagation algorithm.,1.1. Our Contributions,[0],[0]
"This connection can be used to design a decentralized distributed algorithm, which can work with distributed data storage.",1.1. Our Contributions,[0],[0]
In Section 2 we describe the problem formally.,1.2. Organization,[0],[0]
In Section 3 we present our algorithm for rank aggregation under the MNL/BTL model.,1.2. Organization,[0],[0]
"In Section 4 we analyze the mixing time of our random walk, showing that our random walk converges much faster than existing approaches.",1.2. Organization,[0],[0]
In Section 5 we give bounds on sample complexity for recovery of MNL parameters with respect to the total variation distance.,1.2. Organization,[0],[0]
In Section 6 we give a message passing view of our algorithm.,1.2. Organization,[0],[0]
In Section 7 we provide experimental results on synthetic and real world datasets.,1.2. Organization,[0],[0]
"We consider a setting where there are n items, and one observes outcomes of noisy pairwise or multiway comparisons between these items.",2. Problem Setting and Preliminaries,[0],[0]
"We will assume that the outcome of these comparisons is generated according to the multinomial logit (MNL) model, which posits that each item i ∈",2. Problem Setting and Preliminaries,[0],[0]
"[n] is associated with a (unknown) weight/score wi > 0, and the probability that item i wins a comparison is proportional to its weight wi.",2. Problem Setting and Preliminaries,[0],[0]
"More formally, when there is a (multiway) comparison between items of a set S ⊆",2. Problem Setting and Preliminaries,[0],[0]
"[n], for i ∈ S, we have pi|S := Pr(i is the most preferred item in S) =",2. Problem Setting and Preliminaries,[0],[0]
"wi∑ j∈S wj .
",2. Problem Setting and Preliminaries,[0],[0]
"This model is also referred to as the Plackett-Luce model, and it reduces to the Bradley-Terry-Luce (BTL) model in the special case of pairwise comparisons, i.e. |S| = 2.",2. Problem Setting and Preliminaries,[0],[0]
"Let
w ∈",2. Problem Setting and Preliminaries,[0],[0]
"Rn+ be the vector of weights, i.e. w = (w1, · · · , wn)>.",2. Problem Setting and Preliminaries,[0],[0]
"Note that this model is invariant to any scaling of w, so for uniqueness we will assume that ∑n i=1",2. Problem Setting and Preliminaries,[0],[0]
"wi = 1, i.e. w ∈ ∆n where ∆n is the n-dimensional probability simplex.
",2. Problem Setting and Preliminaries,[0],[0]
"The comparison data is of the following form: there are d different comparison sets S1, · · · , Sd ⊆",2. Problem Setting and Preliminaries,[0],[0]
"[n], with |Sa| = m for all a ∈",2. Problem Setting and Preliminaries,[0],[0]
[d] and some constant m,2. Problem Setting and Preliminaries,[0],[0]
< n.,2. Problem Setting and Preliminaries,[0],[0]
"For each set Sa, for a ∈",2. Problem Setting and Preliminaries,[0],[0]
"[d], one observes the outcomes of L independent m-way comparisons between items in Sa, drawn according to the MNL model.",2. Problem Setting and Preliminaries,[0],[0]
"The assumptions that each comparison set is of the same size m, and each set is compared an equal L number of times, are only for simplicity of exposition, and we give a generalization in the supplementary material.",2. Problem Setting and Preliminaries,[0],[0]
"We will denote by yla the winner of the l-th comparison amongst items of Sa, for l ∈",2. Problem Setting and Preliminaries,[0],[0]
[L] and a ∈,2. Problem Setting and Preliminaries,[0],[0]
"[d].
",2. Problem Setting and Preliminaries,[0],[0]
"Given comparison data Y = {(Sa,ya)}da=1, where ya = (y1a, · · · , yLa ), the problem is to find a weight vector ŵ ∈ ∆n, which is close to the true weight vector w under some notion of error/distance.",2. Problem Setting and Preliminaries,[0],[0]
"More formally, the problem is to find ŵ ∈ ∆n, such that ‖ŵ −w‖ can be bounded in terms of the parameters n,L, and m, for some norm ‖ · ‖.",2. Problem Setting and Preliminaries,[0],[0]
"We will give results in terms of the total variation distance, which for two vectors u, û ∈ ∆n is defined as
‖u− û‖TV = 1
2 ‖u− û‖1",2. Problem Setting and Preliminaries,[0],[0]
"=
1
2 ∑ i∈[n] |ui",2. Problem Setting and Preliminaries,[0],[0]
"− ûi| .
",2. Problem Setting and Preliminaries,[0],[0]
"In the following sections, we will present an algorithm for recovering an estimate ŵ of w, and give bounds on the error ‖ŵ−w‖TV in terms of the problem parameters under natural assumptions on the comparison data.",2. Problem Setting and Preliminaries,[0],[0]
"In this section, we will describe our algorithm, which we term as accelerated spectral ranking (ASR).",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Our algorithm is based on the idea of constructing a random walk1 on the comparison graph with n vertices, which has an edge between nodes i and j if items i and j are compared in any m-way comparison.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
The key idea is to construct the random walk such that the probability of transition from node i to node j is proportional to wj .,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"If wj is larger than wi, then with other quantities being equal, one would expect the random walk to spend more time in node j than node i in its steady state distribution.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Hence, if we can calculate the stationary distribution of this random walk, it might give us a way to estimate the weight vector w. Moreover, for computational efficiency, we would also want this random walk to have a fast mixing time, i.e. it should rapidly converge to its stationary distribution.
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"The rank centrality (RC) algorithm (Negahban et al., 2017)
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"1Throughout this paper we will use the terminology Markov chain and random walk interchangeably.
for the BTL model, and its generalization the Luce spectral ranking (LSR) algorithm (Maystre & Grossglauser, 2015) for the MNL model, are based on a similar idea of constructing a random walk over the comparison graph.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"These algorithms construct a random walk whose stationary distribution, in expectation, is exactly w. However, this construction forces their Markov chain to have self loops with large mass, slowing down the convergence rate.
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
In this section we will show that it is possible to design a significantly faster mixing random walk that belongs to a different class of random walks over the comparison graph.,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"More precisely, the random walk that we construct is such that it is possible to recover the weight vector w from its stationary distribution using a fixed linear transformation, while for RC and LSR, the stationary distribution is exactly",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"w. Our theoretical analysis in Section 5 as well as experiments on synthetic and real world datasets in Section 7 will show that this difference can lead to vastly improved results.
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Given comparison data Y, let us denote by Gc([n], E) the undirected graph on n vertices, with an edge (i, j) ∈ E for any i, j that are a part of an m-way comparison.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"More formally, (i, j) ∈ E if there exists an index a ∈",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"[d] such that i, j ∈ Sa.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"We will call Gc the comparison graph, and throughout this paper, we will assume that Y is such that Gc is connected.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
We will denote by di the number of unique m-way comparisons of which i ∈,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"[n] was a part, i.e. di = ∑ a∈[d]",3. Accelerated Spectral Ranking Algorithm,[0],[0]
1[i ∈ Sa].,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Let D ∈ Rn×n be a diagonal matrix, with Dii being equal to di, ∀i ∈",3. Accelerated Spectral Ranking Algorithm,[0],[0]
[n].,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Also, let dmax := maxi di and dmin := mini di.
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
Suppose for each a ∈,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"[d] and j ∈ Sa, one had access to the true probability pj|Sa of j being the most preferred item in Sa.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
Then one could define a random walk on Gc with transition probability from node i ∈,3. Accelerated Spectral Ranking Algorithm,[0],[0]
[n] to j ∈,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"[n] given by
Pij := 1
di ∑ a∈[d]:i,j∈Sa pj|Sa = 1 di ∑ a∈[d]:i,j∈Sa wj∑ j′∈Sa wj′ .
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"(1)
Let P :=",3. Accelerated Spectral Ranking Algorithm,[0],[0]
[Pij ].,3. Accelerated Spectral Ranking Algorithm,[0],[0]
One can verify that P corresponds to a valid transition probability matrix as it is non-negative and row stochastic.,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Furthermore, P defines a reversible Markov chain as it satisfies the detailed balance equations
wi di Pij = wj dj Pji ,
for all i, j ∈",3. Accelerated Spectral Ranking Algorithm,[0],[0]
[n].,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"If the graph Gc is connected then π = Dw/‖Dw‖1 is the unique stationary distribution of P, and one can recover the true weight vector w from this stationary distribution using a linear transform D−1.
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"In practice one does not have access to P, so we propose an empirical estimate of P that can be computed from the given comparison data.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Formally, define p̂i|Sa to be the fraction of times that i won a m-way comparison amongst items in the
Algorithm 1 ASR Input Markov chain P̂ according to Eq.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
(2) Initialize π̂ =,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"( 1n , · · · , 1 n )",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"> ∈ ∆n
while estimates do not converge do π̂",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"← P̂>π̂ end while Output ŵ = D
−1π̂ ‖D−1π̂‖1
set Sa, i.e. p̂i|Sa :",3. Accelerated Spectral Ranking Algorithm,[0],[0]
= 1 L ∑L l=1 1[y l a = i].,3. Accelerated Spectral Ranking Algorithm,[0],[0]
Let us then define a random walk where the probability of transition from node,3. Accelerated Spectral Ranking Algorithm,[0],[0]
i ∈,3. Accelerated Spectral Ranking Algorithm,[0],[0]
[n] to node j ∈,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"[n] is given by
P̂ij := 1
di ∑ a∈[d]:i,j∈Sa p̂j|Sa .",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"(2)
Let P̂ :=",3. Accelerated Spectral Ranking Algorithm,[0],[0]
[P̂ij ].,3. Accelerated Spectral Ranking Algorithm,[0],[0]
One can again verify that P̂ corresponds to a valid transition probability matrix.,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"We can think of P̂ as a perturbation of P, with the error due to perturbation decreasing with more and more comparisons.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"There is a rich literature (Cho & Meyer, 2001; Mitrophanov, 2005) on analyzing sensitivity of the stationary distribution of a Markov chain under small perturbations.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"Hence, given a large number of comparisons, one can expect the stationary distribution of P̂ to be close to that of P. Since we take a linear transform of these stationary distributions, one also needs to show that closeness is preserved under this linear transform.",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"We defer this analysis to Section 5.
",3. Accelerated Spectral Ranking Algorithm,[0],[0]
The pseudo-code for our algorithm is given in Algorithm 1.,3. Accelerated Spectral Ranking Algorithm,[0],[0]
"The algorithm computes the stationary distribution π̂ of the Markov chain P̂ using the power method.2 It then outputs the (normalized) vector ŵ that is obtained after applying the linear transform D−1 to π̂, i.e. ŵ = D
−1π̂ ‖D−1π̂‖1 .",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"In the
next section we will compare the convergence time of our algorithm with previous algorithms (Negahban et al., 2017; Maystre & Grossglauser, 2015).",3. Accelerated Spectral Ranking Algorithm,[0],[0]
"The random walk PRC constructed by the RC (Negahban et al., 2017) algorithm for the BTL model is given by
PRCij :=
{ 1
dmax ∑ a∈[d]:i,j∈Sa pj|Sa",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"if i 6= j
1− 1dmax ∑ j′ 6=i P RC ij′ if i = j
, (3)
2The stationary distribution of the Markov chain may also be computed using other linear algebraic techniques, but these techniques typically have a running time ofO(n3) which is impractical for most modern applications.
and the random walk PLSR constructed by LSR (Maystre & Grossglauser, 2015) for the MNL model is given by
P LSRij",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
":=
{ ∑
a∈[d]:i,j∈Sa pj|Sa if i",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
6=,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"j 1− ∑ j′ 6=i P LSR ij′ if i = j
, (4)
where > 0 is chosen such that the diagonal entries are non-negative.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
In general would be O( 1dmax ).,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"The random walks P̂RC and P̂LSR constructed from the comparison data are defined analogously using empirical probabilities p̂j|Sa instead of pj|Sa .
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"We first begin by showing that for any given comparison data Y, both RC/LSR and our algorithm will return the same estimate upon convergence.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Proposition 1.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Given items [n] and comparison data Y = {(Sa,ya)}da=1, let π̂ be the stationary distribution of the Markov chain P̂ constructed by ASR, and let ŵLSR be the stationary distribution of the Markov chain P̂LSR.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Then ŵLSR = D
−1π̂ ‖D−1π̂‖1 .",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"The same result is also true for ŵ RC for the case of pairwise comparisons.
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
We give a proof of this result in the supplementary material.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Although the above lemma shows that in a convergent state both these algorithms will return the same estimates, it does not say anything about the time it takes to reach this convergent state.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"This is where the key difference lies.
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Observe that each row,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
i ∈,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"[n] of our matrix P is divided by di, whereas each row of PRC is divided by dmax except the diagonal entries.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Now if dmax is very large, a row i ∈",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
[n] of PRC that corresponds to an item i with small di would have very small non-diagonal entries.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
This can make the diagonal entry PRCii,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"very large, which amounts to having a heavy self loop at node i.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"This heavy self loop can significantly reduce the time it takes for the random walk to reach its stationary distribution, since a lot of transitions starting from i will return back to i. The same analysis holds true for LSR under multiway comparisons.
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"To formalize this intuition, we need to analyze the spectral gap of a random walk X, which we denote by µ(X), which plays an important role in determining its mixing time.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
The spectral gap of a reversible random walk (or Markov chain) X is defined as µ(X) :,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"= 1 − λ2(X), where λ2(X) is the second largest eigenvalue of X in terms of absolute value.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
The following lemma (see Levin et al. (2008) for more details) gives both upper and lower bounds on the mixing time (w.r.t.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
the total variation distance) of a random walk in terms of the spectral gap.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Lemma 1.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"(Levin et al., 2008) Let X be the transition probability matrix of a reversible, irreducible Markov chain with state space [n], π be the stationary distribution of X, and πmin := mini∈[n] πi, and let
d(r) = sup p∈∆n
‖pXr",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"− π‖TV .
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"For any γ > 0, let t(γ) = min{r ∈ N : d(r) ≤ γ}; then
log( 1 2γ ) ( 1 µ(X)",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"− 1 ) ≤ t(γ) ≤ log( 1 γπmin ) 1 µ(X) .
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
The above lemma states that the mixing time of a Markov chain X is inversely proportional to its spectral gap µ(X).,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Now, we will compare the spectral gap of our Markov chain P with the spectral gap of PRC (and PLSR).
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Proposition 2.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Let the probability transition matrix P for our random walk be as defined in Eq.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
(1).,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Let PRC and PLSR be as defined in Eq.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"(3) and Eq. (4), respectively.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Then
dmin dmax µ(P) ≤ µ(PRC) ≤ µ(P) , (5)
and dminµ(P) ≤ µ(PLSR) ≤ µ(P) , (6)
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"where = O( 1dmax ).
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"A formal proof of this lemma is given in the supplementary material, and uses comparison theorems for reversible Markov chains due to Diaconis & Saloff-Coste (1993).",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"This lemma shows that the spectral gap of P is always lower bounded by that of PRC (and PLSR), but can be much larger than it.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"In the latter case one would observe, using Lemma 1, that our algorithm will converge faster than the RC algorithm (and LSR).",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
In fact there are instances where O(dmax/dmin) = Ω(n) and the leftmost inequalities in both Eq.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
(5) and Eq. (6) hold with equality.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
In these instances the convergence of our algorithm will be Ω(n) times faster.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"We give examples of two such instances.
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Example 1.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Let n = 3, m = 2, w1 = 1/2, w2 = 1/4 and w3 = 1/4.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
In the comparison data 1 is compared to both 2 and 3; but items 2 and 3 are not compared to each other.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"This implies that d1 = 2, and di = 1 for i 6= 1.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"One can calculate the matrices P and PRC, and their respective eigenvalues, and observe that µ(P) = 2µ(PRC).
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
Example 2.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Let m = 2, w = (1/n, · · · , 1/n)>, and the comparison data be such that item 1 is compared to every other item, and no other items are compared to each other.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"This implies that d1 = n−1, and di = 1 for i 6= 1.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"One can calculate the matrix P and PRC again, and their respective eigenvalues, and observe that µ(P) =",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"(n− 1) · µ(PRC).
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Note that in the above lemma, we only show the relation between the spectral gaps of the matrices P and PRC, and not for any particular realization P̂ and P̂RC.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"If the Markov chains P̂ and P̂RC are reversible, then identical results hold.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"However, similar results are very hard to prove for nonreversible Markov chains (Dyer et al., 2006).",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Nevertheless, for large L, one can expect the realized matrices P̂ and P̂RC to be close to their expected matrices P and PRC, respectively.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"Hence, using eigenvalue perturbation bounds (Horn
& Johnson, 1990), one can show that the spectrum of P̂ and P̂RC is close to the spectrum of P and PRC, respectively.",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
The same analysis holds true for LSR under multiway comparisons.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"In Section 7 we perform experiments on synthetic and real world datasets which empirically show that the mixing times of the realized Markov chains behave as predicted.
",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
"It has been observed that faster mixing rates of Markov chains gives us the ability to prove sharper perturbation bounds for these Markov chains (Mitrophanov, 2005).",4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
In the following section we will use these perturbation bounds to prove sharper sample complexity bounds for our algorithm.,4. Comparison of Mixing Time with Rank Centrality (RC) and Luce Spectral Ranking (LSR),[0],[0]
In this section we will present sample complexity bounds for the estimates returned by ASR in terms of total variation distance.,5. Sample Complexity Bounds,[0],[0]
"The following theorem gives an error bound in terms of the total variation distance for estimates ŵ of the MNL weights returned by our algorithm
Theorem 1.",5. Sample Complexity Bounds,[0],[0]
"Given items [n] and comparison data Y = {(Sa,ya)}da=1, let each set Sa of cardinality m be compared L times, with outcomes ya = (y1a, · · · , yLa ) produced as per a MNL model with parameters w =",5. Sample Complexity Bounds,[0],[0]
"(w1, . . .",5. Sample Complexity Bounds,[0],[0]
", wn), such that ‖w‖1 = 1.",5. Sample Complexity Bounds,[0],[0]
"If the random walk P̂ (Eq. (2)) on the comparison graph Gc([n], E) induced by the comparison data Y is strongly connected, then the ASR algorithm (Algorithm 1) converges to a unique distribution ŵ, which with probability ≥ 1 − 3n−(C2−50)/25 satisfies the following error bound3
‖w",5. Sample Complexity Bounds,[0],[0]
"− ŵ‖TV ≤ C κdavg µ(P) dmin
√ max{m, log(n)}
L ,
where κ = log (
davg dminwmin ) , wmin = mini∈[n] wi, davg =∑
i∈[n] widi, dmin = mini∈[n] di, µ(P) is the spectral gap of the random walk P (Eq. (1)), and C is any constant.
",5. Sample Complexity Bounds,[0],[0]
"Recall from Section 3 that the Markov chain P̂ can be viewed as a perturbation of P. To show that the stationary distributions of P̂ and P are close, we use the results of Mitrophanov (2005) on the stability of Markov chains under perturbations.",5. Sample Complexity Bounds,[0],[0]
"We also show closeness is preserved under the linear transformation D−1, giving the final bound stated in the aforementioned theorem.",5. Sample Complexity Bounds,[0],[0]
"We present a formal proof in the supplementary material.
",5. Sample Complexity Bounds,[0],[0]
"In the error bound of Theorem 1, one can further bound the spectral gap µ(P) of P in terms of the spectral gap of the random walk normalized Laplacian of Gc, which is a
3The dependence on κ is due to the dependence on 1 πmin in the mixing time upper bounds in Lemma 1.",5. Sample Complexity Bounds,[0],[0]
"There are other bounds for κ in terms of the condition number for Markov chains, for example see (Mitrophanov, 2005), and any improvement on these bounds will lead to an improvement in our sample complexity.",5. Sample Complexity Bounds,[0],[0]
"In the worst case, κ has a trivial upper bound of O(logn).
fundamental quantity associated with Gc.",5. Sample Complexity Bounds,[0],[0]
The Laplacian represents a random walk onGc that transitions from a node i to one of its neighbors uniformly at random.,5. Sample Complexity Bounds,[0],[0]
"Formally, the Laplacian L := C−1A, where C is a diagonal matrix with Cii = ∣∣⋃ a∈[d]:i∈Sa Sa
∣∣, i.e. the number of unique items i was compared with, and A is the adjacency matrix, such that for i, j ∈",5. Sample Complexity Bounds,[0],[0]
"[n], Aij = 1 if (i, j) ∈ E, and Aij = 0 otherwise.",5. Sample Complexity Bounds,[0],[0]
"Let ξ := µ(L) be the spectral gap of L. Then we can lower bound µ(P) as follows (proof in the supplement)
µ(P) ≥ ξ m b2 ,
where b is the ratio of the maximum to the minimum weight, i.e. b = maxi,j∈[n] wi/wj .",5. Sample Complexity Bounds,[0],[0]
This gives us the following.,5. Sample Complexity Bounds,[0],[0]
Corollary 1.,5. Sample Complexity Bounds,[0],[0]
"In the setting of Theorem 1, the ASR algorithm converges to a unique distribution ŵ, which with probability ≥ 1 − 3n−(C2−50)/25 satisfies the following error bound:
‖w − ŵ‖TV ≤ Cmb2 κ davg
ξ dmin
√ max{m, log(n)}
L ,
where b = maxi,j∈[n] wiwj .
",5. Sample Complexity Bounds,[0],[0]
"In the discussion that follows, we will assume b = O(1), and hence, µ(P) = Ω(ξ/m).",5. Sample Complexity Bounds,[0],[0]
The quantity davg has an interesting interpretation: it is the weighted average of the number of sets in which each item was shown.,5. Sample Complexity Bounds,[0],[0]
"It has a trivial upper bound of dmax, however, a careful analysis will reveal a better bound of O(|E|/n) where E is the set of edges in the comparison graph Gc.",5. Sample Complexity Bounds,[0],[0]
Using this observation we can give the following corollary of the above theorem.,5. Sample Complexity Bounds,[0],[0]
Corollary 2.,5. Sample Complexity Bounds,[0],[0]
"If the conditions of Theorem 1 are satisfied, and if the number of edges in the comparison graph Gc are O(n poly(log n)), i.e. |E| = O(n poly(log n)), then in order to ensure a total variation error of o(1), the required number of comparisons per set is upper bounded as
L =",5. Sample Complexity Bounds,[0],[0]
O,5. Sample Complexity Bounds,[0],[0]
( µ(P)−2 poly(log n) ),5. Sample Complexity Bounds,[0],[0]
"= O ( ξ−2m3 poly(log n) ) .
",5. Sample Complexity Bounds,[0],[0]
"Hence, the sample complexity, i.e. total number of m-way comparisons needed to estimate w with error o(1), is given by |E| × L = O ( ξ−2m3 n poly(log n) ) .
",5. Sample Complexity Bounds,[0],[0]
We again provide a proof of this corollary in the appendix.,5. Sample Complexity Bounds,[0],[0]
Note that the case when the total number of edges in the comparison graph is O(n poly(log n)) captures the most interesting case in ranking and sorting.,5. Sample Complexity Bounds,[0],[0]
"Also, in most practical settings the size m of comparison sets will be O(log n).",5. Sample Complexity Bounds,[0],[0]
"In this case, the above corollary implies a sample complexity bound of O ( ξ−2 n poly(log n) ) , which is sometimes referred to as quasi-linear complexity.",5. Sample Complexity Bounds,[0],[0]
The following simple example illustrates this sample complexity bound.,5. Sample Complexity Bounds,[0],[0]
Example 3.,5. Sample Complexity Bounds,[0],[0]
"Consider a star comparison graph, discussed in Example 2, where there is one item i ∈",5. Sample Complexity Bounds,[0],[0]
[n] that is compared to all other n,5. Sample Complexity Bounds,[0],[0]
"− 1 items, and no other items are
compared to each other.",5. Sample Complexity Bounds,[0],[0]
Let w =,5. Sample Complexity Bounds,[0],[0]
"( 1n , · · · , 1 n )",5. Sample Complexity Bounds,[0],[0]
>.,5. Sample Complexity Bounds,[0],[0]
One can calculate the spectral gap µ(P) to be 0.5 exactly.,5. Sample Complexity Bounds,[0],[0]
"In this case, the sample complexity bound given by our result is O(n poly(log n)).
",5. Sample Complexity Bounds,[0],[0]
Discussion/Comparison.,5. Sample Complexity Bounds,[0],[0]
"For the special case of pairwise comparisons under the BTL model (m = 2), Negahban et al. (2017) give a sample complexity bound of O ( dmax dmin ξ−2 n poly(log n) )
for recovering the estimates ŵ with low (normalized) L2 error.",5. Sample Complexity Bounds,[0],[0]
"Using Proposition 1 one can see that this bound also applies to the estimates returned by our algorithm, and our bound in terms of L1 applies to rank centrality as well.",5. Sample Complexity Bounds,[0],[0]
"However, the bounds due to Negahban et al. (2017) have a dependence on the ratio dmaxdmin due to the large spectral gap of their Markov chain as compared to ξ, the spectral gap of the Laplacian.",5. Sample Complexity Bounds,[0],[0]
"In Section 7 we show that for many real world datasets dmaxdmin can be much larger than log n, and hence, their bounds are no longer quasi-linear.",5. Sample Complexity Bounds,[0],[0]
A large class of graphs that occur in many real world scenarios and exhibit this behavior are the power-law graphs.,5. Sample Complexity Bounds,[0],[0]
"Another real world scenario in which dmaxdmin = Ω(n) arises is choice modeling (Agrawal et al., 2016), where one explicitly models the ‘no choice option’ where the user has an option of not selecting any item from the set of items presented to her.",5. Sample Complexity Bounds,[0],[0]
"In this case the ‘no choice option’ will be present in each comparison set, and the comparison graph will behave like a star graph discussed in Example 2.",5. Sample Complexity Bounds,[0],[0]
"In fact for such graphs, the results of (Negahban et al., 2017) give a trivial bound of poly(n) in terms of the L2 error.
",5. Sample Complexity Bounds,[0],[0]
For the general case of multiway comparisons we are not aware of any other sample complexity bounds.,5. Sample Complexity Bounds,[0],[0]
It is also important to note that the dependence on the number of comparison sets comes only through the spectral gap ξ of the natural random walk on the comparison graph.,5. Sample Complexity Bounds,[0],[0]
"For example, if the graph is a cycle (d = n), then the spectral gap is O(1/n2), whereas if the graph is a clique (d = O(n2))",5. Sample Complexity Bounds,[0],[0]
"the spectral gap is O(1).
6.",5. Sample Complexity Bounds,[0],[0]
"Message Passing Interpretation of ASR In this section, we show our spectral ranking algorithm can be interpreted as a message passing/belief propagation algorithm.",5. Sample Complexity Bounds,[0],[0]
"This connection can be used to design a decentralized distributed version of our algorithm.
",5. Sample Complexity Bounds,[0],[0]
"Let us introduce the factor graph, which is an important data structure used in message passing algorithms.",5. Sample Complexity Bounds,[0],[0]
"The factor graph is a bipartite graph Gf ([n] ∪ [d], Ef ) which has two type of nodes– item nodes which correspond to the n items, and set nodes which correspond to the d sets.",5. Sample Complexity Bounds,[0],[0]
"More formally, there is an item node i for each item i ∈",5. Sample Complexity Bounds,[0],[0]
"[n], and there is a set node a for each set Sa, ∀a ∈",5. Sample Complexity Bounds,[0],[0]
[d].,5. Sample Complexity Bounds,[0],[0]
"There is an edge (i, a) ∈ Ef between node i and a if and only if i ∈ Sa.",5. Sample Complexity Bounds,[0],[0]
"There is a weight p̂i|Sa on the edge (i, a) which corresponds
to the fraction of times i won in the set Sa.
",5. Sample Complexity Bounds,[0],[0]
Algorithm 2,5. Sample Complexity Bounds,[0],[0]
Message Passing Input Graph Gf = (,5. Sample Complexity Bounds,[0],[0]
"[n] ∪ [d], Ef ), edge (i, a) ∈ E has weight p̂i|Sa",5. Sample Complexity Bounds,[0],[0]
"Initialize Set m(0)a→i ← m/n, ∀a ∈",5. Sample Complexity Bounds,[0],[0]
"[d], ∀i ∈ Sa for t = 1, 2, · · · until convergence do
for all i ∈",5. Sample Complexity Bounds,[0],[0]
"[n] dom(t)i→a = 1di ∑ a′:i∈Sa′ p̂i|Sa′ ·m (t−1) a′→i
for all a ∈",5. Sample Complexity Bounds,[0],[0]
"[d] do m(t)a→i = ∑ i′∈Sa m (t) i′→a
end for Set ŵi ← m(t−1)i→a , ∀i ∈",5. Sample Complexity Bounds,[0],[0]
"[n] Output ŵ/‖ŵ‖1
We shall now describe the algorithm.",5. Sample Complexity Bounds,[0],[0]
"In each iteration of this algorithm, the item nodes send a message to their neighboring set nodes, and the set nodes respond to these messages.",5. Sample Complexity Bounds,[0],[0]
"A message from an item node i to a set node a represents an estimate of the weight wi of item i, and a message from a set node a to an item i represents an estimate of the sum of weights of items contained in set Sa.
",5. Sample Complexity Bounds,[0],[0]
"In each iteration, the item nodes update their estimates based on the messages they receive in the previous iteration, and send these estimates to their neighboring set nodes.",5. Sample Complexity Bounds,[0],[0]
"The set nodes then update their estimate by summing up the messages they receive from their neighboring item nodes, and then send these estimates to their neighboring item nodes.",5. Sample Complexity Bounds,[0],[0]
"This process continues until the messages converge.
",5. Sample Complexity Bounds,[0],[0]
"Formally, let m(t−1)i→a be the message from item node i to set node a in iteration t− 1, and m(t−1)a→i be the corresponding message from the set node",5. Sample Complexity Bounds,[0],[0]
a to item node i.,5. Sample Complexity Bounds,[0],[0]
"Then the messages in the next iteration are updated as follows:
m (t) i→a =
1
di ∑ a′∈[d]:i∈Sa′ p̂i|Sa′ ·m (t−1) a′→i ,
m (t) a→i = ∑ i′∈Sa m (t) i′→a .
",5. Sample Complexity Bounds,[0],[0]
"Now, suppose that the empirical edge weights p̂i|Sa are equal to the true weights pi|Sa",5. Sample Complexity Bounds,[0],[0]
"= wi∑ j∈Sa wj
, ∀i ∈",5. Sample Complexity Bounds,[0],[0]
"[n], a ∈",5. Sample Complexity Bounds,[0],[0]
[d].,5. Sample Complexity Bounds,[0],[0]
"Also, suppose on some iteration t ≥ 1, the item messages m(t)i→a become equal to the item weights wi, ∀i ∈",5. Sample Complexity Bounds,[0],[0]
[n].,5. Sample Complexity Bounds,[0],[0]
"Then it is easy to observe that the next iteration of messages m
(t+1) i→a are also equal to wi.",5. Sample Complexity Bounds,[0],[0]
"Therefore, the true weights w, in some sense, are a fixed point of the above set of equations.",5. Sample Complexity Bounds,[0],[0]
"The following lemma shows that the ASR algorithm is equivalent to this message passing algorithm.
",5. Sample Complexity Bounds,[0],[0]
Lemma 2.,5. Sample Complexity Bounds,[0],[0]
"For any realization of comparison data Y, there is a one-to-one correspondence d each iteration of the message passing algorithm (2) and the corresponding power iteration of the ASR algorithm (1), and both algorithms return the same estimates ŵ for any Y.
We give a proof of the above lemma in the supplementary material.",5. Sample Complexity Bounds,[0],[0]
The above lemma gives an interesting connection between spectral ranking under the MNL model and message passing/belief propagation.,5. Sample Complexity Bounds,[0],[0]
"Such connections have been observed for other problem such as the problem of aggregating crowdsourced binary tasks (Khetan & Oh, 2016).",5. Sample Complexity Bounds,[0],[0]
A consequence of this connection is that it facilitates a fully decentralized distributed implementation of the ASR algorithm.,5. Sample Complexity Bounds,[0],[0]
"This can be very useful for modern applications, where machines can communicate local parameter updates to each other, without explicitly communicating the data.",5. Sample Complexity Bounds,[0],[0]
"In this section we perform experiments on both synthetic and real data to compare our algorithm to the existing LSR (Maystre & Grossglauser, 2015) and RC (Negahban et al., 2017) algorithms for recovering the weight vector w under the MNL and BTL model, respectively.",7. Experiments,[0],[0]
The implementation4 of our algorithm is based on applying the power method on P̂ (Eq. (2)).,7. Experiments,[0],[0]
"The power method was chosen due to its simplicity, efficiency, and scalability to large problem sizes.",7. Experiments,[0],[0]
"Similarly, the implementations of LSR and RC are based on applying the power method on P̂LSR (Eq. (4)), and P̂RC (Eq. (3)), respectively.",7. Experiments,[0],[0]
"In the definition of P̂LSR, the parameter was chosen to be the maximum possible value that ensures P̂LSR is a Markov chain.",7. Experiments,[0],[0]
"We conducted experiments on synthetic data generated according to the MNL model, with weight vectors w generated randomly (details below).",7.1. Synthetic Data,[0],[0]
"We compared our algorithm with the LSR algorithm for comparison sets of size m = 5, and with the RC algorithm for sets of size m = 2.",7.1. Synthetic Data,[0],[0]
"We used two different graph topologies for generating the comparison graph Gc, or equivalently the comparison sets:
1.",7.1. Synthetic Data,[0],[0]
Random Topology:,7.1. Synthetic Data,[0],[0]
This graph topology corresponds to random graphs where n log2(n) comparison sets are chosen uniformly at random from all the ( n m ) unique sets of cardinality m. This topology is very close to the Erdős-Rényi topology which has been well-studied in the literature.,7.1. Synthetic Data,[0],[0]
"In fact the degree distributions of nodes in this random topology are very close to the degree distributions in the ErdősRényi topology (Mezard & Montanari, 2009).",7.1. Synthetic Data,[0],[0]
"The only reason we study the former is computational, as iterating over all ( n m ) hyper-edges is computationally challenging.
2.",7.1. Synthetic Data,[0],[0]
Star Topology:,7.1. Synthetic Data,[0],[0]
"In this graph topology, there is a single item that belongs to all sets; the remaining (m − 1) items in each set are contained only in that set.",7.1. Synthetic Data,[0],[0]
"We study this topology because it corresponds to the choice sets used in Example 2, where there was a factor of Ω(n) gap in the
4code available: https://github.com/agarpit/asr
2 4 6 8 10 12 14 16 18 20 0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
ASR RC
1 2 3 4 5 6 7 8 0
0.05
0.1
0.15
0.2
ASR LSR
200 400 600 800 1000 1200 1400 1600 0
0.1
0.2
0.3
0.4
0.5
ASR RC
100 200 300 400 500 600 700 800 900 0
0.1
0.2
0.3
0.4
0.5
ASR LSR
Figure 1.",7.1. Synthetic Data,[0],[0]
"Results on synthetic data: L1 error vs. number of iterations for our algorithm, ASR, compared with the RC algorithm (for m = 2) and the LSR algorithm (for m = 5), on data generated from the MNL/BTL model with the random and star graph topologies.
50 100 150 200 -9
-8.5
-8
-7.5 10
5
ASR RC
20 40 60 80 100 120 140 160 180 -4
-3.8
-3.6
-3.4
-3.2
-3
-2.8
-2.6
-2.4
-2.2 10
4
ASR RC
5 10 15 20 -5030
-5020
-5010
-5000
-4990
-4980
-4970
-4960
ASR LSR
5 10 15 20 25 30 35 40 45 50 -4400
-4350
-4300
-4250
-4200
-4150
-4100
ASR LSR
Figure 2.",7.1. Synthetic Data,[0],[0]
"Results on real data: Log-likelihood vs. number of iterations for our algorithm, ASR, compared with the RC algorithm (for pairwise comparison data) and the LSR algorithm (for multi-way comparison data), all with regularization parameter set to 0.2.
spectral gap between our algorithm and the other algorithms.
",7.1. Synthetic Data,[0],[0]
"In our experiments we selected n = 5005, and the weightwi of each item i ∈",7.1. Synthetic Data,[0],[0]
"[n] was drawn uniformly at random from the range (0, 1); the weights were then normalized so they sum to 1.",7.1. Synthetic Data,[0],[0]
A comparison graph Gc was generated according to each of the graph topologies above.,7.1. Synthetic Data,[0],[0]
The parameter L was set to 300 log2 n. The winner for each comparison set was drawn according to the MNL model with weights w.,7.1. Synthetic Data,[0],[0]
The convergence criterion for all algorithms was the same: we run the algorithm until the L1 distance between the new estimates and the old estimates is ≤ 0.0001.,7.1. Synthetic Data,[0],[0]
Each experiment was repeated 100 times and the average values over all trials are reported.,7.1. Synthetic Data,[0],[0]
"For n = 500, m ∈ {2, 5}, and both graph topologies described above, we compared the convergence as a function of the number of iterations6 for each algorithm.",7.1. Synthetic Data,[0],[0]
We plotted the L1 error of the estimates produced by these algorithms after each iteration.,7.1. Synthetic Data,[0],[0]
The plots are given in Figure 1.,7.1. Synthetic Data,[0],[0]
"These plots verify the mixing time analysis of Section 4, and show that our algorithm converges much faster than RC and LSR, and orders of magnitude faster in the case of the star graph.",7.1. Synthetic Data,[0],[0]
"We conducted experiments on the YouTube dataset (Shetty, 2012), GIF-anger dataset (Rich et al.), and the SFwork and SFshop (Koppelman & Bhat, 2006) datasets.",7.2. Real World Datasets,[0],[0]
Table 1 gives some statistics about these datasets.,7.2. Real World Datasets,[0],[0]
"We also plot the degree distributions of these datasets in the supplementary material.
5Results for other values of n are given in the supplement.",7.2. Real World Datasets,[0],[0]
"6We also plotted the convergence as a function of the running time; the results were similar as the running time of each iteration is similar for all these algorithm.
",7.2. Real World Datasets,[0],[0]
"For these datasets, a ground-truth w is either unknown or undefined; and hence, we compare our algorithm and the RC/LSR algorithm with respect to the log-likelihood of the estimates as a function of number of iterations.",7.2. Real World Datasets,[0],[0]
"Due to the number of comparisons per set (or pair) being very small, in order to ensure irreducibility of random walks, we use a regularized version of all algorithms (see supplementary material, and also Section 3.3 in Negahban et al. (2017), for more details).",7.2. Real World Datasets,[0],[0]
"Here, we give results when the regularization parameter λ is set to 0.2, and defer the results for other parameter values to the supplementary material.",7.2. Real World Datasets,[0],[0]
The results are given in Figure 2.,7.2. Real World Datasets,[0],[0]
We observe that our algorithm converges rapidly to the peak log-likelihood value while RC and LSR are always slower in converging to this value.,7.2. Real World Datasets,[0],[0]
We presented a spectral algorithm for the problem of rank aggregation from pairwise and multiway comparisons.,8. Conclusion and Future Work,[0],[0]
"Our algorithm is considerably faster than previous algorithms; in addition, our analysis yields improved sample complexity results for estimation under the BTL and MNL model.",8. Conclusion and Future Work,[0],[0]
We also give a message passing/belief propagation interpretation for our algorithm.,8. Conclusion and Future Work,[0],[0]
It would be interesting to see if one can use our algorithm to give better guarantees for recovery of top-k items under MNL.,8. Conclusion and Future Work,[0],[0]
We would like to thank Rohan Ghuge and Ashish Khetan for helpful discussions.,Acknowledgements,[0],[0]
We would also like to thank the anonymous reviewers for helpful comments.,Acknowledgements,[0],[0]
This material is based upon work supported by the US National Science Foundation under Grant No. 1717290.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgements,[0],[0]
"The problem of rank aggregation from pairwise and multiway comparisons has a wide range of implications, ranging from recommendation systems to sports rankings to social choice.",abstractText,[0],[0]
"Some of the most popular algorithms for this problem come from the class of spectral ranking algorithms; these include the rank centrality algorithm for pairwise comparisons, which returns consistent estimates under the Bradley-Terry-Luce (BTL) model for pairwise comparisons (Negahban et al., 2017), and its generalization, the Luce spectral ranking algorithm, which returns consistent estimates under the more general multinomial logit (MNL) model for multiway comparisons (Maystre & Grossglauser, 2015).",abstractText,[0],[0]
"In this paper, we design a provably faster spectral ranking algorithm, which we call accelerated spectral ranking (ASR), that is also consistent under the MNL/BTL models.",abstractText,[0],[0]
Our accelerated algorithm is achieved by designing a random walk that has a faster mixing time than the random walks associated with previous algorithms.,abstractText,[0],[0]
"In addition to a faster algorithm, our results yield improved sample complexity bounds for recovery of the MNL/BTL parameters: to the best of our knowledge, we give the first general sample complexity bounds for recovering the parameters of the MNL model from multiway comparisons under any (connected) comparison graph (and improve significantly over previous bounds for the BTL model for pairwise comparisons).",abstractText,[0],[0]
"We also give a message-passing interpretation of our algorithm, which suggests a decentralized distributed implementation.",abstractText,[0],[0]
Our experiments on several real world and synthetic datasets confirm that our new ASR algorithm is indeed orders of magnitude faster than existing algorithms.,abstractText,[0],[0]
"Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USA.",abstractText,[0],[0]
Correspondence to: Arpit Agarwal <,abstractText,[0],[0]
"aarpit@seas.upenn.edu>, Shivani Agarwal <ashivani@seas.upenn.edu>.",abstractText,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",abstractText,[0],[0]
Copyright 2018 by the author(s).,abstractText,[0],[0]
Accelerated Spectral Ranking,title,[0],[0]
The dynamics of a large number of physical phenomenon are governed by the incompressible Navier-Stokes equations.,1 INTRODUCTION,[0],[0]
"In this work, we follow the Eulerian viewpoint for simulating these equations, which approximates quantities on a regular grid (Foster & Metaxas, 1996).",1 INTRODUCTION,[0],[0]
"Euler methods are able to produce precise results simulating fluids like water or smoke, at the cost of a high computational load.
",1 INTRODUCTION,[0],[0]
"The most demanding portion of this method is solving the discrete Poisson equation, which enforces the incompressibility condition.",1 INTRODUCTION,[0],[0]
Exact solutions can be found using the Preconditioned Conjugate Gradient (PCG) algorithm or via stationary iterative methods such as the Jacobi or Gauss-Seidel methods.,1 INTRODUCTION,[0],[0]
"A number of numerical methods have been proposed to mitigate this limitation for offline applications, notably multi-grid approximations (McAdams et al., 2010).",1 INTRODUCTION,[0],[0]
"However, in real-time Jacobi iterations are truncated before reaching convergence, rendering these methods inexact and the obtained velocity fields divergent.",1 INTRODUCTION,[0],[0]
"A natural approach is to tackle the problem in a data-driven manner, adapting the solver to the specifics of the data of interest.",1 INTRODUCTION,[0],[0]
"For instance, by operating on a representation of the simulation space of significantly lower dimensionality (Treuille et al., 2006; De Witt et al., 2012).",1 INTRODUCTION,[0],[0]
"More recently, approaches have been proposed which train black-box machine learning systems to predict the output produced by an exact solver, e.g. using random regression forests (Ladický et al., 2015) or neural networks (Yang et al., 2016) for Lagrangian and Eulerian methods respectively.",1 INTRODUCTION,[0],[0]
A major limitation of these methods is that they require a dataset of linear system solutions provided by an exact solver.,1 INTRODUCTION,[0],[0]
"Hence, targets cannot be computed during training and models are trained to predict the ground-truth output always starting from an initial frame produced by an exact solver, while at test time this initial frame is actually generated by the model itself.",1 INTRODUCTION,[0],[0]
This discrepancy between training and simulation can yield errors that can accumulate quickly along the generated sequence.,1 INTRODUCTION,[0],[0]
"Additionally, the ConvNet architecture proposed by Yang et al. is not suited to our more general use-case; in particular it cannot accurately simulate long-range phenomena, such as gravity or buoyancy.",1 INTRODUCTION,[0],[0]
"While providing encouraging results that offer a significant speedup over their PCG baseline, their work is limited to data closely matching the training conditions (as we will discuss in Section 3).
",1 INTRODUCTION,[0],[0]
"The contributions of this work are as follows: (i) the learning task can be phrased as a completely unsupervised learning problem; since obtaining ground-truth data is no longer necessary, we can incorporate loss information from a composition of multiple time-steps and perform various forms of non-trivial data-augmentation.",1 INTRODUCTION,[0],[0]
"(ii) we propose a collection of domain-specific ConvNet architectural optimizations motivated by the linear system structure itself, which lead to both qualitative and
0 10 20 30 40 50 60 0
5
10
15
20
Timestep
E (| |∇
· û || )
Jacobi 34 iterations this work: small model this work: multi-frame this work: single frame
Figure 3: Test-set E (‖∇ · ûi‖) versus time-step
quantitative improvements.",1 INTRODUCTION,[0],[0]
"(iii) the proposed simulator is stable and permits real-time simulation, showing good generalization properties to unseen settings.
",1 INTRODUCTION,[0],[0]
An alternative to our approach is learning an end-to-end mapping that predicts the velocity field directly at each time-step.,1 INTRODUCTION,[0],[0]
"We argue that our hybrid approach restricts the learning task to a stable projection step, relieving the need for modeling the well understood advection and external body forces and enabling the use of enhancing tools such as vorticity confinement.",1 INTRODUCTION,[0],[0]
"In addition to the above the technical contributions, we contribute a dataset that can be of interest for people working on real-time simulations and as a benchmarking framework for end-to-end approaches.",1 INTRODUCTION,[0],[0]
"When a fluid has zero viscosity and is incompressible it can be modeled by the Euler equations:
∂u ∂t = −u · ∇u− 1 ρ ∇p+ f subject to, ∇ · u = 0, (1)
where u is the velocity (a 2D or 3D vector field), t is time, p is the pressure (a scalar field), f is the summation of external forces applied to the fluid body (buoyancy, gravity, etc) and ρ is fluid density.",2 MODEL,[0],[0]
We numerically calculate all partial derivatives using finite difference (FD) methods on a MAC grid Harlow & Welch (1965).,2 MODEL,[0],[0]
Equations 1 can be solved via the standard operator splitting method described in Algorithm 1.,2 MODEL,[0],[0]
"At a high level, step 1 ignores the pressure term (−∇p of (1)) to create an advected velocity field, u?t , which includes unwanted divergence (see see (Selle et al., 2008) for details), and then step 7 solves for pressure, p, to satisfy the constraint in (1).",2 MODEL,[0],[0]
"This produces a divergence free velocity field, ut.",2 MODEL,[0],[0]
"In addition, we use vorticity confinement (Steinhoff & Underhill, 1994) to counteract unwanted numerical dissipation.",2 MODEL,[0],[0]
Step 8 is computationally demanding as it involves solving the Poisson equation: ∇2pt = 1∆t∇ · u?t .,2 MODEL,[0],[0]
"Rewriting this equation results in a large sparse linear system Apt = b, where A is referred to in the literature as the 5 or 7 point Laplacian matrix (for 2D and 3D grids respectively).",2 MODEL,[0],[0]
"After solving for pressure, the divergence free velocity is calculated by subtracting the FD gradient of pressure, ut = u?t − 1ρ∇p.",2 MODEL,[0],[0]
We propose a learned approximate inference mechanism to find fast and efficient solutions to the linear system Apt = b.,2 MODEL,[0],[0]
"The key observation is that, while there is no closed form solution, the function mapping input data to the optimum of an optimization problem is deterministic.",2 MODEL,[0],[0]
Therefore one can attempt to approximate it using a powerful regressor such as a deep neural network.,2 MODEL,[0],[0]
"A block diagram of our high-level model architecture is depicted in Figure 1, and shows the computational blocks required to calculate ût for a single time-step.",2 MODEL,[0],[0]
The advect block is a fixed function unit solving step 1 of Algorithm 1.,2 MODEL,[0],[0]
"Then we add the body and vorticity confinement forces and obtain the divergence of the velocity field ∇ · u?t which, along with geometry, is fed through a multi-stage ConvNet to produce p̂t.",2 MODEL,[0],[0]
"We then calculate the pressure divergence, and subtract it from the divergent velocity to produce ût.",2 MODEL,[0],[0]
"Note that the only block with trainable parameters is the ConvNet model.
",2 MODEL,[0],[0]
"We define an objective function and formulate the inference solution as an unsupervised machine learning task where the loss function is given by,
fobj = ∑ i wi {∇ · ût}2i = ∑ i wi { ∇ · ( u?t − 1 ρ ∇p̂t )}2 i
(2)
Where ût and p̂t are the predicted divergence free velocity and pressure fields respectively and wi is a per-vertex weighting term which emphasizes the divergence of voxels on geometry boundaries.",2 MODEL,[0],[0]
"Note that the bottle-neck architecture in the ConvNet avoids obtaining trivial solutions.
",2 MODEL,[0],[0]
The internal structure of the ConvNet architecture is shown in Figure 2.,2 MODEL,[0],[0]
It consists of 5 stages of convolution (spatial or volumetric) and Rectifying Linear layers (ReLU).,2 MODEL,[0],[0]
The convolutional operator itself mimics the local sparsity structure of our linear system.,2 MODEL,[0],[0]
"However a single resolution network would have limited context, which limits the network‘s ability to model long-range external forces (such as gravity or buoyancy).",2 MODEL,[0],[0]
"As such, we add multi-resolution features to enable modeling long range physical phenomenon, processing each resolution in parallel then upsampling the resultant low resolution features before accumulating them.",2 MODEL,[0],[0]
"The model of Section 2 was implemented in Torch7 Collobert et al. (2011), with two CUDA baseline methods for comparison; a Jacobi-based iterative solver and a PCG-based solver (with incomplete Cholesky L0 preconditioner).",3 RESULTS AND ANALYSIS,[0],[0]
"All tests are performed on a tailored dataset, see Appendix A.
To implement the model of Yang et al. (2016) for comparison, we rephrase their fully-connected architecture as an equivalent, but significantly faster, sliding window model (on a 96x128x96 grid, Yang et al. report 515ms/frame, while our implementation takes 9.4ms/frame).",3 RESULTS AND ANALYSIS,[0],[0]
"Unfortunately, their loss function fails to learn an accurate projection on our dataset.",3 RESULTS AND ANALYSIS,[0],[0]
"This is because our divergent velocity frames include gravity and buoyancy terms, which result in a high amplitude, low frequency gradient in the ground-truth pressure.",3 RESULTS AND ANALYSIS,[0],[0]
"The small 3x3x3 context of the Yang et al. model cannot infer such low frequency output, which dominates the loss function and results in over-training.",3 RESULTS AND ANALYSIS,[0],[0]
"By contrast, our unsupervised objective minimizes divergence after the pressure gradient operator, whose FD calculation acts as a high-pass filter.",3 RESULTS AND ANALYSIS,[0],[0]
This is a significant advantage; our objective function is “softer” on the divergence contribution for phenomena that the network cannot easily infer.,3 RESULTS AND ANALYSIS,[0],[0]
"For the remaining experimental results, we will evaluate an improved version of the Yang et al. model as our “small model” (i.e. a single resolution with only 3x3x3 context, trained using the loss function, top level architectural improvements and training procedure of this work).
",3 RESULTS AND ANALYSIS,[0],[0]
"For fair quantitative comparison of output residual, we choose the number of Jacobi iterations (34) to approximately match the FPROP time of our network.",3 RESULTS AND ANALYSIS,[0],[0]
PCG is orders of magnitude slower at all resolutions.,3 RESULTS AND ANALYSIS,[0],[0]
The “small-model” provides a significant speedup over other methods.,3 RESULTS AND ANALYSIS,[0],[0]
"The runtime for the PCG, Jacobi, this work, and the “small model” are 2521ms, 47.6ms, 39.9ms and 16.9ms respectively.",3 RESULTS AND ANALYSIS,[0],[0]
"See Appendix B for details, including timing as a function of resolution in Figure 5.
",3 RESULTS AND ANALYSIS,[0],[0]
"We simulated a 3D smoke plume using our system and baseline methods (visual results are shown in Appendix C, figures 6 and 7) 1.",3 RESULTS AND ANALYSIS,[0],[0]
Note that this boundary condition is not present in the training set; it is a difficult test of generalization performance.,3 RESULTS AND ANALYSIS,[0],[0]
"Qualitatively, the PCG and 100-iteration Jacobi solvers and our network produce visually similar results.",3 RESULTS AND ANALYSIS,[0],[0]
"The “small model”, cannot accurately simulate the large vortex under the plume, and as a result the plume rises too quickly and exhibits density blurring.",3 RESULTS AND ANALYSIS,[0],[0]
"Similarly the Jacobi method, when truncated early at 34 iterations, introduces implausible high frequency noise and has an elongated shape due to inaccurate modeling of buoyancy.",3 RESULTS AND ANALYSIS,[0],[0]
Both ConvNet based methods lose some smoke density inside the arch model due to residual negative divergence at the fluid-geometry boundary.,3 RESULTS AND ANALYSIS,[0],[0]
"The maximum residual norm was <1e-3, 1.235, 1.966, 0.872 for the PCG, Jacobi, small model and this work respectively.
",3 RESULTS AND ANALYSIS,[0],[0]
"As a test of long-term stability, we record the mean residual norm (E (‖∇ · ûi‖)) across all samples in our test-set for each frame after the initial condition, shown in Figure 1.",3 RESULTS AND ANALYSIS,[0],[0]
"Our model outperforms the small model (Yang et al. sizing), and is competitive with Jacobi.",3 RESULTS AND ANALYSIS,[0],[0]
"We also present the results of our model when a single time-step loss is used; without the multi-frame loss, single time-step accuracy is degraded, and the divergence increases over time as error is accumulated.
1Video examples of these experiments can be found in at http://cims.nyu.edu/˜schlacht/CNNFluids.htm.",3 RESULTS AND ANALYSIS,[0],[0]
"While we do not need ground-truth label information to train the ConvNet model of Section 2, we need a collection of ground-truth pressure solutions to evaluate the precision of our model, and additionally our model does benefit from an efficient sampling of “realistic” initial conditions.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"That is, the space of all divergent velocity fields is unconstrained, and so our network’s generalization performance is improved when using a dataset of natural initial conditions that approximately samples the manifold of real-world fluid simulation states.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"To this end, we propose a procedural method to generate a corpus of initial frames for use in training.
",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"We use synthetic data generated using an offline 3D solver, mantaflow Pfaff & Thuerey - an opensource research library for solving incompressible fluid flow.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
We then seed this solver with initial condition states generated via a simple procedure using a combination of i. a pseudo-random turbulent field to initialize the velocity ii.,A DATASET CREATION AND MODEL TRAINING,[0],[0]
"a random placement of geometry within this field, and iii. procedurally adding localized input perturbations.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"We will now describe this procedure in detail.
",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"Firstly, we use the wavelet turbulent noise of Kim et al. (2008) to initialize a pseudo-random, divergence free velocity field.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"At the beginning of each simulation we randomly sample a set of noise parameters (uniformly sampling the wavelet spatial scale and amplitude) and we generate a random seed, which we then use to generate the velocity field.
",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"Next, we generate an occupancy grid by selecting objects from a database of models and randomly scaling, rotating and translating these objects in the simulation domain.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
We use a subset of 100 objects from the NTU 3D Model Database Pu & Ramani (2006); 50 models are used only when generating training set initial conditions and 50 models are used when generating test samples.,A DATASET CREATION AND MODEL TRAINING,[0],[0]
Figure 4 shows a selection of these models.,A DATASET CREATION AND MODEL TRAINING,[0],[0]
Each model is voxelized using the binvox library Min (2016).,A DATASET CREATION AND MODEL TRAINING,[0],[0]
"For generating 2D simulation data, we simply take a 2D slice of the 3D voxel grid.
",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"Finally, we simulate small divergent input perturbations by modeling inflow moving across the velocity field using a collection of emitter particles.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"We do this by generating a random set of emitters (with random time duration, position, velocity and size) and adding the output of these emitters to the velocity field throughout the simulation.
",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"With the above initial conditions defined, we use manta to calculate u?t by advecting the velocity field and adding forces.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"We also step the simulator forward 256 frames (using Manta’s PCG-based solver), recording the divergent velocity every 8 frame steps.
",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"Using the above procedure, we generate a training set of 320 “scenes” (each with a random initial condition) and a test set of an additional 320 scenes.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
"Each “scene” contains 32 frames, each 0.8 seconds apart.",A DATASET CREATION AND MODEL TRAINING,[0],[0]
We use a disjoint set of geometry for the test and training sets to test generalization performance.,A DATASET CREATION AND MODEL TRAINING,[0],[0]
We will make this dataset public (as well as the code for generating it) for future research use.,A DATASET CREATION AND MODEL TRAINING,[0],[0]
All materials are located at http://cims.nyu.edu/˜schlacht/CNNFluids.htm.,A DATASET CREATION AND MODEL TRAINING,[0],[0]
"Figure 5, shows the computation time of the Jacobi method, the small-model version (with Yang et al. sizing) and this work.",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"This runtime includes the pressure projection steps only: including velocity divergence calculation, the linear system solve, and the velocity update.",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"Note that for
fair quantitative comparison of output residual (shown in Section 3 of the paper), we choose the number of Jacobi iterations (34) to approximately match the FPROP time of our network.",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"Since the asymptotic complexity as a function of resolution is the same for Jacobi and our ConvNet, the FPROP times are equivalent.",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
We use an NVIDIA Titan X GPU with 12GB of ram and an Intel Xeon E5-2690 CPU.,B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
PCG is orders of magnitude slower at all resolutions and has been left off for clarity.,B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
The model of Yang et al. provides a significant speedup over other methods.,B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"The runtime for the PCG, Jacobi, this work, and Yang et al. at 1283 grid resolution are 2521ms, 47.6ms, 39.9ms and 16.9ms respectively.
",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"Note that with custom hardware Movidius; Google Inc., separable convolutions and other architectural enhancements, we believe the runtime of our ConvNet could be reduced significantly.",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"However, we leave this to future work.",B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY,[0],[0]
"This appendix shows rendered frames for the proposed method as well as baseline alternatives.
",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
Figure 6 shows a rendered frame of our plume simulation (without geometry) for all methods.,C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
Note that this boundary condition is not present in the training set and represents an input divergent flow approximately 5 times wider than the largest impulse present during training.,C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
It is a difficult test of generalization performance for data-driven methods.,C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"Qualitatively, the PCG and Jacobi (with 100 iterations) and our network produce visually similar results.",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"The model of Yang et al., trained using the loss function of this work, cannot accurately simulate the large vortex under the plume, and as a result the plume rises too quickly and exhibits density blurring under the plume itself.",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"Similarly the Jacobi method, when truncated early at 34 iterations, introduces implausible high frequency noise and has an elongated shape due to inaccurate modeling of buoyancy forces.
",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
We also repeat the above simulation with solid cells from the “arch” model held out of our training set.,C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
Single frame results for this simulation are shown in Figure 7.,C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"Since this scene exhibits lots of turbulent flow, qualitative comparison is less useful.",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"However, the network of Yang et al. has difficulty minimizing divergence around large flat boundaries and results in high-frequency density artifacts as shown.",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"Both ConvNet based methods lose some smoke density inside the arch model due to negative divergence at the fluid-geometry boundary (specifically at the large flat ceiling), like a result of this wide plume interaction being outside the scope of the training samples.",C QUALITATIVE COMPARISON OF SIMULATIONS,[0],[0]
"Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources.",abstractText,[0],[0]
"In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations.",abstractText,[0],[0]
"Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large linear system with many free-parameters must be solved.",abstractText,[0],[0]
"We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system.",abstractText,[0],[0]
We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.,abstractText,[0],[0]
ACCELERATING EULERIAN FLUID SIMULATION WITH CONVOLUTIONAL NETWORKS,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1183–1191, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Automatic segment-level machine translation (MT) metrics have the potential to greatly advance MT by providing more fine-grained error analysis, increasing efficiency of system tuning methods and leveraging techniques for system hybridization.",1 Introduction,[0],[0]
"However, a major obstacle currently hindering the development of segment-level metrics is their evaluation.",1 Introduction,[0],[0]
"Human assessment is the gold standard against which metrics must be evaluated, but when it comes to the task of evaluating translation quality, human annotators
are notoriously inconsistent.",1 Introduction,[0],[0]
"For example, the main venue for evaluation of metrics, the annual Workshop on Statistical Machine Translation (WMT), reports disturbingly low inter-annotator agreement levels and highlights the need for better human assessment of MT.",1 Introduction,[0],[0]
"WMT-13, for example, report Kappa coefficients ranging from 0.075 to 0.324 for assessors from crowd-sourcing services, only increasing to between 0.315 and 0.457 for MT researchers (Bojar et al., 2013a).",1 Introduction,[0],[0]
"For evaluation of metrics that operate at the system or document-level such as BLEU, inconsistency in individual human judgments can, to some degree, be overcome by aggregation of individual human assessments over the segments within a document.",1 Introduction,[0],[0]
"However, for evaluation of segment-level metrics, there is no escaping the need to boost the consistency of human annotation of individual segments.
",1 Introduction,[0],[0]
"This motivates our analysis of current methods of human evaluation of segment-level metrics, and proposal of an alternative annotation mechanism.",1 Introduction,[0],[0]
"We examine the accuracy of segment scores collected with our proposed method by replicating components of the WMT-13 human evaluation (Bojar et al., 2013b), with the sole aim of optimizing agreement in segment scores to provide an effective gold standard for evaluating segment-level metrics.",1 Introduction,[0],[0]
"Our method also supports the use of significance testing of segment-level metrics, and tests applied to the WMT-13 metrics over nine language pairs reveal for the first time which segment-level metrics outperform others.",1 Introduction,[0],[0]
"We have made available code for acquiring accurate segment-level MT human evaluations from the crowd, in addition to significance
1183
testing competing segment-level metrics, at:
https://github.com/ygraham/ segment-mteval",1 Introduction,[0],[0]
"MT Metrics
Since 2008, the WMT workshop series has included a shared task for automatic metrics, and as with the translation shared task, human evaluation remains the official gold standard for evaluation.",2 WMT-style Evaluation of Segment-level,[0],[0]
"In order to minimize the amount of annotation work and enforce consistency between the primary shared tasks in WMT, the same evaluations are used to evaluate MT systems in the shared translation task, as well as MT evaluation metrics in the document-level metrics and segment-level metrics tasks.",2 WMT-style Evaluation of Segment-level,[0],[0]
"Although WMT have trialled several methods of human evaluation over the years, the prevailing method takes the form of ranking a set of five competing translations for a single source language (SL) input segment from best to worst.",2 WMT-style Evaluation of Segment-level,[0],[0]
A total of ten pairwise human relative preference judgments can be extracted from each set of five translations.,2 WMT-style Evaluation of Segment-level,[0],[0]
"Performance of a segment-level metric is assessed by the degree to which it corresponds with human judgment, measured by the number of metric scores for pairs of translations that are either concordant (Con) or discordant (Dis) with those of a human assessor, which the organizers describe as “Kendall’s τ”:
τ = |Con| − |Dis| |Con|+ |Dis|
Pairs of translations deemed equally good by a human assessor are omitted from evaluation of segment-level metrics (Bojar et al., 2014).
",2 WMT-style Evaluation of Segment-level,[0],[0]
"There is a mismatch between the human judgments data used to evaluate segment-level metrics and the standard conditions under which Kendall’s τ is applied, however: Kendall’s τ is used to measure the association between a set of observations of a single pair of joint random variables, X (e.g. the human rank of a translation) and Y (e.g. the metric score for the same translation).",2 WMT-style Evaluation of Segment-level,[0],[0]
A conventional application of Kendall’s τ would be comparison of all pairs of values within X with each corresponding pair within Y .,2 WMT-style Evaluation of Segment-level,[0],[0]
"Since the human assessment data is, however, a large number of separately ranked sets
of five competing translations and not a single ranking of all translations, it is not possible to compute a single Kendall’s τ",2 WMT-style Evaluation of Segment-level,[0],[0]
correlation.1,2 WMT-style Evaluation of Segment-level,[0],[0]
"The formula used to assess the performance of a metric in the task, therefore, is not what is ordinarily understood to be a Kendall’s τ coefficient, but, in fact, equivalent to a weighted average of all Kendall’s τ for each humanranked set of five translations.
",2 WMT-style Evaluation of Segment-level,[0],[0]
"A more significant problem, however, lies in the inconsistency of human relative preference judgments within data sets.",2 WMT-style Evaluation of Segment-level,[0],[0]
"Since overall scores for metrics are described as correlations, possible values achievable by any metric could be expected to lie in the range",2 WMT-style Evaluation of Segment-level,[0],[0]
"[−1, 1] (or “±1”).",2 WMT-style Evaluation of Segment-level,[0],[0]
"This is not the case, and achievements of metrics are obscured by contradictory human judgments.",2 WMT-style Evaluation of Segment-level,[0],[0]
"Before any metric has provided scores for segments, for example, the maximum and minimum correlation achievable by a participating metric can be computed as, in the case of WMT-13:",2 WMT-style Evaluation of Segment-level,[0],[0]
• Russian-to-English:,2 WMT-style Evaluation of Segment-level,[0],[0]
±0.92 • Spanish-to-English: ±0.90 • French-to-English: ±0.90 • German-to-English:,2 WMT-style Evaluation of Segment-level,[0],[0]
"±0.92 • Czech-to-English: ±0.89 • English-to-Russian: ±0.90 • English-to-Spanish: ±0.90 • English-to-French: ±0.91 • English-to-German: ±0.90 • English-to-Czech: ±0.87 If we are interested in the relative performance of metrics and take a closer look at the formula used to contribute a score to metrics, we can effectively ignore the denominator (|Con|+ |Dis|), as it is constant for all metrics.",2 WMT-style Evaluation of Segment-level,[0],[0]
"The numerator (|Con| − |Dis|) is what determines our evaluation of the relative performance of metrics, and although the formula appears to be a straightforward subtraction of counts of concordant and discordant pairs, due to the large numbers of contradictory human relative preference judgments in data sets, what this number actually represents is not immediately obvious.",2 WMT-style Evaluation of Segment-level,[0],[0]
"If, for example, translations A and B were scored by a metric such that metric score(A) > metric score(B), one
1This would in fact require all (|MT systems| × |distinct segments|) translations included in the evaluation to be placed in a single rank order.
might expect an addition or subtraction of 1 depending on whether or not the metric’s scores agreed with those of a human.",2 WMT-style Evaluation of Segment-level,[0],[0]
"Instead, however, the following is added:
(max(|A > B|, |A < B|) −min(|A > B|, |A < B|))× d
where:
|A > B| = # human judgments where A was preferred over B |A < B| = # human judgments where B was preferred over A
d = {
1 if |A <",2 WMT-style Evaluation of Segment-level,[0],[0]
B| > |A >,2 WMT-style Evaluation of Segment-level,[0],[0]
"B| −1 if |A < B| < |A > B|
For example, translations of segment 971 for Czechto-English systems uedin-heafield and uedin-wmt13 were compared by human assessors a total of 12 times: the first system was judged to be best 4 times, the second system was judged to be best 2 times, and the two systems were judged to be equal 6 times.",2 WMT-style Evaluation of Segment-level,[0],[0]
"This results in a score of 4−2 for a system-level metric that scores the uedin-heafield translation higher than uedin-wmt13 (tied judgments are omitted), or score of 2− 4 in the converse case.
",2 WMT-style Evaluation of Segment-level,[0],[0]
Another challenge is how to deal with relative preference judgments where two translations are deemed equal quality (as opposed to strictly better or worse).,2 WMT-style Evaluation of Segment-level,[0],[0]
"In the current setup, tied translation pairs are excluded from the data, meaning that the ability for evaluation metrics to evaluate similar translations is not directly evaluated, and a metric that manages to score two equal quality translations closer, does not receive credit.",2 WMT-style Evaluation of Segment-level,[0],[0]
"A segment-level metric that can accurately predict not just disparities between translations but also similarities is likely to have high utility for MT system optimization, and is possibly the strongest motivation for developing segment-level metrics in the first place.",2 WMT-style Evaluation of Segment-level,[0],[0]
"In WMT-13, however, 24% of all relative preference judgments were omitted on the basis of ties, broken down as follows:",2 WMT-style Evaluation of Segment-level,[0],[0]
"• Spanish-to-English: 28% • French-to-English: 26% • German-to-English: 27% • Czech-to-English: 25% • Russian-to-English: 24%
• English-to-Spanish: 23% • English-to-French: 23% • English-to-German: 20% • English-to-Czech: 16% • English-to-Russian: 27%",2 WMT-style Evaluation of Segment-level,[0],[0]
"Although significance tests for evaluation of MT systems and document-level metrics have been identified (Koehn, 2004; Graham and Baldwin, 2014; Graham et al., 2014b), no such test has been proposed for segment-level metrics, and it is unfortunately common to conclude success without taking into account the fact that an increase in correlation can occur simply by chance.",2 WMT-style Evaluation of Segment-level,[0],[0]
"In the rare cases where significance tests have been applied, tests or confidence intervals for individual correlations form the basis for drawing conclusions (Aziz et al., 2012; Machacek and Bojar, 2014).",2 WMT-style Evaluation of Segment-level,[0],[0]
"However, such tests do not provide insight into whether or not a metric outperforms another, as all that’s required for rejection of the null hypothesis with such a test is a likelihood that an individual metric’s correlation with human judgment is not equal to zero.",2 WMT-style Evaluation of Segment-level,[0],[0]
"In addition, data sets for evaluation in both document and segment-level metrics are not independent and the correlation that exists between pairs of metrics should also be taken into account by significance tests.",2 WMT-style Evaluation of Segment-level,[0],[0]
"Many human evaluation methodologies attempt to elicit precisely the same quality judgment for individual translations from all assessors, and inevitably produce large numbers of conflicting assessments in the process, including from the same individual human judge (Callison-Burch et al., 2007; CallisonBurch et al., 2008; Callison-Burch et al., 2009).",3 Segment-Level Human Evaluation,[0],[0]
"An alternative approach is to take into account the fact that different judges may genuinely disagree, and allow assessments provided by individuals to each contribute to an overall estimate of the quality of a given translation.
",3 Segment-Level Human Evaluation,[0],[0]
"In an ideal world in which we had access to assessments provided by the entire population of qualified human assessors, for example, the mean of those assessments would provide a statistic that, in theory at least, would provide a meaningful segment-level human score for translations.",3 Segment-Level Human Evaluation,[0],[0]
"If it were possible to collect assessments from the entire
population we could directly compute the true mean score for a translation segment.",3 Segment-Level Human Evaluation,[0],[0]
"This is of course not possible, but thanks to the law of large numbers we can make the following assumption:
Given a sufficiently large assessment sample for a given translation, the mean of assessments will provide a very good estimate of the true mean score of the translation sourced from the entire assessor population.
",3 Segment-Level Human Evaluation,[0],[0]
"What the law of large numbers does not tell us, however, is, for our particular case of translation quality assessment, precisely how large the sample of assessments needs to be, so that the mean of scores provides a close enough estimate to the true mean score for any translation.",3 Segment-Level Human Evaluation,[0],[0]
"For a sample mean for which the variance is known, the required sample size can be computed for a specified standard error.",3 Segment-Level Human Evaluation,[0],[0]
"However, due to the large number of distinct translations we deal with, the variance in sample score distributions may change considerably from one translation to the next.",3 Segment-Level Human Evaluation,[0],[0]
"In addition, the choice as to what exactly is an acceptable standard error in sample means would be somewhat arbitrary.",3 Segment-Level Human Evaluation,[0],[0]
"On the one hand, if we specify a standard error that’s lower than is required, and subsequently collect more repeat assessments than is needed, we would be wasting resources that could, for example, be targeted at the annotation of additional translation segments.
",3 Segment-Level Human Evaluation,[0],[0]
"Our solution is to empirically investigate the impact on sample size of repeat assessments on the mean score for a given segment, and base our determination of sample size on the findings.",3 Segment-Level Human Evaluation,[0],[0]
"Since we later motivate the use of Pearson’s correlation to measure the linear association between human and metric scores (see Section 4), we base our investigation on Pearson’s correlation.
",3 Segment-Level Human Evaluation,[0],[0]
We collect multiple assessments per segment to create score distributions for segments for a fixed set per language pair.,3 Segment-Level Human Evaluation,[0],[0]
"This is repeated twice over the same set of segments to generate two distinct sets of annotations: one set is used to estimate the true mean score, and the second set is randomly downsampled to simulate a set of assessments of fixed sample size.",3 Segment-Level Human Evaluation,[0],[0]
"We measure the Pearson correlation between the true mean score and different numbers of
assessments for a given assessment, to ask the question: how many assessments must be collected for a given segment to obtain mean segment scores that truly reflects translation quality?",3 Segment-Level Human Evaluation,[0],[0]
Scores are sampled according to annotation time to simulate a realistic setting.,3 Segment-Level Human Evaluation,[0],[0]
"MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance).",3.1 Translation Assessment Sample Size,[0],[0]
"The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013).",3.1 Translation Assessment Sample Size,[0],[0]
This supports accurate quality-control as well as normalisation of translation scores for each assessor.,3.1 Translation Assessment Sample Size,[0],[0]
"The assessment
task was posed as a monolingual task, where assessors were asked to rate the degree to which the MT system output adequately expressed the meaning of the corresponding reference translation.",3.1 Translation Assessment Sample Size,[0],[0]
"Translations were sampled at random from the WMT-13 data sets for the four language pairs, as detailed in Table 1.",3.1 Translation Assessment Sample Size,[0],[0]
"Due to low-quality assessors on MTurk and the need for assessments solely for quality assurance purposes, the exercise required a substantial number of individual assessments.",3.1 Translation Assessment Sample Size,[0],[0]
"For Spanish-toEnglish, for example, a total of (280 translations + 120 translations for quality-control purposes) × 40 assessments per translation × 2 separate data collections × ∼2 to allow for filtering of low-quality assessors = ∼64k assessments were collected; after quality control filtering and removing the qualitycontrol translations, around 22k assessments were used for the actual experiment.
",3.1 Translation Assessment Sample Size,[0],[0]
"Figure 1 shows the Pearson correlation between mean segment-level scores calculated for varying numbers of assessments (N ), and the full set of assessments for the second set of assessments.",3.1 Translation Assessment Sample Size,[0],[0]
"For each language pair, we calculate the correlation first over the raw segment scores and second over standardized scores, based on the method of Graham et
al. (2014a).2 For all language pairs, although the correlation is relatively low for single assessments, as the sample size increases, it increases, and by approximately N = 15 assessments, for all four language pairs, the correlation reaches r = 0.9.",3.1 Translation Assessment Sample Size,[0],[0]
"For Spanish-to-English, for which most assessments were collected, when we increase the number of assessments to N = 40 per translation, the correlation reaches r = 0.97.",3.1 Translation Assessment Sample Size,[0],[0]
"Figure 2 is a set of scatter plots for mean segment-level scores for Spanish-toEnglish rising, for varying sample sizes N .
",3.1 Translation Assessment Sample Size,[0],[0]
"As expected, the larger the sample size of assessments, the greater the agreement with the true mean score, but what is more surprising is that with as few as 15 assessments, the scores collected in the two separate experiments correlate extremely well, and provide what we believe to be a sufficient stability to evaluate segment-level metrics.
",3.1 Translation Assessment Sample Size,[0],[0]
"2Standardized segment scores are computed by standardizing individual raw scores according to the mean and standard deviation of individual assessors, and then combined into mean segment scores.",3.1 Translation Assessment Sample Size,[0],[0]
"Since the scores generated by our method are continuous and segment-level metrics are also required to output continuous-valued scores, we can now compare the scores directly using Pearson’s correlation.",4 Segment-level Metric Evaluation,[0],[0]
Pearson’s correlation has three main advantages for this purpose.,4 Segment-level Metric Evaluation,[0],[0]
"Firstly, the measure is unit-free, so metrics do not have to produce scores on the same scale as the human assessments.",4 Segment-level Metric Evaluation,[0],[0]
"Secondly, scores are absolute as opposed to relative and therefore more intuitive and ultimately more powerful; for example, we are able to evaluate metrics over the 20% of translations of highest or lowest quality in the test set.",4 Segment-level Metric Evaluation,[0],[0]
"Finally, the use of Pearson’s correlation facilitates the measurement of statistical significance in correlation differences.
",4 Segment-level Metric Evaluation,[0],[0]
"It is important to point out, however, that moving from Kendall’s τ over relative preference judgments to Pearson’s r over absolute scores does, in fact, change the task required of metrics in one respect: previously, there was no direct evaluation of the scores generated by a metric, nor indeed did the evaluation ever directly compare translations for different source language inputs (as relative preference judgments were always relative to other translations for the same input).",4 Segment-level Metric Evaluation,[0],[0]
"Pearson’s correlation, on the other hand, compares scores across the entire test set.",4 Segment-level Metric Evaluation,[0],[0]
"With the move to Pearson’s correlation, we can also test statistical significance in differences between metrics, based on the Williams test (Williams, 1959),3 which evaluates significance in a difference in dependent correlations (Steiger, 1980).",4.1 Significance Testing of Segment-level Metrics,[0],[0]
"As suggested by Graham and Baldwin (2014), the test is appropriate for evaluation of document-level MT metrics since the data is not independent, and for similar reasons, the test can also be used for evaluation of segment-level metrics.",4.1 Significance Testing of Segment-level Metrics,[0],[0]
We first carry out tests for Spanish-to-English segment-level metrics from WMT-13.,4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"In our experiments in Section 3.1, we used only a sub-sample
3Also sometimes referred to as the Hotelling–Williams test.
of segments, so the first thing is to collect assessments for the remaining Spanish-to-English translation segments using MTurk, based on a sample of at least 15 assessments.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"A total of 24 HITs of 100 translations each were posted on MTurk; after removal of low quality workers (∼50%) and quality control items (a further 30%), this resulted in 840 translation segments with 15 or more assessments each.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"The scores were standardized and combined into mean segment scores.
",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Table 2 shows the Pearson’s correlation for each metric that participated in the WMT-13 Spanish-toEnglish evaluation task, along with the Kendall’s τ based on the original WMT-13 methodology and relative preference assessments.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Overall, when we compare correlations using the new evaluation methodology to those from the original evaluation, even though we have raised the bar by assessing the raw numeric outputs rather than translating them into preference judgments relative to other translations for the same SL input, all metrics achieve higher correlation with human judgment than reported in the original evaluation.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"This indicates that the new evaluation setup is by no means unrealistically difficult, and that even though it was not required of the metrics in the original task setup, the metrics are doing a relatively good job of absolute scoring of translation adequacy.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"In addition,
the new assessment reflects how well metrics score translations of very close or equal quality, and, as described in Section 2, ameliorates the issue of low inter-annotator agreement as well as resolving the original mismatch between discrete human relative preference judgments and continuous metric scores.
",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Figure 3 is a heat map of the Pearson’s correlation between each pair of segment-level metrics for Spanish-to-English from WMT-13, and Figure 4 shows correspondence between scores of three segment-level metrics with our human evaluation data.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
Figure 5 displays the outcome of the Williams significance test as applied to each pairing of competing metrics.,4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Since the power of Williams test increases with the strength of correlation between a pair of metrics, it is important not to conclude the best system by the number of other metrics it outperforms.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Instead, the best choice of metric for that language pair is any metric that is not signicifantly outperformed by any other metric.",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Three metrics prove not to be significantly outperformed by any other metric for Spanish-to-English, and tie for best performance: METEOR (Denkowski and Lavie, 2011), NLEPOR (Han et al., 2013) and SENTBLEU-MOSES (sBLEU-moses).",4.2 Spanish-to-English Segment-level Metrics,[0],[0]
"Since human assessments are now absolute, scores effectively have the same meaning across language pairs, facilitating the combination of data across multiple language pairs.",4.3 9 Language Pairs,[0],[0]
"Since many approaches to MT are language-pair independent, the ability to know what segment-level metric works best across all language pairs is useful for choosing an appropriate default metric or simply avoiding having to swap and change metrics across different language
pairs.
",4.3 9 Language Pairs,[0],[0]
"Assessments of translations were crowd-sourced for nine language pairs used in the WMT-13 shared metrics task: Russian-to-English, Spanish-toEnglish, French-to-English, German-to-English, Czech-to-English, English-to-Russian, Englishto-Spanish, English-to-French and English-toGerman.4 Again, we obtain a minimum of 15 assessments per translation, and collect scores for 100 translations per language pair.",4.3 9 Language Pairs,[0],[0]
"After removal of quality control items, this leaves 70 distinct translations per language pair, combined into a cross-lingual test set of 630 distinct translations spanning nine language pairs.
",4.3 9 Language Pairs,[0],[0]
"Table 3 shows Pearson’s correlation with human assessment for the six segment-level metrics that competed across all language pairs in WMT-13, and Figure 6 shows the outcomes of Williams test for statistical significance between different pairings of metrics.",4.3 9 Language Pairs,[0],[0]
"Results reveal that the same three metrics as before (METEOR, SENTBLEU-MOSES and NLEPOR), in addition to SIMPBLEUP and SIMPBLEUR are not significantly outperformed by any other metric at p<0.05.",4.3 9 Language Pairs,[0],[0]
"However, since the latter two were shown to be outperformed for Spanish-to-English, all else being equal, METEOR, SENTBLEU-MOSES and NLEPOR are still a superior choice of default metric.
",4.3 9 Language Pairs,[0],[0]
"4We were regrettably unable to include English-to-Czech, due to a lack of Czech-speaking MTurk workers.",4.3 9 Language Pairs,[0],[0]
"We presented a new evaluation methodology for segment-level metrics that overcomes the issue of low inter-annotator agreement levels in human assessments, includes evaluation of very close and equal quality translations, and provides a significance test that supports system comparison with confidence.",5 Conclusion,[0],[0]
"Our large-scale human evaluation reveals three metrics to not be significantly outperformed by any other metric in both Spanish-to-
English and a combined evaluation across nine language pairs, namely: METEOR, NLEPOR and SENTBLEU-MOSES.",5 Conclusion,[0],[0]
We wish to thank the anonymous reviewers for their valuable comments.,Acknowledgements,[0],[0]
This research was supported by funding from the Australian Research Council and Science Foundation Ireland (Grant 12/CE/12267).,Acknowledgements,[0],[0]
Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels in human assessments; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of significance testing improvements over a baseline.,abstractText,[0],[0]
"In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed specifically at assessment of segment-level metrics.",abstractText,[0],[0]
We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed.,abstractText,[0],[0]
"Three segment-level metrics — METEOR, NLEPOR and SENTBLEUMOSES — are found to correlate with human assessment at a level not significantly outperformed by any other metric in both the individual language pair assessment for Spanish-toEnglish and the aggregated set of 9 language pairs.",abstractText,[0],[0]
Accurate Evaluation of Segment-level Machine Translation Metrics,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2011–2020 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Recently, deep neural networks (DNNs) have provided promising results for a variety of reading comprehension and question answering tasks (Weston et al., 2014; Hermann et al., 2015; Rajpurkar et al., 2016), which require extracting precise information from documents conditioned on a query.",1 Introduction,[0],[0]
"While a basic sequence to sequence (seq2seq) model (Sutskever et al., 2014) can perform these
∗Work completed while interning at Google Research.",1 Introduction,[0],[0]
"†Work completed while at Google Research.
tasks by encoding a question and document sequence and decoding an answer sequence (Hewlett et al., 2016), it has some disadvantages.",1 Introduction,[0],[0]
"The answer may be encountered early in the text and need to be stored across all the further recurrent steps, leading to forgetting or corruption; Attention can be added to the decoder to solve this problem (Hermann et al., 2015).",1 Introduction,[0],[0]
"Even with attention, approaches based on Recurrent Neural Networks (RNNs) require a number of sequential steps proportional to the document length to encode each document position.",1 Introduction,[0],[0]
"Hierarchical reading models address this problem by breaking the document into sentences (Choi et al., 2017).",1 Introduction,[0],[0]
"In this paper, we introduce a simpler hierarchical model that achieves state-of-the-art performance on our benchmark task without this linguistic structure, and use it as framework to explore semisupervised learning for reading comprehension.
",1 Introduction,[0],[0]
We first develop a hierarchical reader called Sliding-Window Encoder Attentive Reader (SWEAR) that circumvents the aforementioned bottlenecks of existing readers.,1 Introduction,[0],[0]
"SWEAR, illustrated in Figure 1, first encodes each question into a vector space representation.",1 Introduction,[0],[0]
"It then chunks each document into overlapping, fixed-length windows and, conditioned on the question representation, encodes each window in parallel.",1 Introduction,[0],[0]
"Inspired by recent attention mechanisms such as Hermann et al. (2015), SWEAR attends over the window representations and reduces them into a single vector for each document.",1 Introduction,[0],[0]
"Finally, the answer is decoded from this document vector.",1 Introduction,[0],[0]
"Our results show that SWEAR outperforms the previous state-of-the-art on the supervised WikiReading task (Hewlett et al., 2016), improving Mean F1 to 76.8 from the previous 75.6 (Choi et al., 2017).
",1 Introduction,[0],[0]
"While WikiReading is a large dataset with millions of labeled examples, many applications of machine reading have a much smaller number
2011
of labeled examples among a large set of unlabeled documents.",1 Introduction,[0],[0]
"To model this situation, we constructed a semi-supervised version of WikiReading by downsampling the labeled corpus into a variety of smaller subsets, while preserving the full unlabeled corpus (i.e., Wikipedia).",1 Introduction,[0],[0]
"To take advantage of the unlabeled data, we evaluated multiple methods of reusing unsupervised recurrent autoencoders in semi-supervised versions of SWEAR.",1 Introduction,[0],[0]
"Importantly, in these models we are able to reuse all the autoencoder parameters without fine-tuning, meaning the supervised phase only has to learn to condition the answer on the document and query.",1 Introduction,[0],[0]
"This allows for more efficient training and online operation: Documents can be encoded in a single pass offline and these encodings reused by all models, both during training and when answering queries.",1 Introduction,[0],[0]
Our semisupervised learning models achieve significantly better performance than supervised SWEAR on several subsets with different characteristics.,1 Introduction,[0],[0]
"The best-performing model reaches 66.5 with 1% of the WikiReading dataset, compared to the 2016 state of the art of 71.8 (with 100% of the dataset).",1 Introduction,[0],[0]
"Following the recent progress on end-to-end supervised question answering (Hermann et al., 2015; Rajpurkar et al., 2016), we consider the general problem of predicting an answer A given a query-document pair (Q,D).",2 Problem Description,[0],[0]
We do not make the assumption that the answer should be present verbatim in the document.,2 Problem Description,[0],[0]
"Given a document D = {d1, d2, · · · , dND} and a query Q = {q1, q2, · · · , qNQ} as sequences of words, our task is to generate a new sequence of words that matches the correct answer A = {a1, a2, · · · , aNA}.",2.1 Supervised Version,[0],[0]
"Because we do not assume that A is a subsequence of D, the answer may require blending information from multiple parts of the document, or may be precisely copied from a single location.",2.1 Supervised Version,[0],[0]
"Our proposed architecture supports both of these use cases.
",2.1 Supervised Version,[0],[0]
"The WikiReading dataset (Hewlett et al., 2016), which includes a mix of categorization and extraction tasks, is the largest dataset matching this problem description.",2.1 Supervised Version,[0],[0]
"In WikiReading, documents are Wikipedia articles, while queries and answers are Wikidata properties and values,
respectively.",2.1 Supervised Version,[0],[0]
"Example Wikidata property-value pairs are (place of birth, Paris), (genre, Science Fiction).",2.1 Supervised Version,[0],[0]
"The dataset contains 18.58M instances divided into training, validation, and test with an 85/10/5 split.",2.1 Supervised Version,[0],[0]
"The answer is present verbatim in the document only 47.1% of the time, severely limiting models that label document spans, such as those developed for the popular SQUAD dataset (Rajpurkar et al., 2016).",2.1 Supervised Version,[0],[0]
"We also consider a semi-supervised version of the task, where an additional corpus of documents without labeled (Q,A) pairs is available.",2.2 Semi-Supervised Version,[0],[0]
"Taking advantage of the large size of the WikiReading dataset, we created a series of increasingly challenging semi-supervised problems with the following structure:
• Unsupervised: The entire document corpus (about 4M Wikipedia articles), with queries and answers removed.
",2.2 Semi-Supervised Version,[0],[0]
"• Supervised: Five smaller training sets created by sampling a random (1%, 0.5%, 0.1%) of the WikiReading training set, and taking (200, 100) random samples from each property in the original training set.",2.2 Semi-Supervised Version,[0],[0]
"We now present our model, called SlidingWindow Encoder Attentive Reader (SWEAR), shown in Figure 1, and describe its operation in a fully supervised setting.",3 Supervised Model Architecture,[0],[0]
"Given a (Q,D) pair, the model encodes Q into a vector space representation with a Recurrent Neural Network (RNN).",3 Supervised Model Architecture,[0],[0]
"The first layer of the model chunks the document D into overlapping, fixed-length windows and encodes all windows in parallel with an RNN conditioned on the question representation.",3 Supervised Model Architecture,[0],[0]
"The second layer attends over the window representations, reducing them into a single vector representing the latent answer.",3 Supervised Model Architecture,[0],[0]
"Finally, the answer sequence A is decoded from this vector using an RNN sequence decoder.",3 Supervised Model Architecture,[0],[0]
Each word w comes from a vocabulary V and is associated with a vector ew which constitutes the rows of an embedding,3.1 Preliminaries and Notation,[0],[0]
"matrixE. We denote by eD, eQ, and eA",3.1 Preliminaries and Notation,[0],[0]
"the vector sequences corresponding to
the document, question, and answer sequences, respectively.",3.1 Preliminaries and Notation,[0],[0]
"More specifically, we aim at obtaining vector representations for documents and questions, then generating the words of the answer sequence.
",3.1 Preliminaries and Notation,[0],[0]
Our model makes extensive use of RNN encoders to transform sequences into fixed length vectors.,3.1 Preliminaries and Notation,[0],[0]
"For our purposes, an RNN encoder consists of GRU units (Cho et al., 2014) defined as
ht = f(xt;ht−1; θ) (1)
where ht is hidden state at time t. f is a nonlinear function operating on input vector xt and previous state, ht−1 with θ being its parameter vector.",3.1 Preliminaries and Notation,[0],[0]
"Given an input sequence, the encoder runs over the sequence of words producing the hidden vectors at each step.",3.1 Preliminaries and Notation,[0],[0]
We refer to the last hidden state of an RNN encoder as the encoding of a sequence.,3.1 Preliminaries and Notation,[0],[0]
The core of the model is a sequence encoder that operates over sliding windows in a manner analogous to a traditional convolution.,3.2 Sliding Window Recurrent Encoder,[0],[0]
"Before encoding the document, we slide a window of length l with a step size s over the document and produce n = bND−ls c document windows.",3.2 Sliding Window Recurrent Encoder,[0],[0]
"This yields a sequence of sub-documents (D1, D2, · · · , Dn), where each Di contains a subsequence of l words from the original document D. Intuitively, a precise answer may be present verbatim in one or
more windows, or many windows may contain evidence suggestive of a more categorical answer.
",3.2 Sliding Window Recurrent Encoder,[0],[0]
"Next, the model encodes each window conditioned on a question encoding.",3.2 Sliding Window Recurrent Encoder,[0],[0]
"We first encode the question sequence once using a RNN (Enc) as
hq = Enc(eQ; θQ) (2)
where hq is the last hidden state and θQ represents the parameters of the question encoder.",3.2 Sliding Window Recurrent Encoder,[0],[0]
"Initialized with this question encoding, we employ another RNN to encode each document window as
hwi,0 = h q
hwi = Enc(e Di ; θW ) (3)
where hwi,0 is the initial hidden state, h w i is the last hidden state, and θW represents the parameters of the window encoder.",3.2 Sliding Window Recurrent Encoder,[0],[0]
"θW is shared for every window and is decoupled from θQ. As the windows are significantly smaller than the documents, encodings of windows will reflect the effect of question encodings better, mitigating any long-distance dependency problems.",3.2 Sliding Window Recurrent Encoder,[0],[0]
"SWEAR attends over the window encoder states using the question encoding to produce a single vector hd for the document, given by
pi ∝ exp(uTR tanh(WR[hwi , hq]))",3.3 Combining Window Encodings,[0],[0]
(4) hd = ∑,3.3 Combining Window Encodings,[0],[0]
i pih,3.3 Combining Window Encodings,[0],[0]
"w i (5)
",3.3 Combining Window Encodings,[0],[0]
"where [.] is vector concatenation, and pi is the probability window i is relevant to answering the question.",3.3 Combining Window Encodings,[0],[0]
WR and uR are parameters of the attention model.,3.3 Combining Window Encodings,[0],[0]
"Given the document encoding hd, an RNN decoder (Dec) generates the answer word sequence:
ha0 = h d hat = Dec(h a t−1;ωA) (6)
P (a∗t = wj) ∝",3.4 Answer Decoding,[0],[0]
exp(eTj (WAhat + bA)) (7) a∗t,3.4 Answer Decoding,[0],[0]
= argmaxj(P (a ∗ t = wj)),3.4 Answer Decoding,[0],[0]
"(8)
where ha0 is the initial hidden state and h a t is the hidden vector at time t. A∗ = {a∗1, a∗2, · · · , a∗NA} is the sequence of answer words generated.",3.4 Answer Decoding,[0],[0]
"WA, bA, and ωA are the parameters of the answer decoder.",3.4 Answer Decoding,[0],[0]
The training objective is to minimize the average cross-entropy error between the candidate sequence A∗ and the correct answer sequence A.,3.4 Answer Decoding,[0],[0]
"Before exploring unsupervised pre-training, we present summary results for SWEAR in a fully supervised setting, for comparison to previous work on the WikiReading task, namely that of Hewlett et al. (2016) and Choi et al. (2017), which we refer to as HE16 and CH17 in tables.",3.5 Supervised Results,[0],[0]
"For further experiments, results, and discussion see Section 5.2.",3.5 Supervised Results,[0],[0]
"Table 1 shows that SWEAR outperforms the best
results for various models reported in both publications, including the hierarchical models SoftAttend and Reinforce presented by Choi et al. (2017).1 Interestingly, SoftAttend computes an attention over sentence encodings, analogous to SWEAR’s attention over overlapping window encodings, but it does so on the basis of less powerful encoders (BoW or convolution vs RNN), suggesting that the extra computation spent by the RNN provides a meaningful boost to performance.
",3.5 Supervised Results,[0],[0]
"To quantify the effect of initializing the window encoder with the question state, we report results for two variants of SWEAR:",3.5 Supervised Results,[0],[0]
"In SWEAR the window encoder is initialized with the question encoding, while in SWEAR w/ zeros, the window encoder is initialized with zeros.",3.5 Supervised Results,[0],[0]
In both cases the question encoding is used for attention over the window encodings.,3.5 Supervised Results,[0],[0]
For SWEAR w/ zeros it is additionally concatenated with the document encoding and passed through a 2-layer fully connected neural network before the decoding step.,3.5 Supervised Results,[0],[0]
"Conditioning on the question increases Mean F1 by 0.4.
",3.5 Supervised Results,[0],[0]
"Hewlett et al. (2016) grouped properties by answer distribution: Categorical properties have a small list of possible answers, such as countries, Relational properties have an open set of answers, such as spouses or places of birth, and Date properties (a subset of relational properties) have date answers, such as date of birth.",3.5 Supervised Results,[0],[0]
"We reproduce this grouping in Table 2 to show that SWEAR improves performance for Relational and Date properties, demonstrating that it is better able to extract precise information from documents.
",3.5 Supervised Results,[0],[0]
"Finally, we observe that SWEAR outperforms a baseline seq2seq model on longer documents, as
1Document lengths differ between publications: We truncate documents to the first 600 words, while Choi et al. truncate to 1000 words or 35 sentences and Hewlett et al. truncate to 300 words.
shown in Table 3.",3.5 Supervised Results,[0],[0]
"The baseline model is roughly equivalent to the best previously-published result, Placeholder seq2seq (CH17) in Table 1, reaching a Mean F1 of 75.5 on the WikiReading testset.",3.5 Supervised Results,[0],[0]
"SWEAR improves over the baseline in every length category, but the differences are larger for longer documents.",3.5 Supervised Results,[0],[0]
"We now describe semi-supervised versions of the SWEAR model, to address the semi-supervised problem setting described in Section 2.2.",4 Semi-Supervised Model Architecture,[0],[0]
"A wide variety of approaches have been developed for semi-supervised learning with Neural Networks, with a typical scheme consisting of training an unsupervised model first, and then reusing the weights of that network as part of a supervised model.",4 Semi-Supervised Model Architecture,[0],[0]
"We consider each of these problems in turn, describing two types of unsupervised autoencoder models for sequences in Section 4.1 before turning to a series of strategies for incorporating the autoencoder weights into a final supervised model in Section 4.3.",4 Semi-Supervised Model Architecture,[0],[0]
"All of these models reuse the autoencoder weights without modification, meaning a document can be encoded once by an offline process, and the resulting encodings can be used both during training and to answer multiple queries online in a more efficient manner.",4 Semi-Supervised Model Architecture,[0],[0]
"Autoencoders are models that reconstruct their input, typically by encoding it into a latent space and then decoding it back again.",4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
"Autoencoders have recently proved useful for semi-supervised learning (Dai and Le, 2015; Fabius and van Amersfoort, 2014).",4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
We now describe two autoencoder models from the recent literature that we use for unsupervised learning.,4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
"The Recurrent Autoencoder (RAE) is the natural application of the seq2seq framework (Sutskever et al., 2014) to autoencoding documents (Dai and Le, 2015):",4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
"In seq2seq, an encoder RNN already produces a latent representation hN , which is used to initialize a decoder RNN.",4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
"In RAE, the output sequence is replaced with the input sequence, so learning minimizes the cross-entropy between the reconstructed input sequence and the original input sequence.",4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
Encoder and decoder cells share parameters θU .,4.1 Recurrent Autoencoders for Unsupervised Pre-training,[0],[0]
"The Variational Recurrent Autoencoder (VRAE), introduced by Fabius et al. (2014), is a RAE with a variational Bayesian inference step where an unobserved latent random variable generates the sequential data.",4.1.1 Variational Recurrent Autoencoder,[0],[0]
"The encoder and decoder are exactly the same as RAE, but the latent state hN is not directly passed to the decoder.",4.1.1 Variational Recurrent Autoencoder,[0],[0]
"Instead, it is used to estimate the parameters of a Gaussian distribution with a diagonal covariance matrix: The mean is given by µx = WµhN + bµ and the covariance by Σx = WΣhN + bΣ, where Wµ, WΣ, bµ, and bΣ are new variational step parameters.",4.1.1 Variational Recurrent Autoencoder,[0],[0]
"The decoder is initialized with a single vector sampled from this distribution, zx ∼ N (z|µx,Σx).",4.1.1 Variational Recurrent Autoencoder,[0],[0]
"For VRAE, the Kullback-Leibler divergence between trained Normal distribution and standard normal distribution, i.e., KL(N (µx,Σx)|N (0, I)), is added to the loss.",4.1.1 Variational Recurrent Autoencoder,[0],[0]
"We take advantage of the SWEAR architecture by training autoencoders for text windows, as opposed to the standard document autoencoders.",4.1.2 Window Autoencoders,[0],[0]
"These autoencoders operate on the same sliding window subsequences as the supervised SWEAR model, autoencoding all subsequences independently and in parallel.",4.1.2 Window Autoencoders,[0],[0]
This makes them easier to train as they only have to compress short sequences of text into a fixed-length representation.,4.1.2 Window Autoencoders,[0],[0]
"As the task of autoencoding is independent from our supervised problem, we refer to the generated encodings as global encodings.",4.1.2 Window Autoencoders,[0],[0]
Our baseline approach to reusing an unsupervised autoencoder in SWEAR is to initialize all embeddings with the pre-trained parameters and fix them.,4.2 Baseline: Initialization with Autoencoder Embeddings,[0],[0]
We call this model SWEAR-SS (for semisupervised).,4.2 Baseline: Initialization with Autoencoder Embeddings,[0],[0]
The embedding matrix is fixed to the autoencoder embeddings.,4.2 Baseline: Initialization with Autoencoder Embeddings,[0],[0]
All other parameters are initialized randomly and trained as in the fully supervised version.,4.2 Baseline: Initialization with Autoencoder Embeddings,[0],[0]
We found that initializing the encoders and decoder with autoencoder weights hurts performance.,4.2 Baseline: Initialization with Autoencoder Embeddings,[0],[0]
"Unfortunately, this baseline approach to semisupervised learning has significant disadvantages in our problem setting.",4.3 Reviewer Models,[0],[0]
"Pre-trained RNN parameters are not fully exploited since we observed
catastrophic forgetting when initializing and finetuning SWEAR with pre-trained weights.",4.3 Reviewer Models,[0],[0]
This includes fixing window encoder parameters with autoencoders and only fine-tuning question encoders.,4.3 Reviewer Models,[0],[0]
"Second, conditioning the window encoders on the question eliminates the possibility to train window representations offline and utilize them later which causes a significant overhead during testing.
",4.3 Reviewer Models,[0],[0]
"Inspired by recent trends in deep learning models such as Progressive Neural Networks (Rusu et al., 2016) and Reviewer Models (Yang et al., 2016), we propose multiple solutions to these problems.",4.3 Reviewer Models,[0],[0]
"All of the proposed models process text input first through a fixed autoencoder layer: fixed pre-trained embeddings and fixed RNN encoder parameters, both initialized from the autoencoder weights.",4.3 Reviewer Models,[0],[0]
"Above this autoencoder layer, we build layers of abstraction that learn to adapt the pretrained models to the QA task.",4.3 Reviewer Models,[0],[0]
"The most straightforward extension to the baseline model is to fix the pretrained autoencoder RNN as the first layer and introduce a second, trainable reviewer layer.",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"To make this approach more suitable for question answering, reviewer layers utilize corresponding global encodings as well as hidden states of the pre-trained autoencoders as input (Figure 2).",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"The aim is to review both pretrained question and window encodings to compose a single vector representing the window conditioned on the question.
",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"Encoding questions: The question is first encoded by the autoencoder layer, h̃q = Enc(eQ; θU ) where both word embeddings (E and eQ) and encoder (θU ) are fixed and initialized with pretrained parameters.",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"A second, learnable RNN layer then takes the output of the autoencoder layer and corresponding input embeddings as input and produces the final question encoding, hq = Enc(FC([eQ, h̃q, h̃t]); θQ) where FC is a fully connected layer with ReLU activation function, and h̃t is the output of the autoencoder layer at time step t. Figure 3 illustrates a single timestep of the question encoder.
",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"Encoding windows: Similarly, windows are encoded first by the fixed autoencoder layer and then by a reviewer layer, h̃wi = Enc(e
Di ; θU ) and hwi = Enc(FC([e Di , h̃wi , h̃ w t , h
q]); θW ) where h̃wt is the output of the autoencoder layer at time step t. Unlike supervised SWEAR, in SWEARMLR the window encoder is not initialized with the question encoder state.",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"Instead, the question encoder state is an additional input to each unit in the reviewer layer (illustrated as the dashed line in Figure 3).",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"Intuitively, the reviewer layer should reuse the global window and question information and encode only information relevant to the current question.",4.3.1 Multi-Layer Reviewer (SWEAR-MLR),[0],[0]
"Although the reviewer layer in SWEAR-MLR has global window and question encodings as input, it requires a number of sequential steps equal to the window size, plus any additional reviewer steps.",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"The reviewer layer also has to re-encode windows for each question, which is not ideal for online use.
",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"To address these issues, we now present a Progressive Reviewer model (SWEAR-PR) that reviews the outputs of the encoders using a separate RNN that is decoupled from the window size (Figure 4).
",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"Encoding questions and windows: Similar to SWEAR-MLR, SWEAR-PR first encodes the questions and windows independently using autoencoder layers, h̃q = Enc(eQ; θU ) and h̃wi = Enc(eDi ; θU ).",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"To decouple the question and window encoders, however, SWEAR-PR does not have a second layer as a reviewer.
",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
Reviewing questions and windows: SWEARPR employs two other RNNs to review the question and window encodings and to compose a single window representation conditioned on the question.,4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"Question reviewer takes the same pre-trained question encoding at each time step and attends over the hidden states and input embeddings of the pre-trained question encoder, hq = AttnEnc(FC(h̃q);FC([h̃qt , e
Q]); θQ) where AttnEnc is an RNN with an attention cell
which is illustrated in Figure 5.",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"Outputs of the fixed autoencoder layer and fixed word embeddings, [h̃qt , e
Q], are the attendable states.",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"Window reviewer on the other hand takes the pre-trained window encoding and reviewed question encoding at each time step and attends over the hidden states of pre-trained window encoder, hwi = AttnEnc(FC([h̃wi , h q]);FC([h̃wt , e Di ]); θW ) where outputs of the fixed autoencoder layer and fixed word embeddings, [h̃wt , e
Di ], are the attendable states.",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"As length of the windows is smaller than length of the reviewers, SWEAR-PR has significantly smaller overhead compared to other supervised and semi-supervised SWEAR variants.",4.3.2 Progressive Reviewer (SWEAR-PR),[0],[0]
"Reducing window encodings and decoding: As in the supervised case described in 4, both reviewer models attend over the window encodings using the question encoding and reduce them into a single document encoding.",4.3.3 Shared Components,[0],[0]
"Identical to answer decoding described in 6, the answer is decoded using another RNN taking the document state as the initial state.",4.3.3 Shared Components,[0],[0]
"The parameters of this answer decoder are initialized randomly.
",4.3.3 Shared Components,[0],[0]
Adapter layer:,4.3.3 Shared Components,[0],[0]
"As the distribution and scale of parameters may differ significantly between the autoencoder layer and the reviewer layer, we use an adapter layer similar to the adapters in Progressive Neural Networks (Rusu et al., 2016) to normalize the pre-trained parameters:
Wout = a ∗ tanh(b ∗Win) (9)
where a and b are scalar variables to be learnt and Win is a pre-trained input parameter.",4.3.3 Shared Components,[0],[0]
We put adapter layers after every pre-trained parameter connecting to a finetuned parameter such as on the connections from pre-trained embeddings to reviewer layer.,4.3.3 Shared Components,[0],[0]
"We use dropout (Srivastava et al., 2014) regularization on both inputs and outputs of the reviewer cells.",4.3.3 Shared Components,[0],[0]
"As described in Section 2, we evaluate our models on the WikiReading task.",5 Experimental Evaluation,[0],[0]
"In Section 3.5 we presented results for the supervised SWEAR on the full WikiReading dataset, establishing it as the highest-scoring method so far developed for WikiReading.",5 Experimental Evaluation,[0],[0]
"We now compare our semisupervised models SWEAR-MLR and SWEAR-
PR over various subsets of the WikiReading dataset, using SWEAR as a baseline.",5 Experimental Evaluation,[0],[0]
"Following Hewlett et al. (2016), we use the Mean F1 metric for WikiReading, which assigns partial credit when there are multiple valid answers.",5.1 Experimental Setup,[0],[0]
"We ran hyperparameter tuning for all models and report the result for the configuration with the highest Mean F1 on the validation set.
",5.1 Experimental Setup,[0],[0]
The supervised SWEAR model was trained on both the full training (results reported in Section 3.5) and on each subset of training data (results reported below).,5.1 Experimental Setup,[0],[0]
Unsupervised autoencoders were trained on all documents in the WikiReading training set.,5.1 Experimental Setup,[0],[0]
We selected the autoencoder with the lowest reconstruction error for use in semi-supervised experiments.,5.1 Experimental Setup,[0],[0]
"After initialization with weights from the best autoencoder, learnable parameters in the semi-supervised models were trained exactly as in the supervised model.
",5.1 Experimental Setup,[0],[0]
Training Details,5.1 Experimental Setup,[0],[0]
"We implemented all models in a shared framework in TensorFlow (Abadi et al., 2016).",5.1 Experimental Setup,[0],[0]
"We used the Adam optimizer (Kingma and Ba, 2014) for all training, periodically halving the learning rate according to a hyperparameter.",5.1 Experimental Setup,[0],[0]
"Models were trained for a maximum of 4 epochs.
",5.1 Experimental Setup,[0],[0]
"Table 7 shows which hyperparameters were tuned for each type of model, and the range of values for each hyperparameter.",5.1 Experimental Setup,[0],[0]
The parameters in the second group of the table are tuned for supervised SWEAR and the best setting (shown in bold) was used for other models where applicable.,5.1 Experimental Setup,[0],[0]
We fixed the batch size to 8 for autoencoders and 64 for semi-supervised models.,5.1 Experimental Setup,[0],[0]
We used a truncated normal distribution with a standard deviation of 0.01 for VRAE.,5.1 Experimental Setup,[0],[0]
"2Initialization with Word2Vec (Mikolov et al., 2013) embeddings on 1% subset gives 64.0 Mean F1 score.
",5.2 Results and Discussion,[0],[0]
Table 4 and 5 show the results of SWEAR and semi-supervised models with pretrained and fixed embeddings.,5.2 Results and Discussion,[0],[0]
"Results show that SWEAR-SS always improves over SWEAR at small data sizes, with the difference become dramatic as the dataset becomes very small.",5.2 Results and Discussion,[0],[0]
VRAE pretraining yields the best performance.,5.2 Results and Discussion,[0],[0]
"As training and testing datasets have different distributions in perproperty subsets, Mean F1 for supervised and semi-supervised models drops compared to uniform sampling.",5.2 Results and Discussion,[0],[0]
"However, initialization with pretrained VRAE model leads to a substantial improvement on both subsamples.",5.2 Results and Discussion,[0],[0]
"We further experimented by initializing the decoder (vs. only the encoder) with pretrained autoencoder weights but this resulted in a lower Mean F1.
",5.2 Results and Discussion,[0],[0]
Table 6 shows the results of semi-supervised reviewer models.,5.2 Results and Discussion,[0],[0]
"When trained on 1% of the training data, SWEAR-MLR and the supervised SWEAR model perform similarly.",5.2 Results and Discussion,[0],[0]
"Without using skip connections between embedding and hidden layers, the performance drops.",5.2 Results and Discussion,[0],[0]
"The SWEARPR model further improves Mean F1 and outperforms the strongest SWEAR-SS model, even without fine-tuning the weights initialized from the autoencoder.
",5.2 Results and Discussion,[0],[0]
"The success of SWEAR-PR rests on multiple design elements working together, as shown by the reduced performance caused by altering or disabling them.",5.2 Results and Discussion,[0],[0]
"Using dropout only on the inputs, or not using any dropout on reviewer cells, causes a substantial decrease in Mean F1 score (by 1.1 and 1.9, respectively).",5.2 Results and Discussion,[0],[0]
"Configuring the model with many more review steps (15) but with
a smaller hidden vector size (128) reduced Mean F1 to 62.5.",5.2 Results and Discussion,[0],[0]
Increasing the number of review steps for the question to 5 caused a decrease in Mean F1 of 2.1.,5.2 Results and Discussion,[0],[0]
Our model architecture is one of many hierarchical models for documents proposed in the literature.,6 Related Work,[0],[0]
"The most similar is proposed by Choi et al. (2017), which uses a coarse-to-fine approach of first encoding each sentence with a cheap BoW or Conv model, then selecting the top k sentences to form a mini-document which is then processed by a standard seq2seq model.",6 Related Work,[0],[0]
"While they also evaluate their approach on WikiReading, their emphasis is on efficiency rather than model accuracy, with the resulting model performing slightly worse than the full seq2seq model but taking much less time to execute.",6 Related Work,[0],[0]
"SWEAR also requires fewer sequential steps than the document length but still computes at least as many recurrent steps in parallel.
",6 Related Work,[0],[0]
"Our model can also be viewed as containing a Memory Network (MemNet) built from a document (Weston et al., 2014; Sukhbaatar et al., 2015), where the memories are the window encodings.",6 Related Work,[0],[0]
"The core MemNet operation consists of attention over a set of vectors (memories) based on a query encoding, and then reduction of a second set of vectors by weighted sum based on the attention weights.",6 Related Work,[0],[0]
"In particular, Miller et al. (2016) intro-
duce the Key-Value MemNet where the two sets of memories are computed from the keys and values of a map, respectively:",6 Related Work,[0],[0]
"In their QA task, each memory entry consists of a potential answer (the value) and its context bag of words (the key).
",6 Related Work,[0],[0]
"Our reviewer approach is inspired by “Encode, Review, Decode” approach introduced by Yang et al. (2016), which showed the value of introducing additional computation steps between the encoder and decoder in a seq2seq model.
",6 Related Work,[0],[0]
"The basic recurrent autoencoder was first introduced by Dai et al. (2015), a standard seq2seq model with the same input and output.",6 Related Work,[0],[0]
"Fabius et al. (2014) expanded this model into the Variational Recurrent Autoencoder (VRAE), which we describe in Section 4.1.1.",6 Related Work,[0],[0]
"VRAE is an application of the general idea of variational autoencoding, which applies variational approximation to the posterior to reconstruct the input (Kingma and Welling, 2013).",6 Related Work,[0],[0]
"While we train window autoencoders, an alternative approach is hierarchical document autoencoders (Li et al., 2015).
",6 Related Work,[0],[0]
The semi-supervised approach of initializing the weights of an RNN encoder with those of a recurrent autoencoder was first studied by Dai et al. (2015) in the context of document classification and further studied by Ramachandran et al. (2016) for traditional sequence-to-sequence tasks such as machine translation.,6 Related Work,[0],[0]
Our baseline semisupervised model can be viewed as an extension of these approaches to a reading comprehension setting.,6 Related Work,[0],[0]
"Dai et al. (2015) also explore initialization from a language model, but find that the recurrent autoencoder is superior, which is why we do not consider language models in this work.",6 Related Work,[0],[0]
"We have demonstrated the efficacy of the SWEAR architecture, reaching state of the art performance on supervised WikiReading.",7 Conclusions,[0],[0]
The model improves the extraction of precise information from long documents over the baseline seq2seq model.,7 Conclusions,[0],[0]
"In a semi-supervised setting, our method of reusing (V)RAE encodings in a reading comprehension framework is effective, with SWEAR-PR reaching an accuracy of 66.5 on 1% of the dataset against last year’s state of the art of 71.8 using the full dataset.",7 Conclusions,[0],[0]
"However, these methods require careful configuration and tuning to succeed, and making them more robust presents an excellent opportunity for future work.",7 Conclusions,[0],[0]
We introduce a hierarchical architecture for machine reading capable of extracting precise information from long documents.,abstractText,[0],[0]
"The model divides the document into small, overlapping windows and encodes all windows in parallel with an RNN.",abstractText,[0],[0]
"It then attends over these window encodings, reducing them to a single encoding, which is decoded into an answer using a sequence decoder.",abstractText,[0],[0]
This hierarchical approach allows the model to scale to longer documents without increasing the number of sequential steps.,abstractText,[0],[0]
"In a supervised setting, our model achieves state of the art accuracy of 76.8 on the WikiReading dataset.",abstractText,[0],[0]
"We also evaluate the model in a semi-supervised setting by downsampling the WikiReading training set to create increasingly smaller amounts of supervision, while leaving the full unlabeled document corpus to train a sequence autoencoder on document windows.",abstractText,[0],[0]
"We evaluate models that can reuse autoencoder states and outputs without finetuning their weights, allowing for more efficient training and inference.",abstractText,[0],[0]
Accurate Supervised and Semi-Supervised Machine Reading for Long Documents,title,[0],[0]
"Methods for reasoning and making decisions under uncertainty are an important building block of accurate, reliable, and interpretable machine learning systems.",1. Introduction,[0],[0]
In many applications — ranging from supply chain planning to medical diagnosis to autonomous driving — faithfully assessing uncertainty can be as important as obtaining high accuracy.,1. Introduction,[0],[0]
"This paper explores uncertainty estimation over continuous variables in the context of modern deep learning models.
",1. Introduction,[0],[0]
"Bayesian approaches provide a general framework for dealing with uncertainty (Gal, 2016).",1. Introduction,[0],[0]
"Bayesian methods define a probability distribution over model parameters and derive uncertainty estimates by intergrating over all possi-
1Stanford University, Stanford, California 2Afresh Technologies, San Francisco, California.",1. Introduction,[0],[0]
"Correspondence to: Volodymyr Kuleshov <volodymyr@afreshtechnologies.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
ble model weights.,1. Introduction,[0],[0]
"Recent advances in variational inference have greatly increased the scalability and usefulness of these approaches (Blundell et al., 2015).
",1. Introduction,[0],[0]
"In practice, however, Bayesian uncertainty estimates often fail to capture the true data distribution (Lakshminarayanan et al., 2017) — e.g., a 90% posterior credible interval generally does not contain the true outcome 90% of the time (Figure 1).",1. Introduction,[0],[0]
"In such cases, we say that the model is miscalibrated.",1. Introduction,[0],[0]
"This problem arises because of model bias: a predictor may not be sufficiently expressive to assign the right probability to every credible interval, just as it may not be able to always assign the right label to a datapoint.
",1. Introduction,[0],[0]
"Recently, Gal et al. (2017) and Lakshminarayanan et al. (2017) proposed uncertainty estimation techniques for deep neural networks, which include ensemble methods, heteroscedastic regression, and concrete dropout.",1. Introduction,[0],[0]
These methods require modifying the model and may not always produce perfectly calibrated forecasts.,1. Introduction,[0],[0]
"Calibration has been extensively studied in the weather forecasting literature
(Gneiting and Raftery, 2005); however these techniques tend to be specialized and difficult to generalize beyond applications in climate science.
",1. Introduction,[0],[0]
An alternative way to calibrate models has been explored in the support vector classification literature.,1. Introduction,[0],[0]
"These techniques — of which Platt scaling (Platt, 1999) is the most well-known — recalibrate the predictions of a pre-trained classifier in a post-processing step.",1. Introduction,[0],[0]
"As a result, these methods are classifier-agnostic and also typically very simple.
",1. Introduction,[0],[0]
"Here, we propose a new procedure for recalibrating any regression algorithm that is inspired by Platt scaling for classification.",1. Introduction,[0],[0]
"When applied to Bayesian and probabilistic deep learning models, it always produces calibrated credible intervals given a sufficient amount of i.i.d. data.
",1. Introduction,[0],[0]
"We evaluate our proposed algorithm on a range of Bayesian models, including Bayesian linear regression as well as feedforward and recurrent Bayesian neural networks.",1. Introduction,[0],[0]
"Our method consistently produces well-calibrated confidence estimates, which are in turn useful for several tasks in time series forecasting and model-based reinforcement learning.
Contributions.",1. Introduction,[0],[0]
"In summary, we introduce a simple technique for recalibrating the output of any regression algorithm, extending recalibration methods such as Platt scaling that were previously applicable only to classification.",1. Introduction,[0],[0]
We then use this technique to solve an important problem in Bayesian deep learning: the miscalibration of credible intervals.,1. Introduction,[0],[0]
We show that our results are useful in time series forecasting and in model-based reinforcement learning.,1. Introduction,[0],[0]
"This section is a concise overview of calibrated classification (Platt, 1999), and offers a reinterpretation of existing
techniques that will be useful for deriving an extension to the regression and Bayesian settings in the next section.
",2. Calibrated Classification,[0],[0]
Notation.,2. Calibrated Classification,[0],[0]
"We are given a labeled dataset xt, yt ∈ X ×Y for t = 1, 2, ..., T of i.i.d. realizations of random variables X,Y ∼ P, where P is the data distribution.",2. Calibrated Classification,[0],[0]
"Given xt, a forecaster H : X → (Y → [0, 1]) outputs a probability distribution Ft(y) targeting the label yt.",2. Calibrated Classification,[0],[0]
"When Y is continuous, Ft is a cumulative probability distribution (CDF).",2. Calibrated Classification,[0],[0]
"In this section, we assume for simplicity that Y = {0, 1}.",2. Calibrated Classification,[0],[0]
"Intuitively, calibration means that whenever a forecaster assigns a probability of 0.8 to an event, that event should occur about 80% of the time.",2.1. Calibration,[0],[0]
"In binary classification, we have Y = {0, 1}, and we say that H is calibrated if∑T
t=1 ytI{H(xt)",2.1. Calibration,[0],[0]
= p}∑T t=1 I{H(xt),2.1. Calibration,[0],[0]
= p} → p for all p ∈,2.1. Calibration,[0],[0]
"[0, 1] (1)
as T → ∞. Here, for simplicity, we use H(xt) to denote the probability of the event yt = 1.",2.1. Calibration,[0],[0]
"When the xt, yt are i.i.d.",2.1. Calibration,[0],[0]
"realizations of random variables X,Y ∼ P, a sufficient condition for calibration is:
P(Y = 1 | H(X)",2.1. Calibration,[0],[0]
= p) = p for all p ∈,2.1. Calibration,[0],[0]
"[0, 1].",2.1. Calibration,[0],[0]
"(2)
Calibration vs. Sharpness.",2.1. Calibration,[0],[0]
"By itself, calibration is not enough to guarantee a useful forecast.",2.1. Calibration,[0],[0]
"For example, a forecaster that always predicts E[Y ] is calibrated , but not very useful.",2.1. Calibration,[0],[0]
"Good predictions also need to be sharp, which intuitively means that probabilities should be close to zero or one.",2.1. Calibration,[0],[0]
Note that an ideal forecaster is both calibrated and predicts outcomes with 100% confidence.,2.1. Calibration,[0],[0]
"Most classification algorithms — including logistic regression, Naive Bayes, and support vector machines (SVMs) — are not calibrated out-of-the-box.",2.2. Training Calibrated Classifiers,[0],[0]
Recalibration methods train an auxiliary model R :,2.2. Training Calibrated Classifiers,[0],[0]
"[0, 1] → [0, 1] on top of a pre-trained forecaster H such that R ◦H is calibrated.
Estimating a Probability Distribution.",2.2. Training Calibrated Classifiers,[0],[0]
"When the xt, yt are sampled i.i.d.",2.2. Training Calibrated Classifiers,[0],[0]
"from P, choosing R(p)",2.2. Training Calibrated Classifiers,[0],[0]
"= P(Y = 1 | H(X) = p) yields a well-calibrated classifier R ◦ H , according to the definition in Equation 2.",2.2. Training Calibrated Classifiers,[0],[0]
"Thus, recalibration can be framed as estimating the above conditional density.
",2.2. Training Calibrated Classifiers,[0],[0]
"Platt scaling (Platt, 1999) — one of the most widely used recalibration techniques — can be seen as approximating P(Y = 1 | H(X) = p) with a sigmoid (a valid assumption, in practice, when dealing with SVMs).",2.2. Training Calibrated Classifiers,[0],[0]
"Other recalibration methods fit this density with isotonic regression or kernel density estimation.
Projections and Features.",2.2. Training Calibrated Classifiers,[0],[0]
A base classifier H : X → Φ may also output features φ ∈,2.2. Training Calibrated Classifiers,[0],[0]
Φ ⊆ Rd that do not correspond to probabilities.,2.2. Training Calibrated Classifiers,[0],[0]
"For example, an SVM outputs the margin between xt and the separating hyperplane.",2.2. Training Calibrated Classifiers,[0],[0]
Such non-probabilistic H can be similarly recalibrated by fitting R : Φ→,2.2. Training Calibrated Classifiers,[0],[0]
"[0, 1] to P(Y = 1 | H(X) = φ) (see Figure 2).
",2.2. Training Calibrated Classifiers,[0],[0]
"To gain further intuition, note thatH can be seen as projecting the xt into a low-dimensional space Φ (e.g., the SVM margin) such that the data is well separated in Φ. The recalibrator R :",2.2. Training Calibrated Classifiers,[0],[0]
"Φ → [0, 1] then performs density estimation to learn the Bayes-optimal classifier P(Y = 1 | H(X) = φ).",2.2. Training Calibrated Classifiers,[0],[0]
"When φ is low-dimensional, this is tractable; furthermore R ◦H is accurate because the classes Y are well-separated in φ.",2.2. Training Calibrated Classifiers,[0],[0]
"Because P(Y = 1 | H(X) = φ) is Bayes-optimal, R ◦H is also calibrated.
",2.2. Training Calibrated Classifiers,[0],[0]
Diagnostic Tools.,2.2. Training Calibrated Classifiers,[0],[0]
The calibration of a classifier is typically assessed using calibration curves (Figure 2).,2.2. Training Calibrated Classifiers,[0],[0]
"Given a dataset {(xt, yt)}Tt=1, let pt = H(xt) ∈",2.2. Training Calibrated Classifiers,[0],[0]
"[0, 1] be the forecasted probability.",2.2. Training Calibrated Classifiers,[0],[0]
"We group the pt into intervals Ij for j = 1, 2, ...,m that form a partition of [0, 1] (e.g., [0, 0.1], (0.1, 0.2], etc.).",2.2. Training Calibrated Classifiers,[0],[0]
A calibration curve plots the predicted average pj = T−1j ∑ t:pt∈Ij pt in each interval,2.2. Training Calibrated Classifiers,[0],[0]
"Ij against the
observed empirical average pj = T−1j ∑
t:pt∈Ij yt, where Tj = |{t : pt ∈ Ij}|.",2.2. Training Calibrated Classifiers,[0],[0]
"Perfect calibration corresponds to a straight line.
",2.2. Training Calibrated Classifiers,[0],[0]
We can also assess sharpness by looking at the distribution of model predictions.,2.2. Training Calibrated Classifiers,[0],[0]
"When forecasts are sharp, most predictions are close to 0 or 1; unsharp forecasters make predictions closer to 0.5.",2.2. Training Calibrated Classifiers,[0],[0]
"In this section, we extend recalibration methods for classification to to regression (Y = R), and apply the resulting algorithm to Bayesian deep learning models.",3. Calibrated Regression,[0],[0]
"Recall that in regression, the forecaster H outputs at each step t a CDF Ft targeting yt.",3. Calibrated Regression,[0],[0]
We will use F−1t :,3. Calibrated Regression,[0],[0]
"[0, 1] → Y to denote the quantile function F−1t (p) = inf{y : p ≤ Ft(y)}.",3. Calibrated Regression,[0],[0]
"Intuitively, in a regression setting, calibration means than yt should fall in a 90% confidence interval approximately 90% of the time.",3.1. Calibration,[0],[0]
"Formally, we say that the forecaster H is calibrated if∑T
t=1",3.1. Calibration,[0],[0]
"I{yt ≤ F −1 t (p)}
T → p for all p ∈",3.1. Calibration,[0],[0]
"[0, 1] (3)
as T →∞.",3.1. Calibration,[0],[0]
"In other words, the empirical and the predicted CDFs should match as the dataset size goes to infinity.
",3.1. Calibration,[0],[0]
"When the xt, yt are i.i.d.",3.1. Calibration,[0],[0]
"realizations of random variables X,Y ∼ P, a sufficient condition for this is
P(Y ≤",3.1. Calibration,[0],[0]
F−1X (p)),3.1. Calibration,[0],[0]
= p for all p ∈,3.1. Calibration,[0],[0]
"[0, 1], (4)
where we use FX = H(X) to denote the forecast at X .",3.1. Calibration,[0],[0]
"This formulation is related to the notion of probabilistic calibration of Gneiting et al. (2007).
",3.1. Calibration,[0],[0]
Note that our definition also implies that∑T t=1 I{F −1,3.1. Calibration,[0],[0]
t (p1) ≤ yt ≤,3.1. Calibration,[0],[0]
"F−1t (p2)}
T → p2 − p1 (5)
for all p1, p2 ∈",3.1. Calibration,[0],[0]
"[0, 1] as T → ∞.",3.1. Calibration,[0],[0]
"This extends our notion of calibration to general confidence intervals.
",3.1. Calibration,[0],[0]
Calibration and Sharpness.,3.1. Calibration,[0],[0]
"As in classification, calibration by itself is not sufficient to produce a useful forecast.",3.1. Calibration,[0],[0]
"For example, it is easy to see that the forecast F (y) =",3.1. Calibration,[0],[0]
"P(Y ≤ y) is calibrated; however it does even account for the features X and thus cannot be accurate.
",3.1. Calibration,[0],[0]
"In order to be useful, forecasts must also be sharp.",3.1. Calibration,[0],[0]
"In a regression context, this means that the confidence intervals should all be as tight as possible around a single value.",3.1. Calibration,[0],[0]
"More formally, we want the variance var(Ft) of the random variable whose CDF is Ft to be small.",3.1. Calibration,[0],[0]
We propose a simple recalibration scheme for producing calibrated forecasts that is closely inspired by classification techniques such as Platt scaling.,3.2. Training Calibrated Regression Models,[0],[0]
"Given a pre-trained forecaster H , we train an auxiliary model R :",3.2. Training Calibrated Regression Models,[0],[0]
"[0, 1] → [0, 1] such that the forecasts R ◦",3.2. Training Calibrated Regression Models,[0],[0]
"Ft are calibrated (Algorithm 1).
",3.2. Training Calibrated Regression Models,[0],[0]
"This approach is simple, produces calibrated forecasts given enough i.i.d.",3.2. Training Calibrated Regression Models,[0],[0]
"data, and can be applied to any regression model, including recent Bayesian deep learning algorithms.",3.2. Training Calibrated Regression Models,[0],[0]
"Existing methods (Gal et al., 2017; Lakshminarayanan et al., 2017) require modifying the forecaster and may not produce calibrated forecasts even given large amounts of data.
",3.2. Training Calibrated Regression Models,[0],[0]
Algorithm 1 Recalibration of Regression Models.,3.2. Training Calibrated Regression Models,[0],[0]
Input: Uncalibrated model H : X → (Y →,3.2. Training Calibrated Regression Models,[0],[0]
"[0, 1]) and calibration set S = {(xt, yt)}Tt=1.",3.2. Training Calibrated Regression Models,[0],[0]
Output: Auxiliary recalibration model R :,3.2. Training Calibrated Regression Models,[0],[0]
"[0, 1]→",3.2. Training Calibrated Regression Models,[0],[0]
"[0, 1].
1.",3.2. Training Calibrated Regression Models,[0],[0]
"Construct a recalibration dataset: D = {( [H(xt)](yt), P̂ ([H(xt)](yt)) )}T
t=1 ,
where
P̂ (p) =",3.2. Training Calibrated Regression Models,[0],[0]
"|{yt | [H(xt)](yt) ≤ p, t = 1, ..., T}|/T.
2.",3.2. Training Calibrated Regression Models,[0],[0]
"Train a model R (e.g., isotonic regression) on D.
Estimating a Probability Distribution.",3.2. Training Calibrated Regression Models,[0],[0]
Note that setting every forecast Ft to R ◦,3.2. Training Calibrated Regression Models,[0],[0]
Ft where R(p) := P(Y ≤,3.2. Training Calibrated Regression Models,[0],[0]
F−1X (p)) yields a perfectly calibrated forecaster according to the definition in Equation 4.,3.2. Training Calibrated Regression Models,[0],[0]
"Thus, recalibration can be formulated as estimating the above cumulative probability distribution.",3.2. Training Calibrated Regression Models,[0],[0]
"This is similar to the classification setting, where we needed to estimate the conditional density P(Y = 1 | H(X) = p).
",3.2. Training Calibrated Regression Models,[0],[0]
"The intuition behind recalibration is that for any confidence level p, we may estimate from data the true probability P(Y ≤",3.2. Training Calibrated Regression Models,[0],[0]
"F−1X (p)) of a random Y falling in the credible region (−∞, F−1X (p)] below the p-th quantile of FX .",3.2. Training Calibrated Regression Models,[0],[0]
"For example, we may count the fraction of points (xt, yt) in a dataset that have this property or fit a regressor R :",3.2. Training Calibrated Regression Models,[0],[0]
"[0, 1] → [0, 1] to P(Y ≤ F−1X (p)), such that R(p) estimates this probability for every p.",3.2. Training Calibrated Regression Models,[0],[0]
"Then, given a new forecast F , we may adjust the predicted probability F (y) for the credible interval (−∞, y] to the true calibrated probability estimated empirically from data and given by R ◦ F (y).",3.2. Training Calibrated Regression Models,[0],[0]
"For example, if p = 95%, but only 80/100 observed yt fall below the 95% quantile of Ft, then we adjust the 95% quantile to 80% (see Figure 3).
",3.2. Training Calibrated Regression Models,[0],[0]
"Specifically, given a dataset {(xt, yt)}Tt=1, we may learn P(Y ≤",3.2. Training Calibrated Regression Models,[0],[0]
F−1X (p)),3.2. Training Calibrated Regression Models,[0],[0]
"by fitting any regression algorithm to the recalibration set defined by {Ft(yt), P̂ (Ft(yt))}Tt=1, where
P̂ (p) = |{yt | Ft(yt) ≤ p, t = 1, ..., T}|
T (6)
denotes the fraction of the data for which yt lies below the p-th quantile of Ft.
We recommend using isotonic regression (Niculescu-Mizil and Caruana, 2005) as the regression model on this dataset.",3.2. Training Calibrated Regression Models,[0],[0]
This method accounts for the fact that the true function P(Y ≤,3.2. Training Calibrated Regression Models,[0],[0]
"F−1X (p)) is monotonically increasing; it is also non-parametric, hence can learn the true distribution given enough i.i.d.",3.2. Training Calibrated Regression Models,[0],[0]
"data.
",3.2. Training Calibrated Regression Models,[0],[0]
"As in classifier recalibration, it is advisable to fit R on a separate calibration set in order to reduce overfitting.",3.2. Training Calibrated Regression Models,[0],[0]
"Alternatively, one may break the data into K folds, and train K models in a way that is reminiscent of cross-validation: the hold-out fold serves as the calibration set and the model is trained on the remaining folds; at prediction time, the output is the average of the K models.",3.2. Training Calibrated Regression Models,[0],[0]
Probabilistic forecasts Ft are often obtained using Bayesian methods such as Bayesian neural networks or Gaussian processes.,3.3. Recalibrating Bayesian Models,[0],[0]
"In practice, one often uses the mean and the variance µ, σ2 of dropout samples from a Bayesian neural network evaluated at xt to obtain a principled estimate of the predictive distribution over yt (Gal and Ghahramani, 2016a).",3.3. Recalibrating Bayesian Models,[0],[0]
"The result is a probabilistic forecast Ft(xt) taking the form of a Gaussian N (µ(xt), σ2(xt)).
",3.3. Recalibrating Bayesian Models,[0],[0]
"However, if the true data distribution P(Y | X) is not Gaussian, uncertainty estimates derived from the Bayesian model will not be calibrated (see Figures 1 and 3).",3.3. Recalibrating Bayesian Models,[0],[0]
"Algorithm 1 recalibrates uncertainty estimates from any blackbox Bayesian model, making them accurate and useful.",3.3. Recalibrating Bayesian Models,[0],[0]
"We may also use Algorithm 1 to recalibrate nonprobabilistic forecasters, just as Platt scaling recalibrates SVMs.",3.4. Features for Recalibration,[0],[0]
We may generalize the forecast to any increasing function F (y) :,3.4. Features for Recalibration,[0],[0]
Y → Φ where Φ ⊆ R defines a “feature” that correlates with the confidence of the classifier.,3.4. Features for Recalibration,[0],[0]
We transform features into probability estimates by fitting a recalibrator R :,3.4. Features for Recalibration,[0],[0]
Φ→,3.4. Features for Recalibration,[0],[0]
"[0, 1] the following CDF:
P(Y ≤",3.4. Features for Recalibration,[0],[0]
F−1X (φ)).,3.4. Features for Recalibration,[0],[0]
"(7)
The simplest feature φ ∈",3.4. Features for Recalibration,[0],[0]
"Φ is the distance from the mean prediction, i.e. [H(x)](y) = Fx(y) = y−µ(x), where µ(x) is any point estimate of Y .",3.4. Features for Recalibration,[0],[0]
Fitting R essentially means counting the fraction of points that lie at any given distance of µ(x).,3.4. Features for Recalibration,[0],[0]
"Interestingly, this produces calibrated probabilistic forecasts even for an arbitrary (non-probabilistic)
regressor H .",3.4. Features for Recalibration,[0],[0]
"However, confidence intervals will have the same width everywhere independently of x (e.g. Figure 4, left); this makes them less useful at identifying points x where the model is uncertain.
",3.4. Features for Recalibration,[0],[0]
A better feature should account for uncertainty as a function of x.,3.4. Features for Recalibration,[0],[0]
"For example, we may use heteroscedastic regression to directly fit a mean and standard deviation µ(x), σ(x) and use Fx(y) =",3.4. Features for Recalibration,[0],[0]
(y−µ(x))/σ(x).,3.4. Features for Recalibration,[0],[0]
Combining features can further improve the sharpness of forecasts.,3.4. Features for Recalibration,[0],[0]
"Next, we propose a set of diagnostic measures and visualizations in order to assess calibration and sharpness.
Calibration.",3.5. Diagnostic Tools,[0],[0]
We propose a calibration plot for regression inspired by the one for calibration.,3.5. Diagnostic Tools,[0],[0]
"This plot displays the true frequency of points in each confidence interval relative to the predicted fraction of points in that interval.
",3.5. Diagnostic Tools,[0],[0]
"More formally, we choose m confidence levels 0 ≤",3.5. Diagnostic Tools,[0],[0]
p1 < p2 < . . .,3.5. Diagnostic Tools,[0],[0]
"< pm ≤ 1; for each threshold pj , we compute the empirical frequency
p̂j = |{yt | Ft(yt) ≤",3.5. Diagnostic Tools,[0],[0]
"pj , t = 1, ..., T}|
T .",3.5. Diagnostic Tools,[0],[0]
"(8)
To visualize calibration, we plot {(pj , p̂j)}Mj=1; calibrated forecasts correspond to a straight line.",3.5. Diagnostic Tools,[0],[0]
"Note that for best results, the diagnostic dataset should be distinct from the calibration and training sets.
",3.5. Diagnostic Tools,[0],[0]
"Finally, we propose using the calibration error as a numerical score describing the quality of forecast calibration:
cal(F1, y1, ..., FT , yT )",3.5. Diagnostic Tools,[0],[0]
=,3.5. Diagnostic Tools,[0],[0]
m∑ j=1 wj · (pj − p̂j)2.,3.5. Diagnostic Tools,[0],[0]
"(9)
The scalars wj are weights.",3.5. Diagnostic Tools,[0],[0]
"We used wj ≡ 1 in our experiments; alternatively, choosing wj ∝ |{yt | Ft(yt) ≤",3.5. Diagnostic Tools,[0],[0]
"pj , t = 1, ..., T}| decreases the importance of intervals that contain fewer data and that are more difficult to calibrate.
Sharpness.",3.5. Diagnostic Tools,[0],[0]
We propose measuring sharpness using the variance var(Ft) of the random variable whose CDF is Ft. Low-variance predictions are tightly centered around one value.,3.5. Diagnostic Tools,[0],[0]
"A sharpness score can be defined by
sha(F1, ..., FT ) = 1
T T∑ t=1 var(Ft).",3.5. Diagnostic Tools,[0],[0]
"(10)
Note that this definition also applies to categorical variables; for a binary Y with probability mass function f , we have var(f) = f(1)(1− f(1)).",3.5. Diagnostic Tools,[0],[0]
"The latter value is not only maximized at 0 or 1, but corresponds to the “refinement” term in the classical decomposition of the Brier score (Murphy, 1973).",3.5. Diagnostic Tools,[0],[0]
Datasets.,4.1. Setup,[0],[0]
We use eight UCI datasets varying in size from 194 to 8192 examples; examples carry between 6 and 159 continuous features.,4.1. Setup,[0],[0]
"There is generally no standard train/test split, hence we randomly assign 25% of each dataset for testing, and use the rest for training.",4.1. Setup,[0],[0]
We report averages over 5 random splits.,4.1. Setup,[0],[0]
"We also perform depth estimation on the larger Make3D dataset (Saxena et al., 2009), using the setup of Kendall and Gal (2017).
",4.1. Setup,[0],[0]
We also test our method on time series forecasting and reinforcement learning tasks.,4.1. Setup,[0],[0]
We use daily grocery sales from the Corporacion Favorita Kaggle dataset; we forecast the highest-selling item (#1503844) and use data from 2014- 01-01 to 2016-05-31 in stores #1-4 for training and data from 2016-06-01 to 2016-12-31 for testing.,4.1. Setup,[0],[0]
"We use autoregressive features from the past four days as well as binary indicators for the day of the week and the week of the year.
Models.",4.1. Setup,[0],[0]
"The simplest model we study is Bayesian Ridge Regression (MacKay, 1992).",4.1. Setup,[0],[0]
The prior over the weights is a spherical Gaussian with a Gamma prior over the precision parameter.,4.1. Setup,[0],[0]
"Posterior inference can be performed in closed form because the prior is conjugate.
",4.1. Setup,[0],[0]
We also consider feedforward and recurrent neural networks and we use the dropout approximation to variational inference of Gal and Ghahramani (2016a) to produce uncalibrated forecasts.,4.1. Setup,[0],[0]
"In UCI experiments, the feedforward neural network has two layers of 128 hidden units with a dropout rate of 0.5 and parametric ReLU non-linearities.",4.1. Setup,[0],[0]
"Recurrent networks are based on a standard GRU architecture with two stacked layers and a recurrent dropout of 0.5 (Gal and Ghahramani, 2016b).",4.1. Setup,[0],[0]
"We use the DenseNet architecture of Jégou et al. (2017) for the depth regression task.
",4.1. Setup,[0],[0]
"To perform recalibration, we first fit a base model on the training set, and then use isotonic regression as the recalibrator on the same training set.",4.1. Setup,[0],[0]
"We didn’t observe significant overfitting and did not use a distinct calibration set.
",4.1. Setup,[0],[0]
Baselines.,4.1. Setup,[0],[0]
"We compare our approach against two recently proposed methods for improving the calibration of deep learning models: concrete dropout (Gal et al., 2017) and deep ensembles (Lakshminarayanan et al., 2017).",4.1. Setup,[0],[0]
"Concrete dropout is a technique for learning the dropout probabilities based on the concrete distribution (Maddison et al., 2016); we use it to replace standard dropout in our neural network models.",4.1. Setup,[0],[0]
"Discrete ensembles train multiple models with heteroscedastic regression and average their predictive distributions at runtime; in our experiments, we use an ensemble 5 instances of the same neural network that we use as the base predictor (except we add σ(x) as an output).",4.1. Setup,[0],[0]
"Table 1 reports the accuracy (in terms of mean absolute percent error) and the test set calibration error (Equation 9) of Bayesian linear regression, a dense neural network, and two baselines on eight UCI datasets.",4.2. UCI Experiments,[0],[0]
"We also report the re-
calibrated error, which is significantly lower than that of the the original model.",4.2. UCI Experiments,[0],[0]
"Concrete dropout and deep ensembles are often better calibrated than the regular neural network, but not as much as the recalibrated models.",4.2. UCI Experiments,[0],[0]
"Recalibrated forecasts achieve similar accuracies to the baselines, even though the latter have more parameters.",4.2. UCI Experiments,[0],[0]
We follow the setup of Kendall and Gal (2017).,4.3. Depth Estimation,[0],[0]
We compute per-pixel uncertainty estimates using dropout and measure calibration over all the individual pixels.,4.3. Depth Estimation,[0],[0]
We recalibrate on all the pixels in the training set.,4.3. Depth Estimation,[0],[0]
"This yields an improvement in calibration error from 5.50E-02 to 2.41E02, while preserving accuracy.",4.3. Depth Estimation,[0],[0]
The full calibration plot is given in Figure 5.,4.3. Depth Estimation,[0],[0]
"Next, we fit a recurrent network to forecast daily sales of item #1503844; we obtain mean absolute percent errors of 17.3-21.8% on the test set across the four stores.",4.4. Time Series Forecasting,[0],[0]
"In Figure 3, we show that an uncalibrated 90% confidence interval misses most of the data points; however, the recalibrated confidence interval correctly contains about 90% of the true values.
",4.4. Time Series Forecasting,[0],[0]
"Furthermore, we report in Figure 6 true and forecasted sales in store #1, as well as the calibration curves for both methods.",4.4. Time Series Forecasting,[0],[0]
"The two baselines improve on the calibration of the original model, but our recalibration technique is the only one to achieve almost perfectly calibrated forecasts.",4.4. Time Series Forecasting,[0],[0]
"Uncertainty estimation is important in reinforcement learning to balance exploration and exploitation, as well as to improve model-based planning (Ghavamzadeh et al., 2015).",4.5. Model-Based Reinforcement Learning,[0],[0]
"Here, we focus on the latter task, and show how model-based reinforcement learning can be improved by calibrating the learned transition functions between states.",4.5. Model-Based Reinforcement Learning,[0],[0]
"Our task is inventory management, a classical application of reinforcement learning (Van Roy et al., 1997): on a set of days, an agent calculates order quantities for a perishable item in order to maximize store profits; transitions between states are defined by the probabilistic demand forecasts obtained in the previous section.
",4.5. Model-Based Reinforcement Learning,[0],[0]
"We formalize this task as a Markov decision process (S,A, P,R).",4.5. Model-Based Reinforcement Learning,[0],[0]
"States s ∈ S are sets of tuples {(q, `); ` = 1, 2, ..., L}; each (q, `) indicates that the store carries q units of the item that expire in ` days (L being the maximum shelf-life).",4.5. Model-Based Reinforcement Learning,[0],[0]
Transition probabilities P are defined through the following process: on each day the store sells d units (a random quantity sampled from historical data not seen during training) which are removed from the inventory in s (items leave in a first-in first-out manner); the shelflife of the remaining items is decreased (spoiled items are thrown away).,4.5. Model-Based Reinforcement Learning,[0],[0]
Actions a ∈,4.5. Model-Based Reinforcement Learning,[0],[0]
A correspond to orders: the store receives a items with a shelf life of L before entering the next state s′.,4.5. Model-Based Reinforcement Learning,[0],[0]
"Finally, rewards are store profits, defined as sales revenue minus ordering costs.
",4.5. Model-Based Reinforcement Learning,[0],[0]
"We perform a simulation experiment on the grocery dataset: we use the Bayesian neural network from the previous section as our learned model of the environment (i.e., the state transition function).",4.5. Model-Based Reinforcement Learning,[0],[0]
We then use dynamic programming with a 14-day horizon to determine the best action at each step.,4.5. Model-Based Reinforcement Learning,[0],[0]
"We evaluate the agent on the test portion of the Kaggle dataset, using the historical sales to define the state transitions.",4.5. Model-Based Reinforcement Learning,[0],[0]
"Item prices and costs are set to 1.99 and 1.29 respectively; items can be ordered three days a week in packs of 12 and arrive on the next day; the shelflife of new items is always five days.
",4.5. Model-Based Reinforcement Learning,[0],[0]
Table 2 shows the results.,4.5. Model-Based Reinforcement Learning,[0],[0]
A calibrated state transition model allows the agent to better plan its actions and obtain a higher reward.,4.5. Model-Based Reinforcement Learning,[0],[0]
This suggests that our method is useful for planning in model-based reinforcement learning.,4.5. Model-Based Reinforcement Learning,[0],[0]
Calibrated Bayesian Forecasts.,5. Discussion,[0],[0]
Our work proposes techniques for adjusting Bayesian models in a way that matches true empirical frequencies.,5. Discussion,[0],[0]
This approach mixes frequentist and Bayesian ideas; a pure Bayesian would have instead defined a single model family and tried integrating over all possible models.,5. Discussion,[0],[0]
"Instead, we take an approach reminiscent of model criticism techniques (Box and Hunter, 1962) such as posterior predictive checking and its variants (Gelman and Hill, 2007; Kucukelbir et al., 2017).",5. Discussion,[0],[0]
"We fit a model to a dataset, and compare its predictions to real-world data, making adjustments if necessary.
",5. Discussion,[0],[0]
"Interestingly, our work suggests that a fully probabilistic model is not necessary to obtain confidence estimates: we may instead use simple features such as the distance from a point forecast.",5. Discussion,[0],[0]
"The advantage of more complex models is to provide a richer raw signal about their uncertainty, which may be recalibrated into sharper forecasts (Figure 4).
",5. Discussion,[0],[0]
Probabilistic Forecasting.,5. Discussion,[0],[0]
Gneiting et al. (2007) proposed several notions of calibration for continuous variables; our definition is most closely related to his concept of probabilistic calibration.,5. Discussion,[0],[0]
"The main difference is that Gneiting et al. (2007) define it relative to latent generative distributions, whereas our most general definition only considers empirical frequencies.",5. Discussion,[0],[0]
"We found that their notion of marginal calibration was too weak for our purposes, since it only preserves guarantees relative to the average distribution of the yt (which may be too high-variance to be useful).
",5. Discussion,[0],[0]
Gneiting et al. (2007) also proposed that a probabilistic forecast should maximize sharpness subject to calibration.,5. Discussion,[0],[0]
"Interestingly, we take somewhat of an opposite approach: given a pre-trained model, we maximize for its calibration.
",5. Discussion,[0],[0]
This form of recalibration preserves the accuracy of point estimates from the model.,5. Discussion,[0],[0]
"A recalibrated estimate of the median (or any other quantile) only becomes worse if there is insufficient data for recalibration, or there is a shift in the data distribution.",5. Discussion,[0],[0]
"However, the forecasts may become less sharp if the original forecaster was underestimating its uncertainty and producing credible intervals that were too tight around the mean.
Applications.",5. Discussion,[0],[0]
"Calibrated confidence estimates have been extensively studied by practitioners in medicine (Jiang et al., 2012), meteorology (Raftery et al., 2005), natural language processing (Nguyen and O’Connor, 2015), and other fields.",5. Discussion,[0],[0]
"Confidence estimates for continuous variables are important in computer vision applications, such as depth estimation (Kendall and Gal, 2017).",5. Discussion,[0],[0]
"Calibrated probabilities offer significant improvements over ordinary uncertainty estimates because they correspond to real-world empirical frequencies, and hence are interpretable.",5. Discussion,[0],[0]
Calibrated Classification.,6. Previous Work,[0],[0]
"In the binary classification setting, Platt scaling (Platt, 1999) and isotonic regression (Niculescu-Mizil and Caruana, 2005) are effective and widely used to perform recalibration.",6. Previous Work,[0],[0]
"They admit extensions to the multi-class setting (Zadrozny and Elkan, 2002) and to structured prediction (Kuleshov and Liang, 2015).",6. Previous Work,[0],[0]
"They have also been studied in the context of modern neural networks (Guo et al., 2017; Gal et al., 2017; Lakshminarayanan et al., 2017).",6. Previous Work,[0],[0]
Recently Kuleshov and Ermon (2017) proposed methods that can quantify uncertainty via calibrated probabilities without any i.i.d.,6. Previous Work,[0],[0]
"assumptions on the data, allowing inputs to potentially be chosen by a malicious adversary.
",6. Previous Work,[0],[0]
Probabilistic Forecasting.,6. Previous Work,[0],[0]
"The study of calibration originates in the statistics literature (Murphy, 1973; Dawid, 1984), mainly in the context of evaluating probabilistic forecasts using proper loss functions (Gneiting and Raftery, 2007).",6. Previous Work,[0],[0]
"Proper losses decompose into a calibration and sharpness term (Murphy, 1973); this decomposition also extends to probabilistic forecasts (Hersbach, 2000).",6. Previous Work,[0],[0]
"Dawid (1984) also studied calibration in a Bayesian framework.
",6. Previous Work,[0],[0]
"More recently, calibration has been studied in the literature on probabilistic forecasting, especially in the context of meteorology (Gneiting and Raftery, 2005).",6. Previous Work,[0],[0]
"This resulted in specialized calibration systems (Raftery et al., 2005).",6. Previous Work,[0],[0]
"Although most work on calibration focuses on classification, Gneiting et al. (2007) proposed several definitions of calibration for continuous variables.",6. Previous Work,[0],[0]
Their paper does not explore techniques for generating calibrated forecasts; we focus on the study of such algorithms in our work.,6. Previous Work,[0],[0]
"In summary, our paper formalized a notion of calibration for continuous variables, drawing close connections to work in calibrated classification.",7. Conclusion,[0],[0]
"Inspired by these methods, we proposed a simple recalibration technique that generates calibrated probabilistic forecasts given enough i.i.d. data.",7. Conclusion,[0],[0]
"Furthermore, we introduced visualizations and metrics for evaluating calibration and sharpness.",7. Conclusion,[0],[0]
"Finally, we demonstrated the practical importance of calibration by applying our method to Bayesian neural networks.",7. Conclusion,[0],[0]
"Our method consistently produces well-calibrated uncertainty estimates; this result is useful in time series forecasting, reinforcement learning, as well as more generally to construct reliable, interpretable, and interactive machine learning systems.",7. Conclusion,[0],[0]
Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems.,abstractText,[0],[0]
Bayesian methods provide a general framework to quantify uncertainty.,abstractText,[0],[0]
"However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate — for example, a 90% credible interval may not contain the true outcome 90% of the time.",abstractText,[0],[0]
"Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data.",abstractText,[0],[0]
Our procedure is inspired by Platt scaling and extends previous work on classification.,abstractText,[0],[0]
"We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.",abstractText,[0],[0]
Accurate Uncertainties for Deep Learning Using Calibrated Regression,title,[0],[0]
"An active learner is given a model class Θ, a large sample of unlabeled data drawn from an underlying distribution Px and access to a labeling oracle O which can provide a label for any of the unlabeled instances.",1. Introduction,[0],[0]
"The goal of the learner is to find a model θ ∈ Θ that fits the data to a given accuracy while making as few label queries to the oracle as possible.
",1. Introduction,[0],[0]
"There has been a lot of theoretical literature on active learning, most of which has been in the context of binary classification in the PAC model (Balcan et al., 2009; Hanneke, 2007; Dasgupta et al., 2007; Beygelzimer et al., 2009; Awasthi et al., 2014; Zhang and Chaudhuri, 2014).",1. Introduction,[0],[0]
"For classification, the problem is known to be particularly difficult when there is no perfect classifier in the class that best fits the labeled data induced by the oracle’s responses.",1. Introduction,[0],[0]
"Prior work in the PAC model has shown that the difficulty of the problem is alleviated when the “noise” is more benign – for
Authors listed in the alphabetical order 1University of California, San Diego 2Microsoft Research, India.",1. Introduction,[0],[0]
"Correspondence to: Nagarajan Natarajan <t-nanata@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
example, when there is a ground truth classifier that induces a labeling and the oracle’s responses are perturbed versions of these labels (Hanneke, 2007; Awasthi et al., 2014; Zhang and Chaudhuri, 2014; Awasthi et al., 2016) corrupted by certain kinds of noise.",1. Introduction,[0],[0]
"In particular, significant improvements in label complexity have been obtained under what is known as the Tsybakov noise conditions, which model the realistic case of noise that decreases as we move further from the decision boundary.
",1. Introduction,[0],[0]
The case of active learning under regression however is significantly less well-understood.,1. Introduction,[0],[0]
"In particular, we only have a theoretical understanding of the two extreme cases – no noise (as in, no model mismatch) and arbitrary model mismatch.",1. Introduction,[0],[0]
"Chaudhuri et al. (2015) show that allowing the learner to actively select instances for labeling under regression with no model mismatch can only improve the convergence rates by a constant factor; moreover, in many natural cases, such as when the unlabeled data is drawn from a uniform Gaussian, there is no improvement.",1. Introduction,[0],[0]
"Sabato and Munos (2014) look at the other extreme case – when arbitrary model mismatch is allowed – and provide an algorithm that attempts to “learn” the locations of the mismatch through increasingly refined partitions of the space, and then learn a model accordingly.",1. Introduction,[0],[0]
"However if the model mismatch is allowed to be arbitrary, then this algorithm either requires an extremely refined partition leading to a very high running time, or a large number of labels.",1. Introduction,[0],[0]
"More recently, Anava and Mannor (2016) study an online learning approach for estimating heteroscedastic variances and provide general task-dependent regret bounds, but not exact parameter recovery gaurantees.
",1. Introduction,[0],[0]
In this paper we take a step towards closing this gap in understanding by considering active regression under a realistic yet more benign “noise” model – when the variance of the label noise depends linearly on the example,1. Introduction,[0],[0]
"x. Specifically, the oracle’s response on an unlabeled example x is distributed as N (⟨x,β∗⟩,σ2x) with σx = ⟨f∗, x⟩; here β∗ is the unknown vector of regression coefficients and f∗ is an unknown parameter vector.",1. Introduction,[0],[0]
"In classical statistics, this framework is called heteroscedastic regression, and is known to arise in econometric and medical applications.
",1. Introduction,[0],[0]
"While the usual least squares estimator for heteroscedastic regression is still statistically consistent, we find that
even in the passive learning case, optimal convergence rates for heteroscedastic regression are not known.",1. Introduction,[0],[0]
We thus begin with a convergence analysis of heteroscedastic regression for passive learning when the distribution Px over the unlabeled examples is a spherical Gaussian (in d dimensions).,1. Introduction,[0],[0]
"We show that even in this very simple case, the usual least squares estimator is sub-optimal, even when the noise model f∗ is known to the learner.",1. Introduction,[0],[0]
"Instead, we propose a weighted least squares estimator, and show that its rate of convergence is Õ ( ∥f∗∥2(1/n + d2/n2) )",1. Introduction,[0],[0]
"when the noise model is known, and Õ ( ∥f∗∥2(d/n) ) when it needs to be estimated from the data; here, n denotes the number of labeled examples used to obtain the estimator.",1. Introduction,[0],[0]
"The latter matches the convergence rates of the least squares estimator for the usual homoskedastic linear regression, where ∥f∗∥2 plays the role of the variance σ2.
",1. Introduction,[0],[0]
We next turn to active heteroscedastic regression and propose a two-stage active estimator.,1. Introduction,[0],[0]
"We show that when the noise model is known, the convergence rate of active heteroscedastic regression is Õ ( ∥f∗∥2(1/n+d2/n4) ) , a small improvement over passive.",1. Introduction,[0],[0]
"However, in the more realistic case where the noise model is unknown, the rates become O ( ∥f∗∥2(1/n+d2/n2) ) , which improves over the passive estimator by a factor of d. Our results extend to the case when the distribution Px over unlabeled examples is an arbitrary Gaussian with covariance matrix Σ and the norm used is the Σ norm.",1. Introduction,[0],[0]
"Our work illustrates that just like binary classification, even a partial knowledge of the nature of the model mismatch significantly helps the label complexity of active learning.
",1. Introduction,[0],[0]
Our work is just a first step towards a study of active maximum likelihood estimation under controlled yet realistic forms of noise.,1. Introduction,[0],[0]
There are several avenues for future work.,1. Introduction,[0],[0]
"For simplicity, the convergence bounds we present relate to the case when the distribution Px is a Gaussian.",1. Introduction,[0],[0]
"An open problem is to combine our techniques with the techniques of (Chaudhuri et al., 2015) and establish convergence rates for general unlabeled distributions.",1. Introduction,[0],[0]
"Another interesting line of future work is to come up with other, realistic noise models that apply to maximum likelihood estimation problems such as regression and logistic regression, and determine when active learning can help under these noise models.",1. Introduction,[0],[0]
Summary of our main results in this work is given in Table 1.,1. Introduction,[0],[0]
We conclude the paper presenting simulations supporting our theoretical bounds as well as experiments on real-world data.,1. Introduction,[0],[0]
Let x denote instances in Rd.,2. Problem Setup and Preliminaries,[0],[0]
Let Px denote a fixed unknown distribution over instances x.,2. Problem Setup and Preliminaries,[0],[0]
"The response y is gen-
erated according to the model: y = ⟨β∗,x⟩ + ηx, where ηx denotes instance-dependent corruption, and β∗ is the ground-truth parameter.",2. Problem Setup and Preliminaries,[0],[0]
"In this work, we consider the following heteroscedastic model:
ηx ∼ N (0,σ2(x)) , (1)
with a standard parametric model for heteroscedastic noise given by a linear model:
ηx ∼ N (0, ⟨f∗,x⟩2) , (2)
for some unknown f∗ ̸= β∗.",2. Problem Setup and Preliminaries,[0],[0]
Each response is independently corrupted via (2).,2. Problem Setup and Preliminaries,[0],[0]
"The goal is to recover β∗ using instances drawn from Px and their responses sampled from N (⟨β∗,x⟩, ⟨f∗,x⟩2).",2. Problem Setup and Preliminaries,[0],[0]
Remark 1.,2. Problem Setup and Preliminaries,[0],[0]
The noise ηx can be sampled from any subGaussian distribution with E[ηx] = 0 and bounded second moment E[η2x] ≤ σ2 (for some constant σ).,2. Problem Setup and Preliminaries,[0],[0]
"For simplicity, we will consider the Gaussian model (1).
",2. Problem Setup and Preliminaries,[0],[0]
Our approach is based on maximum likelihood estimator (MLE) for regression.,2. Problem Setup and Preliminaries,[0],[0]
"In the homoscedastic setting (i.e. σ(x)2 = σ for all x in (1)), MLE is known to give minimax optimal rates1.",2. Problem Setup and Preliminaries,[0],[0]
"The standard least squares estimator computed on n iid training instances (xi, yi), i = 1, 2, . . .",2. Problem Setup and Preliminaries,[0],[0]
", n is given by:
β̂LS =
( n∑
i=1
xix T i
)−1",2. Problem Setup and Preliminaries,[0],[0]
"n∑
i=1
xiyi , (3)
and is the solution to the minimization problem:
β̂LS = argmin β
n∑
i=1
(⟨β,xi⟩",2. Problem Setup and Preliminaries,[0],[0]
"− yi)2.
",2. Problem Setup and Preliminaries,[0],[0]
"In the heteroscedastic setting, it is easy to show that the standard least squares estimator is consistent.",2. Problem Setup and Preliminaries,[0],[0]
Remark 2.,2. Problem Setup and Preliminaries,[0],[0]
"Standard least squares estimator is consistent for the heteroscedastic noise model (2): Assuming xi ∈ Rd, i = 1, 2, . . .",2. Problem Setup and Preliminaries,[0],[0]
", n are drawn iid from the standard multivariate Gaussian, we have the rate:
∥β̂LS",2. Problem Setup and Preliminaries,[0],[0]
"− β∗∥22 = O ( ∥f∗∥2 d
n
) .
",2. Problem Setup and Preliminaries,[0],[0]
"1A notable exception is the Stein’s estimator that may do better for high dimensional spaces (Stein et al., 1956)
",2. Problem Setup and Preliminaries,[0],[0]
"While the estimator (3) is consistent, it does not exploit the knowledge of the noise model, and does not give better rates even when the noise model f∗ is known exactly.",2. Problem Setup and Preliminaries,[0],[0]
"We look at the maximum likelihood estimator for the heteroscedastic noise (1); which is given by the weighted least squares estimator (or sometimes referred to as generalized least squares estimator):
β̂GLS",2. Problem Setup and Preliminaries,[0],[0]
"=
( n∑
i=1
wixix T i
)−1",2. Problem Setup and Preliminaries,[0],[0]
"n∑
i=1
wixiyi, (4)
where wi = 1σ(xi)2 .",2. Problem Setup and Preliminaries,[0],[0]
"When the weights are known, it has been shown that the weighted estimator is the “correct” estimator to study; in particular, it is the minimum variance unbiased linear estimator (Theorem 10.7, Greene (2002)).",2. Problem Setup and Preliminaries,[0],[0]
"However, we do not know of strong learning rate guarantees for the weighted least squares model in general, or in particular for the model (2), compared to the ordinary least squares estimator.",2. Problem Setup and Preliminaries,[0],[0]
"This raises two important questions for which we provide answers in the subsequent sections.
1.",2. Problem Setup and Preliminaries,[0],[0]
"What are the rates of convergence of the maximum likelihood estimator for the heteroscedastic model when the noise model, aka, f∗ is unknown?
2.",2. Problem Setup and Preliminaries,[0],[0]
"Can we achieve a better label requirement via active learning?
",2. Problem Setup and Preliminaries,[0],[0]
The problem is formally stated as follows.,2. Problem Setup and Preliminaries,[0],[0]
"Given a set of m instances U = {x1,x2, . . .",2. Problem Setup and Preliminaries,[0],[0]
",xm} sampled i.i.d.",2. Problem Setup and Preliminaries,[0],[0]
"from the underlying Px, a label budget n ≤ m, and access to label oracle O that generates responses yi according to the heteroscedastic noise model (2), we want an estimator β̂ of the regression model parameter β∗ such that the estimation error is small, i.e. ∥β̂ − β∗∥2 ≤ O(ϵ).",2. Problem Setup and Preliminaries,[0],[0]
Remark 3.,2. Problem Setup and Preliminaries,[0],[0]
"Existing active regression methods (Sabato and Munos, 2014; Chaudhuri et al., 2015) do not consider the heteroscedastic noise model.",2. Problem Setup and Preliminaries,[0],[0]
"Note that when f∗ is known exactly, one can reduce heteroscedastic model to a homoscedastic model, by scaling instances x and their responses by 1/⟨f∗,x⟩.",2. Problem Setup and Preliminaries,[0],[0]
"However, we still may not be able to apply the existing active learning results to the transformed problem, as the modified data distribution may no longer satisfy required nice properties.",2. Problem Setup and Preliminaries,[0],[0]
"The resulting estimators do not yield advantages over passive learning, even in simple cases such as when Px is spherical Gaussian.
Notation.",2. Problem Setup and Preliminaries,[0],[0]
Id denotes the identity matrix of size d.,2. Problem Setup and Preliminaries,[0],[0]
We use bold letters to denote vectors and capital letters to denote matrices.,2. Problem Setup and Preliminaries,[0],[0]
"To motivate our approach, we begin with the basic heteroscedastic setting: when f∗ is known exactly in (2).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Even
in this arguably simple setting, the rates for passive and active learning are a priori not clear, and the exercise turns out to be non-trivial.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The results and the analysis here help gain insights into label complexities achievable via passive and active learning strategies.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"In the standard (passive) learning setting, we sample n instances uniformly from the set U and compute the maximum likelihood estimator given in (4) with weights set to wi = 1/⟨f∗,xi⟩2.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
The procedure is given in Algorithm 1.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The resulting estimator is unbiased, i.e. E[β̂GLS|X] = β∗.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Let W denote the diagonal weighting matrix with Wii = wi.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
The variance of the estimator is given by: Var(β̂GLS|X) =,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
(XTWX)−1.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
The question of interest is if and when the weighted estimator β̂GLS is qualitatively better than the ordinary least squares estimator β̂LS.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The following theorem shows that the variance of the latter, and in turn the estimation error, can be potentially much larger; and in particular, the difference between their estimation errors is at least a factor of dimensionality d. Theorem 1 (Passive Regression With Noise Oracle).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Let β̂GLS denote the estimator in (4) (or the output of Algorithm 1) where xi ∼ N (0, Id) i.i.d., with label budget n ≥ 2d ln d and β̂LS denote the ordinary least squares estimator (3).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
There exist positive constants C ′,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"and c ≥ 1 such that, with probability at least 1 − dnc , both the statements hold:
∥β̂GLS",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
− β∗∥22 ≤ C ′∥f∗∥2,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"( 1
n +
d2 lnn
n2
) ,
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"∥β̂LS − β∗∥22 = Θ ( ∥f∗∥2 d
n
) .
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Remark 4.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"We present the results for instances sampled from N (0, Id) for clarity.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
The estimation error bounds can be naturally extended to the case of Gaussian distribution with arbitrary covariance matrix Σ.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"In this case, the bounds (in Theorem 1, for example) continue to hold for the estimation error measured w.r.t.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Σ, i.e. (β̂ − β∗)TΣ(β̂ − β∗).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Furthermore, with some calculations, we can obtain analogous bounds for sub-Gaussian distributions, with distribution-specific constants featuring in the resulting bounds.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Remark 5.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"In Theorem 1, when n > d, d2/n2 term is the lower-order term, and thus, up to constants, the error of the weighted least squares estimator is at most ∥f∗∥2(1/n)",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
while that of the ordinary least squares estimator is at least ∥f∗∥2(d/n).,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Thus, if the noise model is known, the weighted least squares estimator can give a factor of d improvement in convergence rate.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Remark 6 (Technical challenges).,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The proofs of key results in this paper involve controlling quantities such as sum of ratios of Gaussian random variables, ratios of chi-squared
random variables, etc. which do not even have expectation, let alone higher moments; so, standard concentration arguments cannot be made.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"However, in many of these cases, we can show that our error bounds hold with sufficiently high probability.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
The following lemma is key to showing Theorem 1; the proof sketch illustrates some of the aforementioned technical challenges.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Unlike typical results in this domain, which bound tr(A−1) by providing concentration bounds for A, we bound tr(A−1) by providing lower bound on each eigenvalue of A.
Lemma 1.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Let X ∈ Rn×d where the rows xi are sampled i.i.d.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"from N (0, Id).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Assume n,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
≥ 2d,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
ln d.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Let W denote a diagonal matrix, with Wii = 1/⟨xi, f⟩2, for fixed f ∈ Rd.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Let σ1 ≥ σ2 ≥ · · · ≥ σd denote the eigenvalues of XTWX .,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Then, with probability at least (1− dnc ):
1. σd(XTWX) ≥ n∥f∥2 and
2.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
σi(XTWX) ≥ C ′ n 2,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"d∥f∥2 lnn for i = 1, 2, . . .",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
", d− 1,
where c ≥ 1 and C ′",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"> 0 are constants.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Proof.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
We give a sketch of the proof here (See Appendix B.2 for details).,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"To show a lower bound on the smallest eigenvalue, we first show that the smallest eigenvector is very close to f , with sufficiently large probability.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"To do so, we exploit the fact that the smallest eigenvalue is at most n/∥f∥2 which can be readily seen.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"For the second part, we consider the variational characterization of d− 1st singular value given by:
σd−1(X TWX) = max
U :dim(U)=d−1 min v∈U,∥v∥=1 vTXTWXv.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
We look at the particular subspace that is orthogonal to f∗ to get the desired upper bound.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
One key challenge here is controlling quantity of the form ∑n i=1,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"g2i h2i
where gi and hi are iid Gaussian variables.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"We use a blocking technique based on the magnitude of ⟨f,xi⟩, and lower bound the quantity with just the first block (as all the quantities involved are positive).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"This requires proving a bound on the order statistics of iid Gaussian random variables (Lemma 7 in Appendix A).
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Theorem 1 shows that weighting “clean” instances (i.e. ⟨f∗,x⟩ ≈ 0) much more than “noisy” instances yields a highly accurate estimator of β∗.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
But can we instead prefer querying labels on instances where we know a priori the response will be relatively noise-free?,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"This motivates a simple active learning solution — in principle, if we actually know f∗, we could query the labels of instances with low noise, and hope to get an accurate estimator.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The active
learning procedure is given in Algorithm 2.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Besides label budget n, it takes another parameter τ as input, which is a threshold on the noise level.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"We state the convergence for Algorithm 2 below:
Theorem 2 (Active Regression with Noise Oracle).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Consider the output estimator β̂ of Algorithm 2, with input label budget n ≥ 2d",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"ln d, unlabeled set U with |U| = m and xi ∼ N (0, Id) i.i.d., and τ = 2n/m. Then, with probability at least 1− 1/nc:
∥β̂ − β∗∥22 ≤ C ′∥f∗∥2 ( 1
n +
d2 lnn
m2
) ,
for some constants",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
c > 1 and C ′,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"> 0.
Remark 7.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
We observe that the estimation error via active learning (Theorem 2) goes to 1/n as the size of the unlabeled examples m becomes larger.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Note that O(1/n) is the error for 1-dimensional problem and is much better than d2/n2 we get from uniform sampling.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Remark 8.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"If we have m ≥ n2 unlabeled samples, then we observe that active learning (Theorem 2) achieves a better convergence rate compared to that of passive learning (Theorem 1) — the lower order term in case of active learning is O( d 2
n4 ) versus passive learning which is O( d2
n2 ).",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The convergence is superior especially when n < d2 (as we also observe in simulations).
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"The proof of Theorem 2 relies on two lemmas stated below.
",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Lemma 2.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Let X ∈ Rn×d denote the design matrix whose rows xi are sampled from U such that they satisfy |⟨xi, f⟩| ≤ ∥f∥τ for fixed f ∈ Rd.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Assume n,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
≥ 2d,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
ln d. Let σ1 ≥ σ2 ≥ . .,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
.σd,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
denote the eigenvalues of XTX .,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Then, with probability at least (1− 1nc ):
1. σd(XTX) ≥ nτ2,
2.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
σi(XTX),3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"≥ 12n, for i = 1, 2, . . .",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
", d− 1,
for some constants C > 0 and c ≥ 1.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
Lemma 3.,3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"For each xi ∈ U , where |U| = m, define gi = ⟨xi, z⟩, for any fixed z ∈ Rd.",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"Then, with probability at least exp(−mτ 3
4∥z∥2 ):
∣∣∣∣ { i : |gi| ≤ ∥z∥τ }∣∣∣∣ ≥ mτ
2 .",3. Basic Idea: Noise Model is Known Exactly,[0],[0]
"In this section, we will first show that we can obtain a consistent estimator of f∗, as long as we have a reasonably good estimate of β∗. Let β0 denote the ordinary least
Algorithm 1 Passive Regression With Noise Oracle Input: Labeling oracle O, instances U = {xi, i ∈",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"[m]}, label budget n, noise model f∗ 1.",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
Choose a subset L of size n from U uniformly at random from U .,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
Query their labels using O. 2.,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"Estimate β̂ using (4) on L, with wi = 1/⟨f∗,xi⟩2.",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"Output: β̂.
Algorithm 2 Active Regression With Noise Oracle Input: Labeling oracle O, noise model f∗, instances U = {xi, i ∈",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"[m]}, label budget n, noise tolerance τ .",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"1. Choose a subset L of size at most n from U with expected noise up to the given tolerance τ , i.e. for all xi ∈ L, |⟨xi, f∗⟩| ≤ τ .",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
Query their labels using O. 2.,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
Estimate β̂,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"as β̂ = (XTWX)−1XTWy where X ∈ Rn×d and y ∈ Rn, and W is a diagonal matrix with Wii =
1 ⟨f∗,xi⟩2 .",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"Output: β̂.
squares estimator of β∗, obtained by using (3), on m1 labeled instances, chosen i.i.d. from N (0, Id).",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"The largest eigenvector of the residual-weighted empirical covariance matrix given by:
Ŝ = 1
m1
m1∑
i=1
(yi − ⟨xi, β̂0⟩)2xixTi .",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"(5)
gives a sufficiently good estimate of f∗.",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
This is established formally in the following lemma.,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
Lemma 4.,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
Let m1 = Ω(d log3 d).,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"Then, with probability at least 1 − 1
m51 , the largest eigenvector f̂ of Ŝ in (5)
converges to f∗:
∥f̂ − f∗∥22 ≤ C1E[∥β0",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"− β∗∥22] +O ( d
m1
) ,
for some positive constant C1, and expectation is wrt the randomness in the estimator β0.
",4. Estimating Noise: Algorithms and Guarantees,[0],[0]
We first discuss the implications of using the estimated f̂ in order to obtain the generalized least square estimator given in (4) and then present the active learning solution.,4. Estimating Noise: Algorithms and Guarantees,[0],[0]
"We now consider a simple (passive) learning algorithm for the setting where the noise model is estimated, based on the weighted least squares solution discussed in Section 3.",4.1. Weighted Least Squares,[0],[0]
"We first get a good estimator of f∗ (as in Lemma 4), and then obtain the weighted least squares estimator: β̂ =",4.1. Weighted Least Squares,[0],[0]
"(XT ŴX)−1XT Ŵy, where Ŵ is the diagonal matrix of inverse noise variances obtained using the estimate f̂ with a small additive offset γ.",4.1. Weighted Least Squares,[0],[0]
The procedure is presented in Algorithm 3.,4.1. Weighted Least Squares,[0],[0]
Remark 9.,4.1. Weighted Least Squares,[0],[0]
"Algorithm 3 can be thought of as a special case of the well-known iterative weighted least squares (i.e. with just one iteration), that has been studied in the past (Carroll et al., 1988).
",4.1. Weighted Least Squares,[0],[0]
"It is well-known heuristic to first estimate the weights and then obtain the weighted estimator in practice; the approach has been widely in use for decades now in multiple communities including Econometrics and Bioinformatics (Harvey, 1976; Greene, 2002).",4.1. Weighted Least Squares,[0],[0]
"However, we do not know of strong convergence rates for the solution.",4.1. Weighted Least Squares,[0],[0]
"To our knowledge, the most comprehensive analysis was done by (Carroll et al., 1988).",4.1. Weighted Least Squares,[0],[0]
"Their analysis is not directly applicable to us for reasons two-fold: (i) they focus on using a maximum likelihood estimate of the parameters in the heteroscedastic noise model, and does not apply to our noise model (2), and (ii) their analysis relies the noise being smooth (for obtaining tighter Taylor series approximation).",4.1. Weighted Least Squares,[0],[0]
"More importantly, their analysis conceals a lot of significant factors in both d and n, and the resulting statements about convergence rates are not useful (See Appendix C).
",4.1. Weighted Least Squares,[0],[0]
Theorem 3.,4.1. Weighted Least Squares,[0],[0]
"Consider the output estimator β̂ of Algorithm 3, with label budget n ≥ 2d",4.1. Weighted Least Squares,[0],[0]
"ln d and offset γ2 =√
d n .",4.1. Weighted Least Squares,[0],[0]
"Then, with probability at least 1− 1/n c:
∥β̂ − β∗∥22 ≤ C∥f∗∥2 d ln4 n
n ,
for some constants C > 0 and c ≥ 1.",4.1. Weighted Least Squares,[0],[0]
Remark 10.,4.1. Weighted Least Squares,[0],[0]
Note that the above result holds for the specific choice of γ.,4.1. Weighted Least Squares,[0],[0]
"When γ = 0, we get the weighted least squares estimator analogous to the one used in Algorithm 1.",4.1. Weighted Least Squares,[0],[0]
"However, when estimating weights with γ = 0, the resulting estimator β̂ has poor convergence rate.",4.1. Weighted Least Squares,[0],[0]
"In particular, we observe empirically that the error ∥β̂ − β∗∥2 scales as O(d 3
n ).",4.1. Weighted Least Squares,[0],[0]
We now show that active learning can help overcome the inadequacy of the passive weighted least squares solution.,4.2. Active Regression,[0],[0]
"The proposed active regression algorithm, presented in Al-
gorithm 4, consists of two stages.",4.2. Active Regression,[0],[0]
"In the first stage, we obtain an estimate f̂ similar to that in Algorithm 3.",4.2. Active Regression,[0],[0]
"Note that if f̂ is indeed very close to f∗, f̂ serves as a good proxy for selecting instances whose labels are nearly noise-free.",4.2. Active Regression,[0],[0]
"To this end, we sample instances that have sufficiently small noise: |⟨f̂ ,x⟩| ≤ τ , where τ is an input parameter to the algorithm.",4.2. Active Regression,[0],[0]
"If f̂ is exact, then the algorithm reduces to the strategy outlined in Algorithm 2.",4.2. Active Regression,[0],[0]
Our algorithm follows the strategy of using a single-round of interaction (in light of the analysis presented in the passive learning setting) to achieve a good estimate of the underlying β∗ akin to the active MLE estimation algorithm studied by Chaudhuri et al. (2015).,4.2. Active Regression,[0],[0]
Lemma 5.,4.2. Active Regression,[0],[0]
Let f̂ denote an estimator of f∗ satisfying ∥f̂,4.2. Active Regression,[0],[0]
− f∗∥2 ≤,4.2. Active Regression,[0],[0]
"∆. For a given τ , let L denote a set of |L| ≥ 2d log d instances sampled from m unlabeled instances U , such that |⟨f̂ ,xi⟩| ≤",4.2. Active Regression,[0],[0]
"τ , for all xi ∈ L, and let yi denote their corresponding labels.",4.2. Active Regression,[0],[0]
"Consider the ordinary least squares estimator obtained using L, i.e.:
β̂ =
( ∑
xi∈L xix
T i
)−1",4.2. Active Regression,[0],[0]
"∑
xi∈L xiyi .
",4.2. Active Regression,[0],[0]
"Then, with probability at least 1− 1nc :
",4.2. Active Regression,[0],[0]
"∥β̂ − β∗∥22 ≤ C∥f∗∥2(τ2 +∆2) ( 1
mτ3 + d− 1 mτ
) .
",4.2. Active Regression,[0],[0]
for some constants C > 0 and c ≥ 1.,4.2. Active Regression,[0],[0]
Remark 11.,4.2. Active Regression,[0],[0]
"The bound in the above theorem recovers the known variance case discussed in Theorem 2, where the estimation error ∆2 = 0 and the choice of τ = 2nm .
",4.2. Active Regression,[0],[0]
"Compared to the passive learning error bound in Theorem 3, we hope to get leverage — as long we can choose τ sufficiently small, and yet guarantee that the number of samples m2 in Step 4 of Algorithm 4 is sufficiently large.",4.2. Active Regression,[0],[0]
"The following theorem shows that this is indeed the case, and that the proposed active learning solution achieves optimal learning rate.",4.2. Active Regression,[0],[0]
Theorem 4 (Active Regression with Noise Estimation).,4.2. Active Regression,[0],[0]
"Consider the output estimator β̂ of Algorithm 4, with input label budget n, unlabeled examples m ≥ n2, m1 = n2 and τ = 2 √ d n .",4.2. Active Regression,[0],[0]
"Then, we have, with probability at least 1− 1/nc:
∥β̂ − β∗∥22 ≤ C∥f∗∥2 ( 1
n +
d2
n2
) .
",4.2. Active Regression,[0],[0]
for some constants C > 0 and c > 1.,4.2. Active Regression,[0],[0]
Remark 12.,4.2. Active Regression,[0],[0]
"We observe that active learning (Theorem 4) has the same convergence rate for sufficiently large n, as that of the case when f∗ is known exactly (Theorem 2).",4.2. Active Regression,[0],[0]
"Note that d2/n2 and d2/m2 are lower-order terms in the compared bounds.
",4.2. Active Regression,[0],[0]
Remark 13.,4.2. Active Regression,[0],[0]
"Unlike in the case when noise model was known (Theorem 2), here we can not do better even with infinite unlabeled examples.",4.2. Active Regression,[0],[0]
"The source of trouble is the estimation error in f̂ , so beyond a point even active learning does not provide improvement.",4.2. Active Regression,[0],[0]
"Note that we do not compute weighted least squares estimator in the final step of Algorithm 4 unlike in Algorithm 2, for the same reason.",4.2. Active Regression,[0],[0]
We now present simulations that support the convergence bounds developed in this work.,5. Simulations,[0],[0]
The setup is as follows.,5. Simulations,[0],[0]
"We sample unlabeled instances x1,x2, . . .",5. Simulations,[0],[0]
",xm from N (0, Id).",5. Simulations,[0],[0]
"Labels are generated according to the heteroscedastic model: yi = ⟨β∗,xi⟩ + gi⟨f∗,xi⟩, where gi are iid standard Gaussian random variables.",5. Simulations,[0],[0]
We fix ∥f∗∥2 = 1 and d = 10.,5. Simulations,[0],[0]
We look at how the model estimation error (in case of Algorithms 1 and 2) ∥β̂ − β∗∥ decays as a function of the label budget n,5. Simulations,[0],[0]
(m = 2n2 for all the simulations).,5. Simulations,[0],[0]
"We also check the estimation error of the noise model in case of Algorithms 3 and 4.
",5. Simulations,[0],[0]
The results for convergence of model estimation when the noise model is known are presented in Figure 1 (a)-(d).,5. Simulations,[0],[0]
"In passive learning, the bounds in Theorem 1 suggest that when n ≤ d2, ∥β∗ − β̂∥ = O( dn ); but once n > d
2, we get a convergence of O(1/ √ n).",5. Simulations,[0],[0]
We observe that the result in Figure 1 (a) closely matches the given bounds2.,5. Simulations,[0],[0]
"In case of active learning, the bounds in Theorem 2, for the case when m ≥ n2, suggest that we get an error rate of ∥β∗ − β̂∥ = O( dn2 ).",5. Simulations,[0],[0]
We observe a similar phenomenon in the Figure 1 (b).,5. Simulations,[0],[0]
"Turning to the noise estimation setting for passive learning, we see in Figure 1 (c) that the estimation error of β∗ as well as f∗ decay as √ d/n",5. Simulations,[0],[0]
"(as suggested by Theorem 3); for active learning, we see in Figure 1 (d) that the estimation error of β∗ is noticeably better, in particular, better than that of f∗, and approaches 1/ √ n as n becomes larger than d2.
",5. Simulations,[0],[0]
"We also study the performance of the algorithms on two real-world datasets from UCI: (1) WINE QUALITY with m = 6500 and d = 11, and (2) MSD (a subset of the million song dataset) with m = 515345 and d = 90.",5. Simulations,[0],[0]
"For each dataset, we create a 70-30 train-test split, and learn the best linear regressor using ordinary least squares, which forms our β∗.",5. Simulations,[0],[0]
We then sample labels using β∗ and a simulated heteroscedastic noise f∗.,5. Simulations,[0],[0]
We compare active and passive learning algorithms on the root mean square error (RMSE) obtained on the test set.,5. Simulations,[0],[0]
"In Figure 1 (e), we see that active learning with noise estimation gives a significant reduction in RMSE early on for WINE QUALITY.",5. Simulations,[0],[0]
"We also see that weighted least squares gives slight benefit over or-
2For better resolution, we plot ∥β∗−β̂∥ rather than ∥β∗−β̂∥2 given in the theorem statements
dinary least squares.",5. Simulations,[0],[0]
"On MSD dataset 3, again we observe that our active learning algorithm consistently achieves a marginal reduction in RMSE as the number of labeled examples increases.",5. Simulations,[0],[0]
"In conclusion, we consider active regression under a heteroscedastic noise model.",6. Conclusions and Future Work,[0],[0]
"Previous work has looked at active regression either with no model mismatch (Chaudhuri et al., 2015) or arbitrary model mismatch (Sabato and Munos, 2014).",6. Conclusions and Future Work,[0],[0]
"In the first case, active learning provided no improvement even in the simple case where the unlabeled examples were drawn from Gaussians.",6. Conclusions and Future Work,[0],[0]
"In the second case, under arbitrary model mismatch, the algorithm either required a very high running time or a large number of labels.",6. Conclusions and Future Work,[0],[0]
We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression.,6. Conclusions and Future Work,[0],[0]
"Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise has the potential to lead to significant gains in the label requirement of active learning.
",6. Conclusions and Future Work,[0],[0]
There are several avenues for future work.,6. Conclusions and Future Work,[0],[0]
"For simplicity, the convergence bounds we present relate to the case when the distribution Px over unlabeled examples is a Gaussian.",6. Conclusions and Future Work,[0],[0]
"An open problem is to combine our techniques with the techniques of (Chaudhuri et al., 2015) and establish convergence rates for general unlabeled distributions.",6. Conclusions and Future Work,[0],[0]
"Another interesting line of future work is to come up with other, realistic noise models that apply to maximum likelihood estimation problems such as regression and logistic regression, and determine when active learning can help under these noise models.
",6. Conclusions and Future Work,[0],[0]
"3here, the response variable is the year of the song; we make the response mean zero in our experiments
Algorithm 3",6. Conclusions and Future Work,[0],[0]
"Least Squares with Estimated Weights Input: Labeling oracle O, unlabeled samples U = {xi, i ∈",6. Conclusions and Future Work,[0],[0]
"[m]}, label budget n, parameter m1, offset γ. 1.",6. Conclusions and Future Work,[0],[0]
Draw m1 examples uniformly at random from U and query their labels y using O. 2.,6. Conclusions and Future Work,[0],[0]
Estimate β̂0 by solving y ≈ Xβ̂0,6. Conclusions and Future Work,[0],[0]
where X ∈ Rm1×d has xi as rows and y ∈ Rm1 is the vector of labels.,6. Conclusions and Future Work,[0],[0]
3.,6. Conclusions and Future Work,[0],[0]
Draw a subset L of n examples uniformly at random from U .,6. Conclusions and Future Work,[0],[0]
Form X ∈ Rn×d and y ∈ Rn. 4.,6. Conclusions and Future Work,[0],[0]
Compute f̂ as the largest eigenvector of the residual-weighted empirical covariance given in (5).,6. Conclusions and Future Work,[0],[0]
"5. Set ŵi = 1⟨xi,f̂⟩2+γ2 ., for xi ∈",6. Conclusions and Future Work,[0],[0]
"L.
6.",6. Conclusions and Future Work,[0],[0]
Estimate β̂,6. Conclusions and Future Work,[0],[0]
"by solving: β̂ = (XT ŴX)−1XT Ŵy, where Ŵ is diagonal matrix with Ŵii = ŵi.",6. Conclusions and Future Work,[0],[0]
"Output: β̂.
Algorithm 4 Active Regression Input:",6. Conclusions and Future Work,[0],[0]
"Labeling oracle O, unlabeled samples U = {xi, i ∈",6. Conclusions and Future Work,[0],[0]
"[m]}, label budget n, parameters m1, τ .",6. Conclusions and Future Work,[0],[0]
1.,6. Conclusions and Future Work,[0],[0]
Draw m1 examples uniformly at random from U and query their labels y using O. 2.,6. Conclusions and Future Work,[0],[0]
Estimate β̂0 by solving y ≈ Xβ̂0,6. Conclusions and Future Work,[0],[0]
where X ∈ Rm1×d and y ∈ Rm1 .,6. Conclusions and Future Work,[0],[0]
3.,6. Conclusions and Future Work,[0],[0]
Compute f̂ as the largest eigenvector of Ŝ given in (5).,6. Conclusions and Future Work,[0],[0]
4. Choose a subset L of m2 = n,6. Conclusions and Future Work,[0],[0]
"− m1 instances from U with estimated noise variance up to tolerance τ2, i.e. for all xi ∈ L, |⟨xi, f̂⟩|2 ≤ τ2.",6. Conclusions and Future Work,[0],[0]
Query their labels using O. 5.,6. Conclusions and Future Work,[0],[0]
Estimate β̂,6. Conclusions and Future Work,[0],[0]
as β̂ = (XTX)−1XTy where X ∈ Rm2×d and y ∈ Rm2 .,6. Conclusions and Future Work,[0],[0]
Output: β̂.,6. Conclusions and Future Work,[0],[0]
"An active learner is given a model class Θ, a large sample of unlabeled data drawn from an underlying distribution and access to a labeling oracle that can provide a label for any of the unlabeled instances.",abstractText,[0],[0]
The goal of the learner is to find a model θ ∈ Θ that fits the data to a given accuracy while making as few label queries to the oracle as possible.,abstractText,[0],[0]
"In this work, we consider a theoretical analysis of the label requirement of active learning for regression under a heteroscedastic noise model, where the noise depends on the instance.",abstractText,[0],[0]
We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression.,abstractText,[0],[0]
"Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise can lead to significant gains in the label requirement of active learning.",abstractText,[0],[0]
Active Heteroscedastic Regression,title,[0],[0]
"different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label’s cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets.",text,[0],[0]
The field of active learning studies how to efficiently elicit relevant information so learning algorithms can make good decisions.,1 Introduction,[1.0],['The field of active learning studies how to efficiently elicit relevant information so learning algorithms can make good decisions.']
"Almost all active learning algorithms are designed for binary classification problems, leading to the natural question: How can active learning address more complex prediction problems?",1 Introduction,[1.0],"['Almost all active learning algorithms are designed for binary classification problems, leading to the natural question: How can active learning address more complex prediction problems?']"
"Multiclass and importance-weighted classification require only minor modifications but we know of no active learning algorithms that enjoy theoretical guarantees for more complex problems.
",1 Introduction,[0.9927520073057489],['Multiclass and importanceweighted classification require only minor modifications but we know of no active learning algorithms that enjoy theoretical guarantees for more complex problems.']
One such problem is cost-sensitive multiclass classification (CSMC).,1 Introduction,[1.0],['One such problem is cost-sensitive multiclass classification (CSMC).']
"In CSMC with K classes, passive learners receive input examples x and cost vectors c ∈ RK , where c(y) is the cost of predicting label y on x.1",1 Introduction,[0],[0]
"A natural design for an active CSMC learner then is to adaptively query the costs of only a (possibly empty) subset of labels on each x. Since measuring label complexity is more nuanced in CSMC (e.g., is it more expensive to query three costs on a single example or one cost on three examples?), we track both the number of examples for which at least one cost is queried, along with the total number of cost queries issued.",1 Introduction,[0],[0]
The first corresponds to a fixed human effort for inspecting the example.,1 Introduction,[0],[0]
"The second captures the additional effort for judging the cost of each prediction, which depends on the number of labels queried.",1 Introduction,[0],[0]
"(By querying a label, we mean querying the cost of predicting that label given an example.)
",1 Introduction,[0],[0]
"In this setup, we develop a new active learning algorithm for CSMC called Cost Overlapped Active Learning (COAL).",1 Introduction,[0],[0]
"COAL assumes access to a set of regression functions, and, when processing an example
akshay@cs.umass.edu, alekha@microsoft.com, tkhuang@protonmail.com, hal@umiacs.umd.edu, jcl@microsoft.com 1Cost here refers to prediction cost and not labeling effort or the cost of acquiring different labels.
",1 Introduction,[0],[0]
"ar X
iv :1
70 3.
01 01
4v 4
[ cs
.L G
] 1
1 O
x, it uses the functions with good past performance to compute the range of possible costs that each label might take.",1 Introduction,[0],[0]
"Naturally, COAL only queries labels with large cost range, akin to uncertainty-based approaches in active regression [11], but furthermore, it only queries labels that could possibly have the smallest cost, avoiding the uncertain, but surely suboptimal labels.",1 Introduction,[0],[0]
The key algorithmic innovation is an efficient way to compute the cost range realized by good regressors.,1 Introduction,[0],[0]
"This computation, and COAL as a whole, only requires that the regression functions admit efficient squared loss optimization, in contrast with prior algorithms that require 0/1 loss optimization [7, 19].
",1 Introduction,[0],[0]
"Among our results, we prove that when processing n (unlabeled) examples withK classes and a regression class with pseudo-dimension d (See Definition 1),
1.",1 Introduction,[0],[0]
The algorithm needs to solve O(Kn5) regression problems over the function class (Corollary 2).,1 Introduction,[0],[0]
"Thus COAL runs in polynomial time for convex regression sets.
2.",1 Introduction,[0],[0]
"With no assumptions on the noise in the problem, the algorithm achieves generalization error Õ( √ Kd/n)
and requests Õ(nθ2 √ Kd) costs from Õ(nθ1 √ Kd) examples (Theorems 3 and 6) where θ1, θ2 are the disagreement coefficients (Definition 2)2.",1 Introduction,[0],[0]
"The worst case offers minimal improvement over passive learning, akin to active learning for binary classification.
3.",1 Introduction,[0],[0]
"With a Massart-type noise assumption (Assumption 3), the algorithm has generalization error Õ(Kd/n) while requesting Õ(Kd(θ2 + Kθ1)",1 Introduction,[0],[0]
"log n) labels from Õ(Kdθ1 log n) examples (Corollary 4, Theorem 7).",1 Introduction,[0],[0]
"Thus under favorable conditions, COAL requests exponentially fewer labels than passive learning.
",1 Introduction,[0],[0]
We also derive generalization and label complexity bounds under a milder Tsybakov-type noise condition (Assumption 4).,1 Introduction,[0],[0]
"Existing lower bounds from binary classification [19] suggest that our results are optimal in their dependence on n, although these lower bounds do not directly apply to our setting.",1 Introduction,[0],[0]
"We also discuss some intuitive examples highlighting the benefits of using COAL.
CSMC provides a more expressive language for success and failure than multiclass classification, which allows learning algorithms to make the trade-offs necessary for good performance and broadens potential applications.",1 Introduction,[0],[0]
"For example, CSMC can naturally express partial failure in hierarchical classification [41].",1 Introduction,[0],[0]
"Experimentally, we show that COAL substantially outperforms the passive learning baseline with orders of
2Õ(·) suppresses logarithmic dependence on n, K, and d.
magnitude savings in the labeling effort on a number of hierarchical classification datasets (see Figure 1 for comparison between passive learning and COAL on Reuters text categorization).
",1 Introduction,[0],[0]
"CSMC also forms the basis of learning to avoid cascading failures in joint prediction tasks like structured prediction and reinforcement learning [15, 37, 13].",1 Introduction,[0],[0]
"As our second application, we consider learning to search algorithms for joint or structured prediction, which operate by a reduction to CSMC.",1 Introduction,[0],[0]
"In this reduction, evaluating the cost of a class often involves a computationally expensive “roll-out,” so using an active learning algorithm inside such a passive joint prediction method can lead to significant computational savings.",1 Introduction,[0],[0]
"We show that using COAL within the AGGRAVATE algorithm [37, 13] reduces the number of roll-outs by a factor of 14 to 3 4 on several joint prediction tasks.
",1 Introduction,[0],[0]
Our code is publicly available as part of the Vowpal Wabbit machine learning library.3,1 Introduction,[0],[0]
Active learning is a thriving research area with many theoretical and empirical studies.,2 Related Work,[0],[0]
We recommend the survey of Settles [39] for an overview of more empirical research.,2 Related Work,[0],[0]
"We focus here on theoretical results.
",2 Related Work,[0],[0]
"Our work falls into the framework of disagreement-based active learning, which studies general hypothesis spaces typically in an agnostic setup (see Hanneke [19] for an excellent survey).",2 Related Work,[0],[0]
"Existing results study binary classification, while our work generalizes to CSMC, assuming that we can accurately predict costs using regression functions from our class.",2 Related Work,[0],[0]
"One difference that is natural for CSMC is that our query rule checks the range of predicted costs for a label.
",2 Related Work,[0],[0]
The other main difference is that we use a square loss oracle to search the version space.,2 Related Work,[0],[0]
"In contrast, prior work either explicitly enumerates the version space",2 Related Work,[0],[0]
"[5, 46] or uses a 0/1 loss classification oracle for the search [14, 7, 8, 24].",2 Related Work,[0],[0]
"In most instantiations, the oracle solves an NP-hard problem and so does not directly lead to an efficient algorithm, although practical implementations using heuristics are still quite effective.",2 Related Work,[0],[0]
"Our approach instead uses a squared-loss regression oracle, which can be implemented efficiently via convex optimization and leads to a polynomial time algorithm.
",2 Related Work,[0],[0]
"In addition to disagreement-based approaches, much research has focused on plug-in rules for active learning in binary classification, where one estimates the class-conditional regression function [10, 32, 20, 9].",2 Related Work,[0],[0]
"Apart from Hanneke and Yang [20], these works make smoothness assumptions and have a nonparametric flavor.",2 Related Work,[0],[0]
"Instead, Hanneke and Yang [20] assume a calibrated surrogate loss and abstract realizable function class, which is more similar to our setting.",2 Related Work,[0],[0]
"While the details vary, our work and these prior results employ the same algorithmic recipe of maintaining an implicit version space and querying in a suitably-defined disagreement region.",2 Related Work,[0],[0]
"Our work has two notable differences: (1) our algorithm operates in an oracle computational model, only accessing the function class through square loss minimization problems, (2) our results apply to general CSMC, which exhibit significant differences from binary classification.",2 Related Work,[0],[0]
"See Subsection 6.1 for further discussion.
",2 Related Work,[0],[0]
"Focusing on linear representations, Balcan et al. [6], Balcan and Long [4] study active learning with distributional assumptions, while the selective sampling framework from the online learning community considers adversarial assumptions [12, 16, 33, 1].",2 Related Work,[0],[0]
"These methods use query strategies that are specialized to linear representations and do not naturally generalize to other hypothesis classes.
",2 Related Work,[0],[0]
"Supervised learning oracles that solve NP-hard optimization problems in the worst case have been used in other problems including contextual bandits [2, 43] and structured prediction [15].",2 Related Work,[0],[0]
"Thus we hope that our work can inspire new algorithms for these settings as well.
",2 Related Work,[0],[0]
"3http://hunch.net/~vw
Lastly, we mention that square loss regression has been used to estimate costs for passive CSMC",2 Related Work,[0],[0]
"[27], but, to our knowledge, using a square loss oracle for active CSMC is new.
",2 Related Work,[0],[0]
Advances over Krishnamurthy et al. [26].,2 Related Work,[0],[0]
Active learning for CSMC was introduced recently in Krishnamurthy et al. [26] with an algorithm that also uses cost ranges to decide where to query.,2 Related Work,[0],[0]
"They compute cost ranges by using the regression oracle to perform a binary search for the maximum and minimum costs, but this computation results in a sub-optimal label complexity bound.",2 Related Work,[0],[0]
We resolve this sub-optimality with a novel cost range computation that is inspired by the multiplicative weights technique for solving linear programs.,2 Related Work,[0],[0]
This algorithmic improvement also requires a significantly more sophisticated statistical analysis for which we derive a novel uniform Freedman-type inequality for classes with bounded pseudo-dimension.,2 Related Work,[0],[0]
"This result may be of independent interest.
",2 Related Work,[0],[0]
Krishnamurthy et al. [26] also introduce an online approximation for additional scalability and use this algorithm for their experiments.,2 Related Work,[0],[0]
Our empirical results use this same online approximation and are slightly more comprehensive.,2 Related Work,[0],[0]
"Finally, we also derive generalization and label complexity bounds for our algorithm in a setting inspired by Tsybakov’s low noise condition [30, 45].
",2 Related Work,[0],[0]
Comparison with Foster et al. [18].,2 Related Work,[0],[0]
"In a follow-up to the present paper, Foster et al. [18] build on our work with a regression-based approach for contextual bandit learning, a problem that bears some similarities to active learning for CSMC.",2 Related Work,[0],[0]
"The results are incomparable due to the differences in setting, but it is worth discussing their techniques.",2 Related Work,[0],[0]
"As in our paper, Foster et al. [18] maintain an implicit version space and compute maximum and minimum costs for each label, which they use to make predictions.",2 Related Work,[0],[0]
"They resolve the sub-optimality in Krishnamurthy et al. [26] with epoching, which enables a simpler cost range computation than our multiplicative weights approach.",2 Related Work,[0],[0]
"However, epoching incurs an additional log(n) factor in the label complexity, and under low-noise conditions where the overall bound is O(polylog(n)), this yields a polynomially worse guarantee than ours.",2 Related Work,[0],[0]
"We study cost-sensitive multiclass classification (CSMC) problems with K classes, where there is an instance space X , a label space Y = {1, . . .",3 Problem Setting and Notation,[0],[0]
",K}, and a distribution D supported on X ×",3 Problem Setting and Notation,[0],[0]
"[0, 1]K .4",3 Problem Setting and Notation,[0],[0]
"If (x, c) ∼ D, we refer to c as the cost-vector, where c(y) is the cost of predicting y ∈ Y .",3 Problem Setting and Notation,[0],[0]
"A classifier h : X → Y has expected cost E(x,c)∼D[c(h(x))]",3 Problem Setting and Notation,[0],[0]
"and we aim to find a classifier with minimal expected cost.
",3 Problem Setting and Notation,[0],[0]
"Let G , {g : X 7→",3 Problem Setting and Notation,[0],[0]
"[0, 1]} denote a set of base regressors and let F , GK denote a set of vector regressors where the yth coordinate of f ∈ F is written as f(·; y).",3 Problem Setting and Notation,[0],[0]
"The set of classifiers under consideration is H , {hf | f ∈ F} where each f defines a classifier hf : X 7→ Y by
hf (x) , argmin y f(x; y).",3 Problem Setting and Notation,[0],[0]
"(1)
When using a set of regression functions for a classification task, it is natural to assume that the expected costs under D can be predicted by some function in the set.",3 Problem Setting and Notation,[0],[0]
"This motivates the following realizability assumption.
",3 Problem Setting and Notation,[0],[0]
Assumption 1 (Realizability).,3 Problem Setting and Notation,[0],[0]
"Define the Bayes-optimal regressor f?, which has f?(x; y) , Ec[c(y)|x],∀x ∈ X (with D(x) > 0), y ∈ Y .",3 Problem Setting and Notation,[0],[0]
"We assume that f? ∈ F .
4In general, labels just serve as indices for the cost vector in CSMC, and the data distribution is over (x, c) pairs instead of (x, y) pairs as in binary and multiclass classification.
",3 Problem Setting and Notation,[0],[0]
"While f? is always well defined, note that the cost itself may be noisy.",3 Problem Setting and Notation,[0],[0]
"In comparison with our assumption, the existence of a zero-cost classifier inH (which is often assumed in active learning) is stronger, while the existence of hf?",3 Problem Setting and Notation,[0],[0]
"inH is weaker but has not been leveraged in active learning.
",3 Problem Setting and Notation,[0],[0]
We also require assumptions on the complexity of the class G for our statistical analysis.,3 Problem Setting and Notation,[0],[0]
"To this end, we assume that G is a compact convex subset of L∞(X ) with finite pseudo-dimension, which is a natural extension of VC-dimension for real-valued predictors.
",3 Problem Setting and Notation,[0],[0]
Definition 1 (Pseudo-dimension).,3 Problem Setting and Notation,[0],[0]
"The pseudo-dimension Pdim(F) of a function class F : X → R is defined as the VC-dimension of the set of threshold functionsH+ , {(x, ξ) 7→ 1{f(x) > ξ} :",3 Problem Setting and Notation,[0],[0]
f ∈ F} ⊂ X,3 Problem Setting and Notation,[0],[0]
"× R→ {0, 1}.
",3 Problem Setting and Notation,[0],[0]
Assumption 2.,3 Problem Setting and Notation,[0],[0]
We assume that G is a compact convex set with Pdim(G) =,3 Problem Setting and Notation,[0],[0]
"d <∞.
As an example, linear functions in some basis representation, e.g., g(x) = ∑d i=1",3 Problem Setting and Notation,[0],[0]
"wiφi(x), where weights wi are bounded in some norm, have pseudodimension d.",3 Problem Setting and Notation,[0],[0]
"In fact, our result can be stated entirely in terms of covering numbers, and we translate to pseudo-dimension using the fact that such classes have “parametric"" covering numbers of the form (1/ε)d.",3 Problem Setting and Notation,[0],[0]
"Thus, our results extend to classes with “nonparametric"" growth rates as well (e.g., Holder-smooth functions), although we focus on the parametric case for simplicity.",3 Problem Setting and Notation,[0],[0]
"Note that this is a significant departure from Krishnamurthy et al. [26], which assumed that G was finite.
",3 Problem Setting and Notation,[0],[0]
Our assumption that G is a compact convex set introduces a computational challenging of managing this infinitely large set.,3 Problem Setting and Notation,[0],[0]
"To address this challenge, we follow the trend in active learning of leveraging existing algorithmic research on supervised learning [14, 8, 7] and access G exclusively through a regression oracle.",3 Problem Setting and Notation,[0],[0]
"Given an importance-weighted dataset D = {xi, ci, wi}ni=1 where xi ∈ X , ci ∈ R, wi ∈ R+, the regression oracle computes
ORACLE(D) ∈ argmin g∈G n∑ i=1",3 Problem Setting and Notation,[0],[0]
wi(g(xi)− ci)2.,3 Problem Setting and Notation,[0],[0]
"(2)
Since we assume that G is a compact convex set it is amenable to standard convex optimization techniques, so this imposes no additional restriction.",3 Problem Setting and Notation,[0],[0]
"However, in the special case of linear functions, this optimization is just least squares and can be computed in closed form.",3 Problem Setting and Notation,[0],[0]
"Note that this is fundamentally different from prior works that use a 0/1-loss minimization oracle [14, 8, 7], which involves an NP-hard optimization in most cases of interest.
",3 Problem Setting and Notation,[0],[0]
Remark 1.,3 Problem Setting and Notation,[0],[0]
"Our assumption that G is convex is only for computational tractability, as it is crucial in the efficient implementation of our query strategy, but is not required for our generalization and label complexity bounds.",3 Problem Setting and Notation,[0],[0]
"Unfortunately recent guarantees for learning with non-convex classes [29, 36] do not immediately yield efficient active learning strategies.",3 Problem Setting and Notation,[0],[0]
"Note also that Krishnamurthy et al. [26] obtain an efficient algorithm without convexity, but this yields a suboptimal label complexity guarantee.
",3 Problem Setting and Notation,[0],[0]
"Given a set of examples and queried costs, we often restrict attention to regression functions that predict these costs well and assess the uncertainty in their predictions given a new example x. For a subset of regressors G ⊂ G, we measure uncertainty over possible cost values for x with
γ(x,G) , c+(x,G)− c−(x,G), c+(x,G) , max g∈G g(x), c−(x,G) , min g∈G g(x).",3 Problem Setting and Notation,[0],[0]
"(3)
For vector regressors F ⊂",3 Problem Setting and Notation,[0],[0]
"F , we define the cost range for a label y given x as γ(x, y, F ) , γ(x,GF (y)) where GF (y) , {f(·; y) | f ∈ F} are the base regressors induced by F for y. Note that since we are
Algorithm 1: Cost Overlapped Active Learning (COAL)
1: Input: Regressors G, failure probability δ ≤ 1/e. 2: Set ψi = 1/ √",3 Problem Setting and Notation,[0],[0]
"i, κ = 3, νn = 324(d log(n) + log(8Ke(d+ 1)n2/δ)).",3 Problem Setting and Notation,[0],[0]
"3: Set ∆i = κmin{ νni−1 , 1}. 4: for i = 1, 2, . . .",3 Problem Setting and Notation,[0],[0]
", n",3 Problem Setting and Notation,[0],[0]
"do 5: gi,y ← arg ming∈G R̂i(g; y).",3 Problem Setting and Notation,[0],[0]
(See (5)).,3 Problem Setting and Notation,[0],[0]
"6: Define fi ← {gi,y}Ky=1. 7: (Implicitly define)",3 Problem Setting and Notation,[0],[0]
Gi(y)← {g ∈ Gi−1(y),3 Problem Setting and Notation,[0],[0]
| R̂i(g; y) ≤,3 Problem Setting and Notation,[0],[0]
"R̂i(gi,y; y) + ∆i}. 8: Receive new example x. Qi(y)← 0,∀y ∈ Y .",3 Problem Setting and Notation,[0],[0]
"9: for every y ∈ Y do
10: ĉ+(y)← MAXCOST((x, y), ψi/4) and ĉ−(y)← MINCOST((x, y), ψi/4).",3 Problem Setting and Notation,[0],[0]
11: end for 12: Y ′,3 Problem Setting and Notation,[0],[0]
← {y ∈ Y | ĉ−(y) ≤ miny′ ĉ+(y′)}.,3 Problem Setting and Notation,[0],[0]
13: if |Y ′| > 1 then 14: Qi(y)← 1 if y ∈ Y ′ and ĉ+(y)− ĉ−(y),3 Problem Setting and Notation,[0],[0]
"> ψi. 15: end if 16: Query costs of each y with Qi(y) = 1. 17: end for
assuming realizability, whenever f? ∈ F , the quantities c+(x,GF (y)) and c−(x,GF (y)) provide valid upper and lower bounds on E[c(y)|x].
",3 Problem Setting and Notation,[0],[0]
"To measure the labeling effort, we track the number of examples for which even a single cost is queried as well as the total number of queries.",3 Problem Setting and Notation,[0],[0]
"This bookkeeping captures settings where the editorial effort for inspecting an example is high but each cost requires minimal further effort, as well as those where each cost requires substantial effort.",3 Problem Setting and Notation,[0],[0]
"Formally, we define Qi(y) ∈ {0, 1} to be the indicator that the algorithm queries label y on the ith example and measure
L1 , n∑ i=1",3 Problem Setting and Notation,[0],[0]
∨,3 Problem Setting and Notation,[0],[0]
"y Qi(y), and L2 , n∑ i=1",3 Problem Setting and Notation,[0],[0]
∑ y Qi(y).,3 Problem Setting and Notation,[0],[0]
(4),3 Problem Setting and Notation,[0],[0]
"The pseudocode for our algorithm, Cost Overlapped Active Learning (COAL), is given in Algorithm 1.",4 Cost Overlapped Active Learning,[0],[0]
"Given an example x, COAL queries the costs of some of the labels y for x. These costs are chosen by (1) computing a set of good regression functions based on the past data (i.e., the version space), (2) computing the range of predictions achievable by these functions for each y, and (3) querying each y that could be the best label and has substantial uncertainty.",4 Cost Overlapped Active Learning,[0],[0]
"We now detail each step.
",4 Cost Overlapped Active Learning,[0],[0]
"To compute an approximate version space we first find the regression function that minimizes the empirical risk for each label y, which at round i is:
R̂i(g; y) , 1
i− 1 i−1∑ j=1 (g(xj)− cj(y))2Qj(y).",4 Cost Overlapped Active Learning,[0],[0]
"(5)
Recall that Qj(y) is the indicator that we query label y on the jth example.",4 Cost Overlapped Active Learning,[0],[0]
Computing the minimizer requires one oracle call.,4 Cost Overlapped Active Learning,[0],[0]
"We implicitly construct the version space Gi(y) in Line 7 as the surviving regressors with low
square loss regret to the empirical risk minimizer.",4 Cost Overlapped Active Learning,[0],[0]
"The tolerance on this regret is ∆i at round i, which scales like Õ(d/i), where recall that d is the pseudo-dimension of the class G.
COAL then computes the maximum and minimum costs predicted by the version space Gi(y) on the new example x. Since the true expected cost is f?(x; y) and, as we will see, f?(·; y) ∈ Gi(y), these quantities serve as a confidence bound for this value.",4 Cost Overlapped Active Learning,[0],[0]
"The computation is done by the MAXCOST and MINCOST subroutines which produce approximations to c+(x,Gi(y)) and c−(x,Gi(y))",4 Cost Overlapped Active Learning,[0],[0]
"respectively (See (3)).
",4 Cost Overlapped Active Learning,[0],[0]
"Finally, using the predicted costs, COAL issues (possibly zero) queries.",4 Cost Overlapped Active Learning,[0],[0]
"The algorithm queries any non-dominated label that has a large cost range, where a label is non-dominated if its estimated minimum cost is smaller than the smallest maximum cost (among all other labels) and the cost range is the difference between the label’s estimated maximum and minimum costs.
",4 Cost Overlapped Active Learning,[0.9967897081996636],"['The algorithm queries any non-dominated label that has a large cost range, where a label is nondominated if its estimated minimum cost is smaller than the smallest maximum cost (among all labels) and the cost range is the difference between the label’s estimated maximum and minimum costs.']"
"Intuitively, COAL queries the cost of every label which cannot be ruled out as having the smallest cost on x, but only if there is sufficient ambiguity about the actual value of the cost.",4 Cost Overlapped Active Learning,[0],[0]
"The idea is that labels with little disagreement do not provide much information for further reducing the version space, since by construction all regressors would suffer similar square loss.",4 Cost Overlapped Active Learning,[0.9979654598093333],"['The idea is that labels with little disagreement do not provide much information for further reducing the version space, since by construction all functions would suffer similar loss.']"
"Moreover, only the labels that could be the best need to be queried at all, since the cost-sensitive performance of a hypothesis hf depends only on the label that it predicts.",4 Cost Overlapped Active Learning,[0],[0]
"Hence, labels that are dominated or have small cost range need not be queried.
",4 Cost Overlapped Active Learning,[0],[0]
"Similar query strategies have been used in prior works on binary and multiclass classification [33, 16, 1], but specialized to linear representations.",4 Cost Overlapped Active Learning,[0],[0]
"The key advantage of the linear case is that the set Gi(y) (formally, a different set with similar properties) along with the maximum and minimum costs have closed form expressions, so that the algorithms are easily implemented.",4 Cost Overlapped Active Learning,[0],[0]
"However, with a general set G and a regression oracle, computing these confidence intervals is less straightforward.",4 Cost Overlapped Active Learning,[0],[0]
"We use the MAXCOST and MINCOST subroutines, and discuss this aspect of our algorithm next.",4 Cost Overlapped Active Learning,[0],[0]
"In this section, we describe the MAXCOST subroutine which uses the regression oracle to approximate the maximum cost on label y realized by Gi(y), as defined in (3).",4.1 Efficient Computation of Cost Range,[0],[0]
"The minimum cost computation requires only minor modifications that we discuss at the end of the section.
",4.1 Efficient Computation of Cost Range,[0],[0]
Describing the algorithm requires some additional notation.,4.1 Efficient Computation of Cost Range,[0],[0]
"Let ∆̃j , ∆j + R̂j(gj,y; y) be the right hand side of the constraint defining the version space at round j, where gj,y is the ERM at round j for label y, R̂j(·; y) is the risk functional, and ∆j is the radius used in COAL.",4.1 Efficient Computation of Cost Range,[0],[0]
"Note that this quantity can be efficiently computed since gj,y can be found with a single oracle call.",4.1 Efficient Computation of Cost Range,[0],[0]
"Due to the requirement that g ∈ Gi−1(y) in the definition of Gi(y), an equivalent representation is Gi(y) = ⋂i j=1{g : R̂j(g; y) ≤ ∆̃j}.",4.1 Efficient Computation of Cost Range,[0],[0]
"Our approach is based on the observation that given an example x and a label y at round i, finding a function g ∈ Gi(y) which predicts the maximum cost for the label y on x is equivalent to solving the minimization problem:
minimizeg∈G(g(x)− 1)2 such that ∀1 ≤ j ≤",4.1 Efficient Computation of Cost Range,[0],[0]
"i, R̂j(g; y) ≤ ∆̃j .",4.1 Efficient Computation of Cost Range,[0],[0]
"(7)
Given this observation, our strategy will be to find an approximate solution to the problem (7) and it is not difficult to see that this also yields an approximate value for the maximum predicted cost on x for the label y.
In Algorithm 2, we show how to efficiently solve this program using the regression oracle.",4.1 Efficient Computation of Cost Range,[0],[0]
"We begin by exploiting the convexity of the set G, meaning that we can further rewrite the optimization problem (7) as
minimizeP∈∆(G)Eg∼P [ (g(x)− 1)2 ] such that ∀1 ≤",4.1 Efficient Computation of Cost Range,[0],[0]
j ≤,4.1 Efficient Computation of Cost Range,[0],[0]
"i,Eg∼P [ R̂j(g; y) ] ≤",4.1 Efficient Computation of Cost Range,[0],[0]
∆̃j .,4.1 Efficient Computation of Cost Range,[0],[0]
"(8)
The above rewriting is effectively cosmetic as G = ∆(G) by the definition of convexity, but the upshot is that our rewriting results in both the objective and constraints being linear in the optimization variable P .",4.1 Efficient Computation of Cost Range,[0],[0]
"Thus,
Algorithm 2: MAXCOST
1: Input: (x, y), tolerance TOL, (implicitly) risk functionals {R̂j(·; y)}ij=1. 2: Compute gj,y = argming∈G R̂j(g; y) for each j. 3: Let ∆j = κmin{ νnj−1 , 1}, ∆̃j = R̂j(gj,y; y) + ∆j for each j. 4: Initialize parameters: c` ← 0, ch ← 1, T ← log(i+1)(12/∆i) 2 TOL4 , η ← √ log(i+ 1)/T .",4.1 Efficient Computation of Cost Range,[0],[0]
5: while |c` − ch| > TOL2/2 do 6: c← ch−c`2 7: µ(1),4.1 Efficient Computation of Cost Range,[0],[0]
← 1 ∈ Ri+1.,4.1 Efficient Computation of Cost Range,[0],[0]
.,4.1 Efficient Computation of Cost Range,[0],[0]
Use MW to check feasibility of Program (9).,4.1 Efficient Computation of Cost Range,[0],[0]
"8: for t = 1, . . .",4.1 Efficient Computation of Cost Range,[0],[0]
", T do 9: Use the regression oracle to find
gt ← argmin g∈G µ (t) 0",4.1 Efficient Computation of Cost Range,[0],[0]
(g(x)− 1)2 + i∑ j=1 µ (t) j,4.1 Efficient Computation of Cost Range,[0],[0]
"R̂j(g; y) (6)
10: If the objective in (6) for gt is at least µ (t) 0",4.1 Efficient Computation of Cost Range,[0],[0]
"c+ ∑i j=1 µ (t) j ∆̃j , c` ← c, go to 5.",4.1 Efficient Computation of Cost Range,[0],[0]
"11: Update
µ (t+1) j ← µ (t) j
( 1− η ∆̃j − R̂j(gt; y)
∆j + 1
) , µ
(t+1) 0",4.1 Efficient Computation of Cost Range,[0],[0]
"← µ (t) 0
( 1− η c− (gt(x)− 1) 2
2
) .
12: end for 13: ch ← c. 14: end while 15: Return ĉ+(y) = 1− √ c`.
we effectively wish to solve a linear program in P , with our computational tool being a regression oracle over the set G. To do this, we create a series of feasibility problems, where we repeatedly guess the optimal objective value for the problem (8) and then check whether there is indeed a distribution P which satisfies all the constraints and gives the posited objective value.",4.1 Efficient Computation of Cost Range,[0],[0]
"That is, we check
?",4.1 Efficient Computation of Cost Range,[0],[0]
∃P ∈ ∆(G),4.1 Efficient Computation of Cost Range,[0],[0]
such that Eg∼P (g(x)− 1)2 ≤ c and ∀1 ≤,4.1 Efficient Computation of Cost Range,[0],[0]
j ≤,4.1 Efficient Computation of Cost Range,[0],[0]
"i,Eg∼P R̂j(g; y) ≤ ∆̃j .",4.1 Efficient Computation of Cost Range,[0],[0]
"(9)
If we find such a solution, we increase our guess, and otherwise we reduce the guess and proceed until we localize the optimal value to a small enough interval.
",4.1 Efficient Computation of Cost Range,[0],[0]
It remains to specify how to solve the feasibility problem (9).,4.1 Efficient Computation of Cost Range,[0],[0]
"Noting that this is a linear feasibility problem, we jointly invoke the Multiplicative Weights (MW) algorithm and the regression oracle in order to either find an approximately feasible solution or certify the problem as infeasible.",4.1 Efficient Computation of Cost Range,[0],[0]
MW is an iterative algorithm that maintains weights µ over the constraints.,4.1 Efficient Computation of Cost Range,[0],[0]
"At each iteration it (1) collapses the constraints into one, by taking a linear combination weighted by µ, (2) checks feasibility of the simpler problem with a single constraint, and (3) if the simpler problem is feasible, it updates the weights using the slack of the proposed solution.",4.1 Efficient Computation of Cost Range,[0],[0]
"Details of steps (1) and (3) are described in Algorithm 2.
",4.1 Efficient Computation of Cost Range,[0],[0]
"For step (2), the simpler problem that we must solve takes the form
?",4.1 Efficient Computation of Cost Range,[0],[0]
"∃P ∈ ∆(G) such that µ0Eg∼P (g(x)− 1)2 + i∑
j=1
µjEg∼P R̂j(g; y) ≤ µ0c+ i∑
j=1
µj∆̃j .
",4.1 Efficient Computation of Cost Range,[0],[0]
"This program can be solved by a single call to the regression oracle, since all terms on the left-hand-side involve square losses while the right hand side is a constant.",4.1 Efficient Computation of Cost Range,[0],[0]
Thus we can efficiently implement the MW algorithm using the regression oracle.,4.1 Efficient Computation of Cost Range,[0],[0]
"Finally, recalling that the above description is for a fixed value of objective c, and recalling that the maximum can be approximated by a binary search over c leads to an oraclebased algorithm for computing the maximum cost.",4.1 Efficient Computation of Cost Range,[0],[0]
"For this procedure, we have the following computational guarantee.
",4.1 Efficient Computation of Cost Range,[0],[0]
Theorem 1.,4.1 Efficient Computation of Cost Range,[0],[0]
Algorithm 2 returns an estimate ĉ+(x; y) such that c+(x; y) ≤ ĉ+(x; y) ≤,4.1 Efficient Computation of Cost Range,[0],[0]
"c+(x; y) + TOL and runs in polynomial time with O(max{1, i2/ν2n} log(i) log(1/TOL)/TOL4) calls to the regression oracle.
",4.1 Efficient Computation of Cost Range,[0],[0]
"The minimum cost can be estimated in exactly the same way, replacing the objective (g(x)− 1)2 with (g(x)",4.1 Efficient Computation of Cost Range,[0],[0]
− 0)2 in Program (7).,4.1 Efficient Computation of Cost Range,[0],[0]
"In COAL, we set TOL = 1/ √ i at iteration i and have νn = Õ(d).",4.1 Efficient Computation of Cost Range,[0],[0]
"As a consequence, we can bound the total oracle complexity after processing n examples.
",4.1 Efficient Computation of Cost Range,[0],[0]
Corollary 2.,4.1 Efficient Computation of Cost Range,[0],[0]
"After processing n examples, COAL makes Õ(K(d3 + n5/d2)) calls to the square loss oracle.
",4.1 Efficient Computation of Cost Range,[0],[0]
Thus COAL can be implemented in polynomial time for any set G that admits efficient square loss optimization.,4.1 Efficient Computation of Cost Range,[0],[0]
"Compared to Krishnamurthy et al. [26] which required O(n2) oracle calls, the guarantee here is, at face value, worse, since the algorithm is slower.",4.1 Efficient Computation of Cost Range,[0],[0]
"However, the algorithm enforces a much stronger constraint on the version space which leads to a much better statistical analysis, as we will discuss next.",4.1 Efficient Computation of Cost Range,[0],[0]
"Nevertheless, these algorithms that use batch square loss optimization in an iterative or sequential fashion are too computational demanding to scale to larger problems.",4.1 Efficient Computation of Cost Range,[0],[0]
"Our implementation alleviates this with an alternative heuristic approximation based on a sensitivity analysis of the oracle, which we detail in Section 7.",4.1 Efficient Computation of Cost Range,[0],[0]
"In this section, we derive generalization guarantees for COAL.",5 Generalization Analysis,[0],[0]
"We study three settings: one with minimal assumptions and two low-noise settings.
",5 Generalization Analysis,[0],[0]
"Our first low-noise assumption is related to the Massart noise condition [31], which in binary classification posits that the Bayes optimal predictor is bounded away from 1/2 for all x.",5 Generalization Analysis,[0],[0]
"Our condition generalizes this to CSMC and posits that the expected cost of the best label is separated from the expected cost of all other labels.
",5 Generalization Analysis,[0],[0]
Assumption 3.,5 Generalization Analysis,[0],[0]
"A distribution D supported over (x, c) pairs satisfies the Massart noise condition with parameter τ > 0, if for all x (with D(x) > 0),
f?(x; y?(x))",5 Generalization Analysis,[0],[0]
≤,5 Generalization Analysis,[0],[0]
min y 6=y?(x),5 Generalization Analysis,[0],[0]
"f?(x; y)− τ,
where y?(x) , argminy f ?(x; y) is the true best label for x.
The Massart noise condition describes favorable prediction problems that lead to sharper generalization and label complexity bounds for COAL.",5 Generalization Analysis,[0],[0]
"We also study a milder noise assumption, inspired by the Tsybakov condition [30, 45], again generalized to CSMC.",5 Generalization Analysis,[0],[0]
"See also Agarwal [1].
",5 Generalization Analysis,[0],[0]
Assumption 4.,5 Generalization Analysis,[0],[0]
"A distribution D supported over (x, c) pairs satisfies the Tsbyakov noise condition with parameters (τ0, α, β) if for all 0 ≤ τ ≤ τ0,
Px∼D [
min y 6=y?(x)
",5 Generalization Analysis,[0],[0]
f?(x; y)− f?(x; y?(x)),5 Generalization Analysis,[0],[0]
≤ τ ],5 Generalization Analysis,[0],[0]
"≤ βτα,
where y?(x) , argminy f ?",5 Generalization Analysis,[0],[0]
"(x; y).
Observe that the Massart noise condition in Assumption 3 is a limiting case of the Tsybakov condition, with τ = τ0 and α → ∞. The Tsybakov condition states that it is polynomially unlikely for the cost of the best label to be close to the cost of the other labels.",5 Generalization Analysis,[0],[0]
"This condition has been used in previous work on cost-sensitive active learning [1] and is also related to the condition studied by Castro and Nowak [10] with the translation that α = 1κ−1 , where κ ∈",5 Generalization Analysis,[0],[0]
"[0, 1] is their noise level.
",5 Generalization Analysis,[0],[0]
Our generalization bound is stated in terms of the noise level in the problem so that they can be readily adapted to the favorable assumptions.,5 Generalization Analysis,[0],[0]
"We define the noise level using the following quantity, given any ζ > 0.
",5 Generalization Analysis,[0],[0]
"Pζ , Px∼D [
min y 6=y?(x)
f?(x; y)− f?(x; y?(x)) ≤ ζ ] .",5 Generalization Analysis,[0],[0]
"(10)
Pζ describes the probability that the expected cost of the best label is close to the expected cost of the second best label.",5 Generalization Analysis,[0],[0]
When Pζ is small for large ζ the labels are well-separated so learning is easier.,5 Generalization Analysis,[0],[0]
"For instance, under a Massart condition Pζ = 0 for all ζ ≤ τ .
",5 Generalization Analysis,[0],[0]
"We now state our generalization guarantee.
",5 Generalization Analysis,[0],[0]
Theorem 3.,5 Generalization Analysis,[0],[0]
"For any δ < 1/e, for all i ∈",5 Generalization Analysis,[0],[0]
"[n], with probability at least 1− δ, we have
Ex,c[c(hfi+1(x))− c(hf?(x))]",5 Generalization Analysis,[0],[0]
"≤ min ζ>0
{ ζPζ +
32Kνn ζi
} ,
where νn, fi are defined in Algorithm 1, and hfi is defined in (1).",5 Generalization Analysis,[0],[0]
"In the worst case, we bound Pζ by 1 and optimize for ζ to obtain an Õ( √ Kd log(1/δ)/i) bound after i samples, where recall that d is the pseudo-dimension of G. This agrees with the standard generalization bound ofO( √ Pdim(F) log(1/δ)/i) for VC-type classes becauseF = GK hasO(Kd) statistical complexity.",5 Generalization Analysis,[0],[0]
"However, since the bound captures the difficulty of the CSMC problem as measured by Pζ , we can obtain sharper results under Assumptions 3 and 4 by appropriately setting ζ.
",5 Generalization Analysis,[0],[0]
Corollary 4.,5 Generalization Analysis,[0],[0]
"Under Assumption 3, for any δ < 1/e, with probability at least 1− δ, for all i ∈",5 Generalization Analysis,[0],[0]
"[n], we have
Ex,c[c(hfi+1(x))− c(hf?(x))]",5 Generalization Analysis,[0],[0]
"≤ 32Kνn iτ .
",5 Generalization Analysis,[0],[0]
Corollary 5.,5 Generalization Analysis,[0],[0]
"Under Assumption 4, for any δ < 1/e, with probability at least 1− δ, for all 32Kνn βτα+20 ≤ i ≤ n, we have
Ex,c[c(hfi+1(x))− c(hf?(x))]",5 Generalization Analysis,[0],[0]
"≤ 2β 1 α+2
( 32Kνn
i
)α+1 α+2
.
",5 Generalization Analysis,[0],[0]
"Thus, Massart and Tsybakov-type conditions lead to a faster convergence rate of Õ(1/n) and Õ(n− α+1 α+2 ).",5 Generalization Analysis,[0],[0]
This agrees with the literature on active learning for classification [31] and can be viewed as a generalization to CSMC.,5 Generalization Analysis,[0],[0]
"Both generalization bounds match the optimal rates for binary classification under the analogous low-noise assumptions [31, 45].",5 Generalization Analysis,[0],[0]
"We emphasize that COAL obtains these bounds as is, without changing any parameters, and hence COAL is adaptive to favorable noise conditions.",5 Generalization Analysis,[0],[0]
"Without distributional assumptions, the label complexity of COAL can be O(n), just as in the binary classification case, since there may always be confusing labels that force querying.",6 Label Complexity Analysis,[1.0],"['Without distributional assumptions, the label complexity of COAL can be O(n), just as in the binary classification case, since there may always be confusing labels that force querying.']"
"In line with prior work, we introduce two disagreement coefficients that characterize favorable distributional properties.",6 Label Complexity Analysis,[0],[0]
"We first define a set of good classifiers, the cost-sensitive regret ball:
Fcsr(r) , { f ∈ F ∣∣∣ E",6 Label Complexity Analysis,[0],[0]
[c(hf (x))− c(hf?(x)),6 Label Complexity Analysis,[0],[0]
] ≤ r} .,6 Label Complexity Analysis,[0],[0]
"We also recall our earlier notation γ(x, y, F ) (see (3) and the subsequent discussion) for a subset F ⊆ F which indicates the range of expected costs for (x, y) as predicted by the regressors corresponding to the classifiers in F .",6 Label Complexity Analysis,[0],[0]
"We now define the disagreement coefficients.
",6 Label Complexity Analysis,[0],[0]
Definition 2 (Disagreement coefficients).,6 Label Complexity Analysis,[0],[0]
"Define
DIS(r, y) , {x | ∃f, f ′ ∈ Fcsr(r), hf (x) = y 6= hf ′(x)} .
",6 Label Complexity Analysis,[0],[0]
"Then the disagreement coefficients are defined as:
θ1 , sup ψ,r>0
ψ r P (∃y | γ(x, y,Fcsr(r))",6 Label Complexity Analysis,[0],[0]
> ψ,6 Label Complexity Analysis,[0],[0]
"∧ x ∈ DIS(r, y)) ,
θ2 , sup ψ,r>0
ψ
r ∑ y P (γ(x, y,Fcsr(r))",6 Label Complexity Analysis,[0],[0]
> ψ,6 Label Complexity Analysis,[0],[0]
"∧ x ∈ DIS(r, y)) .
",6 Label Complexity Analysis,[0],[0]
"Intuitively, the conditions in both coefficients correspond to the checks on the domination and cost range of a label in Lines 12 and 14 of Algorithm 1.",6 Label Complexity Analysis,[0],[0]
"Specifically, when x ∈ DIS(r, y), there is confusion about whether y is the optimal label or not, and hence y is not dominated.",6 Label Complexity Analysis,[0],[0]
"The condition on γ(x, y,Fcsr(r)) additionally captures the fact that a small cost range provides little information, even when y is non-dominated.",6 Label Complexity Analysis,[0],[0]
"Collectively, the coefficients capture the probability of an example x where the good classifiers disagree on x in both predicted costs and labels.",6 Label Complexity Analysis,[0],[0]
"Importantly, the notion of good classifiers is via the algorithm-independent set Fcsr(r), and is only a property of F and the data distribution.
",6 Label Complexity Analysis,[0],[0]
"The definitions are a natural adaptation from binary classification [19], where a similar disagreement region to DIS(r, y) is used.",6 Label Complexity Analysis,[0],[0]
"Our definition asks for confusion about the optimality of a specific label y, which provides more detailed information about the cost-structure than simply asking for any confusion among the good classifiers.",6 Label Complexity Analysis,[0],[0]
"The 1/r scaling is in agreement with previous related definitions [19], and we also scale by the cost range parameter ψ, so that the favorable settings for active learning can be concisely expressed as having θ1, θ2 bounded, as opposed to a complex function of ψ.
",6 Label Complexity Analysis,[0],[0]
"The next three results bound the labeling effort (4), in the high noise and low noise cases respectively.",6 Label Complexity Analysis,[0],[0]
The low noise assumptions enable significantly sharper bounds.,6 Label Complexity Analysis,[0],[0]
"Before stating the bounds, we recall that L1 corresponds to the number of examples where at least one cost is queried, while L2 is the total number of costs queried across all examples.
",6 Label Complexity Analysis,[0],[0]
Theorem 6.,6 Label Complexity Analysis,[0],[0]
"With probability at least 1− δ, the label complexity of the algorithm over n examples is at most L1 = O",6 Label Complexity Analysis,[0],[0]
"( nθ1 √ Kνn + log(1/δ) ) , L2 = O ( nθ2 √ Kνn +K log(1/δ) ) .
",6 Label Complexity Analysis,[0],[0]
Theorem 7.,6 Label Complexity Analysis,[0],[0]
Assume the Massart noise condition holds.,6 Label Complexity Analysis,[0],[0]
"With probability at least 1− δ the label complexity of the algorithm over n examples is at most
L1 = O ( K log(n)νn
τ2 θ1 + log(1/δ)
) , L2 = O (KL1)
Theorem 8.",6 Label Complexity Analysis,[0],[0]
Assume the Tsybakov noise condition holds.,6 Label Complexity Analysis,[0],[0]
"With probability at least 1− δ the label complexity of the algorithm over n examples is at most
L1 = O",6 Label Complexity Analysis,[0],[0]
"( θ α α+1 1 (Kνn) α α+2n 2 α+2 + log(1/δ) ) , L2 =",6 Label Complexity Analysis,[0],[0]
"O (KL1)
",6 Label Complexity Analysis,[0],[0]
"In the high-noise case, the bounds scales with nθ for the respective coefficients.",6 Label Complexity Analysis,[0],[0]
"In comparison, for binary classification the leading term is Õ (nθerror(hf?))",6 Label Complexity Analysis,[0],[0]
which involves a different disagreement coefficient and which scales with the error of the optimal classifier hf?,6 Label Complexity Analysis,[0],[0]
"[19, 24].",6 Label Complexity Analysis,[0],[0]
"Qualitatively the bounds have similar worstcase behavior, demonstrating minimal improvement over passive learning, but by scaling with error(hf?)",6 Label Complexity Analysis,[0],[0]
the binary classification bound reflects improvements on benign instances.,6 Label Complexity Analysis,[0],[0]
"For the special case of multiclass classification, we are able to recover the dependence on error(hf?) and the standard disagreement coefficient with a simple modification to our proof, which we discuss in detail in the next subsection.
",6 Label Complexity Analysis,[0],[0]
"On the other hand, in both low noise cases the label complexity scales sublinearly with n. With bounded disagreement coefficients, this improves over the standard passive learning analysis where all labels are queried on n examples to achieve the generalization guarantees in Theorem 3, Corollary 4, and Corollary 5 respectively.",6 Label Complexity Analysis,[0],[0]
"In particular, under the Massart condition, both L1 and L2 bounds scale with θ log(n) for the respective disagreement coefficients, which is an exponential improvement over the passive learning analysis.",6 Label Complexity Analysis,[0],[0]
"Under the milder Tsybakov condition, the bounds scale with θ α α+1n 2 α+2 , which improves polynomially over passive learning.",6 Label Complexity Analysis,[0],[0]
"These label complexity bounds agree with analogous results from binary classification [10, 19, 21] in their dependence on n.
Note that θ2 ≤ Kθ1 always and it can be much smaller, as demonstrated through an example in the next section.",6 Label Complexity Analysis,[0],[0]
"In such cases, only a few labels are ever queried and the L2 bound in the high noise case reflects this additional savings over passive learning.",6 Label Complexity Analysis,[0],[0]
"Unfortunately, in low noise conditions, we do not benefit when θ2 Kθ1.",6 Label Complexity Analysis,[0],[0]
"This can be resolved by letting ψi in the algorithm depend on the noise level τ , but we prefer to use the more robust choice ψi = 1/ √ i which still allows COAL to partially adapt to low noise and achieve low label complexity.",6 Label Complexity Analysis,[0],[0]
The main improvement over Krishnamurthy et al. [26] is demonstrated in the label complexity bounds under low noise assumptions.,6 Label Complexity Analysis,[0],[0]
"For example, under Massart noise, our bound has the optimal log(n)/τ2 rate, while the bound in Krishnamurthy et al. [26] is exponentially worse, scaling with nβ/τ2 for β ∈ (0, 1).",6 Label Complexity Analysis,[0],[0]
"This improvement comes from explicitly enforcing monotonicity of the version space, so that once a regressor is eliminated it can never force COAL to query again.",6 Label Complexity Analysis,[0],[0]
"Algorithmically, computing the maximum and minimum costs with the monotonicity constraint is much more challenging and requires the new subroutine using MW.",6 Label Complexity Analysis,[0],[0]
In this subsection we show that in many cases we can obtain guarantees in terms of Hanneke’s disagreement coefficient,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"[19], which has been used extensively in active learning for binary classification.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"We also show that, for multiclass classification, the label complexity scales with the error of the optimal classifier h?, a refinement on Theorem 6.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
The guarantees require no modifications to the algorithm and enable a precise comparison with prior results.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Unfortunately, they do not apply to the general CSMC setting, so they have not been incorporated into our main theorems.
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
We start with defining Hanneke’s disagreement coefficient [19].,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Define the disagreement ball F̃(r) , {f ∈ F : P[hf (x) 6= hf?(x)]",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"≤ r} and the disagreement region D̃IS(r) , {x | ∃f, f ′ ∈ F̃(r), hf (x) 6=",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
hf ′(x)}.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"The coefficient is defined as
θ0 , sup r>0
1 r P [ x ∈ D̃IS(r) ] .",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"(11)
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"This coefficient is known to be O(1) in many cases, for example when the hypothesis class consists of linear separators and the marginal distribution is uniform over the unit sphere",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"[19, Chapter 7].",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"In comparison with Definition 2, the two differences are that θ1, θ2 include the cost-range condition and involve the cost-sensitive regret ball Fcsr(r) rather than F̃(r).",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"As F̃(r) ⊂ Fcsr(r), we expect that θ1 and θ2 are typically larger than θ0, so bounds in terms of θ0 are more desirable.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"We now show that such guarantees are possible in many cases.
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
The low noise case.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"For general CSMC, low noise conditions admit the following:
Proposition 1.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Under Massart noise, with probability at least 1− δ the label complexity of the algorithm over n examples is at most L1 = O
( log(n)νn τ2 θ0 + log(1/δ) ) .",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Under Tsybakov noise, the label complexity is
at most L1 = O ( θ0n 2 α+2 (log(n)νn) α α+2 + log(1/δ) ) .",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"In both cases we have L2 = O(KL1).
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"That is, for any low noise CSMC problem, COAL obtains a label complexity bound in terms of Hanneke’s disagreement coefficient θ0 directly.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
Note that this adaptivity requires no change to the algorithm.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
Proposition 1 enables a precise comparison with disagreement-based active learning for binary classification.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"In particular, this bound matches the guarantee for CAL",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"[19, Theorem 5.4] with the caveat that our measure of statistical complexity is the pseudodimension of the F instead of the VC-dimension of the hypothesis class.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"As a consequence, under low noise assumptions, COAL has favorable label complexity in all examples where θ0 is small.
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
The high noise case.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Outside of the low noise setting, we can introduce θ0 into our bounds, but only for multiclass classification, where we always have c , 1 − ey for some y ∈",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
[K].,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Note that f(x; y) is now interpreted as a prediction for 1 − P (y|x), so that the least cost prediction y?(x) corresponds to the most likely label.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"We also obtain a further refinement by introducing error(hf?) , E(x,c)[c(hf?(x))].
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
Proposition 2.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"For multiclass classification, with probability at least 1 − δ, the label complexity of the algorithm over n examples is at most
L1 = 4θ0n · error(hf?)",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
+O ( θ0 (√ Knνn · error(hf?),6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
+Kκνn log(n) ),6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"+ log(1/δ) ) .
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
This result exploits two properties of the multiclass cost structure.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"First we can relate Fcsr(r) to the disagreement ball F̃(r), which lets us introduce Hanneke’s disagreement coefficient θ0.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Second, we can bound Pζ in Theorem 3 in terms of error(hf?).",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Together the bound is comparable to prior results for active learning in binary classification [23, 20, 19], with a slight generalization to the multiclass setting.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"Unfortunately, both of these refinements do not apply for general CSMC.
",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
Summary.,6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"In important special cases, COAL achieves label complexity bounds directly comparable with results for active learning in binary classification, scaling with θ0 and error(hf?).",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"In such cases, whenever θ0 is bounded — for which many examples are known — COAL has favorable label complexity.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"However, in general CSMC without low-noise assumptions, we are not able to obtain a bound in terms of these quantities, and we believe a bound involving θ0 does not hold for COAL.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
"We leave understanding natural settings where θ1 and θ2 are small, or obtaining sharper guarantees as intriguing future directions.",6.1 Recovering Hanneke’s Disagreement Coefficient,[0],[0]
We now describe three examples to give more intuition for COAL and our label complexity bounds.,6.2 Three Examples,[0],[0]
"Even in the low noise case, our label complexity analysis does not demonstrate all of the potential benefits of our query rule.",6.2 Three Examples,[0],[0]
"In this section we give three examples to further demonstrate these advantages.
",6.2 Three Examples,[0],[0]
"Our first example shows the benefits of using the domination criterion in querying, in addition to the cost range condition.",6.2 Three Examples,[0],[0]
"Consider a problem under Assumption 3, where the optimal cost is predicted perfectly, the second best cost is τ worse and all the other costs are substantially worse, but with variability in the predictions.",6.2 Three Examples,[0],[0]
"Since all classifiers predict the correct label, we get θ1 = θ2 = 0, so our label complexity bound is O(1).",6.2 Three Examples,[0],[0]
"Intuitively, since every regressor is certain of the optimal label and its cost, we actually make zero queries.",6.2 Three Examples,[0],[0]
"On the other hand, all of the suboptimal labels have large cost ranges, so querying based solely on a cost range criteria, as would happen with an active regression algorithm",6.2 Three Examples,[0],[0]
"[11], leads to a large label complexity.
",6.2 Three Examples,[0],[0]
"A related example demonstrates the improvement in our query rule over more naïve approaches where we query either no label or all labels, which is the natural generalization of query rules from multiclass classification [1].",6.2 Three Examples,[0],[0]
"In the above example, if the best and second best labels are confused occasionally θ1 may be large, but we expect θ2 Kθ1 since no other label can be confused with the best.",6.2 Three Examples,[0],[0]
"Thus, the L2 bound in Theorem 6 is a factor of K smaller than with a naïve query rule since COAL only queries the best and second best labels.",6.2 Three Examples,[0],[0]
"Unfortunately, without setting ψi as a function of the noise parameters, the bounds in the low noise cases do not reflect this behavior.
",6.2 Three Examples,[0],[0]
The third example shows that both θ0 and θ1 yield pessimistic bounds on the label complexity of COAL in some cases.,6.2 Three Examples,[0],[0]
"The example is more involved, so we describe it in detail.",6.2 Three Examples,[0],[0]
"We focus on statistical issues, using a finite regressor class F .",6.2 Three Examples,[0],[0]
"Note that our results on generalization and label complexity hold in this setting, replacing d log(n) with log |F|, and the algorithm can be implemented by enumerating F .",6.2 Three Examples,[0],[0]
"Throughout this example, we use Õ(·) to further suppress logarithmic dependence on n.
Let X , {x1, . . .",6.2 Three Examples,[0],[0]
", xM}, Y , {0, 1}, and consider functions F , {f?, f1, . . .",6.2 Three Examples,[0],[0]
", fM}.",6.2 Three Examples,[0],[0]
"We have f?(x) , (1/4, 1/2),∀x ∈ X and fi(xi) , (1/4, 0) and fi(xj) , (1/4, 1) for i 6=",6.2 Three Examples,[0],[0]
j.,6.2 Three Examples,[0],[0]
The marginal distribution is uniform and the true expected costs are given by f?,6.2 Three Examples,[0],[0]
so that the problem satisfies the Massart noise condition with τ = 1/4.,6.2 Three Examples,[0],[0]
"The key to the construction is that fis have high square loss on labels that they do not predict.
",6.2 Three Examples,[0],[0]
Observe that as P[hfi(x) 6=,6.2 Three Examples,[0],[0]
hf?(x)],6.2 Three Examples,[0],[0]
= 1/M and hfi(xi) 6=,6.2 Three Examples,[0],[0]
"hf?(xi) for all i, the probability of disagreement is 1 until all fi are eliminated.",6.2 Three Examples,[0],[0]
"As such, we have θ0 = M .",6.2 Three Examples,[0],[0]
"Similarly, we have E[c(hfi(x))− c(hf?(x))]",6.2 Three Examples,[0],[0]
"= 1 4M and γ(x, 1,Fcsr( 1 4M ))",6.2 Three Examples,[0],[0]
"= 1, so θ1 = 4M .",6.2 Three Examples,[0],[0]
"Therefore, the bounds in Theorem 7 and Proposition 1 are both Õ(M log |F|) = Õ(|F|).",6.2 Three Examples,[0],[0]
"On the other hand, since (fi(xj , 1)− f?(xj , 1))2 = 1/4 for all i, j ∈",6.2 Three Examples,[0],[0]
"[M ], COAL eliminates every fi once it has made a total of Õ(log |F|) queries to label y = 1.",6.2 Three Examples,[0],[0]
"Thus the label complexity is actually just Õ(log |F|), which is exponentially better than the disagreement-based analyses.",6.2 Three Examples,[0],[0]
"Thus, COAL can perform much better than suggested by the disagreement-based analyses, and an interesting future direction is to obtain refined guarantees for cost-sensitive active learning.",6.2 Three Examples,[0],[0]
We now turn to an empirical evaluation of COAL.,7 Experiments,[0],[0]
"For further computational efficiency, we implemented an approximate version of COAL using: 1) a relaxed version space Gi(y)",7 Experiments,[0],[0]
← {g ∈ G | R̂i(g; y) ≤,7 Experiments,[0],[0]
"R̂i(gi,y; y) + ∆i}, which does not enforce monotonicity, and 2) online optimization, based on online linear least-squares regression.",7 Experiments,[0],[0]
"The algorithm processes the data in one pass, and the idea is to (1) replace gi,y , the ERM, with an approximation goi,y obtained by online updates, and (2) compute the minimum and maximum costs via a sensitivity analysis of the online update.",7 Experiments,[0],[0]
"We describe this algorithm in detail in Subsection 7.1.
",7 Experiments,[0],[0]
"Then, we present our experimental results, first for simulated active learning (Subsection 7.2) and then for learning to search for joint prediction (Subsection 7.3).",7 Experiments,[0],[0]
"Consider the maximum and minimum costs for a fixed example x and label y at round i, all of which may be suppressed.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
We ignore all the constraints on the empirical square losses for the past rounds.,7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"First, define R̂(g, w, c; y) , R̂(g; y) + w(g(x)− c)2, which is the risk functional augmented with a fake example with weight w and cost c. Also define
g w , arg min g∈G R̂(g, w, 0; y), gw , arg min g∈G R̂(g, w, 1; y),
and recall that gi,y is the ERM given in Algorithm 1.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"The functional R̂(g, w, c; y) has a monotonicity property that we exploit here, proved in Appendix C.
Lemma 1.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"For any c and for w′ ≥ w ≥ 0, define g = argming R̂(g, w, c) and g′ = argming R̂(g, w′, c).",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"Then
R̂(g′)",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"≥ R̂(g) and (g′(x)− c)2 ≤ (g(x)− c)2.
",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"As a result, an alternative to MINCOST and MAXCOST is to find
w , max{w | R̂(g w )",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"− R̂(gi,y) ≤",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"∆i}, (12)
w , max{w | R̂(gw)− R̂(gi,y) ≤",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"∆i}, (13)
and return g w (x) and gw(x) as the minimum and maximum costs.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
We use two steps of approximation here.,7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"Using the definition of gw and gw as the minimizers of R̂(g, w, 1; y) and R̂(g, w, 0; y) respectively, we have
R̂(g w )",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"− R̂(gi,y) ≤ w · gi,y(x)2 − w · gw(x) 2,
R̂(gw)− R̂(gi,y) ≤",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"w · (gi,y(x)− 1)2 − w · (gw(x)− 1)2.
",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"We use this upper bound in place of R̂(gw)− R̂(gi,y) in (12) and (13).",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"Second, we replace gi,y , gw, and gw with approximations obtained by online updates.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"More specifically, we replace gi,y with goi,y, the current regressor produced by all online linear least squares updates so far, and approximate the others by
g w
(x)",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"≈ goi,y(x)− w · s(x, 0, goi,y), gw(x)",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"≈ goi,y(x) + w · s(x, 1, goi,y),
where s(x, y, goi,y) ≥ 0 is a sensitivity value that approximates the change in prediction on x resulting from an online update to goi,y with features x and label y.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
The computation of this sensitivity value is governed by the actual online update where we compute the derivative of the change in the prediction as a function of the importance weight w for a hypothetical example with cost 0 or cost 1 and the same features.,7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"This is possible for essentially all online update rules on importance weighted examples, and it corresponds to taking the limit as w → 0 of the change in prediction due to an update, divided by w. Since we are using linear representations, this requires only O(s) time per example, where s is the average number of non-zero features.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"With these two steps, we obtain approximate minimum and maximum costs using
goi,y(x)− wo · s(x, 0, goi,y), goi,y(x) + wo · s(x, 1, goi,y),
where
wo , max{w | w ( goi,y,(x) 2",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"− (goi,y(x)− w · s(x, 0, goi,y))2 ) ≤",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"∆i}
wo , max{w | w",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"( (goi,y,(x)− 1)2 − (goi,y(x) + w · s(x, 1, goi,y)− 1)2 ) ≤ ∆i}.
",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"The online update guarantees that goi,y(x) ∈",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"[0, 1].",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"Since the minimum cost is lower bounded by 0, we have wo ∈ ( 0, goi,y(x)
",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"s(x,0,goi,y)
] .",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"Finally, because the objective w(goi,y(x))
2 − w(goi,y(x)",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"− w · s(x, 0, goi,y))2 is increasing in w within this range (which can be seen by inspecting the derivative), we can find wo with binary search.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
"Using the same techniques, we also obtain an approximate maximum cost.",7.1 Finding Cost Ranges with Online Approximation,[0],[0]
We performed simulated active learning experiments with three datasets.,7.2 Simulated Active Learning,[0],[0]
"ImageNet 20 and 40 are sub-trees of the ImageNet hierarchy covering the 20 and 40 most frequent classes, where each example has a single zero-cost label, and the cost for an incorrect label is the tree-distance to the correct one.",7.2 Simulated Active Learning,[0],[0]
The feature vectors are the top layer of the Inception neural network [44].,7.2 Simulated Active Learning,[0],[0]
"The third, RCV1-v2 [28], is a multilabel text-categorization dataset, which has 103 labels, organized as a tree with a similar tree-distance cost structure as the ImageNet data.",7.2 Simulated Active Learning,[0],[0]
"Some dataset statistics are in Table 1.
",7.2 Simulated Active Learning,[0],[0]
We compare our online version of COAL to passive online learning.,7.2 Simulated Active Learning,[0],[0]
"We use the cost-sensitive oneagainst-all (CSOAA) implementation in Vowpal Wabbit5, which performs online linear regression for each
5http://hunch.net/~vw
label separately.",7.2 Simulated Active Learning,[0],[0]
There are two tuning parameters in our implementation.,7.2 Simulated Active Learning,[0],[0]
"First, instead of ∆i, we set the radius of the version space to ∆′i = κνi−1 i−1 (i.e. the log(n) term in the definition of νn is replaced with log(i)) and instead tune the constant κ.",7.2 Simulated Active Learning,[0],[0]
"This alternate “mellowness"" parameter controls how aggressive the query strategy is.",7.2 Simulated Active Learning,[0],[0]
"The second parameter is the learning rate used by online linear regression6.
",7.2 Simulated Active Learning,[0],[0]
"For all experiments, we show the results obtained by the best learning rate for each mellowness on each dataset, which is tuned as follows.",7.2 Simulated Active Learning,[0],[0]
We randomly permute the training data 100 times and make one pass through the training set with each parameter setting.,7.2 Simulated Active Learning,[0],[0]
"For each dataset let perf(mel, l, q, t) denote the test performance of the algorithm using mellowness mel and learning rate l on the tth permutation of the training data under a query budget of 2(q−1) · 10 ·K, q ≥ 1.",7.2 Simulated Active Learning,[0],[0]
"Let query(mel, l, q, t) denote the number of queries actually made.",7.2 Simulated Active Learning,[0],[0]
"Note that query(mel, l, q, t) < 2(q−1) ·",7.2 Simulated Active Learning,[0],[0]
10 ·K if the algorithm runs out of the training data before reaching the qth query budget7.,7.2 Simulated Active Learning,[0],[0]
"To evaluate the trade-off between test performance and number of queries, we define the following performance measure:
AUC(mel, l, t)",7.2 Simulated Active Learning,[0],[0]
"= 1
2 qmax∑ q=1",7.2 Simulated Active Learning,[0],[0]
"( perf(mel, l, q + 1, t) + perf(mel, l, q, t) ) · log2 query(mel, l, q + 1, t) query(mel, l, q, t) , (14)
where qmax is the minimum q such that 2(q−1) ·10 is larger than the size of the training data.",7.2 Simulated Active Learning,[0],[0]
This performance measure is the area under the curve of test performance against number of queries in log2 scale.,7.2 Simulated Active Learning,[0],[0]
A large value means the test performance quickly improves with the number of queries.,7.2 Simulated Active Learning,[0],[0]
"The best learning rate for mellowness mel is then chosen as
l?(mel) , arg max l median1≤t≤100 AUC(mel, l, t).
",7.2 Simulated Active Learning,[0],[0]
The best learning rates for different datasets and mellowness settings are in Table 2.,7.2 Simulated Active Learning,[0],[0]
"In the top row of Figure 2, we plot, for each dataset and mellowness, the number of queries against the median test cost along with bars extending from the 15th to 85th quantile.",7.2 Simulated Active Learning,[0],[0]
"Overall, COAL achieves a better 6We use the default online learning algorithm in Vowpal Wabbit, which is a scale-free [38] importance weight invariant [25] form of AdaGrad [17].",7.2 Simulated Active Learning,[0],[0]
"7In fact, we check the test performance only in between examples, so query(mel, l, q, t) may be larger than 2(q−1) ·",7.2 Simulated Active Learning,[0],[0]
"10 ·K by an additive factor of K, which is negligibly small.
trade-off between performance and queries.",7.2 Simulated Active Learning,[0],[0]
"With proper mellowness parameter, active learning achieves similar test cost as passive learning with a factor of 8 to 32 fewer queries.",7.2 Simulated Active Learning,[0],[0]
"On ImageNet 40 and RCV1-v2 (reproduced in Figure 1), active learning achieves better test cost with a factor of 16 fewer queries.",7.2 Simulated Active Learning,[0],[0]
"On RCV1-v2, COAL queries like passive up to around 256k queries, since the data is very sparse, and linear regression has the property that the cost range is maximal when an example has a new unseen feature.",7.2 Simulated Active Learning,[0],[0]
"Once COAL sees all features a few times, it queries much more efficiently than passive.",7.2 Simulated Active Learning,[0],[0]
"These plots correspond to the label complexity L2.
",7.2 Simulated Active Learning,[0],[0]
"In the bottom row, we plot the test error as a function of the number of examples for which at least one query was requested, for each dataset and mellowness, which experimentally corresponds to the L1 label complexity.",7.2 Simulated Active Learning,[0],[0]
"In comparison to the top row, the improvements offered by active learning are slightly less dramatic here.",7.2 Simulated Active Learning,[0],[0]
"This suggests that our algorithm queries just a few labels for each example, but does end up issuing at least one query on most of the examples.",7.2 Simulated Active Learning,[0],[0]
"Nevertheless, one can still achieve test cost competitive with passive learning using a factor of 2-16 less labeling effort, as measured by L1.
",7.2 Simulated Active Learning,[0],[0]
We also compare COAL with two active learning baselines.,7.2 Simulated Active Learning,[1.0],['We also compare COAL with two active learning baselines.']
Both algorithms differ from COAL only in their query rule.,7.2 Simulated Active Learning,[0],[0]
ALLORNONE queries either all labels or no labels using both domination and cost-range conditions and is an adaptation of existing multiclass active learners [1].,7.2 Simulated Active Learning,[0],[0]
"NODOM just uses the cost-range condition, inspired by active regression [11].",7.2 Simulated Active Learning,[0],[0]
"The results for ImageNet 40 and RCV1-v2 are displayed in Figure 3, where we use the AUC strategy to choose the learning rate.",7.2 Simulated Active Learning,[0],[0]
We choose the mellowness by visual inspection for the baselines and use 0.01 for COAL8.,7.2 Simulated Active Learning,[0],[0]
"On ImageNet 40, the ablations provide minimal improvement over passive learning, while on RCV1-v2, ALLORNONE does provide marginal improvement.",7.2 Simulated Active Learning,[0],[0]
"However, on both datasets, COAL substantially outperforms both baselines and passive learning.
",7.2 Simulated Active Learning,[0],[0]
"While not always the best, we recommend a mellowness setting of 0.01 as it achieves reasonable performance on all three datasets.",7.2 Simulated Active Learning,[0],[0]
"This is also confirmed by the learning-to-search experiments, which we discuss next.",7.2 Simulated Active Learning,[0],[0]
"We also experiment with COAL as the base leaner in learning-to-search [15, 13], which reduces joint prediction problems to CSMC.",7.3 Learning to Search,[0],[0]
"A joint prediction example defines a search space, where a sequence of decisions are made to generate the structured label.",7.3 Learning to Search,[0],[0]
"We focus here on sequence labeling tasks, where the input is a sentence and the output is a sequence of labels, specifically, parts of speech or named entities.
",7.3 Learning to Search,[0],[0]
"8We use 0.01 for ALLORNONE and 10−3 for NODOM.
",7.3 Learning to Search,[0],[0]
"Learning-to-search solves such problems by generating the output one label at a time, conditioning on all past decisions.",7.3 Learning to Search,[0],[0]
"Since mistakes may lead to compounding errors, it is natural to represent the decision space as a CSMC problem, where the classes are the “actions” available (e.g., possible labels for a word) and the costs reflect the long term loss of each choice.",7.3 Learning to Search,[0],[0]
"Intuitively, we should be able to avoid expensive computation of long term loss on decisions like “is ‘the’ a DETERMINER?”",7.3 Learning to Search,[0],[0]
once we are quite sure of the answer.,7.3 Learning to Search,[0],[0]
"Similar ideas motivate adaptive sampling for structured prediction [40].
",7.3 Learning to Search,[0],[0]
"We specifically use AGGRAVATE [37, 13, 42], which runs a learned policy to produce a backbone sequence of labels.",7.3 Learning to Search,[0],[0]
"For each position in the input, it then considers all possible deviation actions and executes an oracle for the rest of the sequence.",7.3 Learning to Search,[0],[0]
The loss on this complete output is used as the cost for the deviating action.,7.3 Learning to Search,[0],[0]
"Run in this way, AGGRAVATE requires len×K roll-outs when the input sentence has len words and each word can take one of K possible labels.
",7.3 Learning to Search,[0],[0]
"Since each roll-out takes O(len) time, this can be computationally prohibitive, so we use active learning to reduce the number of roll-outs.",7.3 Learning to Search,[0],[0]
We use COAL and a passive learning baseline inside AGGRAVATE on three joint prediction datasets (statistics are in Table 1).,7.3 Learning to Search,[0],[0]
"As above, we use several mellowness values and the same AUC criteria to select the best learning rate (see Table 2).",7.3 Learning to Search,[0],[0]
"The results are in Figure 4, and again our recommended mellowness is 0.01.
",7.3 Learning to Search,[0],[0]
"Overall, active learning reduces the number of roll-outs required, but the improvements vary on the three datasets.",7.3 Learning to Search,[0],[0]
"On the Wikipedia data, COAL performs a factor of 4 fewer rollouts to achieve similar performance to passive learning and achieves substantially better test performance.",7.3 Learning to Search,[0.9984137092024975],"['On the Wikipedia data, COAL performs a factor of 4 less rollouts to achieve similar performance to passive learning and achieves substantially better test performance.']"
"A similar, but less dramatic, behavior arises on the NER task.",7.3 Learning to Search,[0],[0]
"On the other hand, COAL offers minimal improvement over passive learning on the POS-tagging task.",7.3 Learning to Search,[0],[0]
"This agrees with our theory and prior empirical results [23], which show that active learning may not always improve upon passive learning.",7.3 Learning to Search,[0],[0]
"In this section we provide proofs for the main results, the oracle-complexity guarantee and the generalization and label complexity bounds.",8 Proofs,[0],[0]
"We start with some supporting results, including a new uniform freedman-type inequality that may be of independent interest.",8 Proofs,[0],[0]
"The proof of this inequality, and the proofs for several other supporting lemmata are deferred to the appendices.",8 Proofs,[0],[0]
A deviation bound.,8.1 Supporting Results,[0],[0]
"For both the computational and statistical analysis of COAL, we require concentration of the square loss functional R̂j(·; y), uniformly over the class G. To describe the result, we introduce the central random variable in the analysis:
Mj(g; y) , Qj(y)",8.1 Supporting Results,[0],[0]
"[ (g(xj)− cj(y))2 − (f?(xj ; y)− cj(y))2 ] , (15)
where (xj , cj) is the jth example and cost presented to the algorithm andQj(y) ∈ {0, 1} is the query indicator.",8.1 Supporting Results,[0],[0]
For simplicity we often write Mj when the dependence on g and y is clear from context.,8.1 Supporting Results,[0],[0]
Let Ej [·] and,8.1 Supporting Results,[0],[0]
"Varj [·] denote the expectation and variance conditioned on all randomness up to and including round j − 1.
Theorem 9.",8.1 Supporting Results,[0],[0]
"Let G be a function class with Pdim(G) = d, let δ ∈",8.1 Supporting Results,[0],[0]
"(0, 1) and define νn , 324(d log(n) + log(8Ke(d+ 1)n2/δ)).",8.1 Supporting Results,[0],[0]
"Then with probability at least 1− δ, the following inequalities hold simultaneously for all g ∈ G, y ∈",8.1 Supporting Results,[0],[0]
"[K], and i < i′ ∈",8.1 Supporting Results,[0],[0]
"[n].
i′∑ j=i Mj(g; y) ≤ 3 2 i′∑ j=i EjMj(g; y) + νn, (16)
1
2 i′∑ j=i EjMj(g; y) ≤",8.1 Supporting Results,[0],[0]
"i′∑ j=i Mj(g; y) + νn. (17)
",8.1 Supporting Results,[0],[0]
This result is a uniform Freedman-type inequality for the martingale difference sequence ∑ iMi − EiMi.,8.1 Supporting Results,[0],[0]
"In general, such bounds require much stronger assumptions (e.g., sequential complexity measures [35]) on G than the finite pseudo-dimension assumption that we make.",8.1 Supporting Results,[0],[0]
"However, by exploiting the structure of our particular martingale, specifically that the dependencies arise only from the query indicator, we are able to establish this type of inequality under weaker assumptions.",8.1 Supporting Results,[0],[0]
"The result may be of independent interest, but the proof, which is based on arguments from Liang et al. [29], is quite technical and deferred to Appendix A. Note that we did not optimize the constants.
",8.1 Supporting Results,[0],[0]
The Multiplicative Weights Algorithm.,8.1 Supporting Results,[0],[0]
We also use the standard analysis of multiplicative weights for solving linear feasibility problems.,8.1 Supporting Results,[0],[0]
"We state the result here and, for completeness, provide a proof in Appendix B. See also Arora et al. [3], Plotkin et al. [34] for more details.
",8.1 Supporting Results,[0],[0]
"Consider a linear feasibility problem with decision variable v ∈ Rd, explicit constraints 〈ai, v〉 ≤ bi for i ∈",8.1 Supporting Results,[0],[0]
[m] and some implicit constraints v ∈ S,8.1 Supporting Results,[0],[0]
"(e.g., v is non-negative or other simple constraints).",8.1 Supporting Results,[0],[0]
"The MW algorithm either finds an approximately feasible point or certifies that the program is infeasible assuming access to an oracle that can solve a simpler feasibility problem with just one explicit constraint∑ i µi〈ai, v〉 ≤ ∑ i µibi for any non-negative weights µ ∈ Rm+ and the implicit constraint v ∈",8.1 Supporting Results,[0],[0]
"S. Specifically, given weights µ, the oracle either reports that the simpler problem is infeasible, or returns any feasible point v that further satisfies 〈ai, v〉 − bi ∈",8.1 Supporting Results,[0],[0]
"[−ρi, ρi] for parameters ρi that are known to the MW algorithm.
",8.1 Supporting Results,[0],[0]
"The MW algorithm proceeds iteratively, maintaining a weight vector µ(t) ∈ Rm+ over the constraints.",8.1 Supporting Results,[0],[0]
"Starting with µ(1)i = 1 for all i, at each iteration, we query the oracle with the weights µ
(t) and the oracle either returns a point vt or detects infeasibility.",8.1 Supporting Results,[0],[0]
"In the latter case, we simply report infeasibility and in the former, we update the weights using the rule
µ (t+1)",8.1 Supporting Results,[0],[0]
i ← µ (t),8.1 Supporting Results,[0],[0]
"i ×
( 1− η bi",8.1 Supporting Results,[0],[0]
"− 〈ai, vt〉
ρi
) .
",8.1 Supporting Results,[0],[0]
Here η is a parameter of the algorithm.,8.1 Supporting Results,[0],[0]
"The intuition is that if vt satisfies the ith constraint, then we downweight the constraint, and conversely, we up-weight every constraint that is violated.",8.1 Supporting Results,[0],[0]
"Running the algorithm with appropriate choice of η and for enough iterations is guaranteed to approximately solve the feasibility problem.
",8.1 Supporting Results,[0],[0]
"Theorem 10 (Arora et al. [3], Plotkin et al. [34]).",8.1 Supporting Results,[0],[0]
"Consider running the MW algorithm with parameter η = √ log(m)/T for T iterations on a linear feasibility problem where oracle responses satisfy 〈ai, v〉− bi ∈",8.1 Supporting Results,[0],[0]
"[−ρi, ρi].",8.1 Supporting Results,[0],[0]
"If the oracle fails to find a feasible point in some iteration, then the linear program is infeasible.",8.1 Supporting Results,[0],[0]
"Otherwise the point v̄ , 1T ∑T t=1",8.1 Supporting Results,[0],[0]
"vt satisfies 〈ai, v̄〉 ≤ bi + 2ρi √ log(m)/T for all i ∈",8.1 Supporting Results,[0],[0]
"[m].
Other Lemmata.",8.1 Supporting Results,[0],[0]
"Our first lemma evaluates the conditional expectation and variance ofMj , defined in (15), which we will use heavily in the proofs.",8.1 Supporting Results,[0],[0]
"Proofs of the results stated here are deferred to Appendix C.
Lemma 2 (Bounding variance of regression regret).",8.1 Supporting Results,[0],[0]
"We have for all (g, y) ∈ G × Y ,",8.1 Supporting Results,[0],[0]
Ej,8.1 Supporting Results,[0],[0]
"[Mj ] = Ej [ Qj(y)(g(xj)− f?(xj ; y))2 ] , Var
j",8.1 Supporting Results,[0],[0]
"[Mj ] ≤ 4Ei[Mj ].
",8.1 Supporting Results,[0],[0]
The next lemma relates the cost-sensitive error to the random variables Mj .,8.1 Supporting Results,[0],[0]
Define Fi = { f ∈,8.1 Supporting Results,[0],[0]
"GK | ∀y, f(·; y) ∈ Gi(y) } ,
which is the version space of vector regressors at round i. Additionally, recall that Pζ captures the noise level in the problem, defined in (10) and that ψi = 1/ √ i is defined in the algorithm pseudocode.
",8.1 Supporting Results,[0],[0]
Lemma 3.,8.1 Supporting Results,[0],[0]
"For all i > 0, if f? ∈ Fi, then for all f ∈ Fi
Ex,c[c(hf (x))− c(hf?(x))]",8.1 Supporting Results,[0],[0]
"≤ min ζ>0
{ ζPζ + 1 (ζ ≤ 2ψi)",8.1 Supporting Results,[0],[0]
2ψi,8.1 Supporting Results,[0],[0]
"+
4ψ2i ζ + 6 ζ ∑ y Ei",8.1 Supporting Results,[0],[0]
"[Mi]
} .
",8.1 Supporting Results,[0],[0]
Note that the lemma requires that both f? and f belong to the version space Fi.,8.1 Supporting Results,[0],[0]
"For the label complexity analysis, we will need to understand the cost-sensitive performance of all f ∈ Fi, which requires a different generalization bound.",8.1 Supporting Results,[0],[0]
"Since the proof is similar to that of Theorem 3, we defer the argument to appendix.
",8.1 Supporting Results,[0],[0]
Lemma 4.,8.1 Supporting Results,[0],[0]
"Assuming the bounds in Theorem 9 hold, then for all i,Fi ⊂ Fcsr(ri) where ri , minζ>0 { ζPζ + 44K∆i ζ } .
",8.1 Supporting Results,[0],[0]
"The final lemma relates the query rule of COAL to a hypothetical query strategy driven by Fcsr(ri), which we will subsequently bound by the disagreement coefficients.",8.1 Supporting Results,[0],[0]
"Let us fix the round i and introduce the shorthand γ̂(xi, y) =",8.1 Supporting Results,[0],[0]
"ĉ+(xi, y)− ĉ−(xi, y), where ĉ+(xi, y) and ĉ−(xi, y) are the approximate maximum and minimum costs computed in Algorithm 1 on the ith example, which we now call xi.",8.1 Supporting Results,[0],[0]
"Moreover, let Yi be the set of non-dominated labels at round i of the algorithm, which in the pseudocode we call Y ′. Formally, Yi = {y | ĉ−(xi, y) ≤ miny′ ĉ+(xi, y′)}.",8.1 Supporting Results,[0],[0]
Finally recall that for a set of vector regressors F ⊂,8.1 Supporting Results,[0],[0]
"F , we use γ(x, y, F ) to denote the cost range for label y on example x witnessed by the regressors in F .
Lemma 5.",8.1 Supporting Results,[0],[0]
Suppose that the conclusion of Lemma 4 holds.,8.1 Supporting Results,[0],[0]
"Then for any example xi and any label y at round i, we have
γ̂(xi, y) ≤ γ(xi, y,Fcsr(ri))",8.1 Supporting Results,[0],[0]
+,8.1 Supporting Results,[0],[0]
"ψi.
",8.1 Supporting Results,[0],[0]
"Further, with y?i = argminy f ?(xi; y), ȳi = argminy ĉ+(xi, y), and ỹi = argminy 6=y?i ĉ−(xi, y),
y 6= y?i ∧ y ∈ Yi ⇒ f?(xi; y)− f?(xi; y?i ) ≤ γ(xi, y,Fcsr(ri))",8.1 Supporting Results,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))",8.1 Supporting Results,[0],[0]
"+ ψi/2, |Yi| > 1 ∧ y?i ∈",8.1 Supporting Results,[0],[0]
"Yi ⇒ f?(xi; ỹi)− f?(xi; y?i ) ≤ γ(xi, ỹi,Fcsr(ri))",8.1 Supporting Results,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))",8.1 Supporting Results,[0],[0]
+ ψi/2.,8.1 Supporting Results,[0],[0]
"The proof is based on expressing the optimization problem (7) as a linear optimization in the space of distributions over G. Then, we use binary search to re-formulate this as a series of feasibility problems and apply Theorem 10 to each of these.
",8.2 Proof of Theorem 1,[0],[0]
"Recall that the problem of finding the maximum cost for an (x, y) pair is equivalent to solving the program (7) in terms of the optimal g. For the problem (7), we further notice that since G is a convex set, we can instead write the minimization over g as a minimization over P ∈ ∆(G) without changing the optimum, leading to the modified problem (8).
",8.2 Proof of Theorem 1,[0],[0]
"Thus we have a linear program in variable P , and Algorithm 2 turns this into a feasibility problem by guessing the optimal objective value and refining the guess using binary search.",8.2 Proof of Theorem 1,[0],[0]
"For each induced feasibility problem, we use MW to certify feasibility.",8.2 Proof of Theorem 1,[0],[0]
Let c ∈,8.2 Proof of Theorem 1,[0],[0]
"[0, 1] be some guessed upper bound on the objective, and let us first turn to the MW component of the algorithm.",8.2 Proof of Theorem 1,[0],[0]
"The program in consideration is
?",8.2 Proof of Theorem 1,[0],[0]
∃P ∈ ∆(G) s.t. Eg∼P (g(xi)− 1)2 ≤,8.2 Proof of Theorem 1,[0],[0]
c,8.2 Proof of Theorem 1,[0],[0]
and ∀j ∈,8.2 Proof of Theorem 1,[0],[0]
"[i],Eg∼P R̂j(g; y) ≤ ∆̃j .",8.2 Proof of Theorem 1,[0],[0]
"(18)
",8.2 Proof of Theorem 1,[0],[0]
"This is a linear feasibility problem in the infinite dimensional variable P , with i + 1 constraints.",8.2 Proof of Theorem 1,[0],[0]
"Given a particular set of weights µ over the constraints, it is clear that we can use the regression oracle over g to compute
gµ = arg min g∈G µ0(g(xi)− 1)2 + ∑ j∈[i] µjEg∼P R̂j(g; y).",8.2 Proof of Theorem 1,[0],[0]
"(19)
Observe that solving this simpler program provides one-sided errors.",8.2 Proof of Theorem 1,[0],[0]
"Specifically, if the objective of (19) evaluated at gµ is larger than µ0c + ∑ j∈[i] µj∆̃j then there cannot be a feasible solution to problem (18), since the weights µ are all non-negative.",8.2 Proof of Theorem 1,[0],[0]
"On the other hand if gµ has small objective value it does not imply that gµ is feasible for the original constraints in (18).
",8.2 Proof of Theorem 1,[0],[0]
"At this point, we would like to invoke the MW algorithm, and specifically Theorem 10, in order to find a feasible solution to (18) or to certify infeasibility.",8.2 Proof of Theorem 1,[0],[0]
Invoking the theorem requires the ρj parameters which specify how badly gµ might violate the jth constraint.,8.2 Proof of Theorem 1,[0],[0]
"For us, ρj , κ suffices since R̂j(g; y)− R̂j(gj,y; y) ∈",8.2 Proof of Theorem 1,[0],[0]
"[0, 1] (since gj,y is the ERM) and ∆j ≤",8.2 Proof of Theorem 1,[0],[0]
κ.,8.2 Proof of Theorem 1,[0],[0]
"Since κ ≥ 2 this also suffices for the cost constraint.
",8.2 Proof of Theorem 1,[0],[0]
"If at any iteration, MW detects infeasibility, then our guessed value c for the objective is too small since no function satisfies both (g(xi)− 1)2 ≤",8.2 Proof of Theorem 1,[0],[0]
c and the empirical risk constraints in (18) simultaneously.,8.2 Proof of Theorem 1,[0],[0]
"In this case, in Line 10 of Algorithm 2, our binary search procedure increases our guess for c. On the other hand, if we apply MW for T iterations and find a feasible point in every round, then, while we do not have a point that is feasible for the original constraints in (18), we will have a distribution PT such that
EPT (g(xi)− 1)2 ≤",8.2 Proof of Theorem 1,[0],[0]
c+,8.2 Proof of Theorem 1,[0],[0]
"2κ √ log(i+ 1)
T and ∀j ∈",8.2 Proof of Theorem 1,[0],[0]
"[i],EPT R̂j(g; y) ≤ ∆̃j + 2κ
√ log(i+ 1)
T .
",8.2 Proof of Theorem 1,[0],[0]
We will set T toward the end of the proof.,8.2 Proof of Theorem 1,[0],[0]
"If we do find an approximately feasible solution, then we reduce c and proceed with the binary search.",8.2 Proof of Theorem 1,[0],[0]
We terminate when ch − c` ≤ τ2/2,8.2 Proof of Theorem 1,[0],[0]
and we know that problem (18) is approximately feasible with ch and infeasible with c`.,8.2 Proof of Theorem 1,[0],[0]
"From ch we will construct a strictly feasible point, and this will lead to a bound on the true maximum c+(x, y,Gi).
",8.2 Proof of Theorem 1,[0],[0]
Let P̄ be the approximately feasible point found when running MW with the final value of ch.,8.2 Proof of Theorem 1,[0],[0]
"By Jensen’s inequality and convexity of G, there exists a single regressor that is also approximately feasible, which we denote ḡ. Observe that g?",8.2 Proof of Theorem 1,[0],[0]
"satisfies all constraints with strict inequality, since by (20) we know
that R̂j(g?; y)− R̂j(gj,y; y) ≤",8.2 Proof of Theorem 1,[0],[0]
∆j/κ < ∆j .,8.2 Proof of Theorem 1,[0],[0]
We create a strictly feasible point gζ by mixing ḡ with g?,8.2 Proof of Theorem 1,[0],[0]
"with proportion 1− ζ and ζ for
ζ = 4κ
∆i
√ log(i+ 1)
T ,
which will be in [0, 1] when we set T .",8.2 Proof of Theorem 1,[0],[0]
"Combining inequalities, we get that for any j ∈",8.2 Proof of Theorem 1,[0],[0]
"[i]
(1− ζ)R̂j(ḡ; y) + ζR̂j(g?; y) ≤ R̂j(gj,y; y) +",8.2 Proof of Theorem 1,[0],[0]
(,8.2 Proof of Theorem 1,[0],[0]
"1− ζ) ( ∆j + 2κ √ log(i+ 1)
T
)",8.2 Proof of Theorem 1,[0],[0]
"+ ζ ( ∆j κ )
≤ R̂j(gj,y; y) + ∆j",8.2 Proof of Theorem 1,[0],[0]
"−
( ζ∆j(κ− 1)
κ",8.2 Proof of Theorem 1,[0],[0]
"− 2κ
√ log(i+ 1)
T )",8.2 Proof of Theorem 1,[0],[0]
"≤ R̂j(gj,y; y) + ∆j ,
and hence this mixture regressor",8.2 Proof of Theorem 1,[0],[0]
gζ is exactly feasible.,8.2 Proof of Theorem 1,[0],[0]
Here we use that κ ≥ 2 and that ∆i is monotonically decreasing.,8.2 Proof of Theorem 1,[0],[0]
"With the pessimistic choice g?(xi) = 0, the objective value for gζ is at most
(gζ(xi)− 1)2 ≤ (1− ζ)(ḡ(xi)− 1)2 + ζ(g?(xi)−",8.2 Proof of Theorem 1,[0],[0]
"1)2 ≤ (1− ζ) ( ch + 2κ √ log(i+ 1)
T
) + ζ
≤ c` + τ2/2 + ( 2κ+ 4κ
∆i
)√ log(i+ 1)
T .
",8.2 Proof of Theorem 1,[0],[0]
"Thus gζ is exactly feasible and achieves the objective value above, which provides an upper bound on the maximum cost.",8.2 Proof of Theorem 1,[0],[0]
On the other hand c` provides a lower bound.,8.2 Proof of Theorem 1,[0],[0]
"Our setting of T = log(i+1)(8κ2/∆i) 2
τ4
ensures that that this excess term is at most τ2, since ∆i ≤ 1.",8.2 Proof of Theorem 1,[0],[0]
Note that since τ ≤,8.2 Proof of Theorem 1,[0],[0]
"[0, 1], this also ensures that ζ ∈",8.2 Proof of Theorem 1,[0],[0]
"[0, 1].",8.2 Proof of Theorem 1,[0],[0]
"With this choice of T , we know that c` ≤ (c+(y)",8.2 Proof of Theorem 1,[0],[0]
"− 1)2 ≤ c` + τ2, which implies that c+(y) ∈",8.2 Proof of Theorem 1,[0],[0]
"[1− √ c` + τ2, 1− √ c`].",8.2 Proof of Theorem 1,[0],[0]
"Since √ c` + τ2 ≤ √ c` + τ , we obtain the guarantee.
",8.2 Proof of Theorem 1,[0],[0]
"As for the oracle complexity, since we start with c` = 0 and ch = 1 and terminate when ch − c` ≤ τ2/2, we performO(log(1/τ2)) iterations of binary search.",8.2 Proof of Theorem 1,[0],[0]
"Each iteration requires T = O ( max{1, i2/ν2n} log(i) τ4 ) rounds of MW, each of which requires exactly one oracle call.",8.2 Proof of Theorem 1,[0],[0]
"Hence the oracle complexity isO ( max{1, i2/ν2n} log(i) log(1/τ) τ4 ) .",8.2 Proof of Theorem 1,[0],[0]
"Recall the central random variable Mj(g; y), defined in (15), which is the excess square loss for function g on label y for the jth example, if we issued a query.",8.3 Proof of the Generalization Bound,[0],[0]
"The idea behind the proof is to first apply Theorem 9 to argue that all the random variables Mj(g; y) concentrate uniformly over the function class G. Next for a vector regressor f , we relate the cost-sensitive risk to the excess square loss via Lemma 3.",8.3 Proof of the Generalization Bound,[0],[0]
"Finally, using the fact that gi,y minimizes the empirical square loss at round i, this implies a cost-sensitive risk bound for the vector regressor fi = (gi,y) at round i.
First, condition on the high probability event in Theorem 9, which ensures that the empirical square losses concentrate.",8.3 Proof of the Generalization Bound,[0],[0]
We first prove that f? ∈,8.3 Proof of the Generalization Bound,[0],[0]
Fi for all,8.3 Proof of the Generalization Bound,[0],[0]
i ∈,8.3 Proof of the Generalization Bound,[0],[0]
[n].,8.3 Proof of the Generalization Bound,[0],[0]
"At round i, by (17), for each y and for any g we have
0 ≤ 1 2 i∑ j=1 EjMj(g; y) ≤ i∑ j=1 Mj(g; y) + νn.
",8.3 Proof of the Generalization Bound,[0],[0]
The first inequality here follows from the fact that EjMj(g; y) is a quadratic form by Lemma 2.,8.3 Proof of the Generalization Bound,[0],[0]
"Expanding Mj(g; y), this implies that
R̂i+1(f ?(·; y); y) ≤ R̂i+1(g; y) + νn i .
",8.3 Proof of the Generalization Bound,[0],[0]
Since this bound applies to all g ∈ G it proves that f? ∈ Fi+1,8.3 Proof of the Generalization Bound,[0],[0]
"for all i, using the definition of ∆i and κ.",8.3 Proof of the Generalization Bound,[0],[0]
"Trivially, we know that f? ∈ F1.",8.3 Proof of the Generalization Bound,[0],[0]
"Together with the fact that the losses are in [0, 1] and the definition of ∆i, the above analysis yields
R̂i(f ?(·; y); y) ≤ R̂i(g; y) + ∆i κ .",8.3 Proof of the Generalization Bound,[0],[0]
"(20)
",8.3 Proof of the Generalization Bound,[0],[0]
"This implies that f?(·; y) strictly satisfies the inequalities defining the version space, which we used in the MW proof.
",8.3 Proof of the Generalization Bound,[0],[0]
We next prove that fi+1 ∈ Fj for all j ∈,8.3 Proof of the Generalization Bound,[0],[0]
[i].,8.3 Proof of the Generalization Bound,[0],[0]
"Fix some label y and to simplify notation, we drop dependence on y.",8.3 Proof of the Generalization Bound,[0],[0]
If gi+1 /∈,8.3 Proof of the Generalization Bound,[0],[0]
Gt+1,8.3 Proof of the Generalization Bound,[0],[0]
"for some t ∈ {0, . . .",8.3 Proof of the Generalization Bound,[0],[0]
", i} then, first observe that we must have t large enough so that νn/t ≤ 1.",8.3 Proof of the Generalization Bound,[0],[0]
"In particular, since ∆t+1 = κmin{1, νn/t} and we always have R̂t+1(gi+1; y) ≤",8.3 Proof of the Generalization Bound,[0],[0]
"R̂t+1(gt+1; y) + 1 due to boundedness, we do not evict any functions until νn/t ≤ 1.",8.3 Proof of the Generalization Bound,[0],[0]
For t ≥,8.3 Proof of the Generalization Bound,[0],[0]
"νn, we get
t∑ j=1",8.3 Proof of the Generalization Bound,[0],[0]
Mj = t ( R̂t+1(gi+1)− R̂t+1(g?) ),8.3 Proof of the Generalization Bound,[0],[0]
"= t ( R̂t+1(gi+1)− R̂t+1(gt+1) + R̂t+1(gt+1)− R̂t+1(g?)
)",8.3 Proof of the Generalization Bound,[0],[0]
≥ κνn,8.3 Proof of the Generalization Bound,[0],[0]
"− νn = (κ− 1)νn.
",8.3 Proof of the Generalization Bound,[0],[0]
The inequality uses the radius of the version space and the fact that by assumption gi+1 /∈,8.3 Proof of the Generalization Bound,[0],[0]
"Gt+1, so the excess empirical risk is at least ∆t+1 = κνn/t since we are considering large t. We also use (20) on the second term.",8.3 Proof of the Generalization Bound,[0],[0]
"Moreover, we know that since gi+1 is the empirical square loss minimizer for label y after round i, we have∑i j=1Mj(gi+1; y) ≤ 0.",8.3 Proof of the Generalization Bound,[0],[0]
"These two facts together establish that
i∑ j=t+1 Mj(gi+1) ≤ (1− κ)νn.
",8.3 Proof of the Generalization Bound,[0],[0]
"However, by Theorem 9 on this intermediary sum, we know that
0 ≤ 1 2 i∑ j=t+1 EjMj(gi+1) ≤",8.3 Proof of the Generalization Bound,[0],[0]
i∑ j=t+1 Mj(gi+1),8.3 Proof of the Generalization Bound,[0],[0]
+,8.3 Proof of the Generalization Bound,[0],[0]
"νn < (2− κ)νn < 0
using the definition of κ.",8.3 Proof of the Generalization Bound,[0],[0]
"This is a contradiction, so we must have that gi+1 ∈",8.3 Proof of the Generalization Bound,[0],[0]
"Gj for all j ∈ {1, . . .",8.3 Proof of the Generalization Bound,[0],[0]
", i}.",8.3 Proof of the Generalization Bound,[0],[0]
"The same argument applies for all y and hence we can apply Lemma 3 on all rounds to obtain
i ( Ex,c[c(hfi+1(x))− c(hf?(x))] )",8.3 Proof of the Generalization Bound,[0],[0]
≤,8.3 Proof of the Generalization Bound,[0],[0]
"min
ζ>0 iζPζ + i∑
j=1
( 1 (ζ ≤ 2ψj)",8.3 Proof of the Generalization Bound,[0],[0]
2ψj,8.3 Proof of the Generalization Bound,[0],[0]
"+
4ψ2j ζ + 6 ζ ∑ y Ej [Mj(fi+1; y)] ) .
",8.3 Proof of the Generalization Bound,[0],[0]
We study the four terms separately.,8.3 Proof of the Generalization Bound,[0],[0]
The first one is straightforward and contributes ζPζ to the instantaneous cost sensitive regret.,8.3 Proof of the Generalization Bound,[0],[0]
"Using our definition of ψj = 1/ √ j the second term can be bounded as
i∑ j=1 1 (ζ < 2ψj) 2ψj = d4/ζ2e∑ j=1 2√ j ≤ 4 √ d4/ζ2e ≤ 12 ζ .
",8.3 Proof of the Generalization Bound,[0],[0]
"The inequality above, ∑n i=1",8.3 Proof of the Generalization Bound,[0],[0]
1√,8.3 Proof of the Generalization Bound,[0],[0]
"i ≤ 2 √ n, is well known.",8.3 Proof of the Generalization Bound,[0],[0]
"For the third term, using our definition of ψj gives
i∑ j=1 4ψ2j ζ = 4 ζ i∑ j=1 1 j ≤ 4 ζ (1 + log(i)).
",8.3 Proof of the Generalization Bound,[0],[0]
"Finally, the fourth term can be bounded using (17), which reveals
i∑ j=1 Ej [Mj ] ≤ 2 i∑ j=1 Mj + 2νn
Since for each y, ∑i j=1Mj(fi+1; y) ≤ 0 for the empirical square loss minimizer (which is what we are considering now), we get
6
ζ ∑ y i∑ j=1 Ej [Mj(fi+1; y)]",8.3 Proof of the Generalization Bound,[0],[0]
"≤ 12 ζ Kνn.
",8.3 Proof of the Generalization Bound,[0],[0]
"And hence, we obtain the generalization bound
Ex,c[c(x;hfi+1(x))− c(x;hf?(x))]",8.3 Proof of the Generalization Bound,[0],[0]
"≤ min ζ>0
{ ζPζ + 1
ζi (4 log(i) + 16 + 12Kνn) }",8.3 Proof of the Generalization Bound,[0],[0]
"≤ min
ζ>0
{ ζPζ +
32Kνn ζi
} .
",8.3 Proof of the Generalization Bound,[0],[0]
Proof of Corollary 4.,8.3 Proof of the Generalization Bound,[0],[0]
"Under the Massart noise condition, set ζ = τ",8.3 Proof of the Generalization Bound,[0],[0]
so that Pζ = 0 and we immediately get the result.,8.3 Proof of the Generalization Bound,[0],[0]
Proof of Corollary 5.,8.3 Proof of the Generalization Bound,[0],[0]
"Set ζ = min { τ0, ( 32Kνn iβ ) 1 α+2 } , so that for i sufficiently large the second term is
selected and we obtain the bound.",8.3 Proof of the Generalization Bound,[0],[0]
The proof for the label complexity bounds is based on first relating the version space Fi at round i to the cost-sensitive regret ball Fcsr with radius ri.,8.4 Proof of the Label Complexity bounds,[0],[0]
"In particular, the containment Fi ⊂ Fcsr(ri) in Lemma 4 implies that our query strategy is more aggressive than the query strategy induced by Fcsr(ri), except for a small error introduced when computing the maximum and minimum costs.",8.4 Proof of the Label Complexity bounds,[0],[0]
This error is accounted for by Lemma 5.,8.4 Proof of the Label Complexity bounds,[0],[0]
"Since the probability that Fcsr will issue a query is intimately related to the disagreement coefficient, this argument leads to the label complexity bounds for our algorithm.
",8.4 Proof of the Label Complexity bounds,[0],[0]
Proof of Theorem 6.,8.4 Proof of the Label Complexity bounds,[0],[0]
"Fix some round i with example xi, let Fi be the vector regressors used at round i and let Gi(y) be the corresponding regressors for label y. Let ȳi = argminy",8.4 Proof of the Label Complexity bounds,[0],[0]
"ĉ+(xi, y), y?i = argminy f?(xi; y), and ỹi = argminy 6=y?i ĉ−(xi, y).",8.4 Proof of the Label Complexity bounds,[0],[0]
Assume that Lemma 4 holds.,8.4 Proof of the Label Complexity bounds,[0],[0]
"The L2 label complexity is∑
y Qi(y) = ∑ y 1{|Yi| > 1",8.4 Proof of the Label Complexity bounds,[0],[0]
"∧ y ∈ Yi}1{γ̂(xi, y) >",8.4 Proof of the Label Complexity bounds,[0],[0]
"ψi}
≤ ∑ y 1{|Yi| > 1",8.4 Proof of the Label Complexity bounds,[0],[0]
"∧ y ∈ Yi}1{γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"> ψi/2}.
",8.4 Proof of the Label Complexity bounds,[0],[0]
"For the former indicator, observe that y ∈ Yi implies that there exists a vector regressor f ∈",8.4 Proof of the Label Complexity bounds,[0],[0]
Fi ⊂ Fcsr(ri),8.4 Proof of the Label Complexity bounds,[0],[0]
such that hf (xi) = y.,8.4 Proof of the Label Complexity bounds,[0],[0]
This follows since the domination condition means that there exists g ∈ Gi(y) such that g(xi) ≤ miny′ maxg′∈Gi(y′) g′(xi).,8.4 Proof of the Label Complexity bounds,[0],[0]
"Since we are using a factored representation, we can take f to use g on the yth coordinate and use the maximizers for all the other coordinates.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Similarly, there exists",8.4 Proof of the Label Complexity bounds,[0],[0]
another regressor f ′ ∈,8.4 Proof of the Label Complexity bounds,[0],[0]
Fi such that hf ′(xi) 6=,8.4 Proof of the Label Complexity bounds,[0],[0]
y.,8.4 Proof of the Label Complexity bounds,[0],[0]
"Thus this indicator can be bounded by the disagreement coefficient∑
y Qi(y) ≤ ∑ y 1{∃f, f ′ ∈ Fcsr(ri)",8.4 Proof of the Label Complexity bounds,[0],[0]
"| hf (xi) = y 6= hf ′(xi)}1{γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"> ψi/2}
= ∑ y 1{x ∈",8.4 Proof of the Label Complexity bounds,[0],[0]
"DIS(ri, y) ∧ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ ψi/2}.
",8.4 Proof of the Label Complexity bounds,[0],[0]
"We will now apply Freedman’s inequality on the sequence { ∑ y Qi(y)}ni=1, which is a martingale with range K. Moreover, due to non-negativity, the conditional variance is at most K times the conditional mean, and in such cases, Freedman’s inequality reveals that with probability at least 1− δ
X ≤ EX + 2 √ REX log(1/δ) + 2R log(1/δ) ≤ 2EX + 3R log(1/δ),
where X is the non-negative martingale with range R and expectation EX .",8.4 Proof of the Label Complexity bounds,[0],[0]
The last step is by the fact that 2 √ ab ≤ a+,8.4 Proof of the Label Complexity bounds,[0],[0]
b.,8.4 Proof of the Label Complexity bounds,[0],[0]
"For us, Freedman’s inequality implies that with probability at least 1− δ/2
n∑ i=1",8.4 Proof of the Label Complexity bounds,[0],[0]
∑ y Qi(y) ≤ 2 ∑ i,8.4 Proof of the Label Complexity bounds,[0],[0]
"Ei ∑ y Qi(y) + 3K log(2/δ)
≤ 2",8.4 Proof of the Label Complexity bounds,[0],[0]
∑,8.4 Proof of the Label Complexity bounds,[0],[0]
i,8.4 Proof of the Label Complexity bounds,[0],[0]
Ei ∑ y 1{x ∈,8.4 Proof of the Label Complexity bounds,[0],[0]
"DIS(ri, y) ∧ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ ψi/2}+ 3K log(2/δ)
",8.4 Proof of the Label Complexity bounds,[0],[0]
≤ 4,8.4 Proof of the Label Complexity bounds,[0],[0]
∑ i ri,8.4 Proof of the Label Complexity bounds,[0],[0]
"ψi θ2 + 3K log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
The last step here uses the definition of the disagreement coefficient θ2.,8.4 Proof of the Label Complexity bounds,[0],[0]
"To wrap up the proof we just need to upper bound the sequence, using our choices of ψi = 1/ √ i, ri = 2 √ 44K∆i, and ∆i = κmin{1, νni−1}.",8.4 Proof of the Label Complexity bounds,[0],[0]
"With simple calculations this is easily seen to be at most
8nθ2 √ 88Kκνn + 3K log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
Similarly for L1 we can derive the bound L1 ≤ ∑,8.4 Proof of the Label Complexity bounds,[0],[0]
"i 1 (∃y | γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
≥ ψi/2 ∧ x ∈,8.4 Proof of the Label Complexity bounds,[0],[0]
"DIS(ri, y)) ,
and then apply Freedman’s inequality to obtain that with probability",8.4 Proof of the Label Complexity bounds,[0],[0]
"at least 1− δ/2
L1 ≤ 2 n∑ i=1 2ri ψi θ1 + 3 log(2/δ) ≤ 8θ1n √ 88Kκνn + 3 log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
Proof of Theorem 7.,8.4 Proof of the Label Complexity bounds,[0],[0]
"Using the same notations as in the bound for the high noise case we first express the L2 label complexity as ∑
y Qi(y) = ∑ y 1{|Yi| > 1, y ∈ Yi}Qi(y).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"We need to do two things with the first part of the query indicator, so we have duplicated it here.",8.4 Proof of the Label Complexity bounds,[0],[0]
"For the second, we will use the derivation above to relate the query rule to the disagreement region.",8.4 Proof of the Label Complexity bounds,[0],[0]
"For the first, by Lemma 5, for y 6= y?i , we can derive the bound
1{f?(xi;",8.4 Proof of the Label Complexity bounds,[0],[0]
"y)− f?(xi; y?i ) ≤ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
+ ψi/2} ≤ 1{τ,8.4 Proof of the Label Complexity bounds,[0],[0]
"− ψi/2 ≤ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))}.
",8.4 Proof of the Label Complexity bounds,[0],[0]
"For y = y?i , we get the same bound but with ỹi, also by Lemma 5.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Focusing on just one of these terms, say where y 6= y?i and any round where τ ≥ ψi, we get
1{τ − ψi/2 ≤ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))}Qi(y) ≤ 1{τ/2 ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
"γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))}Qi(y) ≤ 1{τ/4 ≤ γ(xi, y,Fcsr(ri))}Qi(y) + 1{τ/4 ≤ γ(xi, y?i ,Fcsr(ri))}Qi(y) ≤ Di(y) +Di(y?i ),
where for shorthand we have definedDi(y) , 1{τ/4 ≤ γ(xi, y,Fcsr(ri))∧xi ∈ DIS(ri, y)}.",8.4 Proof of the Label Complexity bounds,[0],[0]
The derivation for the first term is straightforward.,8.4 Proof of the Label Complexity bounds,[0],[0]
"We obtain the disagreement region for y?i since the fact that we query y (i.e. Qi(y)) implies there is f such that hf (xi) = y, so this function witnesses disagreement to y?i .
",8.4 Proof of the Label Complexity bounds,[0],[0]
"The term involving y?i and ỹi is bounded in essentially the same way, since we know that when |Yi| > 1, there exists two function f,",8.4 Proof of the Label Complexity bounds,[0],[0]
f ′ ∈,8.4 Proof of the Label Complexity bounds,[0],[0]
Fi such that hf (xi) = ỹi and hf ′(xi) = y?i .,8.4 Proof of the Label Complexity bounds,[0],[0]
"In total, we can bound the L2 label complexity at any round i such that τ ≥ ψi by
L2 ≤ Di(ỹi)",8.4 Proof of the Label Complexity bounds,[0],[0]
+Di(y?i ) + ∑ y 6=y?i (Di(y) +Di(y ?,8.4 Proof of the Label Complexity bounds,[0],[0]
i )),8.4 Proof of the Label Complexity bounds,[0],[0]
≤ KDi(y?i ),8.4 Proof of the Label Complexity bounds,[0],[0]
"+ 2 ∑ y Di(y)
",8.4 Proof of the Label Complexity bounds,[0],[0]
"For the earlier rounds, we simply upper bound the label complexity by K. Since the range of this random variable is at most 3K, using Freedman’s inequality just as in the high noise case, we get that with probability at least 1− δ/2
L2 ≤ n∑ i=1",8.4 Proof of the Label Complexity bounds,[0],[0]
K1{τ ≤ ψi}+,8.4 Proof of the Label Complexity bounds,[0],[0]
2 n∑ i=2,8.4 Proof of the Label Complexity bounds,[0],[0]
"Ei
[ KDi(y ?",8.4 Proof of the Label Complexity bounds,[0],[0]
i ),8.4 Proof of the Label Complexity bounds,[0],[0]
+ 2 ∑ y Di(y) ],8.4 Proof of the Label Complexity bounds,[0],[0]
"+ 9K log(2/δ)
≤ Kd1/τ2e+ 8 τ (Kθ1 + θ2) n∑",8.4 Proof of the Label Complexity bounds,[0],[0]
"i=2 ri + 9K log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
The first line here is the application of Freedman’s inequality.,8.4 Proof of the Label Complexity bounds,[0],[0]
"In the second, we evaluate the expectation, which we can relate to the disagreement coefficients θ1, θ2.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Moreover, we use the setting ψi = 1/ √ i to evaluate the first term.",8.4 Proof of the Label Complexity bounds,[0],[0]
"As a technicality, we remove the index i = 1 from the second summation, since we are already accounting for queries on the first round in the first term.",8.4 Proof of the Label Complexity bounds,[0],[0]
"The last step is to evaluate the series, for which we use the definition of ri = minζ>0 {ζPζ + 44K∆i/ζ} and set ζ = τ , the Massart noise level.",8.4 Proof of the Label Complexity bounds,[0],[0]
This gives ri = 44K∆i/τ .,8.4 Proof of the Label Complexity bounds,[0],[0]
"In total, we get
L2 ≤ Kd1/τ2e+ 352Kκνn
τ2 (Kθ1 + θ2)",8.4 Proof of the Label Complexity bounds,[0],[0]
"log(n) + 9K log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"As θ2 ≤ Kθ1 always, we drop θ2 from the above expression to obtain the stated bound.",8.4 Proof of the Label Complexity bounds,[0],[0]
For L1 we use a very similar argument.,8.4 Proof of the Label Complexity bounds,[0],[0]
"First, by Lemmas 4 and 5
L1 ≤ n∑ i=1",8.4 Proof of the Label Complexity bounds,[0],[0]
"1{|Yi| > 1 ∧ ∃y ∈ Yi, γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ ψi/2}.
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Again by Lemma 5, we know that
|Yi| > 1 ∧ y ∈ Yi ⇒ ∃f, f ′ ∈ Fcsr(ri), hf (xi) = y",8.4 Proof of the Label Complexity bounds,[0],[0]
"6= hf ′(xi).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Moreover, one of the two classifiers can be f?, and so, when τ",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ ψi, we can deduce
⇒ f?(xi, y)− f?(xi, y?i ) ≤ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ γ(xi, y?i ,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ ψi/2 ⇒ τ/4 ≤ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
"∧ τ/4 ≤ γ(xi, y?i ,Fcsr(ri)).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Combining this argument, we bound the L1 label complexity as
L1 ≤ n∑ i=1",8.4 Proof of the Label Complexity bounds,[0],[0]
1{τ ≤ ψi}+ n∑ i=2 1{∃y,8.4 Proof of the Label Complexity bounds,[0],[0]
"| xi ∈ DIS(Fcsr(ri), y) ∧ γ(xi, y,Fcsr(ri))",8.4 Proof of the Label Complexity bounds,[0],[0]
≥ τ/4}.,8.4 Proof of the Label Complexity bounds,[0],[0]
"(21)
Applying Freedman’s inequality just as before gives
L1 ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
d1/τ2e+ 2 n∑ i=2,8.4 Proof of the Label Complexity bounds,[0],[0]
4ri τ θ1 + 2 log(2/δ) ≤,8.4 Proof of the Label Complexity bounds,[0],[0]
"d1/τ2e+ 352Kκνn τ2 θ1 log(n) + 2 log(2/δ),
with probability at least 1− δ/2.
",8.4 Proof of the Label Complexity bounds,[0],[0]
Proof of Theorem 8.,8.4 Proof of the Label Complexity bounds,[0],[0]
"For the Tsybakov case, the same argument as in the Massart case gives that with probability at least 1− δ/2
L2 ≤ Kd1/τ2e+ 8
τ (Kθ1 + θ2) n∑ i=2 ri",8.4 Proof of the Label Complexity bounds,[0],[0]
+,8.4 Proof of the Label Complexity bounds,[0],[0]
"2nβτ α + 9K log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"The main difference here is the term scaling with nτα which arises since we do not have the deterministic bound f?(xi, y) − f?(xi, y?i ) ≥ τ",8.4 Proof of the Label Complexity bounds,[0],[0]
"as we used in the Massart case, but rather this happens except with probability βτα (provided τ ≤ τ0).",8.4 Proof of the Label Complexity bounds,[0],[0]
"Now we must optimize ζ in the definition of ri and then τ .
",8.4 Proof of the Label Complexity bounds,[0],[0]
For ζ the optimal setting is (44K∆i/β) 1 α+2 which gives ri ≤ 2β 1 α+2 (44K∆i) α+1 α+2 .,8.4 Proof of the Label Complexity bounds,[0],[0]
"Since we want to set ζ ≤ τ0, this requires i ≥ 1 + 44Kκνnβτα+20 .",8.4 Proof of the Label Complexity bounds,[0],[0]
"For these early rounds we will simply pay K in the label complexity, but this will be dominated by other higher order terms.",8.4 Proof of the Label Complexity bounds,[0],[0]
"For the later rounds, we get
n∑ i=2 ri ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
2β 1 α+2 (44Kκνn) α+1 α+2 n∑ i=2,8.4 Proof of the Label Complexity bounds,[0],[0]
( 1 i− 1 )α+1 α+2 ≤,8.4 Proof of the Label Complexity bounds,[0],[0]
"2(α+ 2) (44Kκνn) α+1 α+2 (βn) 1 α+2 .
",8.4 Proof of the Label Complexity bounds,[0],[0]
This bound uses the integral approximation ∑n i=2(i− 1) −α+1α+2,8.4 Proof of the Label Complexity bounds,[0],[0]
≤ 1 + ∫,8.4 Proof of the Label Complexity bounds,[0],[0]
n−1 1 x− α+1 α+2 dx ≤ (α+ 2)n 1 α+2 .,8.4 Proof of the Label Complexity bounds,[0],[0]
"At this point, the terms involving τ in our bound are
Kd1/τ2e+ 16 τ (Kθ1 + θ2)(α+ 2) (44Kκνn) α+1 α+2 (βn) 1 α+2 + 2nβτα.
",8.4 Proof of the Label Complexity bounds,[0],[0]
"We set τ = (8(α+ 2)(Kθ1 + θ2)) 1 α+1 (44Kκνn) 1 α+2 (βn) −1 α+2 by optimizing the second two terms which gives a final bound of
L2 ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
"O ( (Kθ1 + θ2) α α+1 (Kνn) α α+2n 2 α+2 +K log(1/δ) ) .
",8.4 Proof of the Label Complexity bounds,[0],[0]
"This follows since the 1/τ2 term agrees in the n dependence and is lower order in other parameters, while the unaccounted for querying in the early rounds is independent of n. The bound of course requires that τ ≤ τ0, which again requires n large enough.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Note we are treating α and β as constants and we drop θ2 from the final statement.
",8.4 Proof of the Label Complexity bounds,[0],[0]
The L1 bound requires only slightly different calculations.,8.4 Proof of the Label Complexity bounds,[0],[0]
"Following the derivation for the Massart case, we get
L1 ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
d1/,8.4 Proof of the Label Complexity bounds,[0],[0]
τ2e+ 8θ1 τ,8.4 Proof of the Label Complexity bounds,[0],[0]
n∑,8.4 Proof of the Label Complexity bounds,[0],[0]
i=2 ri,8.4 Proof of the Label Complexity bounds,[0],[0]
"+ 2nβτ α + 2 log(2/δ)
≤",8.4 Proof of the Label Complexity bounds,[0],[0]
d1/τ2e+ 16θ1(α+ 2) τ,8.4 Proof of the Label Complexity bounds,[0],[0]
"(44Kκνn) α+1 α+2 (βn) 1 α+2 + 2nβτα + 2 log(2/δ),
not counting the lower order term for the querying in the early rounds.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Here we set τ = (8(α+2)θ1) 1 α+1 (44Kκνn) 1 α+2 (βn) −1 α+2 to obtain
L1 ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
O,8.4 Proof of the Label Complexity bounds,[0],[0]
"( θ α α+1 1 (Kνn) α α+2n 2 α+2 + log(1/δ) ) .
",8.4 Proof of the Label Complexity bounds,[0],[0]
Proof of Proposition 1: Massart Case.,8.4 Proof of the Label Complexity bounds,[0],[0]
"Observe that with Massart noise, we have Fcsr(r) ⊂ F̃(r/τ), which implies that
E1 {∃y | x ∈ DIS(r, y) ∧ γ(x, y,Fcsr(r))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ τ/4} ≤ E1 { x ∈ D̃IS(r/τ) } ≤ r τ sup r>0 1 r E1 { x ∈ D̃IS(r) } .
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Thus we may replace θ1 with θ0 in the proof of the L1 label complexity bound above.
",8.4 Proof of the Label Complexity bounds,[0],[0]
Proof of Proposition 1: Tsybakov Case.,8.4 Proof of the Label Complexity bounds,[0],[0]
The proof is identical to Theorem 8 except we must introduce the alternative coefficient θ0.,8.4 Proof of the Label Complexity bounds,[0],[0]
"To do so, define X (τ) , {x ∈ X : miny 6=y?(x)",8.4 Proof of the Label Complexity bounds,[0],[0]
"f?(x, y)− f?(x, y?(x))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ τ}, and note that under the Tsybakov noise condition, we have P[x /∈",8.4 Proof of the Label Complexity bounds,[0],[0]
X (τ)] ≤,8.4 Proof of the Label Complexity bounds,[0],[0]
βτα for all 0 ≤ τ ≤ τ0.,8.4 Proof of the Label Complexity bounds,[0],[0]
"For such a value of τ we have
P[hf (x) 6= hf?(x)]",8.4 Proof of the Label Complexity bounds,[0],[0]
≤ E1{x ∈ X,8.4 Proof of the Label Complexity bounds,[0],[0]
(τ) ∧ hf (x) 6= hf?(x)}+ P[x /∈,8.4 Proof of the Label Complexity bounds,[0],[0]
"X (τ)]
≤ 1 τ",8.4 Proof of the Label Complexity bounds,[0],[0]
Ec(hf (x))− c(hf?(x)),8.4 Proof of the Label Complexity bounds,[0],[0]
"+ βτα.
",8.4 Proof of the Label Complexity bounds,[0],[0]
We use this fact to prove that Fcsr(r) ⊂,8.4 Proof of the Label Complexity bounds,[0],[0]
F̃(2r/τ) for τ sufficiently small.,8.4 Proof of the Label Complexity bounds,[0],[0]
This can be seen from above by noting that if f ∈ Fcsr(r) then the right hand side is rτ,8.4 Proof of the Label Complexity bounds,[0],[0]
"+ βτ α, and if τ ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
"(r/β) 1
1+α the containment holds.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Therefore, we have
E1 {∃y | x ∈ DIS(r, y) ∧ γ(x, y,Fcsr(r))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ τ/4} ≤ E1 {∃y | x ∈ DIS(r, y)} ≤ E1 { x ∈ D̃IS(2r/τ) } ≤ 2r
τ · sup r>0
E1 { x ∈ D̃IS(r) } .
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Thus, provided τ ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
"(rn/β) 1 α+1 , we can replace θ1 with θ0 in the above argument.",8.4 Proof of the Label Complexity bounds,[0],[0]
"This gives
L1 ≤",8.4 Proof of the Label Complexity bounds,[0],[0]
d1/τ2e+,8.4 Proof of the Label Complexity bounds,[0],[0]
4θ0 τ n∑,8.4 Proof of the Label Complexity bounds,[0],[0]
i=2 ri,8.4 Proof of the Label Complexity bounds,[0],[0]
"+ 2nβτ α + 2 log(2/δ)
≤",8.4 Proof of the Label Complexity bounds,[0],[0]
d1/τ2e+ 8θ0(α+ 2) τ,8.4 Proof of the Label Complexity bounds,[0],[0]
"(44Kκνn) α+1 α+2 (βn) 1 α+2 + 2nβτα + 2 log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"As above, we have taken ri = 2β 1 α+2 (44K∆i) α+1 α+2 and approximated the sum by an integral.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Since ∆n = κνn/(n− 1), we can set τ",8.4 Proof of the Label Complexity bounds,[0],[0]
= 2 α+1 α+2 (44Kκνn) 1 α+2 (βn)− 1 α+2 .,8.4 Proof of the Label Complexity bounds,[0],[0]
"This is a similar choice to what we used in the proof of Theorem 8 except that we are not incorporating θ0 into the choice of τ , and it yields a final bound of O ( θ0n 2 α+2 ν α α+2 n",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ log(1/δ) ) .
",8.4 Proof of the Label Complexity bounds,[0],[0]
Proof of Proposition 2.,8.4 Proof of the Label Complexity bounds,[0],[0]
First we relate θ1 to θ0 in the multiclass case.,8.4 Proof of the Label Complexity bounds,[0],[0]
"For f ∈ Fcsr(r), we have
P[hf (x) 6= hf?(x)] ≤ P[hf (x) 6=",8.4 Proof of the Label Complexity bounds,[0],[0]
y] + P[hf?(x) 6=,8.4 Proof of the Label Complexity bounds,[0],[0]
y] = Ec(hf (x))− c(hf?(x)) ≤ 2error(hf?),8.4 Proof of the Label Complexity bounds,[0],[0]
+,8.4 Proof of the Label Complexity bounds,[0],[0]
"r
Therefore for any r > 0 and any x,
1{∃y | x ∈ DIS(r, y) ∧ γ(x, y,Fcsr(r))",8.4 Proof of the Label Complexity bounds,[0],[0]
≥ ψ/2} ≤ 1{x ∈,8.4 Proof of the Label Complexity bounds,[0],[0]
"D̃IS(ri + 2error(hf?))}.
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Applying this argument in the L1 derivation above, we obtain L1 ≤ 2 ∑",8.4 Proof of the Label Complexity bounds,[0],[0]
i Ei1{x ∈,8.4 Proof of the Label Complexity bounds,[0],[0]
"D̃IS(ri + 2error(hf?))}+ 3 log(2/δ)
≤ 2 ∑",8.4 Proof of the Label Complexity bounds,[0],[0]
"i (ri + 2error(hf?))θ0 + 3 log(2/δ).
",8.4 Proof of the Label Complexity bounds,[0],[0]
We now bound ri via Lemma 4.,8.4 Proof of the Label Complexity bounds,[0],[0]
"In multiclass classification, the fact that c = 1− ey for some y implies that f?(x, y) is one minus the probability that the true label is y.",8.4 Proof of the Label Complexity bounds,[0],[0]
"Thus f?(x, y) ∈",8.4 Proof of the Label Complexity bounds,[0],[0]
"[0, 1], ∑ y f
?",8.4 Proof of the Label Complexity bounds,[0],[0]
"(x, y) = K − 1, and for any x we always have
min y 6=y?(x)
",8.4 Proof of the Label Complexity bounds,[0],[0]
"f?(x, y)− f?(x, y?(x))",8.4 Proof of the Label Complexity bounds,[0],[0]
"≥ 1− 2f?(x, y?(x)).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Hence, we may bound Pζ , for ζ ≤ 1/2, as follows
Pζ = Px∼D [
min y 6=y?(x)
",8.4 Proof of the Label Complexity bounds,[0],[0]
"f?(x, y)− f?(x, y?(x))",8.4 Proof of the Label Complexity bounds,[0],[0]
≤ ζ ],8.4 Proof of the Label Complexity bounds,[0],[0]
"≤ Px∼D [1− 2f?(x, y?(x)) ≤ ζ]
≤",8.4 Proof of the Label Complexity bounds,[0],[0]
"Exf?(x, y?(x)) · 2
1− ζ ≤ 4error(hf?).
",8.4 Proof of the Label Complexity bounds,[0],[0]
"Now apply Lemma 4 with ζ = min{1/2, √ 44K∆i/error(hf?)}, and we obtain
ri ≤ √ 44K∆ierror(hf?)",8.4 Proof of the Label Complexity bounds,[0],[0]
"+ 264K∆i.
",8.4 Proof of the Label Complexity bounds,[0],[0]
Using the definition of ∆i the final L1 label complexity bound is L1 ≤ 4θ0n · error(hf?),8.4 Proof of the Label Complexity bounds,[0],[0]
+O ( θ0 (√ Knκνn · error(hf?),8.4 Proof of the Label Complexity bounds,[0],[0]
+Kκνn log(n) ),8.4 Proof of the Label Complexity bounds,[0],[0]
+ log(1/δ) ) .,8.4 Proof of the Label Complexity bounds,[0],[0]
This paper presents a new active learning algorithm for cost-sensitive multiclass classification.,9 Discussion,[0],[0]
"The algorithm enjoys strong theoretical guarantees on running time, generalization error, and label complexity.",9 Discussion,[0],[0]
The main algorithmic innovation is a new way to compute the maximum and minimum costs predicted by a regression function in the version space.,9 Discussion,[0],[0]
"We also design an online algorithm inspired by our theoretical analysis that outperforms passive baselines both in CSMC and structured prediction.
",9 Discussion,[0],[0]
"On a technical level, our algorithm uses a square loss oracle to search the version space and drive the query strategy.",9 Discussion,[0],[0]
This contrasts with many recent results using argmax or 0/1-loss minimization oracles for information acquisition problems like contextual bandits [2].,9 Discussion,[0],[0]
"As these involve NP-hard optimizations in general, an intriguing question is whether we can use a square loss oracle for other information acquisition problems.",9 Discussion,[0],[0]
We hope to answer this question in future work.,9 Discussion,[0],[0]
"Part of this research was completed while TKH was at Microsoft Research and AK was at University of Massachusetts, Amherst.",Acknowledgements,[0],[0]
AK thanks Chicheng Zhang for insightful conversations.,Acknowledgements,[0],[0]
AK is supported in part by NSF Award IIS-1763618.,Acknowledgements,[0],[0]
"In the proof, we mostly work with the empirical `1 covering number for G. At the end, we translate to pseudo-dimension using a lemma of Haussler.
",A Proof of Theorem 9,[0],[0]
Definition 3 (Covering numbers).,A Proof of Theorem 9,[0],[0]
"Given class G ⊂ X → R, α > 0, and sample X =",A Proof of Theorem 9,[0],[0]
"(x1, . . .",A Proof of Theorem 9,[0],[0]
", xn) ∈ Xn, the covering numberN1(α,H, X) is the minimum cardinality of a set V ⊂ Rn such that for any g ∈ G, there exists a (v1, . . .",A Proof of Theorem 9,[0],[0]
", vn) ∈ V with 1n",A Proof of Theorem 9,[0],[0]
∑n i=1,A Proof of Theorem 9,[0],[0]
"|g(xi)− vi| ≤ α.
",A Proof of Theorem 9,[0],[0]
Lemma 6 (Covering number and Pseudo-dimension [22]).,A Proof of Theorem 9,[0],[0]
"Given a hypothesis class G ⊂ X → R with Pdim(G) ≤ d, for any X ∈",A Proof of Theorem 9,[0],[0]
"Xn we have
N1(α,G, X) ≤ e(d+ 1) ( 2e
α
)d .
",A Proof of Theorem 9,[0],[0]
"Fixing i and y, and working toward (16) we seek to bound
P sup g∈G i∑ j=1 Mj(g;",A Proof of Theorem 9,[0],[0]
y)− 3 2 Ej [Mj(g; y)] ≥ τ  .,A Proof of Theorem 9,[0],[0]
(22) The bound on the other tail is similar.,A Proof of Theorem 9,[0],[0]
"In this section, we sometimes treat the query rule as a function which maps an x to a query decision.",A Proof of Theorem 9,[0],[0]
"We use the notation Qj : X → {0, 1} to denote the query function used after seeing the first j − 1 examples.",A Proof of Theorem 9,[0],[0]
"Thus, our query indicator Qj is simply the instantiation Qj(xj).",A Proof of Theorem 9,[0],[0]
"In this section, we work with an individual label y and omit the explicit dependence in all our arguments and notation.",A Proof of Theorem 9,[0],[0]
"For notational convenience, we use zj = (xj , cj , Qj) and with g?(·) = f?(·; y), we define
ξj = g ?",A Proof of Theorem 9,[0],[0]
"(xj)− cj and `(g, x) =",A Proof of Theorem 9,[0],[0]
(g(x)− g?(x)).,A Proof of Theorem 9,[0],[0]
"(23)
Note that ξj is a centered random variable, independent of everything else.",A Proof of Theorem 9,[0],[0]
"We now introduce some standard concepts from martingale theory for the proof of Theorem 9.
",A Proof of Theorem 9,[0],[0]
Definition 4 (Tangent sequence).,A Proof of Theorem 9,[0],[0]
"For a dependent sequence z1, . . .",A Proof of Theorem 9,[0],[0]
", zi we use z′1, . . .",A Proof of Theorem 9,[0],[0]
", z′i to denote a tangent sequence, where z′j |z1:j−1 d = zj |z1:j−1, and, conditioned on z1, . . .",A Proof of Theorem 9,[0],[0]
", zi, the random variables z′1, . . .",A Proof of Theorem 9,[0],[0]
", z′i are independent.
",A Proof of Theorem 9,[0],[0]
"In fact, in our case we have z′j = (x ′ j , c ′",A Proof of Theorem 9,[0],[0]
"j , Q ′ j) where (x ′ j , c ′ j) ∼ D and Q′j : X → {0, 1} is identical to Qj .",A Proof of Theorem 9,[0],[0]
We use M ′j(g) to denote the empirical excess square loss on sample z ′ j .,A Proof of Theorem 9,[0],[0]
"Note that we will continue to use our previous notation of Ej to denote conditioning on z1, . . .",A Proof of Theorem 9,[0],[0]
", zj−1.",A Proof of Theorem 9,[0],[0]
"We next introduce one more random process, and then proceed with the proof.
",A Proof of Theorem 9,[0],[0]
Definition 5 (Tree process).,A Proof of Theorem 9,[0],[0]
"A tree process Q is a binary tree of depth i where each node is decorated with a value from {0, 1}.",A Proof of Theorem 9,[0],[0]
"For a Rademacher sequence ∈ {−1, 1}i we use Qi( ) to denote the value at the node reached when applying the actions 1, . . .",A Proof of Theorem 9,[0],[0]
", i−1 from the root, where +1 denotes left and −1 denotes right.
",A Proof of Theorem 9,[0],[0]
"The proof follows a fairly standard recipe for proving uniform convergence bounds, but has many steps that all require minor modifications from standard arguments.",A Proof of Theorem 9,[0],[0]
"We compartmentalize each step in various lemmata:
1.",A Proof of Theorem 9,[0],[0]
"In Lemma 7, we introduce a ghost sample and replace the conditional expectation",A Proof of Theorem 9,[0],[0]
Ej,A Proof of Theorem 9,[0],[0]
"[·] in (22) with an empirical term evaluated on an tangent sequence.
",A Proof of Theorem 9,[0],[0]
2.,A Proof of Theorem 9,[0],[0]
"In Lemma 8, we perform symmetrization and introduce Rademacher random variables and the associated tree process.
3.",A Proof of Theorem 9,[0],[0]
"In Lemma 9 we control the symmetrized process for finite G.
4.",A Proof of Theorem 9,[0],[0]
"In Lemma 10, we use the covering number to discretize G.
We now state and prove the intermediate results.
",A Proof of Theorem 9,[0],[0]
Lemma 7 (Ghost sample).,A Proof of Theorem 9,[0],[0]
"Let Z = (z1, . . .",A Proof of Theorem 9,[0],[0]
", zi) be the sequence of (x, c,Q) triples and let Z ′ =",A Proof of Theorem 9,[0],[0]
"(z′1, . . .",A Proof of Theorem 9,[0],[0]
", z′i) be a tangent sequence.",A Proof of Theorem 9,[0],[0]
Then for β0 ≥ β1 > 0,A Proof of Theorem 9,[0],[0]
"if τ ≥ 4(1+β1) 2
(β0−β1) , then
PZ sup g∈G i∑ j=1 Mj(g)− (1 + 2β0)EjMj(g) ≥ τ  ≤ 2PZ,Z′ sup g∈G i∑ j=1 Mj(g)−M ′j(g)− 2β1Qj(x′j)`2(g, x′j)",A Proof of Theorem 9,[0],[0]
"≥ τ 2
 , PZ sup g∈G i∑ j=1 (1− 2β0)EjMj(g)−Mj(g) ≥ τ
 ≤ 2PZ,Z′ sup g∈G i∑ j=1 M ′j(g)−Mj(g)− 2β1Qj(x′j)`2(g, x′j) ≥",A Proof of Theorem 9,[0],[0]
"τ 2
 .",A Proof of Theorem 9,[0],[0]
Proof.,A Proof of Theorem 9,[0],[0]
"We derive the first inequality, beginning with the right hand side and working toward a lower bound.",A Proof of Theorem 9,[0],[0]
"The main idea is to condition on Z and just work with the randomness in Z ′. To this end, let ĝ achieve the supremum on the left hand side, and define the events
E =  i∑
j=1
Mj(ĝ)− (1 + 2β0)EjMj(ĝ)",A Proof of Theorem 9,[0],[0]
≥ τ  ,A Proof of Theorem 9,[0],[0]
"E′ =
{ n∑ i=1",A Proof of Theorem 9,[0],[0]
M ′j(ĝ) +,A Proof of Theorem 9,[0],[0]
"2β1Qj(x ′ j)` 2(ĝ, x′j)−",A Proof of Theorem 9,[0],[0]
"(1 + 2β0)EjM ′j(ĝ) ≤ τ/2 } .
",A Proof of Theorem 9,[0],[0]
"Starting from the right hand side, by adding and subtracting (1 + 2β0)EjMj(ĝ) we get
PZ,Z′ sup g∈G i∑ j=1 Mj(g)−M ′j(g)− 2β1Qj(x′j)`2(g, x′j) ≥ τ/2  ",A Proof of Theorem 9,[0],[0]
"≥ PZ,Z′
 i∑ j=1 Mj(ĝ)−M ′j(ĝ)− 2β1Qj(x′j)`2(ĝ, x′j) ≥ τ/2  ≥ PZ,Z′",A Proof of Theorem 9,[0],[0]
[ E ⋂ E′ ],A Proof of Theorem 9,[0],[0]
"= EZ [1{E} × PZ′ [E′|Z]] .
",A Proof of Theorem 9,[0],[0]
"Since we have defined ĝ to achieve the supremum, we know that
EZ1{E} = PZ sup g∈G i∑ j=1 Mj(g)− (1 + 2β0)EjMj(g) ≥ τ  .",A Proof of Theorem 9,[0],[0]
which is precisely the left hand side of the desired inequality.,A Proof of Theorem 9,[0],[0]
Hence we need to bound the PZ′ [E′|Z] term.,A Proof of Theorem 9,[0],[0]
"For this term, we note that
PZ′ [E′|Z] = PZ′ [
n∑ i=1",A Proof of Theorem 9,[0],[0]
M ′j(ĝ) +,A Proof of Theorem 9,[0],[0]
"2β1Qj(x ′ j)` 2(ĝ, x′j)−",A Proof of Theorem 9,[0],[0]
(1 + 2β0)EjM ′j(ĝ) ≤ τ/2,A Proof of Theorem 9,[0],[0]
"| Z ] (a) = PZ′
[ n∑ i=1",A Proof of Theorem 9,[0],[0]
M ′j(ĝ) + 2β1EjM ′j(ĝ)− (1 + 2β0)EjM ′j(ĝ) ≤ τ/2,A Proof of Theorem 9,[0],[0]
"| Z ]
= PZ′ [
n∑ i=1",A Proof of Theorem 9,[0],[0]
( M ′j(ĝ)− EjM ′j(ĝ) ),A Proof of Theorem 9,[0],[0]
+ 2 ( β1 − β0)EjM ′j(ĝ) ) ≤ τ/2,A Proof of Theorem 9,[0],[0]
"| Z
] .
",A Proof of Theorem 9,[0],[0]
"Here, (a) follows since Ej [M ′j(g) | Z] = Qj(x′j)`2(g, x′j) for any g by Lemma 2.",A Proof of Theorem 9,[0],[0]
"Since we are conditioning on Z, ĝ is also not a random function and the same equality holds when we take expectation over Z ′, even for ĝ.",A Proof of Theorem 9,[0],[0]
"With this, we can now invoke Chebyshev’s inequality:
PZ′ [E′|Z] = 1− PZ′  i∑ j=1 M ′j(ĝ) + 2β1Qj(x ′ j)` 2(ĝ, x′j)−",A Proof of Theorem 9,[0],[0]
"(1 + 2β0)EjM ′j(ĝ) ≥ τ/2 ∣∣∣∣∣∣Z 
≥ 1− Var
[∑i j=1M ′ j(ĝ) + 2β1Qj(x ′",A Proof of Theorem 9,[0],[0]
"j)`(ĝ, x ′ j) ∣∣∣Z](
τ/2 + 2(β0",A Proof of Theorem 9,[0],[0]
"− β1) ∑ i Ej [Qj(x′j)`2(ĝ, x′j) ∣∣Z])2 .",A Proof of Theorem 9,[0],[0]
"Since we are working conditional on Z, we can leverage the independence of Z ′",A Proof of Theorem 9,[0],[0]
"(recall Definition 4) to bound the variance term.
",A Proof of Theorem 9,[0],[0]
"Var  i∑ j=1 M ′j(g) + 2β1Qj(x ′ j)` 2(ĝ, x′j) ∣∣∣∣∣∣Z  ≤",A Proof of Theorem 9,[0],[0]
"i∑
j=1
",A Proof of Theorem 9,[0],[0]
Ej [ M ′j(ĝ) 2 + (2β1) 2Qj(x ′,A Proof of Theorem 9,[0],[0]
"j)` 4(ĝ, x′j) + 4β1M ′ j(ĝ)Qj(x ′ j)` 2(ĝ, x′j) ]
≤ 4(1",A Proof of Theorem 9,[0],[0]
"+ β1)2 i∑
j=1
EjQj(x′j)`2(ĝ, x′j).
",A Proof of Theorem 9,[0],[0]
Here we use that Var[X] ≤,A Proof of Theorem 9,[0],[0]
"E[X2] and then we use that `2 ≤ ` since the loss is bounded in [0, 1] along with Lemma 2.",A Proof of Theorem 9,[0],[0]
"Returning to the application of Chebyshev’s inequality, if we expand the quadratic in the denominator and drop all but the cross term, we get the bound
P′Z",A Proof of Theorem 9,[0],[0]
"[E′|Z] ≥ 1− 2(1 + β1)
2
τ(β0 − β1) ≥ 1/2,
where the last step uses the requirement on τ .",A Proof of Theorem 9,[0],[0]
This establishes the first inequality.,A Proof of Theorem 9,[0],[0]
For the second inequality the steps are nearly identical.,A Proof of Theorem 9,[0],[0]
"Let ĝ achieve the supremum on the left hand side and define
E =  i∑
j=1
(1− 2β0)EjMj(ĝ)−Mj(ĝ) ≥ τ  ",A Proof of Theorem 9,[0],[0]
E′,A Proof of Theorem 9,[0],[0]
"=  i∑
j=1
(1− 2β0)EjM ′j(ĝ)−M ′j(ĝ) + 2β1Qj(x′j)`2(ĝ, x′j) ≤ τ/2  .",A Proof of Theorem 9,[0],[0]
"Using the same argument, we can lower bound the right hand side by
P sup g∈G i∑ j=1 M ′j(g)−Mj(g)− 2β1Qj(x′j)`2(g, x′j) ≥ τ/2  ≥ EZ",A Proof of Theorem 9,[0],[0]
[1{E} × PZ′ [E′|Z]],A Proof of Theorem 9,[0],[0]
"Applying Chebyshev’s inequality yields the same expression as for the other tail.
",A Proof of Theorem 9,[0],[0]
Lemma 8 (Symmetrization).,A Proof of Theorem 9,[0],[0]
"Using the same notation as in Lemma 7, we have
PZ,Z′ sup g∈G i∑ j=1 Mj(g)−M ′j(g)− 2β1Qj(x′j)`2(g, x′j)",A Proof of Theorem 9,[0],[0]
"≥ τ  ≤ 2E sup
Q P sup g∈G i∑ j=1 jQj( )((1 + β1)`(g, xj)2 + 2ξj`(g, xj))− β1Qj( )`(g, xj)2 ≥ τ/2  .",A Proof of Theorem 9,[0],[0]
"the same bound holds on the lower tail with (1− β1) replacing (1 + β1).
",A Proof of Theorem 9,[0],[0]
Proof.,A Proof of Theorem 9,[0],[0]
"For this proof, we think of Qj as a binary variable that is dependent on z1, . . .",A Proof of Theorem 9,[0],[0]
", zj−1 and xj .",A Proof of Theorem 9,[0],[0]
"Similarly Q′j depends on z1, . . .",A Proof of Theorem 9,[0],[0]
", zj−1 and x ′ j .",A Proof of Theorem 9,[0],[0]
"Using this notation, and decomposing the square loss, we get
Mj(g) =",A Proof of Theorem 9,[0],[0]
Qj [ (g(xj)− cj)2,A Proof of Theorem 9,[0],[0]
"− (g?(xj)− cj)2 ] = Qj` 2(g, xj) + 2Qjξj`(g, xj).
",A Proof of Theorem 9,[0],[0]
"As such, we can write
Mj(g)−M ′j(g)− 2β1Q′j`2(g, x′j) = (1 + β1)Qj`2(g, xj) + 2Qjξj`(g, xj)︸ ︷︷ ︸ ,T1,j −β1Qj`2(g, xj)︸ ︷︷ ︸ ,T2,j
− (1 + β1)Q′j`2(g, x′j)−",A Proof of Theorem 9,[0],[0]
"2Q′jξ′j`(g, x′j)︸ ︷︷ ︸ ,T ′1,j −β1Q′j`2(g, x′j)︸ ︷︷ ︸ ,T ′2,j .
",A Proof of Theorem 9,[0],[0]
"Here we have introduce the short forms T1,j , T2,j and the primed version just to condense the derivations.",A Proof of Theorem 9,[0],[0]
"Overall we must bound
P sup g∈G",A Proof of Theorem 9,[0],[0]
"i∑ j=1 T1,j − T2,j − T ′1,j − T",A Proof of Theorem 9,[0],[0]
"′2,j ≥ τ  ",A Proof of Theorem 9,[0],[0]
"= E1 supg∈G i∑ j=1 T1,j − T2,j − T ′1,j − T ′2,j ≥ τ  .
",A Proof of Theorem 9,[0],[0]
"Observe that in the final term T1,i, T ′1,i are random variables with identical conditional distribution, since there are no further dependencies and (xi, ξi, Qi) are identically distributed to (x′i, ξ ′",A Proof of Theorem 9,[0],[0]
"i, Q ′",A Proof of Theorem 9,[0],[0]
i).,A Proof of Theorem 9,[0],[0]
"As such, we can symmetrize the ith term by introducing the Rademacher random variable i ∈ {−1,+1} to obtain
EZ,Z′E i1 supg∈G i−1∑ j=1 T1,j − T ′1,j + i(T1,i − T ′1,i)− i∑ j=1 (T2,j + T ′",A Proof of Theorem 9,[0],[0]
"2,j)",A Proof of Theorem 9,[0],[0]
≥ τ  ≤,A Proof of Theorem 9,[0],[0]
"EZ,Z′ sup
Qi,Q′i
E i1",A Proof of Theorem 9,[0],[0]
"supg∈G i−1∑ j=1 T1,j − T ′1,j +",A Proof of Theorem 9,[0],[0]
"i(T1,i − T ′1,i)− i∑ j=1 (T2,j + T ′",A Proof of Theorem 9,[0],[0]
"2,j) ≥ τ  .",A Proof of Theorem 9,[0],[0]
"Here in the second step, we have replaced the expectation over Qi, Q′i with supremum, which breaks the future dependencies for the (i − 1)st term.",A Proof of Theorem 9,[0],[0]
"Note that while we still write EZ,Z′ , we are no longer taking expectation over Qi, Q′i here.",A Proof of Theorem 9,[0],[0]
"The important point is that since xj , ξj are all i.i.d., the only dependencies in the martingale are through Qjs and by taking supremum over Qi, Q′i, swapping the role of T1,i−1 and T ′ 1,i−1 no longer has any future effects.",A Proof of Theorem 9,[0],[0]
Thus we can symmetrize the (i− 1)st term.,A Proof of Theorem 9,[0],[0]
"Continuing in this way, we get
EE i−1 sup Qi,Q′i E i1",A Proof of Theorem 9,[0],[0]
"supg∈G i−2∑ j=1 T1,j − T ′1,j + i∑ j=i−1 j(T1,j − T ′1,j)−",A Proof of Theorem 9,[0],[0]
"i∑ j=1 (T2,j + T ′ 2,j)",A Proof of Theorem 9,[0],[0]
≥ τ  ≤,A Proof of Theorem 9,[0],[0]
"E
〈 sup Qj ,Q′j E j 〉i j=1 1 supg∈G i∑ j=1 j(T1,j − T ′1,j)−",A Proof of Theorem 9,[0],[0]
"i∑ j=1 (T2,j + T ′ 2,j) ≥ τ  .",A Proof of Theorem 9,[0],[0]
"Here in the final expression the outer expectation is just over the variables xj , x′j , ξj , ξ ′ j and the bracket notation denotes interleaved supremum and expectation.",A Proof of Theorem 9,[0],[0]
"Expanding the definitions of T1,i, T2,i, we currently have
E 〈 sup Qj ,Q′j E j 〉i j=1 1 supg∈G i∑ j=1 j",A Proof of Theorem 9,[0],[0]
"[ (1 + β1)Qj` 2(g, xj) + 2Qjξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− i∑ j=1 β1Qj` 2(g, xj)
− i∑
j=1
j [ (1 + β1)Q ′",A Proof of Theorem 9,[0],[0]
"j` 2(g, x′j) +",A Proof of Theorem 9,[0],[0]
2Q ′,A Proof of Theorem 9,[0],[0]
jξ ′,A Proof of Theorem 9,[0],[0]
"j`(g, x ′ j) ]",A Proof of Theorem 9,[0],[0]
"− i∑ j=1 β1Q ′ j` 2(g, x′j)",A Proof of Theorem 9,[0],[0]
≥ τ  .,A Proof of Theorem 9,[0],[0]
"Next we use the standard trick of splitting the supremum over g into a supremum over two functions g, g′, where g′ optimizes the primed terms.",A Proof of Theorem 9,[0],[0]
"This provides an upper bound, but moreover if we replace τ with τ/2 we can split the indicator into two and this becomes
2E 〈 sup Qj E j 〉i j=1 1 supg∈G i∑ j=1 jQj [ (1 + β1)` 2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− β1Qj`2(g, xj) ≥ τ/2  = 2E sup
Q P sup g∈G i∑ j=1 jQj( ) [ (1 + β1)` 2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− β1Qj( )`2(g, xj) ≥ τ/2  .",A Proof of Theorem 9,[0],[0]
"The tree process Q arises here because the interleaved supremum and expectation is equivalent to choosing a binary tree decorated with values from {0, 1} and then navigating the tree using the Rademacher random variables .",A Proof of Theorem 9,[0],[0]
"The bound for the other tail is proved in the same way, except (1+β1) is replaced by (1−β1).
",A Proof of Theorem 9,[0],[0]
"The next lemma is more standard, and follows from the union bound and the bound on the Rademacher moment generating function.
",A Proof of Theorem 9,[0],[0]
Lemma 9 (Finite class bound).,A Proof of Theorem 9,[0],[0]
"For any x1:i, ξ1:i,Q, and for finite G, we have
P max g∈G i∑ j=1 jQj( ) [ (1 + β1)` 2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− β1Qj( )`2(g, xj) ≥ τ  ≤ |G| exp( −2β1τ (3 + β1)2 ) .
",A Proof of Theorem 9,[0],[0]
"The same bound applies for the lower tail.
",A Proof of Theorem 9,[0],[0]
Proof.,A Proof of Theorem 9,[0],[0]
"Applying the union bound and the Chernoff trick, we get that for any λ > 0",A Proof of Theorem 9,[0],[0]
"the LHS is bounded by
∑ g exp(−τλ)E",A Proof of Theorem 9,[0],[0]
"exp λ  i∑ j=1 jQj( ) [ (1 + β1)` 2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− β1Qi( )`2(g, xj)︸ ︷︷ ︸
,T3,j
 
= ∑ g exp(−τλ)E 1:i−1 exp λ i−1∑ j=1 T3,j × E i| 1:i−1 exp(λT3,i).",A Proof of Theorem 9,[0],[0]
Let us examine the ith term conditional on 1:i−1.,A Proof of Theorem 9,[0],[0]
"Conditionally on 1:i−1, Qi( ) is no longer random, so we can apply the MGF bound for Rademacher random variables to get
E i| 1:i−1 exp { λ iQi( ) [ (1 + β1)` 2(g, xi) + 2ξi`(g, xi) ]",A Proof of Theorem 9,[0],[0]
"− λβ1Qi( )`2(g, xi) } ≤ exp { λ2
2 Qi( )
[ (1 + β1)` 2(g, xi) + 2ξi`(g, xi) ]2 − λβ1Qi( )`2(g, xi)}
≤ exp { λ2Qi( ) (3 + β1) 2
2 `2(g, xi)− λβ1Qi( )`2(g, xi)
} ≤ 1.
Here the first inequality is the standard MGF bound on Rademacher random variables E exp(a )",A Proof of Theorem 9,[0],[0]
≤,A Proof of Theorem 9,[0],[0]
exp(a2/2).,A Proof of Theorem 9,[0],[0]
"In the second line we expand the square and use that `, ξ ∈",A Proof of Theorem 9,[0],[0]
"[−1, 1] to upper bound all the terms.",A Proof of Theorem 9,[0],[0]
"Finally, we use the choice of λ = 2β1(3+β1)2 .",A Proof of Theorem 9,[0],[0]
"Repeating this argument from i down to 1, finishes the proof of the upper tail.",A Proof of Theorem 9,[0],[0]
"The same argument applies for the lower tail, but we actually get (3− β1)2 in the denominator, which is of course upper bounded by (3 + β1)2, since β1 > 0.
",A Proof of Theorem 9,[0],[0]
Lemma 10 (Discretization).,A Proof of Theorem 9,[0],[0]
"Fix x1, . . .",A Proof of Theorem 9,[0],[0]
", xi and let V ⊂",A Proof of Theorem 9,[0],[0]
"Ri be a cover of G at scale α on points x1, . . .",A Proof of Theorem 9,[0],[0]
", xi.",A Proof of Theorem 9,[0],[0]
"Then for any Q
P sup g∈G i∑ j=1 jQj( ) [ (1 + β1)` 2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− β1Qj( )`2(g, xj) ≥ τ  ≤ P sup v∈V i∑ j=1 jQj( ) [ (1 + β1)` 2(v, xj) + 2ξj`(v, xj) ]",A Proof of Theorem 9,[0],[0]
"− β1Qj( )`2(v, xj) ≥ τ",A Proof of Theorem 9,[0],[0]
"− 4i(1 + β)α
 .",A Proof of Theorem 9,[0],[0]
The same bound holds for the lower tail with 1− β1.,A Proof of Theorem 9,[0],[0]
"Here `(v, xj) = (",A Proof of Theorem 9,[0],[0]
"vj − g?(xj)).
",A Proof of Theorem 9,[0],[0]
Proof.,A Proof of Theorem 9,[0],[0]
"Observe first that if v is the covering element for g, then we are guaranteed that
1
i i∑ j=1 |`(g, xj)−",A Proof of Theorem 9,[0],[0]
"`(v, xj)| = 1",A Proof of Theorem 9,[0],[0]
i i∑ j=1 |g(xj)−,A Proof of Theorem 9,[0],[0]
"vj | ≤ α,
1
i i∑ j=1 |`2(g, xj)− `2(v, xj)| = 1",A Proof of Theorem 9,[0],[0]
"i i∑ j=1 |(g(xj)− vj)(g(xj) + vj − 2g?(xj)| ≤ 2α,
since g, v, g? ∈",A Proof of Theorem 9,[0],[0]
"[0, 1].",A Proof of Theorem 9,[0],[0]
"Thus, adding and subtracting the corresponding terms for v, and applying these bounds, we get a residual term of iα(2(1 +",A Proof of Theorem 9,[0],[0]
β1) + 2 + 2β1),A Proof of Theorem 9,[0],[0]
= 4iα(1,A Proof of Theorem 9,[0],[0]
"+ β1).
",A Proof of Theorem 9,[0],[0]
Proof of Theorem 9.,A Proof of Theorem 9,[0],[0]
Finally we can derive the deviation bound.,A Proof of Theorem 9,[0],[0]
"We first do the upper tail,",A Proof of Theorem 9,[0],[0]
Mj − EjMj .,A Proof of Theorem 9,[0],[0]
"Set β0 = 1/4, β1 = 1/8 and apply Lemmas 7 and 8 to (22).
",A Proof of Theorem 9,[0],[0]
P sup g∈G i∑ j=1 Mj(g)− 3 2,A Proof of Theorem 9,[0],[0]
EjMj(g),A Proof of Theorem 9,[0],[0]
≥ τ  ≤ 2P sup g∈G,A Proof of Theorem 9,[0],[0]
"i∑ j=1 Mj(g)−M ′j(g)− 1 4 Qj(x ′ j)` 2(g, x′j) ≥ τ/2
 ≤ 4E sup
Q P sup g∈G",A Proof of Theorem 9,[0],[0]
"i∑ j=1 jQj( ) ( 9 8 `2(g, xj) + 2ξj`(g, xj) )",A Proof of Theorem 9,[0],[0]
"− 1 8 Qj( i)`2(g, xj) ≥ τ 4  .",A Proof of Theorem 9,[0],[0]
"Now let V (X) be the cover for G at scale α = τ32i(9/8) = τ 36i , which makes τ/4− 4i(1 + β1)α",A Proof of Theorem 9,[0],[0]
= τ,A Proof of Theorem 9,[0],[0]
8 .,A Proof of Theorem 9,[0],[0]
"Thus we get the bound
≤ 4EX,ξ sup Q P  sup h∈H(X) i∑ j=1 jQj( ) ( 9 8 `2(h, xj) + 2ξj`(h, xj) )",A Proof of Theorem 9,[0],[0]
"− 1 8 Qj( i)`2(h, xj) ≥ τ 8  ≤ 4EX |V (X)| exp ( −2(1/8)(τ/8)
(3 + 1/8)2
) = 2 exp(−2τ/625)EXN ( τ 36i ,G, X ) .
",A Proof of Theorem 9,[0],[0]
"This entire derivation requires that τ ≥ 4(9/8) 2
(1/8)2 = 324.",A Proof of Theorem 9,[0],[0]
The lower tail bound is similar.,A Proof of Theorem 9,[0],[0]
"By Lemmas 7 and 8, with β0 = 1/4 and β1 = 1/8,
P [ sup g∈G n∑ i=1",A Proof of Theorem 9,[0],[0]
1 2 EjMj(g)−Mj(g),A Proof of Theorem 9,[0],[0]
"≥ τ ]
≤ 2P [ sup g∈G n∑ i=1",A Proof of Theorem 9,[0],[0]
"1 2 EjMj(g)−Mj(g)− 2(1/8)Qj(x′j)`2(g, x′j) ≥",A Proof of Theorem 9,[0],[0]
"τ 2 ]
≤",A Proof of Theorem 9,[0],[0]
"4E sup Q P sup g∈G i∑ j=1 jQj( ) [ 7 8 `2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− 1 8 i∑ j=1 Qi( )`2(g, xj) ≥ τ 4  ≤ 4E sup
Q P sup g∈G i∑ j=1 jQj( ) [ 9 8 `2(g, xj) + 2ξj`(g, xj) ]",A Proof of Theorem 9,[0],[0]
"− 1 8 i∑ j=1 Qi( )`2(g, xj) ≥ τ 4  .
",A Proof of Theorem 9,[0],[0]
"This is the intermediate term we had for the upper tail, so we obtain the same bound.",A Proof of Theorem 9,[0],[0]
"To wrap up the proof, apply Haussler’s Lemma 6, to bound the covering number
EXN ( τ
36i ,G, X
)",A Proof of Theorem 9,[0],[0]
"≤ e(d+ 1) ( 72ie
τ
)d .
",A Proof of Theorem 9,[0],[0]
"Finally take a union bound over all pairs of starting and ending indices i < i′, all labels y, and both tails to get that the total failure probability is at most
8Ke(d+ 1) exp (−2τ/625)",A Proof of Theorem 9,[0],[0]
"∑
i<i′∈[n]
( 72e(i′ − i)
τ
)d .
",A Proof of Theorem 9,[0],[0]
The result now follows from standard approximations.,A Proof of Theorem 9,[0],[0]
"Specifically we use the fact that we anyway require τ ≥ 324 to upper bound that 1/τd term, use (i′ − i)d ≤ nd and set the whole expression to be at most δ.",A Proof of Theorem 9,[0],[0]
For completeness we prove Theorem 10 here.,B Multiplicative Weights,[0],[0]
For this section only let q(t) ∝,B Multiplicative Weights,[0],[0]
"p(t) be the distribution used by the algorithm at round t. If the program is feasible, then there exists a point that is also feasible against every distribution q. By contraposition, if on iteration t, the oracle reports infeasibility against q(t), then the original program must be infeasible.
",B Multiplicative Weights,[0],[0]
Now suppose the oracle always finds vt that is feasible against q(t).,B Multiplicative Weights,[0],[0]
"This implies
0 ≤ T∑ t=1",B Multiplicative Weights,[0],[0]
n∑ i=1,B Multiplicative Weights,[0],[0]
q,B Multiplicative Weights,[0],[0]
(t),B Multiplicative Weights,[0],[0]
"i (bi − 〈ai, vt〉).
",B Multiplicative Weights,[0],[0]
"Define `t(i) = bi−〈ai,vt〉
ρi which is in [−1, 1] by assumption.",B Multiplicative Weights,[0],[0]
"We compare this term to the corresponding term for a single constraint i. Using the standard potential-based analysis, with Φ(t) =",B Multiplicative Weights,[0],[0]
∑ i p (t),B Multiplicative Weights,[0],[0]
"i , we get
Φ(T+1) =",B Multiplicative Weights,[0],[0]
m∑ i=1,B Multiplicative Weights,[0],[0]
p (T ),B Multiplicative Weights,[0],[0]
i (1− η`T (i)),B Multiplicative Weights,[0],[0]
≤,B Multiplicative Weights,[0],[0]
"Φ (T ) exp
( −η
m∑ i=1",B Multiplicative Weights,[0],[0]
q,B Multiplicative Weights,[0],[0]
(T ),B Multiplicative Weights,[0],[0]
"i `T (i)
) ≤ m exp ( −η
T∑ t=1",B Multiplicative Weights,[0],[0]
m∑ i=1 q,B Multiplicative Weights,[0],[0]
"(t) i `t(i)
) .
",B Multiplicative Weights,[0],[0]
"For any i, we also have
Φ(T+1) ≥ T∏ t=1",B Multiplicative Weights,[0],[0]
"(1− η`t(i)).
",B Multiplicative Weights,[0],[0]
"Thus, taking taking logarithms and re-arranging we get
0 ≤ T∑ t=1",B Multiplicative Weights,[0],[0]
m∑ i=1 q,B Multiplicative Weights,[0],[0]
(t),B Multiplicative Weights,[0],[0]
i `t(i) ≤,B Multiplicative Weights,[0],[0]
log(m) η + 1 η T∑ t=1 log ( 1 1− η`t(i) ),B Multiplicative Weights,[0],[0]
"≤ logm η + T∑ t=1 `t(i) + ηT.
Here we use standard approximations log(1/(1− x))",B Multiplicative Weights,[0],[0]
≤,B Multiplicative Weights,[0],[0]
x+ x2 (which holds for x ≤ 1/2) along with the fact that |`t(i)| ≤ 1.,B Multiplicative Weights,[0],[0]
"Using the definition of `t(i) we have hence proved that
T∑ t=1 〈ai, vt〉 ≤",B Multiplicative Weights,[0],[0]
"Tbi + ρi logm η + ρiηT.
Now with our choice of η = √
log(m)/T we get the desired bound.",B Multiplicative Weights,[0],[0]
If η ≥ 1/2 then the result is trivial by the boundedness guarantee on the oracle.,B Multiplicative Weights,[0],[0]
Proof of Lemma 1.,C Proofs of Lemmata,[0],[0]
"By the definitions,
R̂(g′) + w′(g′(x)− c)2 = R̂(g′, w′, c) ≤ R̂(g) + w′(g(x)− c)2
= R̂(g) + w(g(x)− c)2 + (w′",C Proofs of Lemmata,[0],[0]
"− w)(g(x)− c)2
≤ R̂(g′) + w(g′(x)− c)2 + (w′ − w)(g(x)− c)2.
",C Proofs of Lemmata,[0],[0]
Rearranging shows that (w′−w)(g′(x)−c)2 ≤ (w′−w)(g(x)−c)2.,C Proofs of Lemmata,[0],[0]
Since w′ ≥,C Proofs of Lemmata,[0],[0]
"w, we have (g′(x)−c)2 ≤ (g(x)− c)2, which is the second claim.",C Proofs of Lemmata,[0],[0]
"For the first, the definition of g gives
R̂(g) + w(g(x)− c)2 ≤ R̂(g′) + w(g′(x)− c)2.
",C Proofs of Lemmata,[0],[0]
"Rearranging this inequality gives, R̂(g′)",C Proofs of Lemmata,[0],[0]
"− R̂(g) ≥ w((g(x) − c)2 − (g′(x) − c)2) ≥ 0, which yields the result.",C Proofs of Lemmata,[0],[0]
Proof of Lemma 2.,C Proofs of Lemmata,[0],[0]
We take expectation of Mj over the cost conditioned on a fixed example xj = x and a fixed query outcome,C Proofs of Lemmata,[0],[0]
"Qj(y):
E[Mj | xj = x,Qj(y)]",C Proofs of Lemmata,[0],[0]
= Qj(y)× Ec[g(x)2,C Proofs of Lemmata,[0],[0]
− f?(x; y)2 − 2c(y)(g(x)− f?(x; y)),C Proofs of Lemmata,[0],[0]
| xj = x] = Qj(y) ( g(x)2 − f?(x; y)2 − 2f?(x; y)(g(x)− f?(x; y)) ),C Proofs of Lemmata,[0],[0]
"= Qj(y)(g(x)− f?(x; y))2.
",C Proofs of Lemmata,[0],[0]
"The second equality is by Assumption 1, which implies E[c(y)",C Proofs of Lemmata,[0],[0]
| xj = x] = f?(x; y).,C Proofs of Lemmata,[0],[0]
"Taking expectation over xj and Qj(y), we have
Ej",C Proofs of Lemmata,[0],[0]
"[Mj ] = Ej [ Qj(y)(g(xj)− f?(xj ; y))2 ] .
",C Proofs of Lemmata,[0],[0]
"For the variance:
Var j
[Mj ] ≤",C Proofs of Lemmata,[0],[0]
Ej,C Proofs of Lemmata,[0],[0]
[M2j ] = Ej [ Qj(y)(g(xj)− f?(xj ; y))2(g(xj) + f?(xj ; y)− 2c(y))2 ] ≤ 4 ·,C Proofs of Lemmata,[0],[0]
Ej [ Qj(y)(g(xj)− f?(xj ; y))2 ] = 4Ej,C Proofs of Lemmata,[0],[0]
"[Mj ].
",C Proofs of Lemmata,[0],[0]
This concludes the proof.,C Proofs of Lemmata,[0],[0]
Proof of Lemma 3.,C Proofs of Lemmata,[0],[0]
"Fix some f ∈ Fi, and let ŷ = hf (x) and y? = hf?(x) for shorthand, but note that both depend on x. Define
Sζ(x) = 1 (f ?(x, ŷ)",C Proofs of Lemmata,[0],[0]
≤,C Proofs of Lemmata,[0],[0]
"f?(x, y?)",C Proofs of Lemmata,[0],[0]
"+ ζ) , S′ζ(x) = 1 ( min y 6=y?",C Proofs of Lemmata,[0],[0]
"f?(x, y) ≤",C Proofs of Lemmata,[0],[0]
"f?(x, y?)",C Proofs of Lemmata,[0],[0]
"+ ζ ) .
",C Proofs of Lemmata,[0],[0]
"Observe that for fixed ζ, Sζ(x)1 (ŷ 6= y?) ≤",C Proofs of Lemmata,[0],[0]
S′ζ(x),C Proofs of Lemmata,[0],[0]
"for all x. We can also majorize the complementary indicator to obtain the inequality
SCζ (x) ≤",C Proofs of Lemmata,[0],[0]
"f?(x, ŷ)−",C Proofs of Lemmata,[0],[0]
"f?(x, y?)
ζ .
",C Proofs of Lemmata,[0],[0]
"We begin with the definition of realizability, which gives
Ex,c[c(hf (x))− c(hf?(x)]",C Proofs of Lemmata,[0],[0]
= Ex,C Proofs of Lemmata,[0],[0]
"[(f?(x, ŷ)− f?(x, y?))1 (ŷ 6= y?)]",C Proofs of Lemmata,[0],[0]
"= Ex [( Sζ(x) + S C ζ (x) ) (f?(x, ŷ)−",C Proofs of Lemmata,[0],[0]
"f?(x, y?))1 (ŷ 6= y?) ]",C Proofs of Lemmata,[0],[0]
≤ ζExS′ζ(x) +,C Proofs of Lemmata,[0],[0]
"Ei [ SCζ (x)1 (ŷ 6= y?) (f?(x, ŷ)−",C Proofs of Lemmata,[0],[0]
"f?(x, y?)) ] .
",C Proofs of Lemmata,[0],[0]
The first term here is exactly the ζPζ term in the bound.,C Proofs of Lemmata,[0],[0]
"We now focus on the second term, which depends on our query rule.",C Proofs of Lemmata,[0],[0]
"For this we must consider three cases.
",C Proofs of Lemmata,[0],[0]
Case 1.,C Proofs of Lemmata,[0],[0]
"If both ŷ and y? are not queried, then it must be the case that both have small cost ranges.",C Proofs of Lemmata,[0],[0]
This follows since f ∈ Fi and hf (x) = ŷ,C Proofs of Lemmata,[0],[0]
"so y? does not dominate ŷ. Moreover, since the cost ranges are small on both ŷ and y?",C Proofs of Lemmata,[0],[0]
"and since we know that f? is well separated under event SCζ (x), the relationship between ζ and ψi governs whether we make a mistake or not.",C Proofs of Lemmata,[0],[0]
"Specifically, we get that SCζ (x)1 (ŷ 6=",C Proofs of Lemmata,[0],[0]
"y?)1 (no query) ≤ 1 (ζ ≤ 2ψi) at round i. In other words, if we do not query and the separation is big but we make a mistake, then it must mean that the cost range threshold ψi is also big.
",C Proofs of Lemmata,[0],[0]
"Using this argument, we can bound the second term as,
Ei [ SCζ (x)1 (ŷ 6= y?)",C Proofs of Lemmata,[0],[0]
(1−Qi(ŷ))(1−Qi(y?)),C Proofs of Lemmata,[0],[0]
"(f?(x, ŷ)−",C Proofs of Lemmata,[0],[0]
"f?(x, y?))",C Proofs of Lemmata,[0],[0]
] ≤,C Proofs of Lemmata,[0],[0]
"Ei [ SCζ (x)1 (ŷ 6= y?) (1−Qi(ŷ))(1−Qi(y?))2ψi
] ≤",C Proofs of Lemmata,[0],[0]
Ei [1 (ζ ≤ 2ψi),C Proofs of Lemmata,[0],[0]
"2ψi] = 1 (ζ ≤ 2ψi) 2ψi.
",C Proofs of Lemmata,[0],[0]
Case 2.,C Proofs of Lemmata,[0],[0]
"If both ŷ and y? are queried, we can relate the second term to the square loss,
Ei [ SCζ (x)Qi(ŷ)Qi(y ?)",C Proofs of Lemmata,[0],[0]
"(f?(x, ŷ)−",C Proofs of Lemmata,[0],[0]
"f?(x, y?))",C Proofs of Lemmata,[0],[0]
"]
≤ 1 ζ",C Proofs of Lemmata,[0],[0]
Ei [ Qi(ŷ)Qi(y ?),C Proofs of Lemmata,[0],[0]
"(f?(x, ŷ)− f?(x, y?))2 ]
≤ 1 ζ",C Proofs of Lemmata,[0],[0]
Ei [ Qi(ŷ)Qi(y ?),C Proofs of Lemmata,[0],[0]
"(f?(x, ŷ)− f(x, ŷ) + f(x, y?)− f?(x, y?))2 ]
≤ 2 ζ",C Proofs of Lemmata,[0],[0]
"Ei [ Qi(ŷ)(f ?(x, ŷ)− f(x, ŷ))2 +Qi(y?)(f(x, y?)− f?(x, y?))2 ]
≤ 2 ζ ∑ y Ei [ Qi(y)(f ?",C Proofs of Lemmata,[0],[0]
"(x, y)− f(x, y))2 ] = 2 ζ ∑ y Ei",C Proofs of Lemmata,[0],[0]
"[Mi(f ; y)] .
",C Proofs of Lemmata,[0],[0]
"Passing from the second to third line here is justified by the fact that f?(x, ŷ) ≥",C Proofs of Lemmata,[0],[0]
"f?(x, y?) and f(x, ŷ) ≤ f(x, y?)",C Proofs of Lemmata,[0],[0]
so we added two non-negative quantities together.,C Proofs of Lemmata,[0],[0]
The last step uses Lemma 2.,C Proofs of Lemmata,[0],[0]
"While not written, we also use the event 1 (ŷ 6= y?) to save a factor of 2.
",C Proofs of Lemmata,[0],[0]
Case 3.,C Proofs of Lemmata,[0],[0]
The last case is if one label is queried and the other is not.,C Proofs of Lemmata,[0],[0]
"Both cases here are analogous, so we do the derivation for when y(x) is queried but y?(x) is not.",C Proofs of Lemmata,[0],[0]
"Since in this case, y?(x) is not dominated (hf (x) is never dominated provided f ∈ Fi), we know that the cost range for y?(x) must be small.",C Proofs of Lemmata,[0],[0]
"Using this fact,
and essentially the same argument as in case 2, we get Ei [ SCζ (x)Qi(ŷ)(1−Qi(y?))",C Proofs of Lemmata,[0],[0]
"(f?(x, ŷ)−",C Proofs of Lemmata,[0],[0]
"f?(x, y?))",C Proofs of Lemmata,[0],[0]
] ≤ 1 ζ,C Proofs of Lemmata,[0],[0]
Ei [ Qi(ŷ)(1−Qi(y?)),C Proofs of Lemmata,[0],[0]
"(f?(x, ŷ)− f?(x, y?))2
] ≤ 2 ζ",C Proofs of Lemmata,[0],[0]
"Ei [ Qi(ŷ) (f ?(x, ŷ)− f(x, ŷ))2 + (1−Qi(y?))",C Proofs of Lemmata,[0],[0]
"(f(x, y?)− f?(x, y?))2 ]
≤",C Proofs of Lemmata,[0],[0]
2ψ 2,C Proofs of Lemmata,[0],[0]
"i
ζ +
2 ζ",C Proofs of Lemmata,[0],[0]
"Ei [ Qi(ŷ) (f ?(x, ŷ)− f(x, ŷ))2 ] ≤",C Proofs of Lemmata,[0],[0]
2ψ 2 i ζ + 2 ζ ∑,C Proofs of Lemmata,[0],[0]
y Ei,C Proofs of Lemmata,[0],[0]
"[Mi(f ; y)] .
",C Proofs of Lemmata,[0],[0]
We also obtain this term for the other case where y? is queried but ŷ is not.,C Proofs of Lemmata,[0],[0]
"To summarize, adding up the contributions from these cases (which is an over-estimate since at most one case can occur and all are non-negative), we get
Ex,c[c(hf (x))− c(hf?(x)]",C Proofs of Lemmata,[0],[0]
≤ ζPζ + 1 (ζ ≤ 2ψi),C Proofs of Lemmata,[0],[0]
2ψi + 4ψ2i ζ + 6 ζ ∑,C Proofs of Lemmata,[0],[0]
y Ei,C Proofs of Lemmata,[0],[0]
"[Mi(f ; y)] .
",C Proofs of Lemmata,[0],[0]
"This bound holds for any ζ, so it holds for the minimum.",C Proofs of Lemmata,[0],[0]
Proof of Lemma 4.,C Proofs of Lemmata,[0],[0]
The proof here is an easier version of the generalization bound proof for fi.,C Proofs of Lemmata,[0],[0]
"First, condition on the high probability event in Theorem 9, under which we already showed that f? ∈",C Proofs of Lemmata,[0],[0]
Fi for all,C Proofs of Lemmata,[0],[0]
i ∈,C Proofs of Lemmata,[0],[0]
[n].,C Proofs of Lemmata,[0],[0]
Now fix some f ∈ Fi.,C Proofs of Lemmata,[0],[0]
"Since by the monotonicity property defining the sets Gi, we must have f ∈ Fj for all 1 ≤ j ≤",C Proofs of Lemmata,[0],[0]
"i, we can immediately apply Lemma 3 on all rounds to bound the cost sensitive regret by
1
i− 1 i−1∑ j=1 min ζ>0
{ ζPζ + 1{ζ ≤ 2ψj}2ψj",C Proofs of Lemmata,[0],[0]
"+
4ψ2j ζ + 6 ζ ∑ y Ej",C Proofs of Lemmata,[0],[0]
"[Mj(f ; y)]
} .
",C Proofs of Lemmata,[0],[0]
"As in the generalization bound, the first term contributes ζPζ , the second is at most 12ζ and the third is at most 4/ζ(1 + log(i− 1)).",C Proofs of Lemmata,[0],[0]
The fourth term is slightly different.,C Proofs of Lemmata,[0],[0]
"We still apply (17) to obtain
1
i− 1 i−1∑ j=1",C Proofs of Lemmata,[0],[0]
Ej [Mj(f ; y)] ≤ 2 i− 1 i−1∑ j=1 Mj(f ; y) +,C Proofs of Lemmata,[0],[0]
"2νn i− 1
= 2 ( R̂i(f ; y)− R̂i(gi,y; y) + R̂i(gi,y; y)− R̂i(f?; y) )",C Proofs of Lemmata,[0],[0]
"+
2νn i− 1
≤ 2∆i + 2νn i− 1 = 2(κ+ 1)νn i− 1 .
",C Proofs of Lemmata,[0],[0]
We use this bound for each label.,C Proofs of Lemmata,[0],[0]
"Putting terms together, the cost sensitive regret is
min ζ>0
{ ζPζ + 12
ζ + 4(1 + log(i− 1)) ζ +",C Proofs of Lemmata,[0],[0]
12K(κ+,C Proofs of Lemmata,[0],[0]
"1)νn ζ(i− 1)
} ≤ min
ζ>0
{ ζPζ +
44κKνn ζ(i− 1)
} .
",C Proofs of Lemmata,[0],[0]
"This proves containment, since this upper bounds the cost sensitive regret of every f ∈ Fi.",C Proofs of Lemmata,[0],[0]
Proof of Lemma 5.,C Proofs of Lemmata,[0],[0]
"The first claim is straightforward, since Fi ⊂ Fcsr(ri) and since we set the tolerance parameter in the calls to MAXCOST and MINCOST to ψi/4.",C Proofs of Lemmata,[0],[0]
"Specifically,
γ̂(xi, y) ≤ γ(xi, y,Fi) + ψi/2 ≤ γ(xi, y,Fcsr(ri))",C Proofs of Lemmata,[0],[0]
"+ ψi/2.
For the second claim, suppose y 6= y?i .",C Proofs of Lemmata,[0],[0]
"Then
y ∈",C Proofs of Lemmata,[0],[0]
Yi ⇒,C Proofs of Lemmata,[0],[0]
"ĉ−(xi, y) ≤",C Proofs of Lemmata,[0],[0]
"ĉ+(xi, ȳi) ⇒",C Proofs of Lemmata,[0],[0]
"ĉ−(xi, y) ≤",C Proofs of Lemmata,[0],[0]
"ĉ+(xi, y?i ) ⇒",C Proofs of Lemmata,[0],[0]
"c−(xi, y,Fcsr(ri))",C Proofs of Lemmata,[0],[0]
"≤ c+(xi, y?i ,Fcsr(ri))",C Proofs of Lemmata,[0],[0]
"+ ψi/2 ⇒ f?(xi; y)− γ(xi, y,Fcsr(ri))",C Proofs of Lemmata,[0],[0]
"≤ f?(xi; y?i ) + γ(xi, y?i ,Fcsr(ri))",C Proofs of Lemmata,[0],[0]
"+ ψi/2.
",C Proofs of Lemmata,[0],[0]
"This argument uses the tolerance setting ψi/4, Lemma 4 to translate between the version space and the cost-sensitive regret ball, and finally the fact that f? ∈ Fcsr(ri) since it has zero cost-sensitive regret.",C Proofs of Lemmata,[0],[0]
"This latter fact lets us lower (upper) bound the minimum (maximum) cost by f? prediction minus (plus) the cost range.
",C Proofs of Lemmata,[0],[0]
For y?i we need to consider two cases.,C Proofs of Lemmata,[0],[0]
First assume y ?,C Proofs of Lemmata,[0],[0]
i = ȳi.,C Proofs of Lemmata,[0],[0]
"Since by assumption |Yi| > 1, it must be the case that ĉ−(xi, ỹi) ≤",C Proofs of Lemmata,[0],[0]
"ĉ+(xi, y?i ) at which point the above derivation produces the desired implication.",C Proofs of Lemmata,[0],[0]
"On the other hand, if y?i 6= ȳi then ĉ+(xi, ȳi) ≤",C Proofs of Lemmata,[0],[0]
"ĉ+(xi, y?i ), but this also implies that ĉ−(xi, ỹi) ≤",C Proofs of Lemmata,[0],[0]
"ĉ+(xi, y?i ), since minimum costs are always smaller than maximum costs, and ȳi is included in the search defining ỹi.",C Proofs of Lemmata,[0],[0]
We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs.,abstractText,[0],[0]
"Our algorithm, COAL, makes predictions by regressing to each label’s cost and predicting the smallest.",abstractText,[0],[0]
"On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label.",abstractText,[0],[0]
"It queries only the labels that could be the best, ignoring the sure losers.",abstractText,[0],[0]
We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort.,abstractText,[0],[0]
"We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets.",abstractText,[0],[0]
Active Learning for Cost-Sensitive Classification,title,[0],[0]
