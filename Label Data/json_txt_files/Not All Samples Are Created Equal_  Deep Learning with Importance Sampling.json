{"sections": [{"text": "We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \u201cinformative\u201d examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the persample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup.\nThe resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%."}, {"heading": "1. Introduction", "text": "The dramatic increase in available training data has made the use of deep neural networks feasible, which in turn has significantly improved the state-of-the-art in many fields, in particular computer vision and natural language processing. However, due to the complexity of the resulting optimization problem, computational cost is now the core issue in training these large architectures.\nWhen training such models, it appears to any practitioner that not all samples are equally important; many of them are properly handled after a few epochs of training, and most could be ignored at that point without impacting the final\n1Idiap Research Institute, Switzerland 2E\u0301cole Polytechique Fe\u0301de\u0301rale de Lausanne, Switzerland. Correspondence to: Angelos Katharopoulos <firstname.lastname@idiap.ch>.\nmodel. To this end, we propose a novel importance sampling scheme that accelerates the training of any neural network architecture by focusing the computation on the samples that will introduce the biggest change in the parameters which reduces the variance of the gradient estimates.\nFor convex optimization problems, many works (Bordes et al., 2005; Zhao & Zhang, 2015; Needell et al., 2014; Cane\u0301vet et al., 2016; Richta\u0301rik & Taka\u0301c\u030c, 2013) have taken advantage of the difference in importance among the samples to improve the convergence speed of stochastic optimization methods. On the other hand, for deep neural networks, sample selection methods were mainly employed to generate hard negative samples for embedding learning problems or to tackle the class imbalance problem (Schroff et al., 2015; Wu et al., 2017; Simo-Serra et al., 2015).\nRecently, researchers have shifted their focus on using importance sampling to improve and accelerate the training of neural networks (Alain et al., 2015; Loshchilov & Hutter, 2015; Schaul et al., 2015). Those works, employ either the gradient norm or the loss to compute each sample\u2019s importance. However, the former is prohibitively expensive to compute and the latter is not a particularly good approximation of the gradient norm.\nCompared to the aforementioned works, we derive an upper bound to the per sample gradient norm that can be computed in a single forward pass. This results in reduced computational requirements of more than an order of magnitude compared to Alain et al. (2015). Furthermore, we quantify the variance reduction achieved with the proposed importance sampling scheme and associate it with the batch size increment required to achieve an equivalent variance reduction. The benefits of this are twofold, firstly we provide an intuitive metric to predict how useful importance sampling is going to be, thus we are able to decide when to switch on importance sampling during training. Secondly, we also provide theoretical guarantees for speedup, when variance reduction is above a threshold. Based on our analysis, we propose a simple to use algorithm that can be used to accelerate the training of any neural network architecture.\nOur implementation is generic and can be employed by adding a single line of code in a standard Keras model\nar X\niv :1\n80 3.\n00 94\n2v 3\n[ cs\n.L G\n] 2\n8 O\nct 2\n01 9\ntraining. We validate it on three independent tasks: image classification, fine-tuning and sequence classification with recurrent neural networks. Compared to existing batch selection schemes, we show that our method consistently achieves lower training loss and test error for equalized wall-clock time."}, {"heading": "2. Related Work", "text": "Existing importance sampling methods can be roughly categorized in methods applied to convex problems and methods designed for deep neural networks."}, {"heading": "2.1. Importance Sampling for Convex Problems", "text": "Importance sampling for convex optimization problems has been extensively studied over the last years. Bordes et al. (2005) developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines. Later, Richta\u0301rik & Taka\u0301c\u030c (2013) proposed a generalized coordinate descent algorithm that samples coordinate sets in a way that optimizes the algorithm\u2019s convergence rate.\nMore recent works (Zhao & Zhang, 2015; Needell et al., 2014) make a clear connection with the variance of the gradient estimates of stochastic gradient descent and show that the optimal sampling distribution is proportional to the per sample gradient norm. Due to the relatively simple optimization problems that they deal with, the authors resort to sampling proportionally to the norm of the inputs, which in simple linear classification is proportional to the Lipschitz constant of the per sample loss function.\nSuch simple importance measures do not exist for Deep Learning and the direct application of the aforementioned theory (Alain et al., 2015), requires clusters of GPU workers just to compute the sampling distribution."}, {"heading": "2.2. Importance Sampling for Deep Learning", "text": "Importance sampling has been used in Deep Learning mainly in the form of manually tuned sampling schemes. Bengio et al. (2009) manually design a sampling scheme inspired by the perceived way that human children learn; in practice they provide the network with examples of increasing difficulty in an arbitrary manner. Diametrically opposite, it is common for deep embedding learning to sample hard examples because of the plethora of easy non informative ones (Simo-Serra et al., 2015; Schroff et al., 2015).\nMore closely related to our work, Schaul et al. (2015) and Loshchilov & Hutter (2015) use the loss to create the sampling distribution. Both approaches keep a history of losses for previously seen samples, and sample either proportionally to the loss or based on the loss ranking. One of the\nmain limitations of history based sampling, is the need for tuning a large number of hyperparameters that control the effects of \u201cstale\u201d importance scores; i.e. since the model is constantly updated, the importance of samples fluctuate and previous observations may poorly reflect the current situation. In particular, Schaul et al. (2015) use various forms of smoothing for the losses and the importance sampling weights, while Loshchilov & Hutter (2015) introduce a large number of hyperparameters that control when the losses are computed, when they are sorted as well as how the sampling distribution is computed based on the rank.\nIn comparison to all the above methods, our importance sampling scheme based on an upper bound to the gradient norm has a solid theoretical basis with clear objectives, very easy to choose hyperparameters, theoretically guaranteed speedup and can be applied to any type of network and loss function."}, {"heading": "2.3. Other Sample Selection Methods", "text": "For completeness, we mention the work of Wu et al. (2017), who design a distribution (suitable only for the distance based losses) that maximizes the diversity of the losses in a single batch. In addition, Fan et al. (2017) use reinforcement learning to train a neural network that selects samples for another neural network in order to optimize the convergence speed. Although their preliminary results are promising, the overhead of training two networks makes the wall-clock speedup unlikely and their proposal not as appealing."}, {"heading": "2.4. Stochastic Variance Reduced Gradient", "text": "Finally, a class of algorithms that aim to accelerate the convergence of Stochastic Gradient Descent (SGD) through variance reduction are SVRG type algorithms (Johnson & Zhang, 2013; Defazio et al., 2014; Allen-Zhu, 2017; Lei et al., 2017). Although asymptotically better, those algorithms typically perform worse than plain SGD with momentum for the low accuracy optimization setting of Deep Learning. Contrary to the aforementioned algorithms, our proposed importance sampling does not improve the asymptotic convergence of SGD but results in pragmatic improvements in all the metrics given a fixed time budget."}, {"heading": "3. Variance Reduction for Deep Neural Networks", "text": "Importance sampling aims at increasing the convergence speed of SGD by focusing computation on samples that actually induce a change in the model parameters. This formally translates into a reduced variance of the gradient estimates for a fixed computational cost. In the following sections, we analyze how this works and present an efficient algorithm that can be used to train any Deep Learning model."}, {"heading": "3.1. Introduction to Importance Sampling", "text": "Let xi, yi be the i-th input-output pair from the training set, \u03a8(\u00b7; \u03b8) be a Deep Learning model parameterized by the vector \u03b8, and L(\u00b7, \u00b7) be the loss function to be minimized during training. The goal of training is to find\n\u03b8\u2217 = arg min \u03b8\n1\nN N\u2211 i=1 L(\u03a8(xi; \u03b8), yi) (1)\nwhere N corresponds to the number of examples in the training set.\nWe use an SGD procedure with learning rate \u03b7, where the update at iteration t depends on the sampling distribution pt1, . . . , p t N and re-scaling coefficients w t 1, . . . , w t N . Let It be the data point sampled at that step, we have P (It = i) = pti and\n\u03b8t+1 = \u03b8t \u2212 \u03b7wIt\u2207\u03b8tL(\u03a8(xIt ; \u03b8t), yIt) (2)\nPlain SGD with uniform sampling is achieved with wti = 1 and pti = 1 N for all t and i.\nIf we define the convergence speed S of SGD as the reduction of the distance of the parameter vector \u03b8 from the optimal parameter vector \u03b8\u2217 in two consecutive iterations t and t+ 1\nS = \u2212EPt [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u201622 \u2212 \u2016\u03b8t \u2212 \u03b8 \u2217\u201622 ] , (3)\nand if we have wi = 1Npi such that\nEPt [wIt\u2207\u03b8tL(\u03a8(xIt ; \u03b8t), yIt)] (4) = \u2207\u03b8t 1N \u2211N i=1 L(\u03a8(xi; \u03b8t), yi), (5)\nand set Gi = wi\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi), then we get (this is a different derivation of the result by Wang et al., 2016)\nS = \u2212EPt [ (\u03b8t+1\u2212\u03b8\u2217)T (\u03b8t+1\u2212\u03b8\u2217)\u2212 (\u03b8t\u2212\u03b8\u2217)T (\u03b8t\u2212\u03b8\u2217) ] = \u2212EPt [ \u03b8Tt+1\u03b8t+1\u22122\u03b8t+1\u03b8\u2217 \u2212 \u03b8Tt \u03b8t + 2\u03b8t\u03b8\u2217\n] = \u2212EPt [ (\u03b8t\u2212\u03b7GIt) T (\u03b8t\u2212\u03b7GIt) + 2\u03b7GTIt\u03b8 \u2217\u2212\u03b8Tt \u03b8t ]\n= \u2212EPt [ \u22122\u03b7 (\u03b8t\u2212\u03b8\u2217)GIt + \u03b72GTItGIt ] = 2\u03b7 (\u03b8t\u2212\u03b8\u2217)EPt [GIt ]\u2212 \u03b72 EPt [GIt ]\nTEPt [GIt ]\u2212 \u03b72Tr (VPt [GIt ])\n(6)\nSince the first two terms, in the last expression, are the speed of batch gradient descent, we observe that it is possible to gain a speedup by sampling from the distribution that minimizes Tr (VPt [GIt ]). Several works (Needell et al., 2014; Zhao & Zhang, 2015; Alain et al., 2015) have shown the optimal distribution to be proportional to the per-sample gradient norm. However, computing this distribution is computationally prohibitive."}, {"heading": "3.2. Beyond the Full Gradient Norm", "text": "Given an upper bound G\u0302i \u2265 \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162 and due to\narg min P Tr (VPt [GIt ]) = arg min P\nEPt [ \u2016GIt\u2016 2 2 ] , (7)\nwe propose to relax the optimization problem in the following way\nmin P\nEPt [ \u2016GIt\u2016 2 2 ] \u2264 min P EPt [ w2ItG\u0302 2 It ] . (8)\nThe minimizer of the second term of equation 8, similar to the first term, is pi \u221d G\u0302i. All that remains, is to find a proper expression for G\u0302i which is significantly easier to compute than the norm of the gradient for each sample.\nIn order to continue with the derivation of our upper bound G\u0302i, let us introduce some notation specific to a multi-layer perceptron. Let \u03b8(l) \u2208 RMl\u00d7Ml\u22121 be the weight matrix for layer l and \u03c3(l)(\u00b7) be a Lipschitz continuous activation function. Then, let\nx(0) = x (9)\nz(l) = \u03b8(l) x(l\u22121) (10)\nx(l) = \u03c3(l)(z(l)) (11)\n\u03a8(x; \u0398) = x(L) (12)\nAlthough our notation describes simple fully connected neural networks without bias, our analysis holds for any affine operation followed by a slope-bounded non-linearity (|\u03c3\u2032(x)| \u2264 K). With\n\u03a3\u2032l(z) = diag ( \u03c3\u2032(l)(z1), . . . , \u03c3 \u2032(l)(zMl) ) , (13)\n\u2206 (l) i = \u03a3 \u2032 l(z (l) i )\u03b8 T l+1 . . .\u03a3 \u2032 L\u22121(z (L\u22121) i )\u03b8 T L , (14)\n\u2207 x (L) i L = \u2207 x (L) i L(\u03a8(xi; \u0398), yi) (15)\nwe get\n\u2016\u2207\u03b8lL(\u03a8(xi; \u0398), yi)\u20162 (16)\n= \u2225\u2225\u2225\u2225(\u2206(l)i \u03a3\u2032L(z(L)i )\u2207x(L)i L)(x(l\u22121)i )T \u2225\u2225\u2225\u2225\n2\n(17)\n\u2264 \u2225\u2225\u2225\u2206(l)i \u2225\u2225\u2225\n2 \u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252 \u2225\u2225\u2225x(l\u22121)i \u2225\u2225\u22252 (18) \u2264 max\nl,i (\u2225\u2225\u2225x(l\u22121)i \u2225\u2225\u2225 2 \u2225\u2225\u2225\u2206(l)i \u2225\u2225\u2225 2 ) \ufe38 \ufe37\ufe37 \ufe38\n\u03c1\n\u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252(19)\nVarious weight initialization (Glorot & Bengio, 2010) and activation normalization techniques (Ioffe & Szegedy, 2015; Ba et al., 2016) uniformise the activations across samples. As a result, the variation of the gradient norm is mostly captured by the gradient of the loss function with respect\nto the pre-activation outputs of the last layer of our neural network. Consequently we can derive the following upper bound to the gradient norm of all the parameters\n\u2016\u2207\u0398L(\u03a8(xi; \u0398), yi)\u20162 \u2264 L\u03c1 \u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252\ufe38 \ufe37\ufe37 \ufe38\nG\u0302i\n,\n(20)\nwhich is marginally more difficult to compute than the value of the loss since it can be computed in a closed form in terms of z(L). However, our upper bound depends on the time step t, thus we cannot generate a distribution once and sample from it during training. This is intuitive because the importance of each sample changes as the model changes."}, {"heading": "3.3. When is Variance Reduction Possible?", "text": "Computing the importance score from equation 20 is more than an order of magnitude faster compared to computing the gradient norm for each sample. Nevertheless, it still costs one forward pass through the network and can be wasteful. For instance, during the first iterations of training, the gradients with respect to every sample have approximately equal norm; thus we would waste computational resources trying to sample from the uniform distribution. In addition, computing the importance score for the whole dataset is still prohibitive and would render the method unsuitable for online learning.\nIn order to solve the problem of computing the importance for the whole dataset, we pre-sample a large batch of data points, compute the sampling distribution for that batch and re-sample a smaller batch with replacement. The above procedure upper bounds both the speedup and variance reduction. Given a large batch consisting of B samples and a small one consisting of b, we can achieve a maximum variance reduction of 1b \u2212 1 B and a maximum speedup of B+3b 3B assuming that the backward pass requires twice the amount of time as the forward pass.\nDue to the large cost of computing the importance per sample, we only perform importance sampling when we know that the variance of the gradients can be reduced. In the following equation, we show that the variance reduction is proportional to the squared L2 distance of the sampling distribution, g, to the uniform distribution u. Due to lack of space, the complete derivation is included in the supplementary material. Let gi \u221d \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162 = \u2016Gi\u20162 and u = 1B the uniform probability.\nTr (Vu[Gi])\u2212 Tr (Vg[wiGi]) (21) = Eu [ \u2016Gi\u201622 ] \u2212 Eg [ w2i \u2016Gi\u2016 2 2 ] (22)\n=\n( 1\nB B\u2211 i=1 \u2016Gi\u20162 )2 B \u2016g \u2212 u\u201622 . (23)\nEquation 23 already provides us with a useful metric to decide if the variance reduction is significant enough to justify using importance sampling. However, choosing a suitable threshold for the L2 distance squared would be tedious and unintuitive. We can do much better by dividing the variance reduction with the original variance to derive the increase in the batch size that would achieve an equivalent variance reduction. Assuming that we increase the batch size by \u03c4 , we achieve variance reduction 1\u03c4 ; thus we have 1\n( 1 B \u2211B i=1 \u2016Gi\u20162 )2 B \u2016g \u2212 u\u201622 Tr (Vu[Gi]) \u2265 (24)(\n1 B \u2211B i=1 \u2016Gi\u20162 )2 B \u2016g \u2212 u\u201622\n1 B \u2211B i=1 \u2016Gi\u2016 2 2\n= (25)\n1\u2211B i=1 g 2 i \u2016g \u2212 u\u201622 = 1\u2212 1 \u03c4 \u21d0\u21d2 (26) 1\n\u03c4 = 1\u2212 1\u2211B\ni=1 g 2 i\n\u2016g \u2212 u\u201622 (27)\nUsing equation 27, we have a hyperparameter that is very easy to select and can now design our training procedure which is described in pseudocode in algorithm 1. Computing \u03c4 from equation 27 allows us to have guaranteed speedup when B + 3b < 3\u03c4b. However, as it is shown in the experiments, we can use \u03c4th smaller than B+3b3b and still get a significant speedup.\nAlgorithm 1 Deep Learning with Importance Sampling 1: Inputs B, b, \u03c4th, a\u03c4 , \u03b80 2: t\u2190 1 3: \u03c4 \u2190 0 4: repeat 5: if \u03c4 > \u03c4th then 6: U \u2190 B uniformly sampled datapoints 7: gi \u221d G\u0302i \u2200i \u2208 U according to eq 20 8: G \u2190 b datapoints sampled with gi from U 9: wi \u2190 1Bgi \u2200i \u2208 G 10: \u03b8t \u2190 sgd step(wi,G, \u03b8t\u22121) 11: else 12: U \u2190 b uniformly sampled datapoints 13: wi \u2190 1 \u2200i \u2208 U 14: \u03b8t \u2190 sgd step(wi,U , \u03b8t\u22121) 15: gi \u221d G\u0302i \u2200i \u2208 U 16: end if\n17: \u03c4 \u2190 a\u03c4\u03c4 + (1\u2212 a\u03c4 ) (\n1\u2212 1\u2211 i g 2 i \u2225\u2225\u2225g \u2212 1|U|\u2225\u2225\u22252 2 )\u22121 18: until convergence\n1In the first version we mistakenly assume 1 \u03c42 which made the algorithm unnecessarily conservative. All the experiments are run using the square root of line 17 in Algorithm 1.\nThe inputs to the algorithm are the pre-sampling size B, the batch size b, the equivalent batch size increment after which we start importance sampling \u03c4th and the exponential moving average parameter a\u03c4 used to compute a smooth estimate of \u03c4 . \u03b80 denotes the initial parameters of our deep network. We would like to point out that in line 15 of the algorithm, we compute gi for free since we have done the forward pass in the previous step.\nThe only parameter that has to be explicitly defined for our algorithm is the pre-sampling size B because \u03c4th can be set using equation 27. We provide a small ablation study for B in the supplementary material."}, {"heading": "4. Experiments", "text": "In this section, we analyse experimentally the performance of the proposed importance sampling scheme based on our upper-bound of the gradient norm. In the first subsection, we compare the variance reduction achieved with our upper bound to the theoretically maximum achieved with the true gradient norm. We also compare against sampling based on the loss, which is commonly used in practice. Subsequently, we conduct experiments which demonstrate that we are able to achieve non-negligible wall-clock speedup for a variety of tasks using our importance sampling scheme.\nIn all the subsequent sections, we use uniform to refer to the usual training algorithm that samples points from a uniform distribution, we use loss to refer to algorithm 1 but instead of sampling from a distribution proportional to our upperbound to the gradient norm G\u0302i (equations 8 and 20), we sample from a distribution proportional to the loss value and finally upper-bound to refer to our proposed method. All the other baselines from published methods are referred to using the names of the authors.\nIn addition to batch selection methods, we compare with various SVRG implementations including the accelerated Katyusha (Allen-Zhu, 2017) and the online SCSG (Lei et al., 2017) method. In all cases, SGD with uniform sampling performs significantly better. Due to lack of space, we report the detailed results in the supplementary material.\nExperiments were conducted using Keras (Chollet et al., 2015) with TensorFlow (Abadi et al., 2016), and the code can be found at http://github.com/idiap/ importance-sampling. For all the experiments, we use Nvidia K80 GPUs and the reported time is calculated by subtracting the timestamps before starting one epoch and after finishing one; thus it includes the time needed to transfer data between CPU and GPU memory.\nOur implementation provides a wrapper around models that substitutes the standard uniform sampling with our importance-sampling method. This means that adding a sin-\ngle line of code to call this wrapper before actually fitting the model is sufficient to switch from the standard uniform sampling to our importance-sampling scheme. And, as specified in \u00a7 3.3 and Algorithm 1, our procedure reliably estimates at every iteration if the importance sampling will provide a speed-up and sticks to uniform sampling otherwise."}, {"heading": "4.1. Ablation study", "text": "As already mentioned, several works (Loshchilov & Hutter, 2015; Schaul et al., 2015) use the loss value, directly or indirectly, to generate sampling distributions. In this section, we present experiments that validate the superiority of our method with respect to the loss in terms of variance reduction. For completeness, in the supplementary material we include a theoretical analysis that explains why sampling based on the loss also achieves variance reduction during\nthe late stages of training.\nOur experimental setup is as follows: we train a wide residual network (Zagoruyko & Komodakis, 2016) on the CIFAR100 dataset (Krizhevsky, 2009), following closely the training procedure of Zagoruyko & Komodakis (2016) (the details are presented in \u00a7 4.2). Subsequently, we sample 1, 024 images uniformly at random from the dataset. Using the weights of the trained network, at intervals of 3, 000 updates, we resample 128 images from the large batch of 1, 024 images using uniform sampling or importance sampling with probabilities proportional to the loss, our upper-bound or the gradient-norm. The gradient-norm is computed by running the backpropagation algorithm with a batch size of 1.\nFigure 1 depicts the variance reduction achieved with every sampling scheme in comparison to uniform. We measure this directly as the distance between the mini-batch gradient and the batch gradient of the 1, 024 samples. For robustness we perform the sampling 10 times and report the average. We observe that our upper bound and the gradient norm result in very similar variance reduction, meaning that the bound is relatively tight and that the produced probability distributions are highly correlated. This can also be deduced by observing figure 2, where the probabilities proportional to the loss and the upper-bound are plotted against the optimal ones (proportional to the gradient-norm). We observe that our upper bound is almost perfectly correlated with the gradient norm, in stark contrast to the loss which is only correlated at the regime of very small gradients. Quantitatively the sum of squared error of 16, 384 points in figure 2 is 0.017 for the loss and 0.002 for our proposed upper bound.\nFurthermore, we observe that sampling hard examples (with high loss), increases the variance, especially in the beginning of training. Similar behaviour has been observed in problems such as embedding learning where semi-hard sample mining is preferred over sampling using the loss (Wu et al., 2017; Schroff et al., 2015)."}, {"heading": "4.2. Image classification", "text": "In this section, we use importance sampling to train a residual network on CIFAR10 and CIFAR100. We follow the experimental setup of Zagoruyko & Komodakis (2016), specifically we train a wide resnet 28-2 with SGD with momentum. We use batch size 128, weight decay 0.0005, momentum 0.9, initial learning rate 0.1 divided by 5 after 20, 000 and 40, 000 parameter updates. Finally, we train for a total of 50, 000 iterations. In order for our history based baselines to be compatible with the data augmentation of the CIFAR images, we pre-augment both datasets to generate 1.5 \u00d7 106 images for each one. Our method does not have this limitation since it can work on infinite datasets in a true online fashion. To compare between methods, we\nuse a learning rate schedule based on wall-clock time and we also fix the total seconds available for training. A faster method should have smaller training loss and test error given a specific time during training.\nFor this experiment, we compare the proposed method to uniform, loss, online batch selection by Loshchilov & Hutter (2015) and the history based sampling of Schaul et al. (2015). For the method of Schaul et al. (2015), we use their proportional sampling since the rank based is very similar to Loshchilov & Hutter (2015) and we select the best parameters from the grid a = {0.1, 0.5, 1.0} and \u03b2 = {0.5, 1.0}. Similarly, for online batch selection, we use s = {1, 10, 102} and a recomputation of all the losses every r = {600, 1200, 3600} updates.\nFor our method, we use a presampling size of 640. One of the goals of this experiment is to show that even a smaller reduction in variance can effectively stabilize training and provide wall-clock time speedup; thus we set \u03c4th = 1.5. We perform 3 independent runs and report the average.\nThe results are depicted in figure 3. We observe that in the relatively easy CIFAR10 dataset, all methods can provide some speedup over uniform sampling. However, for the more complicated CIFAR100, only sampling with our proposed upper-bound to the gradient norm reduces the variance of the gradients and provides faster convergence. Examining the training evolution in detail, we observe that on CIFAR10 our method is the only one that achieves a significant improvement in the test error even in the first stages of training (4, 000 to 8, 000 seconds). Quantitatively, on CIFAR10 we achieve more than an order of magnitude lower training loss and 8% lower test error from 0.087 to 0.079 while on CIFAR100 approximately 3 times lower training loss and 5% lower test error from 0.34 to 0.32 compared to uniform sampling.\nAt this point, we would also like to discuss the performance of the loss compared to other methods that also select batches based on this metric. Our experiments show, that using \u201cfresh\u201d values for the loss combined with a warmup stage so that importance sampling is not started too early outperforms all the other baselines on the CIFAR10 dataset."}, {"heading": "4.3. Fine-tuning", "text": "Our second experiment shows the application of importance sampling to the significant task of fine tuning a pre-trained large neural network on a new dataset. This task is of particular importance because there exists an abundance of powerful models pre-trained on large datasets such as ImageNet (Deng et al., 2009).\nOur experimental setup is the following, we fine-tune a ResNet-50 (He et al., 2015) previously trained on ImageNet. We replace the last classification layer and then train the\nwhole network end-to-end to classify indoor images among 67 possible categories (Quattoni & Torralba, 2009). We use SGD with learning rate 10\u22123 and momentum 0.9. We set the batch size to 16 and for our importance sampling algorithm we pre-sample 48. The variance reduction threshold is set to 2 as designated by equation 27.\nTo assess the performance of both our algorithm and our gradient norm approximation, we compare the convergence speed of our importance sampling algorithm using our upper-bound and using the loss. Once again, for robustness, we run 3 independent runs and report the average.\nThe results of the experiment are depicted in figure 4. As expected, importance sampling is very useful for the task of fine-tuning since a lot of samples are handled correctly very early in the training process. Our upper-bound, once again, greatly outperforms sampling proportionally to the loss when the network is large and the problem is non trivial. Compared to uniform sampling, in just half an hour importance sampling has converged close to the best performance (28.06% test error) that can be expected on this dataset without any data augmentation or multiple crops (Razavian et al.,\n2014), while uniform achieves only 33.74%."}, {"heading": "4.4. Pixel by Pixel MNIST", "text": "To showcase the generality of our method, we use our importance sampling algorithm to accelerate the training of an LSTM in a sequence classification problem. We use the pixel by pixel classification of randomly permuted MNIST digits (LeCun et al., 2010), as defined by Le et al. (2015). The problem may seem trivial at first, however as shown by Le et al. (2015) it is particularly suited to benchmarking the training of recurrent neural networks, due to the long range dependency problems inherent in the dataset (784 time steps).\nFor our experiment, we fix a permutation matrix for all the pixels to generate a training set of 60, 000 samples with 784 time steps each. Subsequently, we train an LSTM (Hochreiter & Schmidhuber, 1997) with 128 dimensions in the hidden space, tanh(\u00b7) as an activation function and sigmoid(\u00b7) as the recurrent activation function. Finally, we use a linear classifier on top of the LSTM to choose a digit based on the hidden representation. To train the aforemen-\ntioned architecture, we use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 10\u22123 and a batch size of 32. We have also found gradient clipping to be necessary for the training not to diverge; thus we clip the norm of all gradients to 1.\nThe results of the experiment are depicted in figure 5. Both for the loss and our proposed upper-bound, importance sampling starts at around 2, 000 seconds by setting \u03c4th = 1.8 and the presampling size to 128. We could set \u03c4th = 2.33 (equation 27) which would only result in our algorithm being more conservative and starting importance sampling later. We clearly observe that sampling proportionally to the loss hurts the convergence in this case. On the other hand, our algorithm achieves 20% lower training loss and 7% lower test error in the given time budget."}, {"heading": "5. Conclusions", "text": "We have presented an efficient algorithm for accelerating the training of deep neural networks using importance sampling. Our algorithm takes advantage of a novel upper bound to the\ngradient norm of any neural network that can be computed in a single forward pass. In addition, we show an equivalence of the variance reduction with importance sampling to increasing the batch size; thus we are able to quantify both the variance reduction and the speedup and intelligently decide when to stop sampling uniformly.\nOur experiments show that our algorithm is effective in reducing the training time for several tasks both on image and sequence data. More importantly, we show that not all data points matter equally in the duration of training, which can be exploited to gain a speedup or better quality gradients or both.\nOur analysis opens several avenues of future research. The two most important ones that were not investigated in this work are automatically tuning the learning rate based on the variance of the gradients and decreasing the batch size. The variance of the gradients can be kept stable by increasing the learning rate proportionally to the batch increment or by decreasing the number of samples for which we compute the backward pass. Thus, we can speed up convergence by increasing the step size or reducing the time per update."}, {"heading": "6. Acknowledgement", "text": "This work is supported by the Swiss National Science Foundation under grant number FNS-30209 \u201cISUL\u201d."}, {"heading": "A. Differences of variances", "text": "In the following equations we quantify the variance reduction achieved with importance sampling using the gradient norm. Let gi \u221d \u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162 = \u2016Gi\u20162 and u = 1B the uniform probability.\nWe want to compute\nTr (Vu[Gi])\u2212 Tr (Vg[wiGi]) = Eu [ \u2016Gi\u201622 ] \u2212 Eg [ w2i \u2016Gi\u2016 2 2 ] . (28)\nUsing the fact that wi = 1Bgi we have\nEg [ w2i \u2016Gi\u2016 2 2 ] =\n( 1\nB B\u2211 i=1 \u2016Gi\u20162\n)2 , (29)\nthus\nTr (Vu[Gi])\u2212 Tr (Vg[wiGi]) (30)\n= 1\nB B\u2211 i=1 \u2016Gi\u201622 \u2212\n( 1\nB B\u2211 i=1 \u2016Gi\u20162\n)2 (31)\n=\n(\u2211B i=1 \u2016Gi\u20162 )2 B3 B\u2211 i=1 ( B2 \u2016Gi\u201622 ( \u2211B i=1 \u2016Gi\u20162)2 \u2212 1 ) (32)\n=\n(\u2211B i=1 \u2016Gi\u20162 )2 B B\u2211 i=1 ( g2i \u2212 u2 ) . (33)\nCompleting the squares at equation 33 and using the fact that \u2211B i=1 u = 1 we complete the derivation.\nTr (Vu[Gi])\u2212 Tr (Vg[wiGi]) (34)\n=\n(\u2211B i=1 \u2016Gi\u20162 )2 B B\u2211 i=1 (gi \u2212 u)2 (35)\n=\n( 1\nB B\u2211 i=1 \u2016Gi\u20162 )2 B \u2016g \u2212 u\u201622 . (36)"}, {"heading": "B. An upper bound to the gradient norm", "text": "In this section, we reiterate the analysis from the main paper (\u00a7 3.2) with more details.\nLet \u03b8(l) \u2208 RMl\u00d7Ml\u22121 be the weight matrix for layer l and \u03c3(l)(\u00b7) be a Lipschitz continuous activation function. Then, let\nx(0) = x (37)\nz(l) = \u03b8(l) x(l\u22121) (38)\nx(l) = \u03c3(l)(z(l)) (39)\n\u03a8(x; \u0398) = x(L). (40)\nEquations 37-40 define a simple fully connected neural network without bias to simplify the closed form definition of the gradient with respect to the parameters \u0398.\nIn addition we define the gradient of the loss with respect to the output of the network as\n\u2207 x (L) i L = \u2207 x (L) i L(\u03a8(xi; \u0398), yi) (41)\nand the gradient of the loss with respect to the output of layer l as\n\u2207 x (l) i L = \u2206(l)i \u03a3 \u2032 L(z (L) i )\u2207x(L)i L (42)\nwhere\n\u2206 (l) i = \u03a3 \u2032 l(z (l) i )\u03b8 T l+1 . . .\u03a3 \u2032 L\u22121(z (L\u22121) i )\u03b8 T L (43)\npropagates the gradient from the last layer (pre-activation) to layer l and\n\u03a3\u2032l(z) = diag ( \u03c3\u2032(l)(z1), . . . , \u03c3 \u2032(l)(zMl) )\n(44)\ndefines the gradient of the activation function of layer l.\nFinally, the gradient with respect to the parameters of the l-th layer can be written\n\u2016\u2207\u03b8lL(\u03a8(xi; \u0398), yi)\u20162 (45)\n= \u2225\u2225\u2225\u2225(\u2206(l)i \u03a3\u2032L(z(L)i )\u2207x(L)i L)(x(l\u22121)i )T \u2225\u2225\u2225\u2225\n2\n(46)\n\u2264 \u2225\u2225\u2225x(l\u22121)i \u2225\u2225\u2225\n2 \u2225\u2225\u2225\u2206(l)i \u2225\u2225\u2225 2 \u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252 . (47) We observe that x(l)i and \u2206 (l) i depend only on zi and \u0398. However, we theorize that due to various weight initialization and activation normalization techniques those quantities do not capture the important per sample variations of the\ngradient norm. Using the above, which is also shown experimentally to be true in \u00a7 4.1, we deduce the following upper bound per layer\n\u2016\u2207\u03b8lL(\u03a8(xi; \u0398), yi)\u20162 (48)\n\u2264 max l,i (\u2225\u2225\u2225x(l\u22121)i \u2225\u2225\u2225 2 \u2225\u2225\u2225\u2206(l)i \u2225\u2225\u2225 2 )\u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252(49) = \u03c1\n\u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252 , (50) which can then be used to derive our final upper bound\n\u2016\u2207\u0398L(\u03a8(xi; \u0398), yi)\u20162 \u2264 L\u03c1 \u2225\u2225\u2225\u03a3\u2032L(z(L)i )\u2207x(L)i L\u2225\u2225\u22252\ufe38 \ufe37\ufe37 \ufe38\nG\u0302i\n.\n(51)\nIntuitively, equation 51 means that the variations of the gradient norm are mostly captured by the final classification layer. Consequently, we can use the gradient of the loss with respect to the pre-activation outputs of our neural network as an upper bound to the per-sample gradient norm."}, {"heading": "C. Comparison with SVRG methods", "text": "For completeness, we also compare our proposed method with Stochastic Variance Reduced Gradient methods and present the results in this section. We follow the experimental setup of \u00a7 4.2 and evaluate on the augmented CIFAR10 and CIFAR100 datasets. The algorithms we considered were SVRG (Johnson & Zhang, 2013), accelerated SVRG with Katyusha momentum (Allen-Zhu, 2017) and, the most suitable for Deep Learning, SCSG (Lei et al., 2017) which in practice is a mini-batch version of SVRG. SAGA (Defazio et al., 2014) was not considered due to the prohibitive memory requirements for storing the per sample gradients.\nFor all methods, we tune the learning rate and the epochs per batch gradient computation (m in SVRG literature). For SCSG, we also tune the large batch (denoted as Bj in Lei et al. (2017)) and its growth rate. The results are depicted in figure 6. We observe that SGD with momentum performs significantly better than all SVRG methods. Full batch SVRG and Katyusha perform a small number of parameter updates thus failing to optimize the networks. In all cases, the best variance reduced method achieves more than an order of magnitude higher training loss than our proposed importance sampling scheme."}, {"heading": "D. Ablation study on B", "text": "The only hyperparameter that is somewhat hard to define in our algorithm is the pre-sampling size B. As mentioned in the main paper, it controls the maximum possible variance reduction and also how much wall-clock time one iteration with importance sampling will require.\nIn figure 7 we depict the results of training with importance sampling and different pre-sampling sizes on CIFAR10. We follow the same experimental setup as in the paper.\nWe observe that larger presampling size results in lower training loss, which follows from our theory since the maximum variance reduction is smaller with small B. In this experiment we use the same \u03c4th for all the methods and we observe that B = 384 reaches first to 0.6 training loss. This is justified because computing the importance for 1, 024 samples in the beginning of training is wasteful according to our analysis.\nAccording to this preliminary ablation study for B, we conclude that choosing B = kb with 2 < k < 6 is a good strategy for achieving a speedup. However, regardless of the choice of B, pairing it with a threshold \u03c4th designated by the analysis in the paper guarantees that the algorithm will be spending time on importance sampling only when the variance can be greatly reduced.\nE. Importance Sampling with the Loss In this section we will present a small analysis that provides intuition regarding using the loss as an approximation or an upper bound to the per sample gradient norm.\nLet L(\u03c8, y) : D \u2192 R be either the negative log likelihood through a sigmoid or the squared error loss function defined respectively as\nL1(\u03c8, y) = \u2212 log ( exp(y\u03c8)\n1 + exp(y\u03c8)\n) y \u2208 {\u22121, 1} \u03c8 \u2208 R\nL2(\u03c8, y) = \u2016y \u2212 \u03c8\u201622 y \u2208 R d \u03c8 \u2208 Rd\n(52)\nGiven our upper bound to the gradient norm, we can write\n\u2016\u2207\u03b8tL(\u03a8(xi; \u03b8t), yi)\u20162 \u2264 L\u03c1 \u2016\u2207\u03c8L(\u03a8(xi; \u03b8t), yi)\u20162 . (53)\nMoreover, for the losses that we are considering, when L(\u03c8, y)\u2192 0 then \u2016\u2207\u03c8L(\u03a8(xi; \u03b8t), yi)\u20162 \u2192 0. Using this fact in combination to equation 53, we claim that so does the per sample gradient norm thus small loss values imply small gradients. However, large loss values are not well correlated with the gradient norm which can also be observed in \u00a7 4.1 in the paper.\nTo summarize, we conjecture that due to the above facts, sampling proportionally to the loss reduces the variance only when the majority of the samples have losses close to 0. Our assumption is validated from our experiments, where the loss struggles to achieve a speedup in the early stages of training where most samples still have relatively large loss values."}], "year": 2019, "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "authors": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "year": 2016}, {"title": "Variance reduction in sgd by distributed importance sampling", "authors": ["G. Alain", "A. Lamb", "C. Sankar", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1511.06481,", "year": 2015}, {"title": "Katyusha: The first direct acceleration of stochastic gradient methods", "authors": ["Z. Allen-Zhu"], "venue": "In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,", "year": 2017}, {"title": "Curriculum learning", "authors": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "year": 2009}, {"title": "Fast kernel classifiers with online and active learning", "authors": ["A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "Importance sampling tree for large-scale empirical expectation", "authors": ["O. Can\u00e9vet", "C. Jose", "F. Fleuret"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for nonstrongly convex composite objectives", "authors": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in neural information processing systems,", "year": 2014}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "year": 2009}, {"title": "Learning what data to learn", "authors": ["Y. Fan", "F. Tian", "T. Qin", "J. Bian", "Liu", "T.-Y"], "venue": "arXiv preprint arXiv:1702.08635,", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["X. Glorot", "Y. Bengio"], "year": 2010}, {"title": "Deep residual learning for image recognition", "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "year": 2015}, {"title": "Long short-term memory", "authors": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["S. Ioffe", "C. Szegedy"], "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["R. Johnson", "T. Zhang"], "venue": "In Advances in neural information processing systems,", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "authors": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "authors": ["A. Krizhevsky"], "venue": "Master\u2019s thesis,", "year": 2009}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "authors": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "year": 2015}, {"title": "Mnist handwritten digit database", "authors": ["Y. LeCun", "C. Cortes", "C. Burges"], "venue": "AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist,", "year": 2010}, {"title": "Non-convex finite-sum optimization via scsg methods", "authors": ["L. Lei", "C. Ju", "J. Chen", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Online batch selection for faster training of neural networks", "authors": ["I. Loshchilov", "F. Hutter"], "venue": "arXiv preprint arXiv:1511.06343,", "year": 2015}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "authors": ["D. Needell", "R. Ward", "N. Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Recognizing indoor scenes", "authors": ["A. Quattoni", "A. Torralba"], "venue": "In Computer Vision and Pattern Recognition,", "year": 2009}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "authors": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "year": 2014}, {"title": "On optimal probabilities in stochastic coordinate descent methods", "authors": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1310.3438,", "year": 2013}, {"title": "Facenet: A unified embedding for face recognition and clustering", "authors": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "year": 2015}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "authors": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno-Noguer"], "venue": "In Computer Vision (ICCV),", "year": 2015}, {"title": "Accelerating deep neural network training with inconsistent stochastic gradient descent", "authors": ["L. Wang", "Y. Yang", "M.R. Min", "S. Chakradhar"], "venue": "arXiv preprint arXiv:1603.05544,", "year": 2016}, {"title": "Sampling matters in deep embedding learning", "authors": ["Wu", "C.-Y", "R. Manmatha", "A.J. Smola", "P. Krahenbuhl"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "year": 2017}, {"title": "Wide residual networks", "authors": ["S. Zagoruyko", "N. Komodakis"], "venue": "Proceedings of the British Machine Vision Conference (BMVC),", "year": 2016}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "authors": ["P. Zhao", "T. Zhang"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "year": 2015}], "id": "SP:ec4536fdbd03cd2e9490862fe2f8d084aa5fd106", "authors": [{"name": "Angelos Katharopoulos", "affiliations": []}, {"name": "Fran\u00e7ois Fleuret", "affiliations": []}], "abstractText": "Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \u201cinformative\u201d examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the persample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.", "title": "Not All Samples Are Created Equal:  Deep Learning with Importance Sampling"}