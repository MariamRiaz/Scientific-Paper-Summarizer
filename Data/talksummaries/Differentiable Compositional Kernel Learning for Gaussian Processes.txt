30	13	A Gaussian process (GP) defines a distribution p(f) over functions X !
34	9	Assume we are given a set of training input-output pairs, D = {(x i , y i )}n i=1 = (X,y), and each target yn is gener- ated from the corresponding f(x n ) by adding independent Gaussian noise; i.e., y n = f(x n ) + ✏ n , ✏ n ⇠ N (0, 2) (2) As the prior on f is a Gaussian process and the likelihood is Gaussian, the posterior on f is also Gaussian.
35	18	We can use this to make predictions p(y⇤|x⇤,D) in closed form: p(y⇤|x⇤,D) = N (µ⇤, 2⇤) µ⇤ = K⇤X(KXX + 2 I) 1 y 2⇤ = K⇤⇤ K⇤X(KXX + 2I) 1KX⇤ + 2 (3) Here we assume zero mean function for f .
43	41	For compositional kernel learning, the Automatic Statistician (Lloyd et al., 2014; Duvenaud et al., 2013) used a compositional space of kernels defined as sums and products of a small number of primitive kernels.
45	8	RBF(x,x0) = 2 exp( kx x 0k2 2l2 ) • periodic.
52	44	Any primitive kernel B can be replaced with any other primitive kernel family B0.
53	8	The search procedure relies on a greedy search: at every stage, it searches over all subexpressions and all possible operators, then chooses the highest scoring combination.
58	26	For kernels k1, k2 • For 1, 2 2 R+, 1k1 + 2k2 is a kernel.
60	35	We design the architecture such that every unit of the network computes a kernel, although some of those kernels may be complex-valued.
64	11	Linear combinations and products can be seen as OR-like and AND-like operations, respectively; this is a common pattern in neural net design (LeCun et al., 1989; Poon & Domingos, 2011).
71	10	The Linear layer closely resembles a fully connected layer in deep neural networks, with each layer h l = W l h l 1 representing a nonnegative linear combination of units in the previous layer (i.e. W l is a nonnegative matrix).
74	7	The Linear layer can be seen as a OR-like operation: two points are considered similar if either kernel has a high value, while the Linear layer further controls the balance using trainable weights.
75	27	The Product layer introduces multiplication, in that each unit is the product of several units in the previous layer.
79	13	Analogously to ordinary neural nets, each layer may also include a nonlinear activation function, so that h l = f(z l ), where z l , the pre-activations, are the result of a linear combination or product.
95	22	Composed of summation and multiplication, the NKN naturally forms a positive-weighted polynomial of primitive kernels.
100	23	Define a d-dimensional spectral mixture kernel with n+1 components, k⇤(⌧ ) = n+1P t=1 n 2 2t cos(4 t 1 > ⌧ ).
101	11	of only one complex-valued primitive kernel ei1 > ⌧ , k⇤(⌧ ) = <{ n+1X t=1 ✓ n 2 ◆2t [ei1 > ⌧ ] 4t} (9) We find that an NKN with small width can approximate any complex-valued stationary kernel, as shown in the following theorem (Proof in Appendix F).
102	14	For any d-dimensional complex-valued stationary kernel k⇤ and ✏ 2 R+, 9{ j }d j=1, {µj}2dj=1, and an NKN ¯k with primitive kernels {exp( 2⇡2k⌧ j k2)}d j=1, {exp(iµ> j ⌧ )}2d j=1, and width no more than 6d+6, such that max ⌧2Rd |¯k(⌧ ) k⇤(⌧ )| < ✏ (10) Beyond approximating stationary kernels, NKN can also capture non-stationary structure by incorporating nonstationary primitive kernels.
103	15	In Appendix G, we prove that with the proper choice of primitive kernels, NKN can approximate a broad family of non-stationary kernels called generalized spectral kernels (Kom Samo & Roberts, 2015).
119	27	We conducted a series of experiments to measure the NKN’s predictive ability in several settings: time series, regression benchmarks, and texture images.
121	11	Furthermore, we tested the NKN on Bayesian Optimization, where model structure and calibrated uncertainty can each enable more efficient exploration.
124	22	For all of these experiments, as well as the 2-d experiment in Figure 1, we used the same NKN architecture and training setup (Appendix J.1).
125	27	We validated the NKN on three time series datasets introduced by Duvenaud et al. (2013): airline passenger volume (Airline), Mauna Loa atmospheric CO2 concentration (Mauna), and solar irradiance (Solar).
126	18	Our focus is on extrapolation, since this is a much better test than interpolation for whether the model has learned the underlying structure.
127	16	We compared the NKN with the Automatic Statistician (Duvenaud et al., 2013); both methods used RBF, RQ, PER and LIN as the primitive kernels.
129	11	We refer to this baseline as “heuristic”.
130	15	The results for Airline are shown in Figure 3, while the Table 1.
131	14	Average test RMSE and log-likelihood for regression benchmarks with random splits.
134	8	Extrapolation results of NKN on the Airline dataset.
141	50	However, the heuristic kernel failed to fit the data points well or capture the increasing amplitude, stemming from its lack of PER*LIN structure.
