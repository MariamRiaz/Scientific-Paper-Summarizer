14	13	We demonstrate empirically that our algorithm improves posterior estimates over other variational methods for several practical Bayesian models.
15	15	Given a target distribution with density1 π(x) for a continuous random variable x ∈ X ⊆ RD, variational inference approximates π(x) with a tractable distribution, q(x;λ), from which we can efficiently draw samples and form sample-based estimates of functions of x. Variational methods minimize the KL-divergence, KL(q||π), between q(·;λ) and the true π as a function of variational parameter λ (Bishop, 2006).
20	12	However, when the family Q does not include π then KL(qλ∗ ||π) > 0 which will result in biased estimates of functions f(x), Ex∼qλ∗ [f(x)] 6= Ex∼π[f(x)].
22	13	Expectations with respect to the target distribution can be calculated as an average with respect to these correlated samples.
23	11	MCMC typically enjoys nice asymptotic properties; as the number of samples grows, MCMC samplers represent the true target distribution with increasing fidelity.
29	9	We define our class of approximating distributions to be mixtures of C simpler component distributions: q(C)(x;λ, ρ) = C∑ c=1 ρcqc(x;λc) , s.t.
36	10	Variational boosting (VBoost) begins with a single mixture component, q(1)(x;λ) = q1(x;λ1) with C = 1.
45	15	The re-parameterization trick is used to compute an unbiased estimate of the gradient of an objective that is expressed as an intractable expectation with respect to a continuousvalued random variable.
46	56	This situation arises in variational inference when the ELBO cannot be evaluated analytically.
52	18	We circumvent this by re-writing the variational objective as a weighted combination of expectations with respect to individual mixture components: L(λ, ρ) = ∫ ( C∑ c=1 ρcqc(x;λc) ) [lnπ(x)− ln q(x;λ)] dx = C∑ c=1 ρc ∫ qc(x;λc) [lnπ(x)− ln q(x;λ)] dx = C∑ c=1 ρcEqc [lnπ(x)− ln q(x;λ)] which is a weighted sum of component-specific ELBOs.
53	15	If the qc are continuous and there exists some function fc( ;λ) such that x = fc( ;λ) and x ∼ qc(·;λ) when ∼ p( ), then we can apply the re-parameterization trick to each component to obtain gradients of the ELBO : ∇λcL(λ, ρ) = ∇λc C∑ c=1 ρcEx∼q(x;λ) [lnπ(x)− ln q(x;λ)] = C∑ c=1 ρcE ∼p( ) [ ∇λc lnπ(fc( ;λc)) −∇λc ln q(fc( ;λc);λ) ] .
54	48	VBoost leverages the above formulation of ∇λcL(λ, ρ) to use the re-parameterization trick in a component-bycomponent manner, allowing us to improve the variational approximation as we incorporate new components.
62	13	The new approximate distribution is q(C+1)(x;λ, ρ) = (1− ρC+1)q(C)(x) + ρC+1qC+1(x;λC+1) .
65	13	Because we have fixed q(C), we only need to optimize the new component parameters, λC+1 and ρC+1, allowing us to use the re-parameterization trick to obtain gradients of L(C+1).
75	10	When the approximation only consists of one component, this structure is commonly referred to as the mean field family.
78	8	(5) the resulting mixture may require a large number of components to represent the strong correlations (see Fig.
81	19	This allows Σ to be any positive semi-definite matrix, enabling q to have the full flexibility of a D-dimensional multivariate normal distribution.
91	10	This is effectively approximating the target via a factor analysis model.
94	15	Incorporating more components using the VBoost framework further improves the approximation of the distribution.
99	51	The matrix determinant lemma expresses the determinant of Σ as the product of two determinants |FF ᵀ + I(v))| = |I(v))||Ir + F ᵀI(−v)F | = exp (∑ d vd ) |Ir + F ᵀI(−v)F | where the left term is simply the product of the diagonal component, and the right term is the determinant of an r × r matrix, computable in O(r3) time (Harville, 1997).
100	12	To compute Σ−1, the Woodbury matrix identity states that (FF ᵀ + I(v))−1 = I(−v)− I(−v)F (Ir + F ᵀI(−v)F )−1F ᵀI(−v) which involves the inversion of a smaller, r × r matrix and can be done in O(r3) time (Golub & Van Loan, 2013).
112	13	This initialization procedure is detailed in Section A and Algorithm 1 of the supplement.
135	14	To supplement the previous synthetic examples, we use VBoost to approximate various challenging posterior distributions arising from real statistical models of interest.2 Binomial Regression We first apply VBoost to a nonconjugate hierarchical binomial regression model.3 The model describes the binomial rates of success (batting averages) of baseball players using a hierarchical model (Efron & Morris, 1975), parameterizing the “skill” of each player: θj ∼ Beta(φ · κ, (1− φ) · κ) player j prior yj ∼ Binomial(Kj , θj) player j hits , where yj is the number of successes (hits) player j has attempted in Kj attempts (at bats).
136	9	Each player has a latent success rate θj , which is governed by two global variables κ and φ.
143	15	Figure 3a compares a selection of univariate and bivariate posterior marginals.
144	20	We see that VBoost is able to closely match the NUTS posteriors, improving upon the MFVI approximation.
145	24	Figure 3b compares the VBoost covariance estimates to the “ground truth” estimates of MCMC at various stages of the algorithm.
147	14	Multi-level Poisson GLM We use VBoost to approximate the posterior of a hierarchical Poisson GLM, a common non-conjugate Bayesian model.
148	18	Here, we focus on a specific model that was formulated to measure the relative rates of stop-and-frisk events for different ethnicities and in different precincts (Gelman et al., 2007), and has been used as an illustrative example of multi-level modeling (Gelman & Hill, 2006).
150	17	The prior over the mean offset and group variances is given by µ, lnσ2α, lnσ 2 β ∼ N (0, 102).
