0	127	Despite empirical success of deep learning (e.g., Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al., 2014) and Speech Recognition (Hinton et al., 2012)), it remains elusive how and why simple methods like gradient descent can solve the complicated non-convex optimization during training.
1	25	In this paper, we focus on a two-layered ReLU network: g(x;w) = K ∑ j=1 σ(w⊺j x), (1) Here σ(x) = max(x, 0) is the ReLU nonlinearity.
2	48	We consider the setting that a student network is optimized to minimize the l2 distance between its prediction and the supervision provided by a teacher network of the same architecture with fixed parameters w∗.
3	58	Note that although the network prediction (Eqn.
7	55	Using this formula, critical point and convergence analysis follow.
38	15	Denote N as the number of samples and d as the input dimension.
40	34	Given the current estimation w, we have the following l2 loss: J(w) = 1 2 ‖g(X;w∗)− g(X;w)‖2, (2) Here we focus on population loss EX [J ], where the input X is assumed to follow spherical Gaussian distribution N (0, I).
41	19	Its gradient is the population gradient EX [∇Jw(w)] (abbrev.
52	20	If we define Population Gating (PG) function F (e,w) ≡ X⊺D(e)D(w)Xw, then E [∇J ] can be written as: E [∇J ] = E [F (w/‖w‖,w)]− E [F (w/‖w‖,w∗)] .
61	13	With the close form of F , E [∇J ] also has a close form: E [∇J ] = N 2 (w−w∗)+N 2π ( θw∗ − ‖w ∗‖ ‖w‖ sin θw ) (6) where θ = ∠(w,w∗) ∈ [0, π].
76	17	For convenience, we defineΠ∗ as the Principal Hyperplane spanned byK ground truth weight vectors.
84	30	Then for any out-of-plane critical point, there is one matrix in L that changes at least one of its weights, yielding a non-isolated different critical point.
85	18	2 also works for any general isotropic distribution, in which E [F (e,w)] has the form of Eqn.
88	33	To analyze in-plane critical points, it suffices to study gradient projections on Π∗.
105	11	With Ljj′ , we have the following necessary conditions for critical points: Theorem 3 If w̄∗ 6= 0, and for a given parameter w, Ljj′({θ∗kl },Θ) > 0 (or < 0) for all 1 ≤ k ≤ K, then w cannot be a critical point.
113	21	It characterizes zero-crossings of a 2D function on a closed region [0, 2π]× [0, π].
118	19	When exact one w∗ is inside Cone(w1,w2), whether (w1,w2) is a critical point remains open.
130	24	A better idea is to sample around the origin with very small radius (but not at w = 0), so that Ω looks like a hyperplane near the origin, and thus almost half samples are useful (Fig.
133	28	The idea is to lower-bound the probability of the shaded area (Fig.
135	13	For multiple ReLUs, Lyapunov method on Eqn.
139	26	Without loss of generality, we set P1 as the identity.
149	16	, y0], the system converges to P2w ∗, etc.
150	13	Since |x0 − y0| can be arbitrarily small, a slightest perturbation around x0 = y0 leads to a different fixed point Pjw ∗ for some j.
157	50	8) still converge to w∗ (and its transformations).
162	23	We randomly pick e and w so that their angle ∠(e,w) is uniformly distributed in [0, π].
171	18	We also examine other zero-mean distributions of X , e.g., U [−1/2, 1/2].
196	17	This formula leads to interesting critical point and convergence analysis.
203	25	What if the input distribution has different symmetries?
205	98	Second, empirically we see convergence cases that are not covered by the theorems, suggesting the conditions imposed by the theorems can be weaker.
206	48	Finally, how to apply similar analysis to broader distributions and how to generalize the analysis to multiple layers are also open problems.
