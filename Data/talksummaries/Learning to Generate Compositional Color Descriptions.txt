1	62	Basic color terms like “red” and “blue” provide a rich set of semantic building blocks in a continuous meaning space; in addition, people employ compositional color descriptions to express meanings not covered by basic terms, such as “greenish blue” or “the color of the rust on my aunt’s old Chevrolet” (Berlin and Kay, 1991).
3	21	We consider color description generation as a grounded language modeling problem.
5	19	We compare our model with LUX (McMahan and Stone, 2015), a Bayesian generative model of color semantics.
7	13	3); (2) it learns correct denotations for underspecified modifiers, which name a variety of colors (“dark”, “dull”; Fig.
8	32	2); and (3) it can model non-convex denotations, such as that of “greenish”, which includes both greenish yellows and blues (Fig.
9	31	As a result, our model also produces significant improvements on several grounded language modeling metrics.
10	34	Formally, a model of color description generation is a probability distribution S(d | c) over sequences of 2243 tokens d conditioned on a color c, where c is represented as a 3-dimensional real vector in HSV space.1 Architecture Our main model is a recurrent neural network sequence decoder (Fig.
12	13	At each time step, the model takes in a concatenation of f and an embedding for the previous output token di, starting with the start token d0 = <s>.
19	89	Color features We compare three representations: • Raw: The original 3-dimensional color vectors, in HSV space.
20	19	• Buckets: A discretized representation, dividing HSV space into rectangular regions at three resolutions (90×10×10, 45×5×5, 1×1×1) and assigning a separate embedding to each region.
21	21	• Fourier: Transformation of HSV vectors into a Fourier basis representation.
25	56	This representation has the property that most regions of color space denoted by some description are extreme along a single direction in Fourier space, thus largely avoiding the need for the model to learn non-monotonic functions of the color representation.
30	19	We demonstrate the effectiveness of our model using the same data and statistical modeling metrics as McMahan and Stone (2015).
31	43	Data The dataset used to train and evaluate our model consists of pairs of colors and descriptions collected in an open online survey (Munroe, 2010).
32	17	Participants were shown a square of color and asked to write a free-form description of the color in a text box.
39	11	This quantifies a tradeoff between accurate modeling and model complexity.
52	113	Given the general success of LSTM-based models at generation tasks, it is perhaps not surprising that they yield good raw performance when applied to color description.
53	11	The color domain, however, has the advantage of admitting faithful visualization of descriptions’ semantics: colors exist in a 3- dimensional space, so a two-dimensional visualization can show an acceptably complete picture of an entire distribution over the space.
55	63	We construct visualizations by querying the model for the probability S(d | c) of the same description for each color in a uniform grid, summing the probabilities over the hue dimension (left crosssection) and the saturation dimension (right crosssection), normalizing them to sum to 1, and plotting the log of the resulting values as a grayscale image.
63	17	Compositionality Our model generalizes to compositional descriptions not found in the training set.
64	34	3 visualizes the probability assigned to the novel utterance “faded teal”, along with “faded” and “teal” individually.
65	34	The meaning of “faded teal” is intersective: “faded” colors are lower in saturation, excluding the colors of the rainbow (the V on the right side of the left panel); and “teal” denotes colors with a hue near 180° (center of the right panel).
68	14	The description “greenish” (Fig.
70	29	Error analysis Table 3 shows some examples of errors found in samples taken from the model.
71	23	The main type of error the system makes is ungrammatical descriptions, particularly fragments lacking a basic color term (e.g., “robin’s”).
72	54	Rarer are grammatical but meaningless compositions (“reddish green”) and false descriptions.
73	37	When queried for its single most likely prediction, argmaxd S(d | c), the result is nearly always an acceptable, “safe” description— manual inspection of 200 such top-1 predictions did not identify any errors.
74	73	We presented a model for generating compositional color descriptions that is capable of producing novel descriptions not seen in training and significantly outperforms prior work at conditional language modeling.3 One natural extension is the use of character-level sequence modeling to capture complex morphology (e.g., “-ish” in “greenish”).
76	16	Their model uses a Labspace color representation and uses the color to initialize the LSTM instead of feeding it in at each time step; they also focus on visualizing point predictions of their description-to-color model, whereas we examine the full distributions implied by our color-todescription model.
77	55	Another extension we plan to investigate is modeling of context, to capture how people describe colors differently to contrast them with other colors via pragmatic reasoning (DeVault and Stone, 2007; Golland et al., 2010; Monroe and Potts, 2015).
