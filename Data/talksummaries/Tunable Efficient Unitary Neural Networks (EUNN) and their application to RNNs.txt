7	26	A recently proposed solution strategy is using orthogonal hidden weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015; Henaff et al., 2016), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers.
8	24	This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al., 2015) or by alternating gradient-descent steps with projections onto the unitary subspace (Wisdom et al., 2016).
16	21	We first review the basic RNN architecture.
22	11	For an RNN, the vanishing or exploding gradient problem is most significant during back propagation from hidden to hidden layers, so we will only focus on the gradient for hidden layers.
24	50	In order to evaluate ∂C∂Wij , one first computes the derivative ∂C ∂h(t) using the chain rule: ∂C ∂h(t) = ∂C ∂h(T ) ∂h(T ) ∂h(t) (3) = ∂C ∂h(T ) T−1∏ k=t ∂h(k+1) ∂h(k) (4) = ∂C ∂h(T ) T−1∏ k=t D(k)W, (5) where D(k) = diag{σ′(Ux(k) + Wh(k−1))} is the Jacobian matrix of the pointwise nonlinearity.
28	10	In a breakthrough paper, Arjovsky, Shah & Bengio (Arjovsky et al., 2015) showed that unitary RNNs can overcome the exploding and vanishing gradient problems and perform well on long term memory tasks if the hiddento-hidden matrix in parametrized in the following unitary form: W = D3T2F−1D2ΠT1FD1.
33	13	This model uses O(N) parameters, which spans merely a part of the whole O(N2)-dimensional space of unitary N × N matrices to enable computational efficiency.
35	16	In order to maximize the power of Unitary RNNs, it is preferable to have the option to optimize the weight matrix W over the full space of unitary matrices rather than a subspace as above.
43	9	Any N × N unitary matrix WN can be represented as a product of rotation matrices {Rij} and a diagonal matrix D, such that WN = D ∏N i=2 ∏i−1 j=1 Rij , where Rij is defined as the N -dimensional identity matrix with the elements Rii, Rij , Rji and Rjj replaced as follows (Reck et al., 1994; Clements et al., 2016):( Rii Rij Rji Rjj ) = ( eiφij cos θij −eiφij sin θij sin θij cos θij ) .
47	12	This generalizes the familiar factorization of a 3D rotation matrix into 2D rotations parametrized by the three Euler angles.
51	18	The unitary matrix WN is multiplied from the right by a succession of unitary matrices RNj for j = N − 1, · · · , 1.
52	21	Once all elements of the last row except the one on the diagonal are zero, this row will not be affected by later transformations.
58	20	For example (Clements et al., 2016), a unitary matrix can be decomposed as WN = D ( R (1) 1,2R (1) 3,4 .
59	11	.R (l) N/2−1,N/2 is a block diagonal matrix, with N angle parameters in total, and F (l) B = R (l) 2,3R (l) 4,5 .
64	39	Inspired by (Mathieu & LeCun, 2014a), an alternative way to organize the rotation matrices is implementing an FFTstyle architecture.
65	13	Instead of using adjacent rotation matrices, each F here performs a certain distance pairwise rotations as shown in Fig.
66	22	(16) The rotation matrices in Fi are performed between pairs of coordinates (2pk + j, p(2k + 1) + j) (17) where p = N2i , k ∈ {0, ..., 2 i−1} and j ∈ {1, ..., p}.
67	34	This requires only log(N) matrices, so there are a total of N log(N)/2 rotational pairs.
68	17	This is also the minimal number of rotations that can have all input coordinates interacting with each other, providing an approximation of arbitrary unitary matrices.
74	17	Input: input x, size N ; parameters θ and φ, size N/2; constant permuatation index list ind1 and ind2.
96	22	This results show that the EURNN architectures introduced in both Sec.4.2 (EURNN with N=512, selecting L=2) and Sec.4.3 (FFT-style EURNN with N=512) outperform the LSTM model (which suffers from long term memory problems and only performs well on the copy task for small time delays T ) and all other unitary RNN models, both in-terms of learnability and in-terms of convergence rate.
108	51	It learns faster, in fewer iteration steps, and converges to a higher classifi- Model hidden size number of validation test (capacity) parameters accuracy accuracy LSTM 80 16k 0.908 0.902 URNN 512 16k 0.942 0.933 PURNN 116 16k 0.922 0.921 EURNN (tunable style) 1024 (2) 13.3k 0.940 0.937 EURNN (FFT style) 512 (FFT) 9.0k 0.928 0.925 cation accuracy.
118	30	The frame prediction task is as follows: given all the log-magnitudes of STFT frames up to time t, predict the log-magnitude of the STFT frame at time t+ 1 that has the minimum mean square error (MSE).
125	17	Furthermore, in this particular task, full-capacity EURNNs outperform small capacity EURNNs and FFT-style EURNNs.
126	36	We have presented a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational cost is merely O(1) per parameter, which is O(logN) more efficient than the other methods discussed above.
128	15	It also performs well on real tasks such as speech prediction, outperforming an LSTM on TIMIT data speech prediction.
129	87	We want to emphasize the generality and tunability of our method.
130	19	The ordering of the rotation matrices we presented in Fig.
135	46	Finally, we note that our method remains applicable even if the unitary matrix is decomposed into a different product of matrices (Eq.
136	54	This powerful and robust unitary RNN architecture also might be promising for natural language processing because of its ability to efficiently handle tasks with long-term correlation and very high dimensionality.
