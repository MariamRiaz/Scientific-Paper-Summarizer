0	43	In many applications, one needs to deal with data containing a very large number of irregular timestamped events that are recorded in continuous time.
1	22	These events can reflect, for instance, the activity of users on a social network (Subrahmanian et al., 2016), high-frequency variations of signals in finance (Bacry & Muzy, 2014), earthquakes and aftershocks in geophysics (Ogata, 1998), crime activity (Mohler et al., 2011) or position of genes in genomics (Reynaud-Bouret & Schbath, 2010).
2	33	In this context, multidimensional counting processes based models play a paramount role.
6	15	The events of all nodes can be represented as a vector of counting processesN t = [N1t · · ·Ndt ]>, where N it counts the number of events of node i until time t ∈ R+, namely N it = ∑ τ∈Zi 1{τ≤t}.
8	20	The vector λt characterizes the distribution ofN t, see (Daley & Vere-Jones, 2003), and patterns in the events time-series can be captured by structuring these intensities.
9	16	The Hawkes process framework (Hawkes, 1971) corresponds to an autoregressive structure of the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon typically observed in social networks (Crane & Sornette, 2008).
10	43	Namely, N t is called a Hawkes point process if the stochastic intensities can be written as λit = µ i + d∑ j=1 ∫ t 0 φij(t− t′)dN jt′ , where µi ∈ R+ is an exogenous intensity and φij are positive, integrable and causal (with support in R+) functions called kernels encoding the impact of an action by node j on the activity of node i.
11	27	Note that when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.
21	21	Since the question of a real causality is too complex in general, most econometricians agreed on the simpler definition of Granger causality (Granger, 1969).
25	38	In the following, we’ll refer to learning the kernels’ integrals as uncovering causality since each integral encodes the notion of Granger causality, and is also linked to the number of events directly caused from a node to another node, as described below at Eq.
26	25	Our paper solves this problem with a different and more direct approach.
28	33	Namely, we want to estimate the matrixG = [gij ] where gij = ∫ +∞ 0 φij(u) du ≥ 0 for 1 ≤ i, j ≤ d. (1) From the definition of Hawkes process as a Poisson cluster process (Jovanović et al., 2015), gij can be simply interpreted as the average total number of events of node i whose direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other intermediate event are not counted).
30	48	Namely, introducing the counting function N i←jt that counts the number of events of i whose direct ancestor is an event of j, we know from (Bacry et al., 2015) that E[dN i←jt ] = gijE[dN j t ] = g ijΛjdt, (2) where we introduced Λi as the intensity expectation, namely satisfying E[dN it ] = Λidt.
45	17	Given 1 ≤ i, j, k ≤ d, the first three integrated cumulants of the Hawkes process can be defined as follows thanks to stationarity: Λidt = E(dN it ) (3) Cijdt = ∫ τ∈R ( E(dN itdN j t+τ )− E(dN it )E(dN j t+τ ) ) (4) Kijkdt = ∫ ∫ τ,τ ′∈R2 ( E(dN itdN j t+τdN k t+τ ′) + 2E(dN it )E(dN j t+τ )E(dNkt+τ ′) − E(dN itdN j t+τ )E(dNkt+τ ′) − E(dN itdNkt+τ ′)E(dN j t+τ ) − E(dN jt+τdNkt+τ ′)E(dN it ) ) , (5) where Eq.
61	12	This framework allows to determine the optimal weighting matrix involved in the loss function, which is a question to be addressed in an extended version of the present work.
104	13	The ODE-based algorithm is an EM algorithm that parametrizes the kernel function with M basis functions, each being discretized to L points.
113	19	Similarly, our method first computes the integrated cumulants, then minimize the objective function with Niter iterations, and invert the resulting matrix R̂ to obtain Ĝ.
115	11	The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM).
118	13	, xn of X , the GMM method minimizes the norm of the empirical mean over n samples, ‖ 1n ∑n i=1 g(xi, θ)‖, as a function of θ, to obtain an estimate of θ0.
126	17	Theorem 2.1 (Consistency of NPHC).
145	18	We apply our method to financial data, in order to understand the self and cross-influencing dynamics of all event types in an order book.
146	62	An order book is a list of buy and sell orders for a specific financial instrument, the list being updated in real-time throughout the day.
149	25	downward) price moves, T (a) (resp.
158	36	We evaluate the performance of the proposed methods using the computing time, the Relative Error RelErr(A,B) = 1 d2 ∑ i,j |aij − bij | |aij | 1{aij 6=0}+|bij |1{aij=0} and the Mean Kendall Rank Correlation MRankCorr(A,B) = 1 d d∑ i=1 RankCorr([ai•], [bi•]), where RankCorr(x, y) = 2d(d−1) (Nconcordant(x, y) − Ndiscordant(x, y)) with Nconcordant(x, y) the number of pairs 3i.e. buy orders that are executed and removed from the list 4i.e. buy orders added to the list 5i.e. the number of times a limit order at the ask is cancelled: in our dataset, almost 95% of limit orders are cancelled before execution.
160	20	Note that RankCorr score is a value between −1 and 1, representing rank matching, but can take smaller values (in absolute value) if the entries of the vectors are not distinct.
161	17	We perform the ADM4 estimation, with exponential kernel, by giving the exact value β = β0 of one block.
169	18	On these simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves a lot the relative error.
172	27	Indeed, it has been shown in (Zhou et al., 2013a) that kernels of MemeTracker data are not exponential, nor power law.
173	43	This partly explains why our approach behaves better.
176	12	This shows empirically that ask and bid have symmetric roles.
