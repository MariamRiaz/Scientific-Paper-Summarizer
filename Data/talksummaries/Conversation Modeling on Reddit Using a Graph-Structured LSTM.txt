30	22	The proposed model is a bidirectional graph LSTM that characterizes a full threaded discussion, assuming a tree-structured response network and accounting for the relative order of the comments.
31	36	Each comment in a conversation corresponds to a node in the tree, where its parent is the comment that it is responding to and its children are the responding comments that it spurs ordered in time.
33	19	In the forward direction, the state vector can be thought of as a summary of the discussion pursued in a particular branch of the tree, while in the backward direction the state vector summarizes the full response subtree that followed a particular comment.
34	33	The state vectors for the forward and backward directions are concatenated for the purpose of predicting comment karma.
35	19	The RNN updates – both forward and backward – incorporate both temporal and hierarchical (tree-structured) dependencies, since commenters typically consider what has already been said in response to a parent comment.
39	92	Here, the objective is to predict quantized comment karma.
43	33	Each node in the tree is associated with an LSTM unit.
46	65	Specifically, we use two forget gates - one for the previous (or subsequent) hierarchical layer, and one for the previous (or subsequent) timing layer.
53	66	Below we provide the update equations for the forward process, using the subscripts i, f, g, c, and o for the input gate, temporal forget gate, hierarchichal forget gate, cell, and output, respectively.
60	22	Let + and − indicate forward and backward embeddings respectively.
68	19	The comment text features, denoted xct , are generated using a simple average bag-of-words representation learned during the training: xct = 1 N N∑ i=1 W ie where W ie is an embedding of the i-th word in the comment, and N is the number of words in the comment.
76	21	We then assign these nodes to be level 0 and prune them out of the tree, but retain a count of nodes pruned for use in a countweighted bias term in the update to capture information about response volume.
107	20	We compare the graph LSTM to a node-independent baseline, which is a feedforward neural network model consisting of input, hidden and softmax layers.
114	39	In order to evaluate the role of each direction (forward or backward) in the graph-structured model, we also present results using only the forward direction graph-LSTM for comparison to the bidirectional model.
129	23	One case is controversial comments, which have large subtrees but do not have high karma because of downvotes; these tend to have overprediction of karma when using only submission context.
144	23	In order to see how the model benefits from using the language cues in underpredicted and overpredicted scenarios, we look at the size of errors made by the graph-LSTM model with and without text features.
145	28	In Figure 5, the x-axis indicates the error between the actual karma level and the karma level predicted by the graph-LSTM using submission context features only.
146	38	The negative errors represent the overpredicted comments, and the positive errors represent the underpredicted comments.
168	19	Looking at the list of words associated with replies to positive-karma comments we noticed words that indicate humor (“LOL”, “hilarious”), positive feedback (“Like”, “Right”), and emotion indicators (“!
169	47	Words in comments and replies associated with overpredicted (controversial) cases are related to controversial topics (sexual, regulate, liberals), named political parties, and mentions of downvoting or indication that the comment has been edited with the word “Edit.” Since the two sets of lists were generated separately, there are words in the over/under-predicted lists that overlap with the zero/non-zero karma lists (12 in the reply lists, 20 in the comment lists).
173	35	The results are plotted for comments in Figure 6, which shows that the words that are associated with underpredicted comments (red) are aligned with positive-karma words (green) for both comment text and text in replies.
179	20	The model leveraging reply text correctly predicts the low karma.
183	25	Example 4 has only one child, but both models using language correctly predict level 7, probably because the model has learned that references to “Colbert” are popular.
185	25	Finally, the last two examples represent instances where neither model successfully identifies a high karma comment, which often involve analogies.
205	35	The propagation of hidden state information in the graph provides a mechanism for representing contextual language, including the history that a comment is responding to as well as the ensuing discussion it spawns.
207	63	Analyses show that the model benefits prediction over the extent of the discussion, and that language cues are particularly important for distinguishing controversial comments from those that are very positively received.
209	29	While we evaluate the model on predicting the popularity of comments in specific forums on Reddit, it can be applied to other social media platforms that maintain a threaded structure or possibly to citation networks.
210	74	In addition to popularity prediction, we expect the model would be useful for other tasks for which the responses to comments are informative, such as detecting topic or opinion shift, influence or trolls.
211	113	With the more fine-grained feedback increasingly available on social media platforms (e.g. laughter, love, anger, tears), it may be possible to distinguish different types of popularity as well as levels, e.g. shared sentiment vs. humor.
215	24	Finally, it would be useful to evaluate performance using a short window lookahead for responses, rather than the full discussion tree.
