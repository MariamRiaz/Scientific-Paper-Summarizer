{"sections": [{"text": "We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the stateof-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data."}, {"heading": "1 Introduction", "text": "A statistical language model estimates probability values P (W ) for strings of words W in a vocabulary V whose size can be in the tens or hundreds of thousands and sometimes even millions. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are often assumed to be conditionally independent; we will assume that W is such a segment, or sentence.\nEstimating full sentence language models (Rosenfeld et al., 2001) is computationally hard if one\nseeks a properly normalized probability model1 over strings of words of finite length in V\u2217. A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end-of-sentence symbol </S> is predicted with non-zero probability in any context. With W = wN1 = w1, . . . , wN we get:\nP (wN1 ) = N\u220f\nk=1\nP (wk|wk\u221211 ) (1)\nSince the parameter space of P (wk|wk\u221211 ) is too large, the language model is forced to put the context wk\u221211 into an equivalence class determined by a function \u03a6(wk\u221211 ). As a result,\nP (wN1 ) \u223c=\nN\u220f\nk=1\nP (wk|\u03a6(wk\u221211 )) (2)\nResearch in language modeling consists of finding appropriate equivalence classifiers \u03a6 and methods to estimate P (wk|\u03a6(wk\u221211 )). Arguably the most successful paradigm in language modeling uses the n-gram equivalence classification, that is, defines\n\u03a6n-gram(w k\u22121 1 ) . = wk\u2212n+1, wk\u2212n+2, . . . , wk\u22121\nOnce the form \u03a6(wk\u221211 ) is specified, only the problem of estimating P (wk|\u03a6(wk\u221211 )) from training data remains.\nIn order to outperform the n-gram equivalence class, one must find a way to leverage long-distance context. This can be done explicitly, e.g. by combining multiple arbitrary features (Rosenfeld, 1994), or implicitly as is the case for the current state of the art\n1In some practical systems the constraint on using a properly normalized language model is side-stepped at a gain in modeling power and simplicity, see e.g. Chen et al. (1998).\n329\nTransactions of the Association for Computational Linguistics, vol. 4, pp. 329\u2013342, 2016. Action Editor: Jason Eisner. Submission batch: 12/2014; Revision batch: 7/2015; 11/2015; 4/2016; 6/2016; Published 7/2016.\nc\u00a92016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nrecurrent neural network language models (Mikolov, 2012). Unfortunately, either method comes at a large computational cost which makes training and evaluation on a large corpus impractical.\nIn this paper we present a novel probability estimation technique, called Sparse Non-negative Matrix (SNM) estimation. Although SNM estimation is a general approach that can be applied to many problems, its efficient combination of arbitrary features makes it particularly interesting for language modeling. We demonstrate this by training models with variable-length n-gram features and skip-gram features to incorporate long-distance context.\nThe paper is organized as follows: Section 2 discusses work that is related to SNM which is described in Section 3. We then present a complexity analysis in Section 4 and experimental results on two English corpora in Sections 5 and 6. We end with conclusions and future work in Section 7."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Neural networks", "text": "Recently, neural networks (NN) (Bengio et al., 2003; Emami, 2006; Schwenk, 2007), and in particular recurrent neural networks (RNN) (Mikolov, 2012; Sundermeyer et al., 2012) have shown excellent performance in language modeling (Chelba et al., 2014). RNNLMs have two main advantages over n-gram language models: 1) they learn a lowdimensional continuous vector representation for words which allows them to discover fine-grained similarities between words; 2) they are capable of modeling dependencies that span over longer distances, i.e. they can extend the context past the ngram window. Their main disadvantage however is that they take a long time to train and evaluate."}, {"heading": "2.2 Feature-based models", "text": "Another popular method to leverage long-distance context is Maximum Entropy (ME) (Rosenfeld, 1994). ME is interesting because it can mix different types of features extracted from large context windows, e.g. n-gram, skip-gram, bag-of-word and syntactic features. Unfortunately it suffers from the same drawback as neural networks, as we will see in Section 2.4.\nThe above-mentioned features can also be used in\nother ways, e.g. Chelba and Jelinek (2000) use a leftto-right syntactic parser to identify long-distance dependencies (at sentence level), whereas approaches such as Bellegarda (2000) leverage latent semantic information (at document level). Tan et al. (2012) integrate both syntactic and topic-based modeling with n-grams in a unified approach."}, {"heading": "2.3 Skip-grams", "text": "The type of long-distance features that we incorporate into our SNMLMs are skip-grams (Huang et al., 1993; Ney et al., 1994; Rosenfeld, 1994), which can effectively capture dependencies across longer contexts. We are not the first to highlight this effectiveness; previous such results were reported in Singh and Klakow (2013). Recently, Pickhardt et al. (2014) also showed that a backoff generalization using single skips yields significant perplexity reductions. We note though that our SNMLMs are trained by mixing single as well as longer skips, combining both in one model. More fundamentally, the SNM model parameterization and method of estimation are completely original, as far as we know.\nIn our approach, a skip-gram feature extracted from the context wk\u221211 is characterized by the tuple (r, s, a) where:\n\u2022 r denotes the number of remote context words \u2022 s denotes the number of skipped words \u2022 a denotes the number of adjacent context words\nrelative to the target word wk being predicted. The window size of a feature extractor then corresponds to r + s + a. For example, in the sentence <S> The quick brown fox jumps over the lazy dog </S> a (1, 2, 3) skip-gram feature for the target word dog is:\n[brown skip-2 over the lazy]\nFor performance reasons, it is recommended to limit s and to limit either (r + a) or both r and s.\nWe configure the skip-gram feature extractor to produce all features F , defined by the equivalence class \u03a6(wk\u221211 ), that meet constraints on the minimum and maximum values for:\n\u2022 the number of context words r + a \u2022 the number of remote words r \u2022 the number of adjacent words a \u2022 the skip length s\nWe also allow the option of not including the exact value of s in the feature representation; this may help with smoothing by sharing counts for various skip features. The resulting tied skip-gram features will look like:\n[curiosity skip-* the cat]\nIn order to build a good probability estimate for the target word wk in a context wk\u221211 we need a way of combining an arbitrary number of skip-gram features, which do not fall into a simple hierarchy like regular n-gram features. The standard way to combine such predictors is ME, but it is computationally hard. The proposed SNM estimation on the other hand is capable of combining such predictors in a way that is computationally easy, scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view."}, {"heading": "2.4 Log-linear models", "text": "Neural networks and ME are related in the sense that for both models P (wk|\u03a6(wk\u221211 )) takes the following form:\nP (wk|\u03a6(wk\u221211 )) = exp(y\u0302wk)\u2211\nt\u2032\u2208V exp(y\u0302t\u2032)\n(3)\nwhere the y\u0302t\u2032 are the unnormalized log-probabilities for each potential target word t\u2032 and depend on the model in question. For a ME model with features F , they can be represented as follows:\ny\u0302 = xTM (4)\nwhere x is the word feature activation vector and M is a |F|\u00d7|V| feature weight matrix. The y\u0302i of neural networks on the other hand are computed as follows:\ny\u0302 = g(xTH)W (5)\nwhere g(\u00b7) is the activation function of the hidden layer (typically a tanh or sigmoid) and W and H are weight matrices for the output and hidden layer respectively. Feed-forward and recurrent neural networks differ only in their input vectors x: in a feedforward neural network, x is a concatenation of the input features whereas in a recurrent neural network, x is a concatenation of the input word with the previous hidden state. Because of their shared loglinearity, training and evaluating these models becomes computationally complex.\nAlthough log-linear models have been shown to perform better than linear models (Klakow, 1998), their performance is also hampered by their complexity and we will show in the rest of the paper that a linear model can in fact compete with the state of the art when trained with variable-length n-gram and skip-gram features combined."}, {"heading": "3 Sparse Non-negative Matrix Estimation", "text": ""}, {"heading": "3.1 Linear model", "text": "Contrary to neural networks and ME, SNM language models do not estimate P (wk|\u03a6(wk\u221211 )) in a loglinear fashion, but are in fact linear models:\nP (wk|\u03a6(wk\u221211 )) = y\u0302wk\u2211\nt\u2032\u2208V y\u0302t\u2032\n(6)\nwhere y\u0302 is defined as in Eq. (4). Like ME however, SNM uses features F that are predefined and arbitrary, e.g. n-grams, skip-grams, bags of words, syntactic features, ... The features are extracted from the left context of wk and stored in a feature activation vector x = \u03a6(wk\u221211 ), which is binary-valued, i.e. xf represents the presence or absence of the feature with index f .\nIn what follows, we represent the target word wk by a vector y, which is a one-hot encoding of the vocabulary V: yt = 1 for t = wk, yt = 0 otherwise. To further simplify notation, we will not make the distinction between a feature or target and its index, but rather denote both of them by f and t, respectively.\nThe y\u0302t\u2032 in SNM are computed in the same way as ME, using Eq. (4), where M is a |F| \u00d7 |V| feature weight matrix, which is sparse and non-negative. Mft is indexed by feature f and target t and denotes the influence of feature f in the prediction of t. Plugging Eq. (4) into Eq. (6), we can derive the complete form of the conditional distribution P (y|x) = P (wk|\u03a6(wk\u221211 )) in SNMLMs:\nP (y|x) = (x TM)wk\u2211\nt\u2032\u2208V(x TM)t\u2032\n= \u2211 f \u2032\u2208F xf \u2032Mf \u2032wk\u2211\nt\u2032\u2208V \u2211 f \u2032\u2208F xf \u2032Mf \u2032t\u2032\n= \u2211 f \u2032\u2208F xf \u2032Mf \u2032wk\u2211\nf \u2032\u2208F xf \u2032 \u2211 t\u2032\u2208VMf \u2032t\u2032 (7)\nAs required by the denominator in Eq. (7), this computation also involves summing over all the present features for the entire vocabulary. However, because of the linearity of the model, we can precompute the row sums \u2211 t\u2032\u2208VMf \u2032t\u2032 for each f\n\u2032 and store them together with the model. This means that the evaluation can be done very efficiently, since the remaining summation involves a limited number of terms: even though the amount of features |F| gathered over the entire training data is potentially huge, the amount of active, non-zero features for a given x is small. For example, for SNM models using variable-length n-gram features, the maximum number of active features is n; in our experiments with a large variety of skip-grams, it was around 100.\nNotice that this precomputation is not possible for the log-linear ME which is otherwise similar, because the sum over all features does not distribute outside the sum over all targets in the denominator:\nP (y|x) = exp( \u2211 f \u2032\u2208F xf \u2032Mf \u2032wk)\u2211\nt\u2032\u2208V exp(\n\u2211\nf \u2032\u2208F xf \u2032Mf \u2032t\u2032)\n(8)\nThis is a huge difference and essentially makes SNM a more efficient model at runtime."}, {"heading": "3.2 Adjustment function and meta-features", "text": "We let the entries of M be a slightly modified or adjusted version of the relative frequencies:\nMft = e A(f,t)Cft\nCf\u2217 (9)\nwhere A(f, t) is a real-valued function, dubbed the adjustment function (to be defined below), and C is a feature-target count matrix, computed over the entire training corpus T . Cft denotes the cooccurrence count of feature f and target t, whereas Cf\u2217 denotes the total occurrence count of feature f , summed over all targets t\u2032.\nAn unadjusted SNM model, where A(f, t) = 0, is a linear mixture of simple feature models P (t|f) with uniform mixture weights. The adjustment function enables the models to be weighted by the relative importance of each input feature and, because it also parameterized by t, takes into account the current target. The function is computed by a linear model on binary meta-features (Lee et al., 2007):\nA(f, t) = \u03b8 \u00b7 h(f, t) (10)\nwhere h(f, t) is the meta-feature vector extracted from the feature-target pair (f, t).\nEstimating weights \u03b8k on the meta-feature level rather than the input feature level enables similar input features to share weights which improves generalization. We illustrate this by an example.\nGiven the word sequence the quick brown fox, we extract the following elementary metafeatures from the 3-gram feature the quick brown and the target fox:\n\u2022 feature identity: [the quick brown] \u2022 feature type: 3-gram \u2022 feature count: Cf\u2217 \u2022 target identity: fox \u2022 feature-target count: Cft We also allow conjunctions of (single or multiple) elementary meta-features to form more complex meta-features. This explains the absence of the feature-target identity (and others, see Appendix A) in the above list: it is represented by the conjunction of the feature and target identities. The resulting meta-features enable the model to share weights between, e.g. all 3-grams, all 3-grams that have target fox, etc. Although these conjunctions may in theory override Cft/Cf\u2217 in Eq. (9), keeping the relative frequencies allows us to train the adjustment function on part of the data (see also Section 3.4).\nWe apply smoothing to all of the count metafeatures: since count meta-features of the same order of magnitude carry similar information, we group them so they can share weights. We do this by bucketing the count meta-features according to their (floored) log2 value. As this effectively puts the lowest count values, of which there are many, into a different bucket, we optionally introduce a second (ceilinged) bucket to assure smoother transitions. Both buckets are then weighted according to the log2 fraction lost by the corresponding rounding operation. Pseudocode for meta-feature extraction and count bucketing is presented in Appendix A.\nTo control memory usage, we employ a feature hashing technique (Langford et al., 2007; Ganchev and Dredze, 2008) where we store the meta-feature weights in a flat hash table \u03b8 of predefined size. Strings are fingerprinted (converted into a byte array, then hashed), counts are hashed, and the resulting integer is mapped to an index in \u03b8 by taking its\nvalue modulo the pre-defined size(\u03b8). We do not prevent collisions, which has the potentially undesirable effect of tying together the weights of different meta-features. However, as was previously observed by Mikolov et al. (2011), when this happens the most frequent meta-feature will dominate the final value after training, which essentially boils down to a form of pruning. Because of this, the model performance does not strongly depend on the size of the hash table. Note that we only apply hashing to the meta-feature weights: the adjusted and raw relative frequencies are stored as SSTables (Sorted String Table)."}, {"heading": "3.3 Model estimation", "text": "Although it is in principle possible to use regularized maximum likelihood to estimate the parameters of the model, a gradient-based approach would end up with parameter updates involving the gradient of the log of Eq. (7) which works out to:\n\u2202 logP (y|x) \u2202A(f, t) = xfMft ( yt y\u0302wk \u2212 1\u2211\nt\u2032\u2208V y\u0302t\u2032\n) (11)\nFor the complete derivation, see Appendix B. The problem with this gradient is that we need to sum over the entire vocabulary V in the denominator. In Eq. (7) we could get away with this by precomputing the row sums, but here the sums change after each update. Instead, we were inspired by Xu et al. (2011) and chose to use an independent binary predictor for each word in the vocabulary during estimation. Our approach however differs from Xu et al. (2011) in that we do not use |V| Bernoullis, but |V| Poissons2, using the fact that for a large number of trials a Bernoulli with small p is well approximated by a Poisson with small \u03bb.\nIf we consider each yt\u2032 in y to be Poisson distributed with parameter y\u0302t\u2032 , the conditional probability PPois(y|x) is given by:\nPPois(y|x) = \u220f\nt\u2032\u2208V\ny\u0302 yt\u2032 t\u2032 e \u2212y\u0302t\u2032 yt\u2032 ! = \u220f t\u2032\u2208V y\u0302 yt\u2032 t\u2032 e \u2212y\u0302t\u2032 (12)\n2We originally chose Poisson so we could apply the model to tasks with outputs yt > 1. More recent experiments using a multinomial loss can be found in Chelba and Pereira (2016).\nand the gradient of the log-probability works out to:\n\u2202 logPPois(y|x) \u2202A(f, t) = xfMft ( yt y\u0302wk \u2212 1 )\n(13)\nFor the complete derivation, see Appendix C. The parameters \u03b8 of the adjustment function are learned by maximizing the Poisson log-probability, using stochastic gradient ascent. That is, for each feature-target pair (f, t) we compute the gradient in Eq. (13) and propagate it to the meta-feature weights \u03b8k by multiplying it with \u2202A(f, t)/\u2202\u03b8k = hk. At the N th occurrence of feature-target pair (f, t), each weight \u03b8k is updated using the propagated gradient, weighted by a learning rate \u03b7:\n\u03b8k,N \u2190 \u03b8k,N\u22121 + \u03b7\u2202N (f, t) (14)\nwhere \u2202N (f, t) is a short-hand notation for the N th gradient with respect to \u03b8k. Rather than using a single fixed learning rate, we use AdaGrad (Duchi, 2011) which uses a separate adaptive learning rate \u03b7k,N for each weight \u03b8k,N :\n\u03b7k,N = \u03b3\u221a\n\u22060 + \u2211N n=1 \u2202n(f, t) 2\n(15)\nwhere \u03b3 is a constant scaling factor for all learning rates and \u22060 is an initial accumulator constant. Basing the learning rate on historical information tempers the effect of frequently occurring features which keeps the weights small and as such acts as a form of regularization."}, {"heading": "3.4 Optimization and leave-one-out training", "text": "Each feature-target pair (f, t) constitutes a training example where examples with yt = 0 are called negative and others positive. Using the short-hand notations T = |T |, F = |F| and V = |V|, this means that the training data consists of approximately TF (V \u2212 1) negative and only TF positive training examples. If we examine the two terms of Eq. (13) separately, we see that the first term xfMft\nyt y\u0302wk depends on yt which means it becomes zero for all the negative training examples. The second term \u2212xfMft however does not depend on yt and therefore never becomes zero. This also means that the total gradient is never zero and because of\nthis, the vast amount of updates required for the negative examples makes the update algorithm computationally too expensive.\nTo speed up the algorithm we use a heuristic that allows us to express the second term as a function of yt, essentially redistributing the updates for the numerous negative examples to the fewer positive training examples. Appendix D shows that for batch training this has the same effect if run over the entire corpus. We note that for online training this is not strictly correct, sinceMft changes after each update. Nonetheless, we found this to yield good results as well as seriously reducing the computational cost. After applying the redistribution, the online gradient that is applied to each training example becomes:\n\u2202 logPPois(y|x) \u2202A(f, t) = xfytMft\n( 1\ny\u0302wk \u2212 Cf\u2217 Cft\n) (16)\nwhich is non-zero only for positive training examples, hence making training independent of the size of the vocabulary.\nOne practical way to further prevent overfitting and adapt the model to a specific task is to use heldout data, i.e. compute the count matrix C on the training data and estimate the parameters \u03b8 on the held-out data. Unfortunately, since the aggregated gradients in Eq. (16) tie the updates to the counts Cf\u2217 and Cft in the training data, they can\u2019t differentiate between held-out and training data, which means that the meta-feature weights can\u2019t be tuned specifically to the held-out data. Experiments in which we tried to use the held-out counts instead did not yield good results, presumably because we are violating the redistribution heuristic.\nRather than adding a regularizer on the metafeature weights, we instead opted for leave-one-out training. With the notation A(f, t, Cf\u2217, Cft) reflecting the dependence of the adjustment function on feature and feature-target counts, the gradient under leave-one-out training becomes:\nxfyt\n( ( 1\ny\u0302+wk \u2212 1)M+ft \u2212 Cf\u2217 \u2212 Cft Cft M\u2212ft\n) (17)\nwhere M\u2212ft, M + ft and y\u0302 + wk are defined as follows:\nM\u2212ft = e A(f,t,Cf\u2217\u22121,Cft) Cft\nCf\u2217 \u2212 1\nM+ft = e A(f,t,Cf\u2217\u22121,Cft\u22121)Cft \u2212 1\nCf\u2217 \u2212 1 y\u0302+wk = (x TM+)wk\nThe full derivation can be found in Appendix E. We note that in practice, it often suffices to use only a subset of the training examples for leave-oneout training, which has the additional advantage of speeding up training even further."}, {"heading": "4 Complexity analysis", "text": "Besides their excellent results, RNNs have also been shown to scale well with large amounts of data with regards to memory and accuracy (Williams et al., 2015). Compared to n-gram models which grow huge very quickly with only modest improvements, RNNs take up but a fraction of the memory and exhibit a near linear reduction in log perplexity with log training words. Moreover, a larger hidden layer can yield more improvements, whereas ngram models quickly suffer from data sparsity. The problem with RNNs however is that they are computationally complex which makes training and evaluation slow. A standard Elman network (Elman, 1990) with hidden layer of size H trained on a corpus of size T with vocabulary of size V has complexity\nIT (H2 +HV ) (18)\nwhere I indicates the number of iterations. Several attempts have been made to reduce training time, focusing mostly on reducing the large factors T or V :\n\u2022 vocabulary shortlisting (Schwenk and Gauvain, 2004) \u2022 subsampling (Schwenk and Gauvain, 2005; Xu\net al., 2011) \u2022 class-based (Goodman, 2001b; Morin and Ben-\ngio, 2005; Mikolov et al., 2011) \u2022 noise-contrastive estimation (Gutmann and\nHyva\u0308rinen, 2012; Chen et al., 2015)\nHowever, these techniques either come with a serious performance degradation (Le et al., 2013) or\ndo not sufficiently speed up training. The classbased implementation for example, still has a training computational complexity of:\nIT (H2 +HC + CVC) (19)\nwhere C indicates the number of classes and VC the variable amount of words in a class. Although this is a significant reduction in complexity, the dominant term ITH2 is still large. The same applies to noisecontrastive estimation.\nAs was shown in Mikolov et al. (2011), a Maximum Entropy model can be regarded as a neural network with direct connections for the features, i.e. it has no hidden layers. The model uses the same softmax activation at its output and its complexity therefore also depends on the size of the vocabulary:\nIT (F+V ) (20)\nwhere F+ F denotes the number of active features. To achieve state-of-the-art results this model is often combined with an RNN, which yields a total complexity of:\nIT (H2 +HV + F+V ) (21)\nThe computational complexity for training SNM models on the other hand is independent of V :\nTF+ + IT \u2032F+\u0398+ (22)\nwhere \u0398+ is the number of meta-features for each of the F+ input features. The first term is related to counting features and feature-target pairs and the second term to training the adjustment model on a subset T \u2032 of the training data. If we compare an SNMLM with typical values of F+ \u2248 100 and \u0398+ < 40, to the RNNLM configurations with H = 1024 in Chelba et al. (2014) and Williams et al. (2015), we find that training comes at a reduced complexity of at least two orders of magnitude.\nA even more striking difference in complexity can be seen at test time. Whereas the complexity of a class-based RNN for a single test step is proportional toH2+HC+CVC , testing SNMLMs is linear in F+ because of the reasons outlined in Section 3.1."}, {"heading": "5 Experiment 1: 1B Word Benchmark", "text": "Our first experimental setup used the One Billion Word Benchmark3 made available by Chelba et al. (2014). It consists of an English training and test set of about 0.8 billion and 159658 tokens, respectively. The vocabulary contains 793471 words and was constructed by discarding all words with count below 3. OOV words are mapped to an <UNK> token which is also part of the vocabulary. The OOV rate of the test set is 0.28%. Sentence order is randomized.\nAll of the described SNM models are initialized with meta-feature weights \u03b8k = 0 which are updated using AdaGrad with accumulator \u22060 = 1 and scaling factor \u03b3 = 0.02 over a single epoch of 30M training examples. The hash table for the metafeatures was limited to 200M entries as increasing it yielded no significant improvements."}, {"heading": "5.1 N-gram experiments", "text": "In the first set of experiments, we used all variablelength n-gram features that appeared at least once in the training data up to a given length. This yields at most n active features: one for each m-gram of length 0 \u2264 m < n where m = 0 corresponds to an empty feature which is always present and produces the unigram distribution. The number of features is smaller than n when the context is shorter than n\u22121 words (near sentence boundaries) and during evaluation where an n-gram that did not occur in the training data is discarded.\nWhen trained using these features, SNMLMs come very close to n-gram models with interpolated Kneser-Ney (KN) smoothing (Kneser and Ney, 1995), where no count cut-off was applied and the discount does not change with the order of the model. Table 1 shows that Katz smoothing (Katz, 1987) performs considerably worse than both SNM and KN. KN and SNM are not very complementary as linear interpolation with weights optimized on the test data only yields an additional perplexity reduction of about 1%. The difference between KN and SNM becomes smaller when we increase the size of the context, going from 5% for 5-grams to 3% for 8-grams, which indicates that SNMLMs might be better suited to a large number of features.\n3http://www.statmt.org/lm-benchmark"}, {"heading": "5.2 Integrating skip-gram features", "text": "To incorporate skip-gram features, we can either build a \u2018pure\u2019 skip-gram SNMLM that contains no regular n-gram features (except for unigrams) and interpolate this model with KN, or we can build a single SNMLM that has both the regular n-gram features and the skip-gram features. We compared the two approaches by choosing skip-gram features that can be considered the skip-equivalent of 5-grams, i.e. they contain at most 4 context words. In particular, we configured the following feature extractors:\n\u2022 1 \u2264 r \u2264 3; 1 \u2264 s \u2264 3; 1 \u2264 r + a \u2264 4 \u2022 1 \u2264 r \u2264 2; s \u2265 4 (tied); 1 \u2264 r + a \u2264 4\nWe then built a model that uses both these features and regular 5-grams (SNM5-skip), as well as one that only uses the skip-gram features (SNM5-skip (no n-grams)). In addition, both models were interpolated with a KN 5-gram model (KN5).\nAs can be seen from Table 2, it is better to incorporate all features into one single SNM model than to interpolate with a KN 5-gram model (KN5). This is not surprising as linear interpolation uses a fixed weight for the evaluation of every word sequence, whereas the SNM model applies a variable weight that is dependent both on the context and the target\nword. Finally, interpolating the all-in-one SNM5skip with KN5 yields almost no additional gain."}, {"heading": "5.3 Skip-gram experiments", "text": "The best SNMLM results so far (SNM10-skip) were achieved using 10-grams, together with skip-grams defined by the following feature extractors:\n\u2022 s = 1; 1 \u2264 r + a \u2264 5 \u2022 r = 1; 1 \u2264 s \u2264 10 (tied); 1 \u2264 r + a \u2264 4 This mixture of rich (large context) short-distance and shallow long-distance features enables the model to achieve state-of-the-art results. Table 3 compares its perplexity to KN5 as well as to the following language models:\n\u2022 Stupid Backoff LM (SBO) (Brants et al., 2007) \u2022 Hierarchical Softmax Maximum Entropy LM\n(HSME) (Goodman, 2001b; Morin and Bengio, 2005) \u2022 Recurrent Neural Network LM with Maximum\nEntropy (RNNME) (Mikolov, 2012)\nDescribing these models however is beyond the scope of this paper. Instead we refer the reader to Chelba et al. (2014) for a detailed description. The table also lists the number of model parameters, which in the case of SNMLMs consist of the non-zero entries and precomputed row sums of M.\nWhen we compare the perplexity of SNM10-skip with the state-of-the-art RNNLM with 1024 hidden neurons (RNNME-1024), the difference is only 3%. Moreover, this small advantage comes at the cost of increased training and evaluation complexity. Interestingly, when we interpolate the two models, we have an additional gain of 20%, which shows that SNM10-skip and RNNME-1024 are also complementary. As far as we know, the resulting perplexity of 41.3 is already the best ever reported on this corpus, beating the optimized combination of several models, reported in Chelba et al. (2014) by 6%. Finally, interpolation over all models shows that the contribution of other models as well as the additional perplexity reduction of 0.3 is negligible."}, {"heading": "5.4 Runtime experiments", "text": "In this Section we present actual runtimes to give some idea of how the theoretical complexity analysis of Section 4 translates to a practical application.\nMore specifically, we compare the training runtime (in machine hours) of the best SNM model to the best RNN and n-gram models:\n\u2022 KN5: 28 machine hours \u2022 SNM5: 115 machine hours \u2022 SNM10-skip: 487 machine hours \u2022 RNNME-1024: 5760 machine hours\nAs these models were trained using different architectures (number of CPUs, type of distributed computing, etc.), a runtime comparison is inherently hard and we would therefore like to stress that these numbers should be taken with a grain of salt. However, based on the order of magnitude we can clearly conclude that SNM\u2019s reduced training complexity shown in Section 4 translates to a substantial reduction in training time compared to RNNs. Moreover, the large difference between KN5 and SNM5 suggests that our vanilla implementation can be further improved to achieve even larger speed-ups."}, {"heading": "6 Experiment 2: 44M Word Corpus", "text": "In addition to the experiments on the One Billion Word Benchmark, we also conducted experiments on a small subset of the LDC English Gigaword corpus. This has the advantage that the experiments are more easily reproducible and, since this corpus preserves the original sentence order, it also allows us to investigate SNM\u2019s capabilities of modeling phenomena that cross sentence boundaries.\nThe corpus is the one used in Tan et al. (2012), which we acquired with the help of the authors and is now available at http://www.esat. kuleuven.be/psi/spraak/downloads/4. It consists of a training set of 44M tokens, a check set of 1.7M tokens and a test set of 13.7M tokens. The vocabulary contains 56k words which corresponds to an OOV rate of 0.89% and 1.98% for the check and test set, respectively. OOV words are mapped to an <UNK> token. The large difference in OOV rate between the check and test set is explained by the fact that the training data and check data are from the same source (Agence France-Presse), whereas the test data is drawn from CNA (Central News Agency of Taiwan) which seems to be out of domain relative to the training data. This discrepancy also shows in the perplexity results, presented in Table 4.\nAll of the described SNM models are initialized with meta-feature weights \u03b8k = 0 which are updated using AdaGrad with accumulator \u22060 = 1 and scaling factor \u03b3 = 0.02 over a single epoch of 10M training examples. The hash table for the metafeatures was limited to 10M entries as increasing it yielded no significant improvements.\nWith regards to n-gram modeling, the results are analogous to the 1B word experiment: SNM5 is close to KN5; both outperform Katz5 by a large mar-\n4In order to comply with the LDC license, the data was encrypted using a key derived from the original data.\ngin. This is the case for the check set and the test set. Tan et al. (2012) showed that by crossing sentence boundaries, perplexities can be drastically reduced. Although they did not publish any results on the check set, their mixture of n-gram, syntactic language models and topic models achieved a perplexity of 176 on the test set, a 23% relative reduction compared to KN5. A similar observation was made for the SNM models by adding a feature extractor (r, s, a) analogous to regular skip-grams, but with s now denoting the number of skipped sentence boundaries </S> instead of words. Adding skip-</S> features with r + a = 4, 1 \u2264 r \u2264 2 and 1 \u2264 s \u2264 10, yielded an even larger reduction of 26% than the one reported by Tan et al. (2012). On the check set we observed a 25% reduction.\nThe RNNME results are achieved with a setup similar to the one in Chelba et al. (2014). The main differences are related to the ME features (3-grams only instead of 10-grams and bag-of-words features) and the number of iterations over the training data (20 epochs instead of 10). These choices are related to the size of the training data. It can be seen from Table 4 that the best RNNME model outperforms the best SNM model by 13% on the check set. The outof-domain test set shows that due to its compactness, RNNME is better suited for LM adaptation."}, {"heading": "7 Conclusions and Future Work", "text": "We have presented SNM, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. A first set of empirical evaluations on two data sets shows that SNM n-gram LMs perform almost as well as the well-established KN models. When we add skipgram features, the models are able to match the state-of-the-art RNNLMs on the One Billion Word Benchmark (Chelba et al., 2014). Combining the two modeling techniques yields the best known result on the benchmark which shows that the two models are complementary.\nOn a smaller subset of the LDC English Gigaword corpus, SNMLMs are able to exploit cross-sentence dependencies and outperform a mixture of n-gram models, syntactic language models and topic models. Although RNNLMs still outperform SNM by 13% on this corpus, a complexity analysis and mea-\nsured runtimes show that the RNN comes at an increased training and evaluation time.\nWe conclude that the computational advantages of SNMLMs over both Maximum Entropy and RNN estimation promise an approach that has large flexibility in combining arbitrary features effectively and yet scales gracefully to large amounts of data.\nFuture work includes exploring richer features similar to Goodman (2001a), as well as richer metafeatures in the adjustment model. A comparison of SNM models with Maximum Entropy at feature parity is also planned. One additional idea was pointed out to us by action editor Jason Eisner. Rather than using one-hot target vectors which emphasizes fit, it is possible to use low-dimensional word embeddings. This would most likely yield a smaller model with improved generalization."}, {"heading": "8 Acknowledgments", "text": "We would like to thank Mike Schuster for his help with training and evaluating the RNN models. Thanks also go to Tan et al. who provided us with the 44M word corpus and to action editor Jason Eisner and the anonymous reviewers whose helpful comments certainly improved the quality of the paper."}, {"heading": "Appendix A Meta-feature Extraction Pseudocode", "text": "// Meta-features are represented as tuples (hash value, weight). // New meta-features are either added (metafeatures.Add(mf new)) or // joint (metafeatures.Join(mf new)) with the existing meta-features. // Strings are fingerprinted, counts are hashed. function COMPUTE METAFEATURES(FeatureTargetPair pair)\n// feature-related meta-features metafeatures = {} metafeatures.Add(Fingerprint(pair.feature identity), 1.0) metafeatures.Add(Fingerprint(pair.feature type), 1.0) log count = log(pair.feature count) / log(2) bucket1 = floor(log count) bucket2 = ceil(log count) weight1 = bucket2 - log count weight2 = log count - bucket1 metafeatures.Add(Hash(bucket1), weight1) metafeatures.Add(Hash(bucket2), weight2)\n// target-related meta-features metafeatures.Join(Fingerprint(pair.target identity), 1.0)\n// feature-target-related meta-features log count = log(pair.feature target count) / log(2) bucket1 = floor(log count) bucket2 = ceil(log count) weight1 = bucket2 - log count weight2 = log count - bucket1 metafeatures.Join(Hash(bucket1), weight1) metafeatures.Join(Hash(bucket2), weight2)\nreturn metafeatures\nAppendix B Multinomial Gradient\n\u2202 logPmulti(y|x) \u2202A(f, t) =\n( \u2202 log(xTM)wk\n\u2202Mft \u2212 \u2202 log\n\u2211 t\u2032\u2208V(x TM)t\u2032\n\u2202Mft\n) \u2202Mft \u2202Aft\n=\n( 1\n(xTM)wk \u2202(xTM)wk \u2202Mft \u2212 1\u2211 t\u2032\u2208V(x TM)t\u2032\n\u2202 \u2211\nt\u2032\u2208V(x TM)t\u2032\n\u2202Mft\n) Mft\n= ( xfyt y\u0302wk \u2212 xf\u2211 t\u2032\u2208V(x TM)t\u2032 ) Mft\n= xfMft ( yt y\u0302wk \u2212 1\u2211 t\u2032\u2208V y\u0302t\u2032 )\nAppendix C Poisson Gradient\n\u2202 logPPois(y|x) \u2202A(f, t) =\n( \u2202 \u2211\nt\u2032\u2208V yt\u2032 log(x TM)t\u2032\n\u2202Mft \u2212 \u2202\n\u2211 t\u2032\u2208V(x TM)t\u2032\n\u2202Mft\n) \u2202Mft \u2202A(f, t)\n=\n( 1\n(xTM)wk \u2202(xTM)wk \u2202Mft\n\u2212 xf ) Mft\n= xfMft ( yt y\u0302wk \u2212 1 )"}, {"heading": "Appendix D Distributing Negative Updates", "text": "Over the entire training set, adding Cf\u2217CftMft once on the target t that occurs with feature f amounts to the same as traversing all targets t\u2032 that co-occur with f in the training set and adding the term Mft to each:\nMft \u2211\n(f,t\u2032)\u2208T xf =\nCf\u2217 Cft MftCft = Cf\u2217 Cft\nMft \u2211\n(f,t\u2032)\u2208T xfyt\u2032\nApplying this to the second term of the Poisson gradient, we get:\n\u2202 logPPois(y|x) \u2202A(f, t) = xfMft yt y\u0302wk \u2212 xfMft = xfMft yt y\u0302wk \u2212 xfytMft Cf\u2217 Cft = xfytMft\n( 1\ny\u0302wk \u2212 Cf\u2217 Cft\n)"}, {"heading": "Appendix E Leave-one-out Training", "text": "In leave-one-out training we exclude the event that generates the gradients from the counts used to compute those gradients. More specifically, for each training example (f, t) we let:\nCf\u2217 \u2190 Cf\u2217 \u2212 1 if xf = 1 Cft \u2190 Cft \u2212 1 if xf = 1, yt = 1\nwhich means that the gradients for the positive and the negative examples are changed in a different way. Since Eq. (16) expresses the general update rule for both type of examples, we first have to separate it into updates for negative and positive examples and then adapt accordingly.\nIn particular, the second term of Eq. (16), i.e. \u2212xfytMft Cf\u2217Cft is a distribution of Cf\u2217 \u2212 Cft negative and Cft positive updates over Cft positive examples:\n\u2212xfytMft Cf\u2217 Cft\n= \u2212xfytMft ( Cf\u2217 \u2212 Cft\nCft + Cft Cft\n) = \u2212xfytMft\nCf\u2217 \u2212 Cft Cft \u2212 xfytMft\nFurthermore, recall that the first term of Eq. (16), i.e. xfytMfty\u0302wk is non-zero only for positive examples, so it can be added to the positive updates. We can then apply leave-one-out to positive and negative updates separately, ending up with:\n\u2202 logPPois(y|x) \u2202A(f, t) = xfyt\n( ( 1\ny\u0302+wk \u2212 1)M+ft \u2212 Cf\u2217 \u2212 Cft Cft M\u2212ft\n)\nwhere M\u2212ft, M + ft and y\u0302 + wk are defined as follows:\nM\u2212ft = e A(f,t,Cf\u2217\u22121,Cft) Cft\nCf\u2217 \u2212 1\nM+ft = e A(f,t,Cf\u2217\u22121,Cft\u22121)Cft \u2212 1\nCf\u2217 \u2212 1 y\u0302+wk = (x TM+)wk"}], "year": 2016, "references": [{"title": "Exploiting Latent Semantic Information in Statistical Language Modeling", "authors": ["Jerome Bellegarda."], "venue": "Proceedings of the IEEE, 88(8), 1279\u20131296.", "year": 2000}, {"title": "A Neural Probabilistic Language Model", "authors": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3, 1137\u20131155.", "year": 2003}, {"title": "Large Language Models in Machine Translation", "authors": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean."], "venue": "Proceedings of EMNLP, 858\u2013 867.", "year": 2007}, {"title": "Structured Language Modeling", "authors": ["Ciprian Chelba", "Frederick Jelinek."], "venue": "Computer Speech and Language, 14(4), 283\u2013332.", "year": 2000}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "authors": ["Ciprian Chelba", "Tom\u00e1\u0161 Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "Proceedings of Interspeech, 2635\u20132639.", "year": 2014}, {"title": "Multinomial Loss on Held-out Data for the Sparse Nonnegative Matrix Language Model", "authors": ["Ciprian Chelba", "Fernando Pereira."], "venue": "arXiv:1511.01574", "year": 2016}, {"title": "Topic Adaptation for Language Modeling Using Unnormalized Exponential Models", "authors": ["Stanley F. Chen", "Kristie Seymore", "Ronald Rosenfeld."], "venue": "Proceedings of ICASSP, 681\u2013684.", "year": 1998}, {"title": "Recurrent Neural Network Language Model Training with Noise Contrastive Estimation for Speech Recognition", "authors": ["Xie Chen", "Xunying Liu", "Mark Gales", "Phil Woodland."], "venue": "Proceedings of ICASSP, 5411\u20135415.", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12, 2121\u20132159.", "year": 2011}, {"title": "Finding Structure in Time", "authors": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2), 179\u2013211.", "year": 1990}, {"title": "A Neural Syntactic Language Model", "authors": ["Ahmad Emami."], "venue": "Ph.D. Thesis, Johns Hopkins University.", "year": 2006}, {"title": "A Bit of Progress in Language Modeling, Extended Version", "authors": ["Joshua T. Goodman."], "venue": "Technical Report MSR-TR-2001-72.", "year": 2001}, {"title": "Classes for Fast Maximum Entropy Training", "authors": ["Joshua T. Goodman."], "venue": "Proceedings of ICASSP, 561\u2013564.", "year": 2001}, {"title": "Noisecontrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics", "authors": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "Journal of Machine Learning Research, 13(1), 307\u2013 361.", "year": 2012}, {"title": "Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer", "authors": ["Slava M. Katz."], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, 35(3), 400\u2013401.", "year": 1987}, {"title": "Log-linear Interpolation of Language Models", "authors": ["Dietrich Klakow."], "venue": "Proceedings of ICSLP, 1695\u20131698.", "year": 1998}, {"title": "Improved Backing-Off for M-Gram Language Modeling", "authors": ["Reinhard Kneser", "Hermann Ney."], "venue": "Proceedings of ICASSP, 181\u2013184.", "year": 1995}, {"title": "Vowpal Wabbit Online Learning Project", "authors": ["John Langford", "Lihong Li", "Alex Strehl."], "venue": "http:// hunch.net/?p=309.", "year": 2007}, {"title": "Small Statistical Models by Random Feature Mixing", "authors": ["Kuzman Ganchev", "Mark Dredze."], "venue": "Proceedings of the ACL-2008 Workshop on Mobile Language Processing, 19\u201320.", "year": 2008}, {"title": "Structured Output Layer Neural Network Language Models for Speech Recognition", "authors": ["Hai-Son Le", "Ilya Oparin", "Alexandre Allauzen", "Jean-Luc Gauvain", "Fran\u00e7ois Yvon."], "venue": "IEEE Transactions on Audio, Speech & Language Processing, 21, 195\u2013204.", "year": 2013}, {"title": "Learning a Meta-level Prior for Feature Relevance from Multiple Related Tasks", "authors": ["Su-in Lee", "Vassil Chatalbashev", "David Vickrey", "Daphne Koller"], "venue": "Proceedings of ICML,", "year": 2007}, {"title": "Strategies for Training Large Scale Neural Network Language Models", "authors": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1s Burget", "Jan Cernock\u00fd."], "venue": "Proceedings of ASRU, 196\u2013201.", "year": 2011}, {"title": "Statistical Language Models Based on Neural Networks", "authors": ["Tom\u00e1\u0161 Mikolov."], "venue": "Ph.D. Thesis, Brno University of Technology.", "year": 2012}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "authors": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of AISTATS, 246\u2013252.", "year": 2005}, {"title": "On Structuring Probabilistic Dependences in Stochastic Language Modeling", "authors": ["Hermann Ney", "Ute Essen", "Reinhard Kneser."], "venue": "Computer Speech and Language, 8, 1\u201338.", "year": 1994}, {"title": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing", "authors": ["Rene Pickhardt", "Thomas Gottron", "Martin K\u00f6rner", "Paul G. Wagner", "Till Speicher", "Steffen Staab."], "venue": "Proceedings of ACL, 1145\u20131154.", "year": 2014}, {"title": "Adaptive Statistical Language Modeling: A Maximum Entropy Approach", "authors": ["Ronald Rosenfeld."], "venue": "Ph.D. Thesis, Carnegie Mellon University.", "year": 1994}, {"title": "Whole-sentence Exponential Language Models: a Vehicle for Linguistic-Statistical Integration", "authors": ["Ronald Rosenfeld", "Stanley F. Chen", "Xiaojin Zhu."], "venue": "Computer Speech and Language, 15, 55\u201373.", "year": 2001}, {"title": "Neural Network Language Models for Conversational Speech Recognition", "authors": ["Holger Schwenk", "Jean-Luc Gauvain."], "venue": "Proceedings of ICSLP, 1215-1218.", "year": 2004}, {"title": "Training Neural Network Language Models On Very Large Corpora", "authors": ["Holger Schwenk", "Jean-Luc Gauvain."], "venue": "Proceedings of EMNLP, 201\u2013208.", "year": 2005}, {"title": "Continuous Space Language Models", "authors": ["Holger Schwenk."], "venue": "Computer Speech and Language, 21, 492\u2013 518.", "year": 2007}, {"title": "Comparing RNNs and Log-linear Interpolation of Improved Skipmodel on Four Babel Languages: Cantonese, Pashto, Tagalog, Turkish", "authors": ["Mittul Singh", "Dietrich Klakow."], "venue": "Proceedings of ICASSP, 8416\u2013 8420.", "year": 2013}, {"title": "LSTM Neural Networks for Language Modeling", "authors": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Proceedings of Interspeech, 194\u2013197.", "year": 2012}, {"title": "A Scalable Distributed Syntactic, Semantic, and Lexical Language Model", "authors": ["Ming Tan", "Wenli Zhou", "Lei Zheng", "Shaojun Wang."], "venue": "Computational Linguistics, 38(3), 631\u2013671.", "year": 2012}, {"title": "Efficient Subsampling for Training Complex Language Models", "authors": ["Puyang Xu", "Asela Gunawardana", "Sanjeev Khudanpur."], "venue": "Proceedings of EMNLP, 1128\u20131136.", "year": 2011}, {"title": "Scaling Recurrent Neural Network Language Models", "authors": ["Will Williams", "Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson."], "venue": "Proceedings of ICASSP, 5391\u20135395.", "year": 2015}], "id": "SP:8f67a85a79dfe3617cb7aaaf5400391cb8fad0a1", "authors": [{"name": "Joris Pelemans", "affiliations": []}, {"name": "Noam Shazeer", "affiliations": []}, {"name": "Ciprian Chelba", "affiliations": []}], "abstractText": "We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the stateof-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data.", "title": "Sparse Non-negative Matrix Language Modeling"}