{"sections": [{"text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 57\u201367 Copenhagen, Denmark, September 7\u201311, 2017. c\u00a92017 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Frequently recurring sequences of events in prototypical scenarios, such as visiting a restaurant and driving to work, are a useful source of world knowledge. Two examples are shown in Figure 1, which are different variations of the \u201crestaurant visiting\u201d scenario, where events are partially ordered and can be flexible. Such knowledge is useful for natural language understanding because texts typically do not include event details when mentioning a scenario. For example, the reader is expected to infer that the narrator could have been\n\u2217This work has been done when the first author worked at SUTD.\n1The term \u201ctemporal order\u201d is used throughout this work to indicate the narrative order in texts, following Chambers and Jurafsky (2008). Strictly speaking, the event order we extract is the narrative order.\ndriving or cycling given the text \u201cI got flat tire\u201d. Another typical use of event chain knowledge is to help infer what is likely to happen next given a previous event sequence in a scenario. We investigate the modeling of stereotypical event chains, which is remotely similar to language modeling, but with events being more sparse and flexibly ordered than words.\nOur work follows a recent line of NLP research on script learning. Stereotypical knowledge about partially-ordered events, together with their participant roles such as \u201ccustomer\u201d, \u201cwaiter\u201d, and \u201ctable\u201d, is conventionally referred to as scripts (Schank et al., 1977). NLP algorithms have been investigated for automatically inducing scripts from unstructured texts (Mooney and DeJong, 1985; Chambers and Jurafsky, 2008). In particular, Chambers and Jurafsky (2008) made a first attempt to learn scripts from test inducing event\n57\nchains by grouping events based on their narrative coherence, calculated based on Pairwise Mutual Information (PMI). Jans et al. (2012) showed that the method can be improved by calculating event relations using skip bi-gram probabilities, which explicitly model the temporal order of pairs event. Jans et al. (2012)\u2019s model is adopted by a line of subsequent methods on inducing event chains from text (Orr et al., 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015).\nWhile the above methods are statistical, neural network models have recently been used for event sequence modeling. Granroth-Wilding and Clark (2016) used a Siamese Network instead of PMI to calculate the coherence between two events. Rudinger et al. (2015) extended the idea of Jans et al. (2012) by using a log-bilinear neural language model (Mnih and Hinton, 2007) to calculate event probabilities. By learning embeddings for reducing sparsity, the above models give much better results compared to the models of Chambers and Jurafsky (2008) and Jans et al. (2012). Similar in spirit, Modi (2016) predicted the probability of an event belonging to a certain event chain by modeling known events in the chain as a bag of vectors, showing that it outperforms discrete statistical methods. These neural methods are consistent with the earlier statistical models in leveraging event-pair relations.\nPichotta and Mooney (2016) experimented with LSTM for script learning, using an existing sequence of events to predict the probability of a next event, which outperformed strong discrete baselines. One advantage of LSTMs is that they can encode unbounded time sequences without losing long-term historical information. LSTMs capture significantly more order information compared to the methods of Granroth-Wilding and Clark (2016), Rudinger et al. (2015), and Modi (2016), which model the temporal order of only pairs of events. On the other hand, a strong-order LSTM model can also suffer the disadvantage of over-fitting, given the flexible order of event chains in a script, as demonstrated by the cases of Figure 1. In this aspect, event-pair models are more adaptive for flexible orders. However, no direct comparisons have been reported between LSTM and various existing neural network methods that model event-pairs.\nWe make such comparisons using the same benchmark, finding that the method of Pichotta\nand Mooney (2016) does not necessarily outperform event-pair models, such as Granroth-Wilding and Clark (2016). LSTM temporal ordering and event pair modeling have their respective strength. To leverage the advantages of both methods, we propose to integrate chain temporal order information into event relation measuring. In particular, we calculate event pair relations by representing events in a chain using LSTM hidden states, which encode temporal information. The LSTM over-fitting issue is mitigated by using the temporal-order in a chain as a feature for event pair modeling, rather than the direct model output. In addition, observing that the importance of existing events can vary for inferring a subsequent event, we use a dynamic memory network model to automatically induce event weights for each event for inferring the next event. In contrast, previous methods give equal weights to existing events (Chambers and Jurafsky, 2008; Modi, 2016; Granroth-Wilding and Clark, 2016).\nResults on a multi-choice narrative cloze benchmark show that our model significantly outperforms both Granroth-Wilding and Clark (2016) and Pichotta and Mooney (2016), improving the state-of-the-art accuracy from 49.57% to 55.12%. Our contributions can be summarized as follows:\n\u2022 We make a systematic comparison of LSTM and pair-based event sequence learning methods using the same benchmarks.\n\u2022 We propose a novel dynamic memory network model, which combines the advantages of both LSTM temporal order learning and traditional event pair coherence learning.\n\u2022 We obtain the best results in the standard multi-choice narrative cloze test.\nOur code is released at https://github. com/wangzq870305/event_chain."}, {"heading": "2 Related Work", "text": "Scripts have been a traditional subject in AI research (Schank et al., 1977), where event sequences are manually encoded in knowledge bases, and used for end tasks such as inference. They are also connected with research in linguistics and psychology, and sometimes referred to as frames (Minsky, 1975; Fillmore, 1982) and schemata (Rumelhart, 1975). The same concept\nis also studied as templates in information extraction (Sundheim, 1991). Chambers and Jurafsky (2008) pioneered the recent line of work on script induction (Jans et al., 2012; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016), where the focus is on modeling narrative event chains, a crucial subtask for script modeling from raw text. Below we summarize such investigations.\nWith respect to event representation, Chambers and Jurafsky (2008) casted narrative events as triples of the form \u3008event, dependency\u3009, where the event is typically represented by a verb and the dependency represents typed dependency relations between the event and a protagonist, such as \u201csubject\u201d and \u201cobject\u201d. Chambers and Jurafsky (2008) organized narrative chains around a central actor, or protagonist, mining events that share a common protagonist from texts by using a syntactic parser and a coreference resolver. Balasubramanian et al. (2013) observed that the protagonist representation of event chains can suffer from weaknesses such as lack of coherence, and proposed to represent events as \u3008arg1, relation, arg2\u3009, where arg1 and arg2 represent the subject and object, respectively. Such representation is inspired by open information extraction (Mausam et al., 2012), and offers richer features for event pair modeling. Pichotta and Mooney (2014) adpoted a similar idea, using v(es, eo, ep) to represent an event, where v is a verb lemma, es is the subject, eo is the object, and ep is an entity with prepositional relation to v. Their representation is used by subsequent work such as Modi (2016) and Granroth-Wilding and Clark (2016). We follow Pichotta and Mooney (2016) in our event representation form.\nWith respect to modeling, existing methods can be classified into two main categories, namely weak-order models, which calculate relations between pairs of events, and strong-order models, which consider the temporal order of events in a full sequence. Event-pair models have so far been the dominant method in the literature. Earlier work used discrete event representations and estimated event relations by statistical counting. As mentioned earlier, Chambers and Jurafsky (2008) used PMI to calculate event relations, and Jans et al. (2012) used skip bigram probabilites to the same end, which is order-sensitive. Most subsequent methods followed Jans et al. (2012) in using skip n-grams (Pichotta and Mooney, 2014; Rudinger et al., 2015).\nEvents being multi-argument structures, counting-based methods can suffer from sparsity issues. Recent work employed embeddings to address this disadvantage. Rudinger et al. (2015) learned event embeddings as a by-product of training a log-bilinear language model for events; Granroth-Wilding and Clark (2016) leveraged the skip-gram model of Mikolov et al. (2013) for training the embeddings of event and arguments by ordering them into a pseudo sentence. Modi (2016) utilized word embeddings of verbs and arguments directly, using a hidden layer to automatically consolidate word embedding into a single structured event embeddings. We follow Modi (2016) and use a hidden layer to learn event argument compositions given word embeddings, training the composition function as a part of the event chain learning process.\nMitigating the sparsity issue of event representations, neural methods can capture temporal orders between events beyond skip n-grams. Our model integrates the advantages of strong-order learning and event-pair learning by using LSTM hidden states as feature representation of existing events in the calculation of event pair relationships. In addition, we use a memory network model to weigh existing events, which gives better results compared to the equal weighting method of existing models.\nWith respect to evaluation, Chambers and Jurafsky (2008) proposed the Narrative Cloze Test, which asks for a missing event in a given event chain with a gap. The task has been adopted by various subsequent work for comparing results with Chambers and Jurafsky (2008) (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015). One issue of the narrative cloze test is that there can sometimes be multiple plausible answers, but only one gold-standard answer, which can make it overly expensive to manually evaluate system outputs. To address this issue, Modi (2016) proposed the Adversarial Narrative Cloze (ANC) task, which is to discriminate between pairs of real and corrupted event chains. Granroth-Wilding and Clark (2016) proposed the Multi-Choice Narrative Cloze (MCNC) task, which is to choose the most likely next event from a set of candidates given a chain of events. We choose MCNC for comparing different models.\nOther related work includes learning temporal relations of events (Modi and Titov, 2014; Uz-\nZaman et al., 2013; Abend et al., 2015), evaluated using different metrics. There has also been work using graph models to induce frames, which emphasize more on learning event structures and less on temporal orders (Chambers, 2013; Cheung et al., 2013). The above methods focus on one of the two subtasks we consider here. Frermann et al. (2014) used a Bayesian model to jointly cluster web collections of explicit event sequence and learn input event-pair temporal orders. However, their work is under a different input setting (Regneri et al., 2010), not learning event chains from texts. Mostafazadeh et al. (2016) proposed the story close task (SCT), which is to predict the ending given a unfinished story. Our narrative chain prediction task can be regarded as a sub task in the story close task, which can contribute as a major approach. On the other hand, information beyond event chains can be useful for the story close task."}, {"heading": "3 Problem Definition", "text": "As shown in Figure 2, given a chain of narrative events e1, e2, ..., en\u22121, our work is to predict the likelihood of a next event candidate en. Formally, an event e is a structure v(a0, a1, a2), where v is a verb describing the event, a0 and a1 are its subject and direct object, respectively, and a2 is a prepositional object. For example, given the sentence \u201cJohn brought Marry to the restaurant\u201d, an event bring{John,Marry , to the restaurant} can be extracted.\nWe follow the standard script induction setting (Chambers and Jurafsky, 2008; GranrothWilding and Clark, 2016), extracting events from a text corpus using a syntactic parser and a named entity resolver. A neural network is used to model chains of extracted events for script learning. In particular, we model the probability of a sub-\nsequent event given a chain of events. For evaluation, we solve the multi-choice narrative cloze task: given a chain of events and a set of candidate next events, the most likely candidate is chosen as the output."}, {"heading": "4 Model", "text": "The overall structure of our model is shown in Figure 3, which has three main components. First, given an event v(a0, a1, a2), a representation layer is used to compose the embeddings of v, a0, a1, and a2 into a single event vector e. Second, a LSTM is used to map a sequence of existing events e1, e2, ..., en\u22121 into a sequence of hidden vectors h1, h2, ..., hn\u22121, which encode the temporal order. Given a next event candidate ec, the recurrent network takes one further step from hn\u22121 to derive its hidden vector hc, which encodes ec. Third, hc is paired with h1, h2, ..., hn\u22121 individually, and passed to a dynamic memory network to learn the relatedness score s. s is used to denote the connectedness between the candidate subsequent event and the context event chain."}, {"heading": "4.1 Event Representation", "text": "We learn vector representations of standard events by composing pre-trained word embeddings of its verb and arguments. The skipgram model (Mikolov et al., 2013) is used to train word vectors. For arguments that consist of more than one word, we use the averaged word for the representation. OOV words are represented simply using zero vectors. For events with less than 3 arguments, such as \u201cJohn fell\u201d, where v = fall, a0 = John, a1 = NULL, and a2 = NULL, the NULL arguments are represented using all-zero vectors.\nDenoting the embeddings of v, a0, a1, and a2 as e(v), e(a0), e(a1), and e(a2), respectively, the embedding of e is calculated using a tanh composition layer\ne(e) = tanh(W ve \u00b7 e(v) +W 0e \u00b7 e(a0)+ W 1e \u00b7 e(a1) +W 2e \u00b7 e(a2) + be)\n(1)\nHere W ve , W 0 e , W 1 e , W 2 e , and b are model parameters, which are randomly initialized and tuned during the training of the main network."}, {"heading": "4.2 Modeling Temporal Orders", "text": "Given the embeddings of the existing chain of events e1, e2, ..., en\u22121, we use a standard LSTM (Hochreiter and Schmidhuber, 1997) without\ncoupled input and forget gates or peephole connections to model the temporal order. We obtain a sequence of hidden state vectors h1, h2, ..., hn\u22121 by recurrently feeding e(e1), e(e2), ..., e(en\u22121) as inputs to the LSTM, where hi = LSTM(e(ei), hi\u22121). The initial state hs and all stand LSTM parameters are randomly initialized and tuned during training.\nNow for each candidate next event ec, we obtain its vector representation e(ec) in the same way as for e1 to en\u22121. e(ec) is then appended to the existing event chain to obtain a temporal-ordersensitive feature vector hc, by advancing the recurrent encoding process for one step from hn\u22121: hc = LSTM(e(ec), hn\u22121). With multiple next event candidates e1c , e 2 c , ..., e m c (m \u2208 [1,\u221e]), m feature vectors are obtained, as shown in Figure 4, each being used as a basis for estimating the probability of the corresponding event candidate."}, {"heading": "4.3 Modeling Pairwise Event Relations", "text": "After obtaining the hidden states for events, we model event pair relations using these hidden state vectors. A straightforward approach to model the relation between two events is using a Siamese network (Granroth-Wilding and Clark, 2016). The order-sensitive LSTM features for existing events h1, h2, ..., hn\u22121 and the candidate event hc are\nused as event representations. Given a pair of events hi (i \u2208 [1..n \u2212 1]) and hc, the relatedness score is calculated by\nsi = sigmoid(Wsihi +Wschc + bs), (2)\nwhere Wsi, Wsc and bs are model parameters. Given the relation score si between hc and each existing event hi, the likelihood of ec given e1, e2, ..., en\u22121 can be calculated as the average of si:\ns = \u2211n\u22121\ni=1 si n\u2212 1 (3)\nWeighting existing events. The drawback of above approach is that it considers the contribution of each event on the chain is same. However, given a chain of existing events, some are more informative for inferring a subsequent event than others. For example, given the events \u201cwait in queue\u201d, \u201cgetting seated\u201d and \u201corder food\u201d, \u201corder food\u201d is more relevant for inferring \u201ceat food\u201d compared with the other two given events. Given information over the full event chain, this link can be more evident since the scenario is likely restaurant visiting.\nWe use an attentional neural network to calculate the relative importance of each existing event according to the subsequent event candidate, using hi (i \u2208 [1..n\u22121]) and hc for event representations:\nui = tanh(Weihi +Wchc + bu) (4)\n\u03b1i = exp(ui)\u2211 j exp(uj)\n(5)\nwhere \u03b1i \u2208 [0, 1] is the weight of hi, and \u2211 i \u03b1 t i = 1. Wei, Wc, and bu are model parameters. After obtaining the weight \u03b1i of each existing event hi, the relatedness of ec with the existing events can be calculated as:\ns = n\u22121\u2211 i=1 \u03b1i \u00b7 si (6)\nMulti-layer attention using Deep memory network. Memory network (Weston et al., 2014; Mikolov et al., 2014) has been used for exploring deep semantic information for semantic tasks. Such as question answering (Sukhbaatar et al., 2015; Kumar et al., 2016) and reading comprehension (Hermann et al., 2015; Weston et al., 2015). Our task is analogous to such semantic tasks in\nthe sense that deep semantic information can be necessary for making the most rational inference. Hence, we are motivated to use a deep memory network model to refine event weight and event relation calculation by recurrently modeling more abstract representations of the scenario. Different from the previous researches, we use the memory network to model the event chain, refining the attention mechanism used to explore the pair-wise relation between events.\nThe memory model consists of multiple dynamic computational layers (hops). For the first layer (hop 1), the weights \u03b1 for existing events e1, e2, ..., en\u22121 can be calculated using the same attention mechanism as Eq.4 and Eq.5. Given the weights \u03b1, we build a consolidated representation of context event chain e1, e2, ..., en\u22121 as a weighted sum of h1, h2, ..., hn\u22121:\nhe = n\u22121\u2211 i\u22121 \u03b1i \u00b7 hi (7)\nThe event candidate hc and the new representation of the existing chain he can be further integrated to deduce a deeper representation of the full event chain hypothesis to the next layer (hop 2), denoted as v. v contains deeper semantic information compared with hc, which encode the temporal order of the event chain [h1, h2, ..., hn\u22121, hc] without differentiating the weights of each event. As a result, in the next hop, better event weights can potentially be deduced by using v instead of hc in the calculation of attention:\nuti = tanh(Weihi +Wvv t + bu) (8)\n\u03b1ti = exp(uti)\u2211 j exp(u t j)\n(9)\nIn the same way, we stack multiple hops and repeat the steps multiple times, so that more abstract evidences can be extracted according to the chain of existing events. The above process can be performed recurrently, by taking hc as an initial scenario representation v0, and then repeatedly calculating hte given h1, h2, ..., hn\u22121 and vt, and using hte and vt to find a deeper scenario representation vt+1. Following Chung et al. (2014) and Tran et al. (2016), a gated recurrent network is used to this end:\nz = \u03c3(Wzhte + Uzv t) r = \u03c3(Wrhte + Urv t)\nh\u0302 = tanh(Whte + U(r vt)) vt+1 = (1\u2212 z) vt + z h\u0302\n(10)\nAt any step, if the value of |vt+1\u2212vt| is less than the threshold \u00b5, we consider that the progress has reached convergence. Figure 5 shows an overview of the memory network at hop t."}, {"heading": "4.4 Training", "text": "Given a set of event chains, each with a goldstandard subsequent event and a number of nonsubsequent events, our training objective is to minimize the cross-entropy loss between the gold subsequent event and the set of non-subsequent events. The loss function of event chain prediction is that:\nL(\u0398) = N\u2211\ni=1\n(si \u2212 yi)2 + \u03bb2 ||\u0398|| 2 (11)\nwhere si is the relation score, yi is the label of the candidate (yi = 1 for positive sample, and yi = 0 for negative sample), \u0398 is the set of model parameters and \u03bb is a parameter for L2 regularization. We apply online training, where model parameters are optimized by using AdaGrad (Duchi et al., 2011). We train word embedding using the Skipgram algorithm (Mikolov et al., 2013)2."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Datasets", "text": "Following Granroth-Wilding and Clark (2016), we extract events from the NYT portion of the Gigaword corpus (Graff et al., 2003). The C&C\n2https://code.google.com/p/word2vec/\ntools (Curran et al., 2007) are used for POS tagging and dependency parsing, and OpenNLP3 for phrase structure parsing and coreference resolution. The training set consists of 1,500,000 event chains. We follow Granroth-Wilding and Clark (2016) and use 10,000 event chains as the test set, and 1,000 event chains for development. There are 5 choices of output event for event input chain, which are given by Granroth-Wilding and Clark (2016). This dataset is referred to as G&C16.\nWe also adapt the Chambers and Jurafsky (2008)\u2019s dataset to the multiple choice setting, and use this dataset as the second benchmark. The dataset contains 69 documents, with 346 multiple choice event chain samples. We randomly sample 4 negative subsequent events for each event chain to make multiple-choice candidates. This dataset is referred to as C&J08. For both datasets, accuracy (Acc.) of the chosen subsequent event is used to measure the performance of our model."}, {"heading": "5.2 Hyper-parameters", "text": "There are several important hyper-parameters in our models, and we tune their values using the development dataset. We set the regularization weight \u03bb = 10\u22128 and the initial learning rate to 0.01. The size of word vectors is set to 300, and the size of hidden vectors in LSTM to 128. In order to avoid over-fitting, dropout (Hinton et al., 2012) is used for word embedding with a ratio of 0.2. The neighbor similarity threshold \u03b7 is set to 0.25. The threshold \u00b5 of the memory network sets to 0.1."}, {"heading": "5.3 Development Experiments", "text": "We conduct a set of development experiments on the G&C16 development set to study the influence of event argument representations and network configurations of the proposed MemNet model."}, {"heading": "5.3.1 Influence of Event Structure", "text": "Existing literature discussed various structures to denote events, such as v(a0, a1) and v(a0, a1, a2). We investigate the influence of integrating argument values of the subject a0, object a1 and preposition a2, by doing ablation experiments on the development data. The results are shown in Table 1, where the system using all arguments gives a 54.36% accuracy. By removing a2, which exists in 17.6% of the events in our developmental data, the\n3https://opennlp.apache.org/\naccuracy drops to 54.02%. In contrast, by removing a0 and a1, which exist in 87.6% and 64.6% of the events in the development data, respectively, the accuracies drop to 53.43% and 53.57%, respectively, which demonstrates the relative importance of a0 (i.e., the subject) and a1 (i.e., the object) for event modelling. While most previous work (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014) modelled only a0 and a1, recent work (Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) modelled a2 also.\nBy removing both a1 and a2, the accuracy drops further to 53.32%. Interestingly, by removing the verb while keeping only the arguments, the accuracy drops to 42.63%. While this demonstrates the central value of the verb in denoting a event, it also suggests that the arguments themselves play a useful role in inferring the stereotypical scenario."}, {"heading": "5.3.2 Influence of Network Configurations", "text": "We study the influence of various network configurations by performing ablation experiments, as shown in Table 2. MemNet is the full model of this paper; -LSTM denotes ablation of the LSTM layer, using e(e1), e(e2), ..., e(en\u22121) instead of h1, h2, ..., hn\u22121 to represent events; -Hop denotes ablation of the dynamic network model, using only attention mechanism to calculate the weights of each existing event; -Attention denotes ablation of the attention mechanism, using the same weight on each existing event when inferring ec. The model \u201c-Attention, -LSTM\u201d is hence similar to the method of Granroth-Wilding and Clark (2016), although we used a different way of deriving event embeddings. The model \u201cLSTM-only\u201d shows a based by using LSTM hidden vector hn\u22121 to directly predict the next event, which is similar to the method of Pichotta and Mooney (2016).\nInfluence of Temporal Order. By comparing \u201cMemNet\u201d and \u201c-LSTM\u201d, and comparing \u201c-\nAttention\u201d with \u201c-Attention, -LSTM\u201d, one can find that temporal order information over the whole event chain does have significant influence on the results (p \u2212 value < 0.01 using t-test). On the other hand, using LSTM to directly predict the subsequent event (\u201cLSTM-only\u201d) does not give better accuracies compared to model event pairs (\u201c-Attention, -LSTM\u201d). This confirms our intuition that strong-oder modelling and event-pair modelling each have their own strength.\nInfluence of Attention. Comparison between \u201c-Attention\u201d and \u201c-Hop\u201d, and between \u201c- Attention, -LSTM\u201d and \u201c-Hop, -LSTM\u201d shows that giving different weights to different events does lead to improving results. Our analysis in Section 4.3 gives more intuitions to this observation. Finally, comparison between \u201c-Hop\u201d and \u201cMemNet\u201d and between \u201c-Hop, -LSTM\u201d and \u201c- LSTM\u201d shows that a multi-hop deep memory network can indeed enhance the model with single level attention by offering more effective semantic representation of the scenarios."}, {"heading": "5.4 Final Results", "text": "Table 3 shows the final results on the C&C 16 and C&J08 datasets, respectively. We compare the results of our final model with the following baselines:\n\u2022 PMI is the co-occurrence based model of Chambers and Jurafsky (2008), who calculate event pair relations based on Pointwise Mutual Information (PMI), scoring each candidate event ec by the sum of PMI scores between the given events e0, e1, ..., en\u22121 and the candidate.\n\u2022 Bigram is the counting based model of Jans et al. (2012), calculating event pair relations based on skip bigram probabilities, trained using maximum likelihood estimation.\n\u2022 Event-Comp is the neural event relation model proposed by Granroth-Wilding and Clark (2016). They learn event representations by calculating pair-wise event scores using a Siamese network.\n\u2022 RNN is the method of Pichotta and Mooney (2016), who model event chains by directly using hc in Section 4.2 to predict the output, rather than taking them as features for event pair relation modeling.\n\u2022 MemNet is the proposed deep memory network model.\nOur reimplementation of PMI and Bigrams follows (Granroth-Wilding and Clark, 2016). It can be seen from the table that the statistical counting-based models PMI and Bigram significantly underperform the neural network models Event-Comp, RNN and MemNet, which is largely due to their sparsity and lack of semantic representation power. Under our event representation, Bigram does not outperform PMI significantly either, although considering the order of event pairs. This is likely due to sparsity of events when all arguments are considered.\nDirect comparison between Event-Comp and RNN shows that the event-pair model gives comparable results to the strong-order LSTM model. Although Granroth-Wilding and Clark (2016) and Pichotta and Mooney (2016) both compared with statistical baselines, they did not make direct comparisons between their methods, which represent two different approaches to the task. Our results show that they each have their unique advantages, which confirm our intuition in the introduction. By considering both pairwise relations and chain temporal orders, our method significantly outperform both Event-Comp and RNN (p \u2212 value < 0.01 using t-test), giving the best reported results on both datasets."}, {"heading": "6 Conclusion", "text": "We proposed a dynamic memory network to integrate chain order information into event relation measuring, calculating event pair relations by representing events in a chain using LSTM hidden states, which encode temporal orders, and using a dynamic memory model to automatically induce event weights for each event. Standard evaluation showed that our method significantly outperforms state-of-the-art event pair models and event chain models, giving the best results reported so far."}, {"heading": "Acknowledgments", "text": "The corresponding author is Yue Zhang. We are grateful for the help of Fei Dong for his initial discussion. We thank our anonymous reviewers for their constructive comments, which helped to improve the paper. This work is supported by the Temasek Lab grant IGDST1403012 at Singapore University of Technology and Design."}], "year": 2017, "references": [{"title": "Lexical event ordering with an edge-factored model", "authors": ["Omri Abend", "Shay B. Cohen", "Mark Steedman."], "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "year": 2015}, {"title": "Generating coherent event schemas at scale", "authors": ["Niranjan Balasubramanian", "Stephen Soderland", "Mausam", "Oren Etzioni."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,", "year": 2013}, {"title": "Event schema induction with a probabilistic entity-driven model", "authors": ["Nathanael Chambers."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle,", "year": 2013}, {"title": "Unsupervised learning of narrative event chains", "authors": ["Nathanael Chambers", "Daniel Jurafsky."], "venue": "ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, June 1520, 2008, Columbus, Ohio, USA, pages 789\u2013797.", "year": 2008}, {"title": "Probabilistic frame induction", "authors": ["Jackie Chi Kit Cheung", "Hoifung Poon", "Lucy Vanderwende."], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of", "year": 2013}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "authors": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1412.3555.", "year": 2014}, {"title": "Linguistically motivated large-scale NLP with c&c and boxer", "authors": ["James R. Curran", "Stephen Clark", "Johan Bos."], "venue": "ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Re-", "year": 2007}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["John C. Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "year": 2011}, {"title": "Frame semantics", "authors": ["Charles Fillmore."], "venue": "Linguistics in the morning calm, pages 111\u2013137.", "year": 1982}, {"title": "A hierarchical bayesian model for unsupervised induction of script knowledge", "authors": ["Lea Frermann", "Ivan Titov", "Manfred Pinkal."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EA-", "year": 2014}, {"title": "English gigaword", "authors": ["David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, Philadelphia.", "year": 2003}, {"title": "What happens next? event prediction using a compositional neural network model", "authors": ["Mark Granroth-Wilding", "Stephen Clark."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, US-", "year": 2016}, {"title": "Teaching machines to read and comprehend", "authors": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems 28: Annual", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "CoRR, abs/1207.0580.", "year": 2012}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "year": 1997}, {"title": "Skip n-grams and ranking functions for predicting script events", "authors": ["Bram Jans", "Steven Bethard", "Ivan Vulic", "MarieFrancine Moens."], "venue": "EACL 2012, 13th Conference of the European Chapter of", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "authors": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."], "venue": "Proceedings of the 33nd In-", "year": 2016}, {"title": "Open language learning for information extraction", "authors": ["Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "year": 2012}, {"title": "Learning longer memory in recurrent neural networks. CoRR, abs/1412.7753", "authors": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Micha\u00ebl Mathieu", "Marc\u2019Aurelio Ranzato"], "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on", "year": 2013}, {"title": "A framework for representing knowledge", "authors": ["Marvin Minsky"], "year": 1975}, {"title": "Three new graphical models for statistical language modelling", "authors": ["Andriy Mnih", "Geoffrey E. Hinton."], "venue": "Machine Learning, Proceedings of the TwentyFourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, pages 641\u2013", "year": 2007}, {"title": "Event embeddings for semantic script modeling", "authors": ["Ashutosh Modi."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 75\u201383.", "year": 2016}, {"title": "Inducing neural models of script knowledge", "authors": ["Ashutosh Modi", "Ivan Titov."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, CoNLL 2014, Baltimore, Maryland, USA, June 26-27, 2014, pages 49\u201357.", "year": 2014}, {"title": "Learning schemata for natural language processing", "authors": ["Raymond Mooney", "Gerald DeJong."], "venue": "Urbana, 51:61801.", "year": 1985}, {"title": "A corpus and cloze evaluation for deeper understanding", "authors": ["Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James F. Allen"], "year": 2016}, {"title": "Learning scripts as hidden markov models", "authors": ["John Walker Orr", "Prasad Tadepalli", "Janardhan Rao Doppa", "Xiaoli Fern", "Thomas G. Dietterich."], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec", "year": 2014}, {"title": "Statistical script learning with multi-argument events", "authors": ["Karl Pichotta", "Raymond J. Mooney."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2014, April 26-30, 2014, Gothen-", "year": 2014}, {"title": "Learning statistical scripts with LSTM recurrent neural networks", "authors": ["Karl Pichotta", "Raymond J. Mooney."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 2800\u20132806.", "year": 2016}, {"title": "Learning script knowledge with web experiments", "authors": ["Michaela Regneri", "Alexander Koller", "Manfred Pinkal."], "venue": "ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden,", "year": 2010}, {"title": "Script induction as language modeling", "authors": ["Rachel Rudinger", "Pushpendre Rastogi", "Francis Ferraro", "Benjamin Van Durme."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal,", "year": 2015}, {"title": "Notes on a schema for stories", "authors": ["David E Rumelhart."], "venue": "Representation and understanding: Studies in cognitive science, 211(236):45.", "year": 1975}, {"title": "Scripts, plans, goals and understanding; an inquiry into human knowledge structures", "authors": ["Roger Schank", "Roger Schank", "Robert P Abelson."], "venue": "Technical report.", "year": 1977}, {"title": "End-to-end memory networks", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-", "year": 2015}, {"title": "Third message understanding evaluation and conference (muc-3): Phase 1 status report", "authors": ["Beth Sundheim."], "venue": "HLT.", "year": 1991}, {"title": "Recurrent memory networks for language", "authors": ["Ke M. Tran", "Arianna Bisazza", "Christof Monz"], "year": 2016}, {"title": "Semeval-2013 task 1: Tempeval3: Evaluating time expressions, events, and temporal relations", "authors": ["Naushad UzZaman", "Hector Llorens", "Leon Derczynski", "James F. Allen", "Marc Verhagen", "James Pustejovsky."], "venue": "Proceedings of the 7th In-", "year": 2013}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "authors": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "year": 2015}, {"title": "Memory networks", "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "year": 2014}], "id": "SP:ef0e11df5452cbaeaaafa0e839647ca5d757e676", "authors": [{"name": "Zhongqing Wang", "affiliations": []}, {"name": "Yue Zhang", "affiliations": []}, {"name": "Ching-Yun Chang", "affiliations": []}], "abstractText": "There has been a recent line of work automatically learning scripts from unstructured texts, by modeling narrative event chains. While the dominant approach group events using event pair relations, LSTMs have been used to encode full chains of narrative events. The latter has the advantage of learning long-range temporal orders1, yet the former is more adaptive to partial orders. We propose a neural model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling. A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event. Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far.", "title": "Integrating Order Information and Event Relation for Script Event Prediction"}