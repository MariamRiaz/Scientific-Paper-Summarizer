0	31	In multi-task reinforcement learning (MTRL) agents are presented several related target tasks (Taylor & Stone, 2009; Caruana, 1998) with shared characteristics.
4	25	Many real-world problems involve multiple agents with partial observability and limited communication (e.g., the AUV example) (Oliehoek & Amato, 2016), but generating accurate models for these domains is difficult due to complex interactions between agents and the environment.
7	45	Learning specialized policies for individual tasks can be problematic, as not only do agents have to store a distinct policy for each task, but in practice face scenarios where the identity of the task is often non-observable.
88	29	We formalize single-task MARL using the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), defined as 〈I,S,A, T ,R,Ω,O, γ〉, where I is a set of n agents, S is the state space, A = ×iA(i) is the joint action space, and Ω = ×iΩ(i) is the joint observation space (Bernstein et al., 2002).1 Each agent i executes action a(i) ∈ A(i), where joint action a = 〈a(1), .
94	22	The team receives a joint reward rt = R(st,at) ∈ R at each timestep t, the objective being to maximize the value (or expected return), V = E[ ∑H t=0 γ trt].
104	66	A partially-observable MT-MARL DomainD is a tuple 〈I,S,A,Ω, γ〉, where I is the set of agents, S is the environment state space, A is the joint action space, Ω is the joint observation space, and γ is the discount factor.
107	20	The objective is to find a joint policy that maximizes average empirical execution-time return in all E episodes, V̄ = 1E ∑E e=0 ∑He t=0 γ tRe(st,at), where He is the time horizon of episode e.
108	61	This section introduces a two-phase approach for partiallyobservable MT-MARL; the approach first conducts singletask specialization, and subsequently unifies task-specific DRQNs into a joint policy that performs well in all tasks.
110	23	This enables agents to learn coordination, while also learning Q-values needed for computation of a unified MT-MARL policy.
113	34	In contrast to policy tables or FSCs, Q-values are amenable to the multi-task distillation process as they inherently measure quality of all actions, rather than just the optimal action.
117	27	This approach uses two learning rates: nominal learning rate, α, is used when the TD-error is non-negative; a smaller learning rate, β, is used otherwise (where 0 < β < α < 1).
119	100	Agents are, therefore, robust against negative learning due to teammate exploration and concurrent actions.
127	26	Experience replay also breaks temporal correlations of samples used for Q-value updates—crucial for reducing generalization error, as the stochastic optimization algorithms used for training DQNs typically assume i.i.d.
133	57	Given independent experience samples for each agent, the first agent may learn action a1 as optimal, whereas the second agent learns a2, resulting in arbitrarily poor joint action 〈a1, a2〉.
135	44	Concurrent experiences induce correlations in local policy updates, so that given existence of multiple equilibria, agents tend to converge to the same one.
150	21	Guaranteed concurrent sampling merely requires a one-time (offline) consensus of agents’ random number generator seeds prior to initiating learning.
156	40	,He}, where He is the timestep of the episode’s final experience.
159	21	This ensures all experiences have equal probability of being used in updates, which we found especially critical for fast training on tasks with only terminal rewards.
170	64	Following task specialization, the second phase involves distillation of each agent’s set of DRQNs into a unified DRQN that performs well in all tasks without explicit provision of task ID.
174	34	For data collection, agents use each specialized DRQN (from Phase I) to execute actions in corresponding tasks, resulting in a set of regression CERTs {MR} (one per task), each containing sequences of regression experiences 〈o(i)t , Q (i) t 〉, where Q (i) t = Q (i) t (~ot (i); θ(i)) is the specialized DRQN’s Q-value vector for agent i at timestep t. Supervised learning of Q-values is then conducted.
178	43	The motivation behind loss function (6) is that low temperatures (0 < T < 1) lead to sharpening of specialized DRQN action-values, Qbt , ensuring that the distilled DRQN ultimately chooses similar actions as the specialized policy it was trained on.
179	33	We refer readers to Rusu et al. (2015) for additional analysis of the distillation loss.
191	28	Target dynamics are unknown to agents and vary across tasks.
193	43	In MAMT, each agent is assigned a unique target to capture, yet is unaware of the assignment (which also varies across tasks).
202	30	The failure point of Dec-DRQN is first compared to DecHDRQN in the MAST domain with n = 2 and Pf = 0 (full observability) for increasing task size, starting from 4× 4.
203	27	Despite the domain simplicity, Dec-DRQN fails to match Dec-HDRQN at the 8×8 mark, receiving value 0.05±0.16 in contrast to Dec-HDRQN’s 0.76±0.11 (full results reported in supplementary material).
223	22	We now evaluate distillation of specialized Dec-HDRQN policies (as learned in Section 6.1) for MT-MARL.
229	81	5, our MT-MARL approach first performs DecHDRQN specialization training on each task for 70K epochs, and then performs distillation for 100K epochs.
232	47	Note that performance is plotted only for the 4× 4 and 6× 6 tasks, simply for plot clarity (see supplementary material for MT-MARL evaluation results on all tasks).
239	48	Future work will investigate incorporation of skills (macro-actions) into the framework, extension to domains with heterogeneous agents, and evaluation on more complex domains with much larger numbers of tasks.
