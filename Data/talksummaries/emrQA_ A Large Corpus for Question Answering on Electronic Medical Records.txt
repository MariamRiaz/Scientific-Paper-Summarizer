0	58	Automatic question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets (Voorhees et al., 1999; Ferrucci et al., 2010; Rajpurkar et al., 2016; Joshi et al., 2017).
5	47	EMRs are a longitudinal record of a patient’s health information in the form of unstructured clinical notes (progress notes, discharge summaries etc.)
6	44	Physicians wish to answer questions about medical entities and relations from the EMR, requiring a deeper understanding of clinical notes.
15	15	The main contributions of this work are as follows: • A novel framework for systematic generation of domain-specific large-scale QA datasets that can be used in any domain where manual annotations are challenging to obtain but limited annotations may be available for other NLP tasks.
36	19	Our general framework for generating a largescale QA corpus given certain resources consists of three steps: (1) collecting questions to capture domain-specific user needs, followed by normalizing the collected questions to templates by replacing entities (that may be related via binary or composite relations) in the question with placeholders.
39	15	(3) We then proceed to the important step of re-purposing existing NLP annotations to populate questionlogical form templates and generate answers.
40	18	QA is a complex task that requires addressing several fundamental NLP problems before accurately answering a question.
45	20	Moreover, in domain specific instances such as EMRs, manually annotated logical forms allow the experts to express information essential for natural language understanding such as domain knowledge, temporal relations, and negation (Gao et al.; Chabierski et al., 2017).
47	90	We apply the proposed framework to generate the emrQA corpus consisting of questions posed by physicians against longitudinal EMRs of a patient, using annotations provided by i2b2 (Figure 2).
48	38	We collect questions for EMR QA by, 1) polling physicians at the Veterans Administration for what they frequently want to know from the EMR (976 questions), 2) using an existing source of 5,696 questions generated by a team of medical experts from 71 patient records (Raghavan et al., 2017) and 3) using 15 prototypical questions from an ob- servational study done by physicians (Tang et al., 1994).
49	44	To obtain templates, the questions were automatically normalized by identifying medical entities (using MetaMap (Aronson, 2001)) in questions and replacing them with generic placeholders.
51	40	We align our entity types to those defined in the i2b2 concept extraction tasks (Uzuner et al., 2010a, 2011) - problem, test, treatment, mode and medication.
52	56	E.g., The question What is the dosage of insulin?
53	16	from the collection gets converted to the template What is the dosage of |medication|?
75	94	The next step in the process is to populate the question and logical form (QL) templates with existing annotations in the i2b2 clinical datasets and extract answer evidence for the questions.
76	28	The i2b2 datasets are expert annotated with fine-grained annotations (Guo et al., 2006) that were developed for various shared NLP challenge tasks, including (1) smoking status classification (Uzuner et al., 2008), (2) diagnosis of obesity and its co-morbidities (Uzuner, 2009), extraction of (3) medication concepts (Uzuner et al., 2010a), (4) relations, concepts, assertions (Uzuner et al., 2010b, 2011) (5) co-reference resolution (Uzuner et al., 2012) and (6) heart disease risk factor identification (Stubbs and Uzuner, 2015).
92	63	The overall process for answer evidence generation was vetted by a physician.
107	24	A quantitative and qualitative analysis of emrQA question templates is shown in Table 3, where logical forms help formalize their characteristics (Su et al., 2016).
108	23	Questions may request specific finegrained information (attribute values like dosage) or may express a more coarse-grained need (event entities like medications etc), or a combination of both.
109	18	25% of questions require complex operators (e.g compare(>)) and 12% of questions express the need for external medical knowledge (e.g. lab.refhigh).
110	45	The questions in emrQA are highly compositional, where 47% of question templates have at least one event relation.
119	35	has all medications in the patient’s longitudinal record as answer evidence.
120	24	In order to analyze the reasoning required to answer emrQA questions, we sampled 35 clinical notes from the corpus and analyzed 3 random questions per note by manually labeling them with the categories described in Table 4.
122	50	We compare and contrast this analysis with SQuAD (Rajpurkar et al., 2016), a popular MC dataset generated through crowdsourcing, to show that the framework is capable of generating a corpus as representative and even more complex.
124	30	Additionally, over two times as many questions in emrQA require reasoning over multiple sentences.
126	23	EMRs are inherently noisy and hence 29% have incomplete context and the document length is 27 times more than SQuAD which offers new challenges to existing QA models.
130	143	We implement baseline models using neural and heuristic methods for question to logical form (QL) and question to answer (Q-A) mapping.
131	31	Heuristic Models: We use a template-matching approach where we first split the data into train/test sets, and then normalize questions in the test set into templates by replacing entities with placeholders.
135	19	Scoring and matching is done using two heuristics: (1) HM-1, which computes an identical match, and (2) HM-2, which generates a GloVe vector (Arora et al., 2016) representation of the templates using sentence2vec and then computes pairwise cosine similarity.
138	23	Hence, for comparison with GeoQuery and ATIS, we use the results of seq2seq model with a single 200 hidden units layer (Jia and Liang, 2016).
