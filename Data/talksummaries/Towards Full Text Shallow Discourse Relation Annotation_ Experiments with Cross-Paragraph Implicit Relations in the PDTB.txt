0	33	Empirical approaches for modeling discourse relations rely on corpora annotated with such relations, such as the PDTB (Prasad et al., 2008), the RST-DT (Carlson et al., 2003), and the ANNODIS corpus (Afantenos et al., 2012).
1	21	The PDTB is currently the largest of these annotated corpora and widely used for theoretical and empirical research on discourse relations.
32	23	Annotated over the ˜1 million word WSJ corpus (Marcus et al., 1993), the PDTB follows a lexically-grounded approach to the representation of discourse relations (Webber et al., 2003) while remaining theoryneutral in its annotation approach.
33	49	Discourse relations are taken to hold between two abstract object arguments, named Arg1 and Arg2 using syntactic conventions, and are triggered either by explicit connectives (Ex.
48	32	3) when it can be expressed by inserting a connective, or an AltLex (alternatively lexicalized) relation (Ex.
50	30	When a discourse relation is not inferred, the context is annotated as EntRel (Ex.
54	57	The Arg2 of explicit relations is always some part of the sentence or clause containing the connective, but the Arg1 can be anywhere in the prior text.
57	58	To identify challenges and explore the feasibility of annotating cross-paragraph implicit relations on a large scale, texts from the PDTB corpus were selected to cover a range of sub-genres (Webber, 2009) and lengths.
58	67	These texts contained 440 current paragraph first sentence (CPFS) tokens (excluding the first sentence in each text) not already related to the prior text by an inter-sentential explicit connective.
59	107	These tokens were annotated in the PDTB Annotation Tool (Lee et al., 2016) over the three phases described below.
60	68	Phase One involved guidelines training and developing a preliminary understanding of the task.
61	32	Two expert annotators worked together to discuss and annotate 10 texts (130 tokens) with the PDTB guidelines, except we did not enforce the PDTB adjacency constraint in order to explore the full complexity of the task.
62	34	Each token was annotated for its type (Implicit, EntRel or Altlex), sense (Fig.
79	21	To this end, a further 103 tokens (10 texts) were separately annotated by each annotator for type, sense and minimal argument spans, regardless of whether arguments were adjacent or non-adjacent.
81	23	As shown, the adjacency distribution of arguments in the 76% (45%+31%) tokens agreed to be adjacent (46/103) or non-adjacent (32/103) supports our hypothesis that non-adjacent crossparagraph implicit relations occur with high frequency (32/78, 41%), approaching half of all agreed tokens.
82	32	For each of these agreed tokens, we computed sense and argument agreement to obtain (a) ‘Exact Match’, i.e., fully agreed for type, sense, and argument spans, (b) ‘Sent-level match’, i.e., slightly relaxing the minimality constraint subsententially to include tokens agreed for type and sense whose argument boundaries only disagreed inside a sentence boundary (e.g. because one annotator included an adjunct clause the other excluded), (c) ‘Agree Sense, Args Overlap’, i.e., relaxing the minimality constraint supra-sententially to include tokens agreed for type and sense whose Arg1 and Arg2 boundaries overlapped but did not exactly match (e.g. because one annotator included additional sentence(s) the other considered non-minimal), (d) ‘Agree Sense, Args Disagree’, i.e., agreed for type and sense but unmatched in all of the aforementioned ways, which can only occur for non-adjacent relations and not adjacent relations, and (e) ‘Disagree Sense’, i.e., disagreed as to type or sense, although arguments may or may not have matched in some way.
85	22	The table also shows that with this metric, agreement was worse for nonadjacent relations ((7+5+3)/32, 47%) than adjacent relations ((11+3+14)/46, 61%).
86	33	Discussion of the disagreements showed that while it was almost always possible to reach consensus, the time and effort required was often much greater for non-adjacent relations – twice the amount of time required for adjacent relations – and therefore prohibitive to large-scale annotation.
88	24	Tokens perceived as forming a non-adjacent implicit relation would be annotated as NoSemRel, as described below, providing an underspecified marking to indicate its presence.
99	73	In the absence of a semantic discourse relation between adjacent sentences, the PDTB-2 labels the relation between them as follows: (a) as EntRel if an entity-based coherence relation holds between Arg1 and Arg2 and the discourse is expanded around that entity in Arg2, either by continuing the narrative around it or supplying background about it; (b) as EntRel if (a) doesn’t hold but some entity co-reference exists between Arg1 and Arg2 (even if an implicit relation also holds between Arg2 and a non-adjacent sentence); (c) as NoRel if neither (a) nor (b) holds (even if an implicit relation also holds between Arg2 and a non-adjacent sentence); and (d) as NoRel if none of (a)-(c) hold, which occurs when Arg2 is not part of the discourse (e.g., bylines or the start of a new article in a single WSJ file).
100	32	However, given our goal to encode the presence of non-adjacent implicit relations, the manner in which these labels are currently assigned is a problem because this information is spread across both labels, by way of scenarios (b) and (c) above.
102	26	Both of these considerations therefore led us to create two new labels for our task: SemEntRel (Semantic EntRel) for scenario (a), to unambiguously identify cases of entity-based coherence relations, and NoSemRel for scenarios (b) and (c), to unambiguously identify cases of non-adjacent implicit relations.
116	30	Employing the enhancements to the PDTB-2 guidelines developed during Phase Two, 207 CPFS-PPLS implicit relation tokens from 34 texts were separately annotated by the two annotators in Phase Three for type, sense and minimal argument spans.
150	24	As shown, agreement on whether a relation was adjacent (95) or nonadjacent (63) was approximately the same as in Phase Two, at 76% (46%+30%), Furthermore, over these 158 (95+63) tokens, the proportion of non-adjacent tokens (63/158, 40%) was similar to Phase Two, again supporting our hypothesis about their high frequency.
151	35	Because of the backoff to annotating only adjacent cross-paragraph implicit relations, overall agreement with the most relaxed metric on argument spans1 is higher in Phase Three (62%) than in Phase Two (43%).
154	43	Exactly matched arguments show an increase to 42% from 24% in Phase Two and there are fewer disagreements due to supra-sentential overlapping spans, which have reduced to 13% from 30% in Phase Two.
190	54	Given this, one goal of our future work is to annotate ˜200 texts of the PDTB corpus with adjacent cross-paragraph implicit relations, following the enhanced guidelines developed here, and publicly distribute the annotations via github.3 The subset of texts to be annotated contain approximately 700 tokens of crossparagraph implicit relations, which we have estimated (from our Phase1 to Phase3 annotations) to require 3 minutes per token on average, i.e., approximately 35 hours of annotation time per annotator.
194	30	We find that annotating non-adjacent cross-paragraph implicit relations is difficult and time-consuming.
197	48	In addition, a two-pass annotation methodology would allow the more difficult cross-paragraph non-adjacent implicit relations to be annotated in a second pass.
198	84	Sequences of inter-sentential relations from the first pass could then reveal systematic structures to inform the second pass.
