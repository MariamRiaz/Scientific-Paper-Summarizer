3	29	Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013).
4	44	However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art.
6	29	The key issues behind the limited success of applying SMT systems directly to semantic parsing lie in the difference between semantic parsing and SMT: MRL is not a real natural language with different properties from natural language.
12	65	Alternatively, this paper proposes an effective, yet simple way to enrich SCFG in hierarchical phrase-based SMT for better semantic parsing.
13	33	Specifically, since the translation rules play a critical role in SMT, we explore to improve translation rule quality and increase its coverage in three ways.
18	23	Evaluation on GeoQuery benchmark dataset shows that our approach obtains consistent improvement and achieves the state-of-the-art across various languages, including English, German and Greek.
23	35	On the target side, we convert these meaning representations to series of strings similar to NL.
24	34	To do so, we simply take a preorder traversal of every functional form, and label every function with the number of arguments it takes.
37	94	In hierarchical phrase-based (HPB) translation models, synchronous rules take the form X → 〈γ, α,∼〉, where X is the non-terminal symbol, γ and α are strings of lexical items and non-terminals in the source and target side respectively, and ∼ indicates the one-to-one cor- respondence between non-terminals in γ and α.
42	23	Given a word sequence eij from position i to position j in MRL′, we enrich the non-terminal symbol X to reflect the internal structure of the word sequence of eij .
44	20	As mentioned earlier, we regard the nested structure in MRL′ as function-argument structure, where each function takes one or more arguments as input while its return serves as an argument to the outside function.
49	25	For an incomplete word sequence, we examine 1) the number of arguments it requires on the right to be complete; and 2) the arity of a function it requires on the left to be complete.
66	22	While alignment of multiple target words to one source word is common in SMT, a trick is then to run IBM model training in both directions.
76	81	According to word alignment in Figure 2(c), a phrase extractor will generate a phrase pair 〈have the highest, largest one@1〉, which is nonintuitive.
87	30	Unknown words usually remain intact in the translation in most machine translation systems (Koehn et al., 2007; Dyer et al., 2010), resulting in the fact that certain translations can not be converted back to tree structures.
88	26	This indicates that in semantic parsing the translation of a word can be from two categories: 1) a token in MRL; or 2) null (i.e., not translated at all), we generate synthetic translation rules for unknown word translation.
93	33	Given an unknown word wu, it generates its synthetic rules in two steps: 1) finding top n (e.g., 5 as in our experiments) close words via Word2Vec;6 and 2) generating synthetic translation rules based on the close Algorithm 1: Generating synthetic translation rules for unknown words Input: Unknown word wu in the source language Source side training data vocabulary: W Lexical translation tables T1 and T2 (two directions) Output: Synthetic translation rule set R for wu 1. foreach word wi in W 2. si = sim(wu, wi) 3. get the top n words WB = {wb1...wbn} with the highest {si} 4.
105	22	As mentioned above, 600 instances are used to train and tune our decoder.
121	23	One natural way to overcome this issue, as in Andreas et al. (2013), would be to simply filter n-best translation till a well-formed one is found.
124	20	Effect of Word Alignment With respect to the performance over different alignment settings, we have the following observations from Table 2: • Semantic parsing is substantially sensitive to alignment.
130	25	• Our approach of tripling the training data achieves comparable performance to the one with gold alignment, suggesting that instead of developing a brand new algorithm for semantic parsing alignment, we can simply make use of GIZA++ alignment output.
150	44	Specifically, while we are not aware of public resources to looking for semantically close words in German, Greek and Thai, we translate unknown words into null for the three languages.
151	20	Table 6 shows the performance over four different languages.
152	23	It shows that our approach, including enriched SCFG, tripling training data with three alignments, and unknown word translation, obtains consistent improvement over the four languages.
159	27	Finally, finding similar words via Word2Vec, however, is quite fast since this is bounded by the vocabulary size of our training set.
160	33	Thanks to the small size of unknown words, adding unknown word translation rules has a very limited impact on the size of phrase table, consequently negligible changes on decoding time.
184	40	In this paper, we have presented an enriched SCFG approach for semantic parsing which realizes the potential of the SMT approach.
189	22	One direc- tion of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data.
190	39	We also noticed recent advance in tree-based SMT.
191	65	Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side.
