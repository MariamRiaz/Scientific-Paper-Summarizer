0	55	The problem of fine-grained opinion analysis involves extraction of opinion targets (or aspect terms) and opinion expressions (or opinion terms) from each review sentence.
1	23	For example, in the sentence: “They offer good appetizers”, the aspect and opinion terms are appetizers and good correspondingly.
7	26	Nevertheless, very few approaches exist for cross-domain fine-grained opinion analysis due to the difficulties in fine-grained adaptation, which is more challenging than coarse-grained problems.
9	21	However, their model requires a seed opinion lexicon in the target domain and pre-mined syntactic patterns as a bridge.
39	16	In unsupervised domain adaptation, we are given a set of labeled review sentences from a source domain DS={(xSi ,ySi)} nS i=1, and a set of unlabeled sentences from a target domain DT = {xTj} nT j=1.
40	20	Our goal is to predict token-level labels on DT .
45	28	On one hand, these previous attempts have verified that syntactic information between words, which can be used as a bridge between domains, is crucial for domain adaptation.
46	20	On the other hand, dependency-tree-based RNN (Socher et al., 2010) has proven to be effective to learn high-level feature representation of each word by encoding syntactic relations between aspect terms and opinion terms (Wang et al., 2016).
47	58	With the above findings, we propose a novel RNN named Recursive Neural Structural Correspondence Network (RNSCN) to learn high-level representation for each word across different domains.
79	35	Each direct edge in the dependency tree associates with a relation feature vector rnm∈Rd and a true relation label vector yRnm∈RK , where K is the total number of dependency relations, n and m denote the indices of the parent and child word of the dependency edge, respectively.
82	67	For non-leaf node appetizer, we first generate the relation vector r43 for the dependency edge x4 (appetizers) amod−−−−→ x3 (good) by r43 = tanh(Whh3 + Wxx4), where Wh ∈ Rd×d transforms the hidden representation to the relation vector space.
84	23	Moreover, the relation vector r43 is used for the auxiliary task on relation prediction: ŷR43 = softmax(WRr43 + bR), where WR ∈ RK×d is the relation classification matrix.
90	23	(3) Through the auxiliary task, similar relations enforce participating words close to each other so that words with similar syntactic functionalities are clustered across domains.
93	37	As discussed in Section 3, it might be hard to learn an accurate relation classifier when each class is a unique relation, because the dependency parser may generate incorrect relations as noisy labels.
95	30	Suppose there is a set of latent groups of relations: G = {1, 2, ..., |G|}, where each relation belongs to only one group.
97	28	The goal is to encode the relation vector to a probability distribution of assigning this relation to any group.
98	18	As can be seen Figure 3, each relation vector rnm is first passed through the autoencoder as follows, p(Gnm = i|rnm) = exp(r>nmWencgi)∑ j∈G exp(r>nmWencgj) , (4) where Gnm denotes the inherent relation group for rnm, gi∈Rd represents the feature embedding for group i, and Wenc∈Rd×d is the encoding matrix that computes bilinear interactions between relation vector rnm and relation group embedding gi.
100	19	An accumulated relation group embedding is computed as: gnm = |G|∑ i=1 p(Gnm = i|rnm)gi.
168	22	The overall comparison results with the baselines are shown in Table 2 with average F1 scores and standard deviations over three random splits.
174	45	To show the effect of the integration of the autoencoder, we conduct experiments over different variants of the proposed model in Table 3.
177	34	To further verify that the autoencoder indeed reduces label noise when the parser is inaccurate, we generate new noisy parse trees by replacing some relations within each sentence with a random relation.
186	18	The above comparisons are made using the test data from target domains which are not available during training (i.e., the inductive setting).
189	20	To do that, we first remove the sequential structure on top, resulting in RNSCN+.
194	21	In general, RNSCN+GRU shows similar performances for both inductive and transductive settings.
196	55	Without opinion labels, RNSCN+-GRU* still achieves better results than Hier-Joint most of the time.
197	19	Its lower performance compared to RNSCN+-GRU also indicates that in the cross-domain setting, the dual information between aspects and opinions is beneficial to find appropriate and discriminative relation feature space.
199	105	To qualitatively show the effect of the auxiliary task with auto-encoders for clustering syntactically similar words across domains, we provide some case studies on the predicted groups of some words in Table 5.
203	21	In the end, we verify the robustness and capability of the model by conducting sensitivity studies and experiments with varying number of unlabeled target data for training, respectively.
207	46	Obviously, our model shows steady improvement with the increasing number of unlabeled target data.
211	30	The adaptation takes place in a common relation feature space, which builds the structural correspondences using syntactic relations among the words in each sentence.
