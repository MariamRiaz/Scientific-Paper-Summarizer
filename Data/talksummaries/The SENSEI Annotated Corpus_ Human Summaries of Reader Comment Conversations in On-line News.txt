1	61	Virtually all major newspapers and news broadcasters now support a reader comment facility, which allows readers to participate in multi-party conversations in which they exchange views and opinion on issues in the news.
8	34	These authors adopt broadly similar approaches: first reader comments are topically clustered, then comments within clusters are ranked and finally one or more top-ranked comments are selected from each cluster, yielding an extractive summary.
10	27	In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015).
14	29	42 In our view, what has been lacking so far is a discussion of and proposed answer to the fundamental question of what a summary of reader comments should be like and human-generated exemplars of such summaries for real sets of reader comments.
24	42	Figure 1 shows a fragment of a typical comment stream, taken from reader comment responses to a Guardian article announcing the decision by Bury town council to reduce bin collection to once every three weeks.
29	47	While threads may be topically cohesive, in practice they rarely are, with the same topic appearing in multiple threads and threads drifting from one topic onto another (see, e.g. comments 5 and 6 in Figure 1 both of which cite plague as a likely outcome of the new policy but are in different threads).
30	47	Our view, based on an analysis of scores of comment sets, is that reader comments are primarily argumentative in nature, with readers making assertions that either (1) express a viewpoint (or stance) on an issue raised in the original article or by an earlier commenter, or (2) provide evidence or grounds for believing a viewpoint or assertion already expressed.
31	23	Issues are questions on which multiple viewpoints are possible; e.g., the issue of whether reducing bin collection to once every three weeks is a good idea, or whether reducing bin collection will lead to an increase in vermin.
32	44	Issues are very often implicit, i.e not directly expressed in the comments (e.g., the issue of whether reducing bin collection will lead to an increase in vermin is never explicitly mentioned yet this is clearly what comments 1-4 are addressing).
37	36	Comments may also express jokes or emotion, though these too are often in the service of advancing some viewpoint (e.g. sarcasm or as in comments 4 and 6 emotive terms like lamebrained and crazy clearly indicating the commenters’ stances, as well as their emotional attitude).
38	25	Given the fundamentally argumentative nature of reader comments as sketched above, one type of summary of wide potential use is a generic informative summary that aims to provide an overview of the argument in the comments.
42	38	characterise opinion on an issue typically involves: identifying alternative viewpoints; indicating the grounds given to support viewpoints; aggregating – indicating how opinion was distributed across different issues, viewpoints and grounds, using quantifiers or qualitative expressions e.g. “the majority discussed x”; indicating where there was consensus or agreement among the comment; indicating where there was disagreement among the comment.
46	56	To help people write overview summaries of reader comments, we have developed a 4-stage method, which is described below2.
53	101	Annotators are asked to write a ‘label’ for each comment, which is a short, free text annotation, capturing its essential content.
61	29	This exercise helps annotators to make better sense of the broad content of the comments, before writing a summary.
62	30	The annotation interface re-displays the labels created in Stage 1 in an edit window, so the annotator can cut/paste the labels (each with its comment id and poster name) into their groups, add Group Labels, and so on.
67	28	Annotators first write an ‘unconstrained summary’, with no word-length requirement, and then (with the first summary still visible) write a ‘constrained-length summary’ of 150–250 words.
72	57	Stage 4: Back-Linking In this stage, annotators link sentences of the constrained-length summary back to the groups (or sub-groups) that informed their creation.
91	25	The SENSEI Social Media Corpus, comprising the full text of the original Guardian articles and reader comments as well as all annotations generated in the four stage summary writing method described in Section 3 above – comment labels, groups, summaries and backlinks – is freely available at: nlp.shef.ac.uk/sensei/.
92	47	There were 18 articles and comment sets, of which 15 were double annotated and 3 were triple annotated, giving a total of 39 sets of complete annotations.
103	31	Whilst the average of groups per annotation set was 9.0, for the annotator who grouped the least this was 4.0, and the maximum average 14.5.
124	33	Analysis revealed that summaries also include examples of: Background about, e.g., an event, practice or person, to clarify an aspect of the debate, e.g. see (S5) of Summary 1, Humour; Feelings and Complaints, about e.g. commenters and reporters.
167	51	We have presented a proposal for a form of informative summary that aims to capture the key content of multi-party, argument-oriented conversations, such as those found in reader comment.
168	72	We have developed a method to help humans author such summaries, and used it to build a corpus of reader comment multiply annotated with summaries and other information.
169	33	We believe the method of labeling and grouping has wide application, i.e. in creating reference summaries of complex, multi-party dialogues in other domains.
171	26	2.2, and exhibit a high degree of consistency, as shown by the content similarity assessment of Sec.
172	52	Informal feedback from media professionals (at the Guardian and elsewhere) suggests that the summaries are viewed very positively as a summary of comments in themselves, and as a target for what an automated system might deliver online.
174	72	We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016).
175	174	Even so, there are limitations to the work done which give pointers to further work.
179	25	Results from a pilot suggest annotators find it much easier to work on sets of 30–50 comments, so we are investigating how annotations for smaller subsets of a comment set might be merged into a single annotation.
