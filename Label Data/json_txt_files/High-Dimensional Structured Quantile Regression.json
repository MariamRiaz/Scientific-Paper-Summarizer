{"sections": [{"heading": "1 Introduction", "text": "Considerable advances have been made over the past decade on fitting high-dimensional structured linear models when the number of samples n is much smaller than the ambient dimensionality p (Banerjee et al., 2014; Negahban et al., 2012; Chandrasekaran et al., 2012). Most of the advances have been made for linear models: yi = \u3008xi, \u03b8\u2217\u3009 + \u03c9i, i = 1, . . . , n, where \u03b8\u2217 \u2208 Rp is assumed to be structured, e.g., sparse, group sparse, etc. Estimation of such structured \u03b8 is usually done using Lasso-type regularized estimators (Negahban et al., 2012; Banerjee et al., 2014) or Dantzig-type constrained estimators (Chandrasekaran et al., 2012; Chatterjee et al., 2014); other related estimators have also been explored (Hsu & Sabato,\n1Department of Computer Science & Engineering, University of Minnesota, Twin Cities. Correspondence to: Vidyashankar Sivakumar <sivak017@umn.edu>, Arindam Banerjee <banerjee@cs.umn.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n2016; Vershynin, 2015). Such models have been extended to generalized linear models (Banerjee et al., 2014; Negahban et al., 2012), matrix completion (Cande\u0300s & Recht, 2009), vector auto-regressive models (Melnyk & Banerjee, 2016) among others.\nIn this paper, we consider the problem of structured quantile regression in high-dimensions, which can be posed as follows: given the response variable yi and covariates xi the \u03c4 th conditional quantile function of yi given xi is given by: F\u22121yi|xi(\u03c4 |xi) = \u3008xi, \u03b8 \u2217 \u03c4 \u3009, \u03c4 \u2208 (0, 1) for some structured \u03b8\u2217\u03c4 whose structure can be captured by a suitable atomic norm R(\u00b7), e.g., l1-norm for sparsity, l1/l2 norm for group sparsity, etc. Here F\u22121yi|xi(\u00b7) is the inverse of the conditional distribution function of yi given xi. We consider the following regularized estimator for the structured quantile regression problem:\n\u03b8\u0302n := argmin\u03b8\u2208Rp L\u03c4 (\u03b8) + \u03bbnR(\u03b8)\n:= argmin\u03b8\u2208Rp 1\nn n\u2211 i=1 \u03c1\u03c4 (yi \u2212 \u3008xi, \u03b8\u3009) + \u03bbnR(\u03b8) ,\n(1)\nwhere \u03c1\u03c4 (u) = (\u03c4 \u2212 I(u \u2264 0))u is the asymmetric absolute deviation function (Koenker, 2005), I(\u00b7) is the indicator function. The goal is to get nonasymptotic bounds on the estimation error \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162.\nMany previous papers analyze the asymptotic performance of the estimator in (1) (Li & Zhu, 2008; Kai et al., 2011; Wu & Liu, 2009; Zou & Yuan, 2008; Wang et al., 2012). In the non-asymptotic setting of interest in this paper, special cases of the estimator in (1) have been studied in recent literature (Belloni & Chernozhukov, 2011; Kato, 2011; Fan et al., 2014a), primarily focusing on specific norms like the l1-norm and nonoverlapping group sparse norm. In contrast, our formulation and analysis is applicable to any atomic norm R(\u00b7) giving considerable flexibility in choosing a suitable structure for real world problems, e.g., hierarchical sparsity, k-support norm, OWL norm etc. More recently (Alquier et al., 2017) consider the more general problem of norm regularized regression with Lipschitz loss functions which includes (1) as a special case. They derive similar results to ours for bounds on the estimation error, but their analysis differs significantly and, in our opinion, does not leverage and highlight the key geometric and sta-\ntistical characteristics of the problem.\nIn the setting of norm regularized regression with square loss, including the widely studied Lasso estimator (Tibshirani, 1996; Negahban et al., 2012; Banerjee et al., 2014), the sample complexity n0 of the estimator gets determined by a certain restricted strong convexity (RSC) property which simplifies to the restricted eigenvalue (RE) condition on the matrix XTX (Bickel et al., 2009; Negahban et al., 2012); in the noiseless setting, i.e., when \u03c9i = 0, the sample complexity determines a phase transition phenomenon so that the probability of recovering the structured \u03b8\u2217 is minimal when n < n0, and one can exactly recover \u03b8\u2217 with high probability when n > n0. Our work gives an equivalent sample complexity characterization for structured quantile regression, which was not available in prior work. The challenge in characterizing RSC in the context of quantile regression stems partly from the nonsmoothness of the objective, so one has to work with subgradients. However, the unique aspect stems from the geometry of quantile regression, or as the authoritative book on the topic puts it: \u201cHow quantile regression works?\u201d (Koenker, 2005)[Section 2.2]. In quantile regression, the n samples get divided into three subsets: \u03bd samples which get exactly interpolated, i.e., yi = \u3008xi, \u03b8\u0302\u3009, (n \u2212 \u03bd)\u03c4 samples which lie below the curve, i.e., yi < \u3008xi, \u03b8\u0302\u3009, and (n \u2212 \u03bd)(1 \u2212 \u03c4) samples which lie above the curve, i.e., yi > \u3008xi, \u03b8\u0302\u3009. Note that when \u03bd = n all samples are interpolated, the loss is zero and the same \u03b8\u0302 is a solution for all quantiles \u03c4 . Quantile regression is clearly not working. The Number of InterPolated Samples (NIPS) \u03bd is an important quantity, inherent to structure in \u03b8\u2217, and determines the sample complexity of structured quantile regression (1). In fact, we show that when n > \u03bd, the RSC condition associated with the estimator in (1) is satisfied. When there is no structure in \u03b8\u2217, then \u03bd = O(p), and quantile regression needs n > O(p) samples to work. However, when \u03b8\u2217 has structure, such as sparsity or group sparsity, \u03bd can be substantially smaller than p. Specifically we show that \u03bd is of the order of square of Gaussian width of the error set (Talagrand, 2014; Chandrasekaran et al., 2012) for a class of atomic norms which includes l1, l1/l2 group sparse, ksupport (Argyriou et al., 2012) and the OWL (Bogdan et al., 2013) norms. The Gaussian width as a measure of complexity of a set has been extensively used in prior literature (Chandrasekaran et al., 2012; Tropp, 2015; Banerjee et al., 2014). For example, when \u03b8\u2217 is sparse with s non-zero entries, we show that \u03bd = cs log p.\nWhen n > \u03bd and the RSC condition is satisfied, building on recent developments in high-dimensional estimation (Negahban et al., 2012; Banerjee et al., 2014), we show that choosing \u03bbn \u2265 2R\u2217(\u2207L\u03c4 (\u03b8\u2217))), where R\u2217(\u00b7) is the dual norm of R(\u00b7), leads to non-asymptotic bounds on the estimation error \u2016\u03b8\u0302n \u2212 \u03b8\u2217\u20162. While the specification of\n\u03bbn looks complex, with its dependency on the dual norm and its dependency on \u03b8\u2217, we show it is sufficient to set \u03bbn based on the Gaussian width (Talagrand, 2014) of the unit norm ball for R(\u00b7) (Banerjee et al., 2014; Sivakumar et al., 2015)\u2018. Our analysis and results on the estimation error bound for quantile regression, interestingly, has the same order as that for regularized least squares regression for general norms (Banerjee et al., 2014). In contrast to the least squares loss the quantile loss is more robust as the estimation error is independent of the two norm of the noise. We discuss results for the l1, l1/l2 group sparse and k-support norms as examples, precisely characterizing the sample complexity for recovery and non-asymptotic error bounds. Specifically, our results for the l1-norm matches those from existing literature on sparse quantile regression (Belloni & Chernozhukov, 2011).\nThe rest of the paper is organized as follows. In Section 2, we discuss the problem formulation along with assumptions, review the general framework for analyzing regularized estimation problems and discuss the three atomic norms used as examples throughout the paper. In Section 3, we analyze the number of interpolated samples and establish precise sample complexities for a class of atomic norms in terms of the Gaussian widths of sets. In Section 4 we establish key ingredients of the analysis and provide the main bound. We present experimental results in Section 5 and conclude in Section 6."}, {"heading": "2 Background and Preliminaries", "text": "Problem formulation: We outline the assumptions on the data and estimator. Similar conditions are present in all prior literature on quantile regression and we refer to Section 2.5 in Belloni & Chernozhukov (2011) for examples of data satisfying the conditions.\nWe consider data is generated as y = X\u03b8 + \u03c9, X \u2208 Rn\u00d7p is the design matrix, \u03b8 \u2208 Rp and \u03c9 \u2208 Rn is the noise. We assume subGaussian design matrices X \u2208 Rn\u00d7p which includes the class of all bounded random variables. Note that this is not a restrictive assumption as to avoid the quantile crossing phenomenon the covariate space has to be bounded. Quantile crossing is when the value of the \u03c41th quantile is greater than the \u03c42th quantile for some \u03c41 < \u03c42 (See Section 2.5 in (Koenker, 2005)). We do not make any assumptions on the noise vector \u03c9 \u2208 Rn. More specifically the noise can be sampled from a heavy tailed distribution, can be heteroscedastic as in the location-scale model where \u03c9i = \u3008xi, \u03b7\u3009 \u00b7 i, \u03b7 \u2208 Rp, i is noise independent of xi, bimodal and so on and so forth. Note that this setting is more general than for the least squares loss.\nWe consider a parametric quantile regression model where the \u03c4 th conditional quantile function of the response vari-\nable yi given any xi \u2208 Rp is given by,\nF\u22121yi|xi(\u03c4 |xi) = \u3008xi, \u03b8 \u2217 \u03c4 \u3009, \u03b8\u2217\u03c4 \u2208 Rp, \u03c4 \u2208 (0, 1) , (2)\nwhere F\u22121yi|xi is the inverse of the conditional distribution function of yi given xi. We will assume the conditional density of yi evaluated at the conditional quantile \u3008xi, \u03b8\u2217\u03c4 \u3009 is bounded away from zero uniformly for all \u03c4 , that is, fyi|xi(\u3008xi, \u03b8\u2217\u03c4 \u3009) > f > 0 for all \u03c4 and all xi. The goal is to estimate parameter \u03b8\u0302\u03c4 close to \u03b8\u2217\u03c4 using n observations of the data when n < p. The estimator in this paper belongs to the family of regularized estimators and is of the form:\n\u03b8\u0302\u03bbn,\u03c4 := argmin\u03b8\u2208Rp L\u03c4 (\u03b8) + \u03bbnR(\u03b8) , (3)\nwhere L\u03c4 (\u03b8) = 1n \u2211n i=1 \u03c1\u03c4 (yi\u2212\u3008xi, \u03b8\u3009), \u03c1\u03c4 (\u00b7) is the quantile loss function and R(\u00b7) is any atomic norm. Examples of atomic norms we consider in this paper are the l1, l1/l2 nonoverlapping group sparse norm and the k-support norm. We present all results assuming any single \u03c4 \u2208 (0, 1) and going forward drop the subscripts from \u03b8\u2217\u03c4 and \u03b8\u0302\u03bbn,\u03c4 .\nGaussian Width: All results will be in terms of the Gaussian width of suitable sets. For any set A \u2208 Rp, the Gaussian width of the set A is defined as (Gordon, 1985; Chandrasekaran et al., 2012):\nw(A) = Eg [ sup u\u2208A \u3008g, u\u3009 ] . (4)\nwhere the expectation is over g \u223c N(0, Ip\u00d7p). Gaussian widths have been widely used in prior literature on structured estimation (Chandrasekaran et al., 2012; Banerjee et al., 2014; Sivakumar et al., 2015; Tropp, 2015).\nHigh-dimensional estimation: Our analysis is built on developments over the past decade for high-dimensional structured regression for linear and generalized linear models using both regularized as well as constrained estimators (Candes & Tao, 2007; Bickel et al., 2009; Chandrasekaran et al., 2012; Negahban et al., 2012; Banerjee et al., 2014). For the regularized formulation considered in this work Banerjee et al. (2014); Negahban et al. (2012) have established a generalized analysis framework when the loss is least squares or more generally the maximum likelihood estimator for generalized linear models. We give a brief overview of the main components of the analysis.\n1. Regularization parameter: In Banerjee et al. (2014); Negahban et al. (2012) the regularization parameter is assumed to satisfy the following assumption,\n\u03bbn \u2265 2R\u2217(\u2207L\u03c4 (\u03b8\u2217)) . (5)\nWith \u2126R = {u|R(u) \u2264 1} denoting the unit norm ball, Banerjee et al. (2014) prove that with high probability a\nvalue \u03bbn = O(w(\u2126R)) satisfies the above condition for subGaussian design matrices, noise and the least squares loss.\n2. Error set: The assumption on the regularization parameter ensures that the error vector \u2206 = \u03b8\u0302\u2212 \u03b8\u2217 lies in the following error set (Banerjee et al., 2014),\nC := {\n\u2206 | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1 2 R(\u2206)\n} . (6)\n3. The norm compatibility constant: It is defined as follows (Negahban et al., 2012; Banerjee et al., 2014),\n\u03a8(C) = sup u\u2208C\nR(u) \u2016u\u20162 . (7)\n4. Restricted Strong Convexity (RSC): In Banerjee et al. (2014); Negahban et al. (2012) the loss function is shown to satisfy the following RSC condition with high probability once the number of samples is of the order of the square of the Gaussian width of the error set, that is, n = O(w2(C)).\ninf u\u2208C \u03b4L(\u03b8\u2217, u) = inf u\u2208C (L(\u03b8\u2217 + u)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217), u\u3009)\n\u2265 \u03ba\u2016u\u20162 . (8)\nFor the squared loss, the RSC condition simplifies to the Restricted Eigenvalue (RE) condition (Bickel et al., 2009)\ninf u\u2208C\n1 n \u2016Xu\u201622 \u2265 \u03ba\u2016u\u201622 . (9)\n5. Recovery Bounds: When RSC and bounds on the regularization parameter are satisfied Banerjee et al. (2014) prove the following deterministic error bound,\n\u2016\u2206\u20162 = \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 c \u03a8(C)w(\u2126R)\n\u03ba . (10)\nwhere c is any constant.\nAtomic Norms: We consider the class of atomic norms for the regularizer. Mathematically consider a set A \u2286 Rp the collection of atoms that is compact, centrally symmetric about the origin (that is, a \u2208 A =\u21d2 \u2212a \u2208 A). Let \u2016\u03b8\u2016A denote the gauge of A,\nR(\u03b8) = \u2016\u03b8\u2016A = inf{t > 0 : \u03b8 \u2208 tconv(A)} (11) = inf{ \u2211 a\u2208A ca : \u03b8 = \u2211 a\u2208A caa, ca \u2265 0,\u2200a \u2208 A} .\n(12)\nFor example when A = {\u00b1ei}pi=1 yields \u2016\u03b8\u2016A = \u2016\u03b8\u20161. Although the atomic set A may contain uncountably many vectors, we assume A can be decomposed as a union of m sets,A = Ai\u222aA2\u222a. . . Am similar to the setting considered\nin Chen & Banerjee (2015). Such a decomposition assumption is satisfied by many popular atomic norms like the l1, l1/l2 group sparse norms, k-support norm, OWL norm etc. Throughout the paper we will illustrate our results on the following norms.\n1. l1 norm: For the l1 norm we will consider that \u03b8\u2217 is an s-sparse vector, that is, \u2016\u03b8\u2217\u20160 = s.\n2. l1/l2 nonoverlapping group sparse norm: Let {G1,G2, . . . , GNG} denote a collection of groups which are blocks of any vector \u03b8 \u2208 Rp. Let \u03b8NG denote a vector with coordinates \u03b8iNG = \u03b8\ni if i \u2208 GNG , else \u03b8iNG = 0. The maximum size of any group is l = max\ni\u2208[1,...,NG ] |Gi|. The norm is given as R(\u03b8) = \u2211NG i=1 \u2016\u03b8i\u20162. Let SG \u2286 {1, 2, . . . , NG} with cardinality |SG | = sG . We consider the true parameter \u03b8\u2217 \u2208 Rp is sG-sparse, that is, \u03b8\u2217NG = ~0,\u2200NG /\u2208 SG .\n3. k-support norm: The k-support norm can be expressed as an infimum convolution given by (Argyriou et al., 2012),\nR(\u03b8) = inf\u2211 i ui=\u03b8 {\u2211 i \u2016ui\u20162 | \u2016ui\u20160 \u2264 k } . (13)\nClearly it is an atomic norm for which A = {a \u2208 Rp | \u2016a\u20160 \u2264 k, \u2016a\u20162 = 1} and A is a union of ( p k ) subsets, that is, A = A1 \u222a A2 \u222a . . . A(pk). More results on the k-support norm can be looked up in Chatterjee et al. (2014); Chen & Banerjee (2015). We consider the setting where \u2016\u03b8\u2217\u20160 = s and k < s. For the results we require the Gaussian widths of the unit norm ball, error set and the norm compatibility constants for the norms. We provide them below for reference. All values are in order notation.\nNorm w(\u2126R) w(C) \u03a8(C) l1 c \u221a log p c \u221a s log p c \u221a s\nl1/l2 c \u221a l + logNG c \u221a lsG + sG logNG c \u221a sG k-sp c \u221a k + k logd p\nk e c\n\u221a s+ s logd p\nk e c\n\u221a 2s/k"}, {"heading": "3 Number of InterPolated Samples (NIPS)", "text": "We begin with intuitions on the geometry of the problem. In the high sample, low dimension setting n >> p, when R(\u03b8) = 0, the quantile loss is a linear program and hence the solutions are at the vertices, that is, where any p of the n samples are interpolated. Mathematically we define the quantity Z = {i : yi = \u3008xi, \u03b8\u0302\u3009 = \u3008xi, \u03b8\u2217 + u\u3009, u \u2208 Rp} and note that \u03bd = sup\nu\u2208Rp |Z| = O(p). In the high di-\nmensional setting considered in this paper n < p and hence with R(\u03b8) = 0 the number of interpolated samples is \u03bd = n. From an optimization perspective there are multiple such solutions and all solutions are optimal for all quantile parameters \u03c4 . But practically quantile regression is not working. Now introducing a regularizer\nwith a suitable choice for the regularization parameter ensures that the error vector lies in a restricted subset of Rp, C := { u \u2223\u2223R(\u03b8\u2217 + u) \u2264 R(\u03b8\u2217) + 12R(u) } \u2286 Rp. We are now interested in characterizing \u03bd = sup u\u2208C |Z|, Z = {i : yi = \u3008xi, \u03b8\u0302\u3009 = \u3008xi, \u03b8\u2217 + u\u3009, u \u2208 C}, that is, the maximum number of interpolated samples with the error restricted to a particular subset of Rp. Again if \u03bd = n, quantile regression is not working. Since there are no restrictions on the number of non-zero elements in the error vector u a first crude estimate will be \u03bd \u2264 min{n, p, \u2016u\u20160}, which implies quantile regression will not work unless we have a minimum of p samples. But intuitively \u03bd should depend on properties of the error set C, which the initial crude estimate is failing to take advantage of.\nBelow we state a result which reinforces the intuition of the relation between \u03bd and the properties of the set C. Specifically we show that for the types of atomic norms considered in this work (which includes all popularly known vector norms) the number of interpolated samples does not exceed the product of the square of the norm compatibility constant and the square of the Gaussian width of the unit norm ball. For the norms considered, this is precisely the square of the Gaussian width of the error set C. For example for the l1 norm for an s-sparse vector this evaluates to an upper bound of \u03bd = O(s log p). While the result statement considers subGaussian design matrices, the result will also hold for design matrices sampled from heavy-tailed distributions using arguments similar to Lecue\u0301 & Mendelson (2014); Sivakumar et al. (2015).\nTheorem 3.1 ConsiderX has isotropic subGaussian rows and \u03b8\u2217 is an s-sparse vector that can be written as a linear combination of k atoms from an atomic set of cardinality m,\n\u03b8\u2217 = k\u2211 i=1 ciai, ai \u2208 A, ci \u2265 0, |A| = m . (14)\nFor the regularized quantile regression problem penalized with the atomic norm R(\u03b8) = \u2016\u03b8\u2016A,\n\u03b8\u0302 = arg min \u03b8\u2208Rp L\u03c4 (\u03b8)+\u03bbR(\u03b8) = arg min \u03b8\u2208Rp\n1\nn n\u2211 i=1 \u03c1\u03c4 (\u03b8)+\u03bbR(\u03b8) ,\n(15) let C := { u|R(\u03b8\u2217 + u) \u2264 R(\u03b8\u2217) + 12R(u) } denote the error set, let \u03bb \u2265 R\u2217(\u2207L\u03c4 (\u03b8\u2217)) and let n \u2265 (c1s + c2\u03a8\n2(C)w(A)) where \u03a8(C) = sup u\u2208C \u2016u\u2016A \u2016u\u20162 is the norm compatibility constant in the error set, w(A) is the Gaussian width of the unit norm ball and c1 and c2 are some constants. Then with probability atleast 1 \u2212 exp(\u2212c2k1 log(em))\u22122 exp(\u2212\u03b7w2(C)) the number of in-\nterpolated samples,\n\u03bd = sup u\u2208C |{i : yi = \u3008xi, \u03b8\u2217+u\u3009, u \u2208 C}| \u2264 c\u03a82(C)w2(A) , (16) where c is a constant.\nTo understand the intuition consider the case of the l1 norm. For an error vector lying in a particular s-dimensional subspace the maximum number of interpolated samples is O(s) with high probability. Extending the argument to all such s-dimensional subspaces by a union bound argument on the ( p s ) subspaces, the maximum number of interpolated samples when the error vector is any s-sparse vector in pdimensional space is O(s log p). Finally the argument is extended to all vectors in the error set using the powerful Maurey\u2019s empirical approximation argument previously employed in Sivakumar et al. (2015); Lecue\u0301 & Mendelson (2014); Rudelson & Zhou (2013).\nSuprisingly in prior literature on structured high dimensional quantile regression, the importance of \u03bd has not been explicitly discussed. This intuition about the importance of \u03bd also shows up in an elegant form in the analysis of the RSC condition in Section 4.2.\nBelow we provide results for the number of interpolated samples for the l1, l1/l2 nonoverlapping group sparse and k-support norms. For the l1/l2 nonoverlapping group sparse norm and the k-support norm we first illustrate that they are atomic norms. The results then follow from substituting known values for the norm compatibility constant and Gaussian width of unit norm ball for the different norms. For computation of these quantities for any general norm we refer the interested reader to work in Vershynin (2015); Chen & Banerjee (2015).\nCorollary 1 For the l1 norm with \u03b8\u2217 being an s-sparse vector, when n > (c1s + c2s log p) then with high probability,\n\u03bd = sup u\u2208C |{i : yi = \u3008xi, \u03b8\u2217 + u\u3009}| \u2264 cs log p , (17)\nfor some constant c.\nBefore applying the result to the nonoverlapping group sparse norm note that for any vector \u03b8 \u2208 Rp,\n\u03b8 = NG\u2211 i=1 \u2211 j cij\u03b2ij , (18)\nwhere \u03b2ij is any unit norm vector in subspace defined by the group i. For any group i, let \u03b8i denote the vector constructed from \u03b8 such that it has component k, \u03b8k = 0 if k /\u2208 i. Now by definition of atomic norm cij = \u2016\u03b8i\u20162 for \u03b8i\nin the same direction of \u03b2ij , otherwise cij = 0. Therefore the group sparse norm is an atomic norm with a hierarchical set structure with the number of elements m = NG in the outer set with each element of the outer set itself being a set of an infinite number of elements with any one element chosen for a particular vector \u03b8, that is, cij 6= 0 for only one j amongst an infinite number of j\u2019s.\nCorollary 2 Consider the l1/l2 nonoverlapping groupsparse norm with n > (c1sGl + c2sG logNG). With high probability,\n\u03bd = sup u\u2208C |{i : yi = \u3008xi, \u03b8\u2217 + u\u3009}| \u2264 c(lsG + sG logNG) ,\n(19) for some constant c.\nFor the k-support norm \u2016\u03b8\u2016spk = inf\u2211 i ui=\u03b8\n{ \u2211 i \u2016ui\u20162 | \u2016ui\u20160 \u2264 k}, can be similarly\nexpressed as,\n\u03b8 = (pk)\u2211 i=1 \u2211 j cij\u03b2ij (20)\nwhere \u03b2ij is a unit vector in k-dimensional subspace i. The difference compared to the nonoverlapping group sparse norm is that many of the cij\u2019s can now be non zero in the inner sum. This is comparably more complex than the group-sparse norm where the inner set becomes a singleton for some \u03b8, but in terms of the analysis nothing changes.\nCorollary 3 Consider the k-support norm with n > (c1s+ c2s logd pk e). With high probability,\n\u03bd = sup u\u2208C |{i : yi = \u3008xi, \u03b8\u2217 + u\u3009}| \u2264 c(s+ s logdp/ke) ,\n(21) for some constant c."}, {"heading": "4 Structured Quantile Regression", "text": "In this section, we present results for the key components in the general analysis framework of Banerjee et al. (2014) which we briefly described in Section 2 of the paper. We start with results on the regularization parameter by analyzing equation (5) before establishing sample complexity bounds when the restricted strong convexity condition in equation (8) is satisfied. Finally an l2 bound on the error is obtained using (10). We will consider subGaussian design matrices throughout. All results are in terms of Gaussian widths of sets and the norm compatibility constant. Results for l1, l1/l2-nonoverlapping group sparse and ksupport norms are given for illustration purposes."}, {"heading": "4.1 Regularization Parameter \u03bbn", "text": "We analyze the bound in equation (5). In prior literature a bound has been established specifically for the l1 norm in Belloni & Chernozhukov (2011) (See Theorem 1). Below we consider the case of any general atomic norm and obtain a result in terms of the Gaussian width of the unit norm ball. The analysis follows from a similar result for the regularization parameter in Banerjee et al. (2014) for the least squares case.\nTheorem 4.1 Let X \u2208 Rn\u00d7p be a design matrix with independent isotropic subGaussian rows with subGaussian norm |||xi|||\u03c82 \u2264 \u03ba. Define \u2126R = {u : R(u) \u2264 1} the unit norm ball and let \u03c6 = sup\nu \u2016u\u20162/R(u). Then the following\nholds E [R\u2217(\u2207L\u03c4 (\u03b8\u2217))] \u2264 c \u221a \u03c4(1\u2212 \u03c4)w(\u2126R)\u221a\nn , (22)\nwhere c is any fixed constant depending only on the subGaussian norm \u03ba. Moreover with probability atleast 1 \u2212\nc1 exp\n( \u2212 (\n\u03c4 c2\u03c6\u03ba\n)2) \u2212 2 exp ( \u22122t2 ) R\u2217(\u2207L\u03c4 (\u03b8\u2217)) \u2264 c \u221a \u03c4(1\u2212 \u03c4)w(\u2126R) + t\u221a\nn , (23)\nwhere c1, c2, t are absolute constants.\nA major difference to the least squares loss setting, is the independence of the regularization parameter to assumptions on the noise vector (see for example Theorem 3 and Theorem 4 in Banerjee et al. (2014) where the noise is explicitly assumed to be subGaussian and homoscedastic and the noise enters the analysis through properties of \u2016\u03c9\u20162). This gives the flexibility of considering noise vectors which are heavy tailed or heteroscedastic. Indeed the most interesting applications of quantile regression arise in such settings.\nBelow we provide bounds for the regularization parameter for different norms by substituting known values of the Gaussian width for the unit norm balls. The result for the l1 norm matches with Theorem 1 in Belloni & Chernozhukov (2011) for the regularization parameter.\nCorollary 4 If R(\u00b7) is the l1 norm, with high probability R\u2217(\u2207L\u03c4 (\u03b8\u2217)) \u2264 c \u221a \u03c4(1\u2212 \u03c4) \u221a log p+ t\u221a\nn . (24)\nCorollary 5 If R(\u00b7) is the l1/l2 nonoverlapping group sparse norm, with high probability,\nR\u2217(\u2207L\u03c4 (\u03b8\u2217)) \u2264 c \u221a \u03c4(1\u2212 \u03c4) \u221a l + logNG + t\u221a n .\nCorollary 6 If R(\u00b7) is the k-support norm, with high probability,\nR\u2217(\u2207L\u03c4 (\u03b8\u2217)) \u2264 c \u221a \u03c4(1\u2212 \u03c4) \u221a k + k logd pk e+ t\u221a n ."}, {"heading": "4.2 Restricted Strong Convexity (RSC)", "text": "The loss needs to satisfy the RSC condition in equation (8). Prior literature on structured quantile regression has not discussed the RSC condition explicitly, though Fan et al. (2014b) considers it for the quantile huber loss function.\nWe start by providing an intuition for the RSC formulation for the quantile loss. The RSC condition equation (8) on the error set C evaluates to the following,\ninf u\u2208C\n1\nn n\u2211 i=1 \u222b \u3008xi,u\u3009 0 (I(yi \u2212 \u3008xi, \u03b8\u2217\u3009 \u2264 z)\u2212 I(yi \u2212 \u3008xi, \u03b8\u2217\u3009 \u2264 0)) dz .\n(25) Let \u03bd = sup\nu\u2208C |Z| = sup u\u2208C |{i|yi = \u3008xi, \u03b8\u2217 + u\u3009}| is the num-\nber of interpolated samples. For any n < p if the model can interpolate all points, that is, \u03bd = n then (25) evaluates to zero. In general, as shown in Section 3, \u03bd gets determined by the structure. For example for the l1 norm \u03bd = O(s log p) rather than the ambient dimensionality p. Thus, the sum over n points in (25) simply reduces to the sum over the (n\u2212\u03bd) points which are not interpolated, and will ensure the RSC condition when n > \u03bd. The intuition of the NIPS property of Section 3 thus shows up elegantly in the RSC condition. In equation (25), let \u03bei = yi \u2212 \u3008xi, \u03b8\u2217\u3009, vi =\u222b \u3008xi,u\u3009 0\n(I(\u03bei \u2264 z)\u2212 I(\u03bei \u2264 0)) and consider 1 n \u2211n i=1E[vi],\n1\nn n\u2211 i=1 E[vi] = 1 n n\u2211 i=1 \u222b \u3008xi,u\u3009 0 (Fi(\u03bei + z)\u2212 Fi(\u03be))\n= 1\nn n\u2211 i=1 \u222b \u3008xi,u\u3009 0 fi(\u03bei)zdz + o(1)\n= 1\n2n n\u2211 i=1 fi(\u03bei)\u3008xi, u\u30092 + o(1)\n\u2265 f\n2n \u2016Xu\u201622 \u2265 f\u03ba\u2016u\u201622 2 .\nThe first line follows from the definition of the cumulative distribution function, the second line by a simple Taylor series expansion, the last line by the assumption that f \u2264 fi(\u03bei),\u2200i and (1/n)\u2016Xu\u201622 \u2265 \u03ba, where \u03ba is the restricted eigenvalue (RE) constant. The RE condition is satisfied as the sample complexity bounds for satisfying the NIPS property is same as the RE condition. More generally RSC is a condition on the minimum eigenvalue of the Jacobian matrix 1n \u2211n i=1 fi(\u03bei)\u3008xi, u\u30092 restricted to the error set\nC. This quantity has also been considered in prior literature (see Section 4.2 in Koenker (2005) and the proof in page 121, also see condition D.1 in Belloni & Chernozhukov (2011)). While the above analysis is in expectation of the quantity vi, we state the following result giving large deviation bounds for the above quantity.\nTheorem 4.2 ConsiderX \u2208 Rn\u00d7p has subGaussian rows. Let 0 < f < fi(\u3008xi, \u03b8\u2217\u3009) be a uniform lower bound on the conditional density for all xi in the support of x. Let \u03ba denote the RE constant satisfying 1n\u2016Xu\u2016 2 2 \u2265 \u03ba\u2016u\u201622. Let the number of samples n > cw2(C) where w(C) is the Gaussian width of the error set C and c is some constant. Then with probability atleast 1\u2212 exp(\u2212\u03c42/2)\u2212 exp ( \u2212\u03c6 2 1f\u03be 2 n ) where \u03c61, \u03be < 1 and \u03c4 are constants,\ninf u\u2208C\n\u03b4L\u03c4 (\u03b8\u2217, u) \u2265 c1\u03baf\u2016u\u201622 . (26)\nwhere c1 < 0 is a constant.\nBelow we provide results for different norms. The sample complexity for the l1 norm matches the result in Belloni & Chernozhukov (2011) (see equation 2.10).\nCorollary 7 For the l1 norm with n > cs log p with high probability the following RSC condition is satisfied,\ninf u\u2208C\n\u03b4L\u03c4 (\u03b8\u2217, u) \u2265 c\u03baf\u2016u\u201622 . (27)\nCorollary 8 For the l1/l2 nonoverlapping group sparse norm with n > c(lsG + sG logNG) with high probability the following RSC condition is satisfied,\ninf u\u2208C\n\u03b4L\u03c4 (\u03b8\u2217, u) \u2265 \u03baf\u2016u\u201622 . (28)\nCorollary 9 For the k-support norm with n > c(s + s logd pk e) with high probability the following RSC condition is satisfied,\ninf u\u2208C\n\u03b4L\u03c4 (\u03b8\u2217, u) \u2265 c\u03baf\u2016u\u201622 . (29)"}, {"heading": "4.3 Recovery Bounds", "text": "Following the general framework outlined in Banerjee et al. (2014) (see Theorem 2), we state the following high probability bound on the two norm of the error vector \u2206 = \u03b8\u0302\u2212\u03b8\u2217.\nTheorem 4.3 For the quantile regression problem, when \u03bbn \u2265 c1 \u221a \u03c4(1\u2212\u03c4)w(\u2126R)\u221a\nn , n > c2w2(C) for some constants\nc1, c2 then with high probability, \u2016\u2206\u20162 \u2264 c \u221a \u03c4(1\u2212 \u03c4)\u03a8(C)w(\u2126R)\nf\u03ba . (30)\nwhere \u03a8(C) is the norm compatibility constant in the error set.\nThe two norm of the error depends on the two terms\u221a \u03c4(1\u2212 \u03c4) and f . The \u221a \u03c4(1\u2212 \u03c4) term is minimized at the tails and hence has the effect of reducing the estimation error. But typically this is dominated by the lower bound on the density f term which makes the estimate less precise in regions of low density. This is to be expected as there are very few samples to make a very precise estimate in low density regions. While similar observations are made in page 72 of Koenker (2005), the results are asymptotic while we show non-asymptotic recovery bounds. Another aspect we reiterate here is the independence of the results from the form of the noise. All results make no assumptions on the noise apart from an assumption on the lower bound of the noise density.\nBelow we provide recovery bounds for the different norms we consider in the paper.\nCorollary 10 For the l1 norm when \u03bbn \u2265 c1 \u221a \u03c4(1\u2212\u03c4)\n\u221a log p\u221a\nn and n > c2s log p with high proba-\nbility\n\u2016\u2206\u20162 \u2264 c \u221a s log p\nf\u03ba \u221a n\n(31)\nCorollary 11 For the l1/l2 nonoverlapping group sparse\nnorm when \u03bbn > c1 \u221a \u03c4(1\u2212\u03c4) \u221a l+logNG\u221a\nn and n > c(lsG +\nsG logNG) with high probability\n\u2016\u2206\u20162 \u2264 c \u221a lsG + sG logNG\nf\u03ba \u221a n\n(32)\nCorollary 12 For the k-support norm when \u03bbn > c1 \u221a \u03c4(1\u2212\u03c4)\n\u221a k+k logd pk e\u221a n\nand n > cs + s logd pk e with high probability\n\u2016\u2206\u20162 \u2264 c \u221a s+ s logd pk e f\u03ba \u221a n\n(33)"}, {"heading": "5 Experiments", "text": "We perform simulations with synthetic data."}, {"heading": "5.1 Phase Transition", "text": "Data is generated as y = X\u03b8\u2217 + \u03c9. \u03b8\u2217 = [1, 1, 1, 1, 1, 1\ufe38 \ufe37\ufe37 \ufe38\n6\n, 0, 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38 p-6 ] \u2208 Rp for the l1 norm and\n\u03b8\u2217 = [1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 5 , 1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 5 , 1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 5 , 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38 5 , . . . , 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38 5 ] for the l1/l2 group sparse norm with p \u2208 [500, 750, 1000]. The noise \u03c9i \u223c N(0, 0.25),\u2200i \u2208 [n] is Gaussian with zero mean and 0.25 variance. The design matrix X \u223c\nN(0, Ip\u00d7p) is multivariate Gaussian with identity covariance. We vary n = [10, 20, 30, . . . , 120, 130]. For each n we generate 100 datasets with the probability of success defined as the fraction of times we are able to faithfully estimate the true parameter. For p = 500 we run simulations for \u03c4 \u2208 [0.1, 0.5, 0.9] and for p \u2208 [750, 1000] we run simulations only for \u03c4 = 0.5. For the optimization, we use the Alternating Direction Method of Multipliers (Boyd et al., 2010). The details of the updates can be found in the flare\ndocumentation Li et al. (2015). The code was implemented in Python. The plots in Figure 1 clearly show a phase transition for both the l1 and l1/l2 group sparse norms for all quantiles exemplifying the NIPS property described earlier."}, {"heading": "5.2 Robustness", "text": "We showcase the robustness enjoyed by quantile regression over ordinary least squares estimation against heavytailed noise and outliers. We consider the l1 norm with y = X\u03b8\u2217 + \u03c9. \u03b8\u2217 = [1, 1, 1, 1, 1, 1\ufe38 \ufe37\ufe37 \ufe38\n6\n, 0, 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38 494 ] \u2208 Rp. For\nheavy-tailed noise we consider the student t-distribution with different degrees of freedom, with lower degrees of freedom corresponding to heavier tailed data. To show the robustness to outliers we randomly pick a certain percentage of samples from the dataset and multiply the noise by 10, that is, \u03c9i = 10 \u2217 \u03c9i for a certain proportion of the dataset. We vary the proportion of contamination from 2.5% to 15%. We fix n = 200 for this simulation. Again for both exercises, we run 100 simulations and plot the mean and standard deviation of the estimation error \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162. The plots in Figure 2 show 1. the estimation error against varying degrees of freedom of the student tdistribution and 2. estimation error against the percent contamination. The observations are in agreement with conventional wisdom on robustness of the quantile regression estimator to heavy-tailed noise and outliers."}, {"heading": "6 Conclusions", "text": "The paper presents a general framework for the analysis of non-asymptotic error and structured recovery for norm regularized quantile regression for any atomic norm. Our results are based on extending the general analysis framework outlined in Banerjee et al. (2014); Negahban et al. (2012) using insights from the geometry of the problem. In particular we introduce the Number of InterPolated Samples (NIPS) as critical for determining the sample complexity for consistent recovery. We prove that once the number of samples crosses the NIPS threshold, we start recovering the true parameter. This phase transition phenomena for norm regularized quantile regression problems has not been discussed in prior literature. We also prove that NIPS is of the order of square of the Gaussian width of the error set for many atomic norms - which is the same order as that for regularized least squares regression and match results from previous work for the l1 norm (Belloni & Chernozhukov, 2011).\nAcknowledgements: We thank reviewers for their valuable comments. This work was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A."}], "year": 2017, "references": [{"title": "Estimation Bounds and Sharp Oracle Inequalities of Regularized Procedures with Lipschitz Loss Functions", "authors": ["P. Alquier", "V. Cottet", "G. Lecue"], "year": 2017}, {"title": "Sparse Prediction with the k-Support Norm", "authors": ["Argyriou", "Andreas", "Foygel", "Rina", "Srebro", "Nathan"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2012}, {"title": "Estimation with Norm Regularization", "authors": ["Banerjee", "Arindam", "Chen", "Sheng", "Fazayeli", "Farideh", "Sivakumar", "Vidyashankar"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2014}, {"title": "l1Penalized Quantile Regression in High-Dimensional Sparse Models", "authors": ["Belloni", "Alexandre", "Chernozhukov", "Victor"], "venue": "The Annals of Statistics,", "year": 2011}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "authors": ["Bickel", "Peter J", "Ritov", "Yaacov", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "Statistical Estimation and Testing via the Sorted L1 Norm", "authors": ["Bogdan", "Malgorzata", "Berg", "Ewout van den", "Su", "Weijie", "Emmanuel", "Candes"], "year": 1969}, {"title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers", "authors": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"], "venue": "Foundations and Trends in Machine Learning,", "year": 2010}, {"title": "Exact Matrix Completion via Convex Optimization", "authors": ["Cand\u00e8s", "Emmanuel J", "Recht", "Benjamin"], "venue": "Foundations of Computational Mathematics,", "year": 2009}, {"title": "The Dantzig selector : statistical estimation when p is much larger than n", "authors": ["Candes", "Emmanuel J", "Tao", "Terence"], "venue": "The Annals of Statistics,", "year": 2007}, {"title": "The Convex Geometry of Linear Inverse Problems", "authors": ["Chandrasekaran", "Venkat", "Recht", "Benjamin", "Parrilo", "Pablo A", "Willsky", "Alan S"], "venue": "Foundations of Computational Mathematics,", "year": 2012}, {"title": "Generalized Dantzig Selector: Application to the k-support Norm", "authors": ["Chatterjee", "Soumyadeep", "Chen", "Sheng", "Banerjee", "Arindam"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Structured Estimation with Atomic Norms: General Bounds and Applications", "authors": ["Chen", "Sheng", "Banerjee", "Arindam"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Adaptive Robust Variable Selection", "authors": ["Fan", "Jianqing", "Yingying", "Barut", "Emre"], "venue": "Annals of Statistics,", "year": 2014}, {"title": "Robust Estimation of High-Dimensional", "authors": ["Fan", "Jianqing", "Li", "Quefeng", "Wang", "Yuyan"], "venue": "Mean Regression", "year": 2014}, {"title": "Some Inequalitites for Gaussian Processes and Applications", "authors": ["Gordon", "Yehoram"], "venue": "Israel Journal of Mathematics,", "year": 1985}, {"title": "Loss Minimization and Parameter Estimation with Heavy Tails", "authors": ["Hsu", "Daniel", "Sabato", "Sivan"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "New Efficient Estimation and Variable Selection Methods for Semiparametric Varying-Coefficient Partially Linear Models", "authors": ["B. Kai", "R. Li", "H. Zou"], "venue": "The Annals of Statistics,", "year": 2011}, {"title": "Group Lasso for High Dimensional Sparse Quantile Regression Models", "authors": ["Kato", "Kengo"], "year": 2011}, {"title": "Quantile Regression", "authors": ["Koenker", "Roger"], "year": 2005}, {"title": "Sparse recovery under weak moment assumptions", "authors": ["Lecu\u00e9", "Guillaume", "Mendelson", "Shahar"], "year": 2014}, {"title": "The flare package for high dimensional linear regression and precision matrix estimation in r", "authors": ["Li", "Xingguo", "Zhao", "Tuo", "Yuan", "Xiaoming", "Liu", "Han"], "venue": "Journal of Machine Learning Research,", "year": 2015}, {"title": "L1-norm Quantile Regression", "authors": ["Y.J. Li", "J. Zhu"], "venue": "Journal of Computational and Graphical Statistics,", "year": 2008}, {"title": "Estimating Structured Vector Autoregressive Model", "authors": ["Melnyk", "Igor", "Banerjee", "Arindam"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "A Unified Framework for HighDimensional Analysis ofM -Estimators with Decomposable Regularizers", "authors": ["Negahban", "Sahand N", "Ravikumar", "Pradeep", "Wainwright", "Martin J", "Yu", "Bin"], "venue": "Statistical Science,", "year": 2012}, {"title": "Reconstruction from anisotropic random measurements", "authors": ["Rudelson", "Mark", "Zhou", "Shuheng"], "venue": "IEEE Transactions on Information Theory, 59(6):3434\u20133447,", "year": 2013}, {"title": "Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with SubExponential Designs", "authors": ["Sivakumar", "Vidyashankar", "Banerjee", "Arindam", "Ravikumar", "Pradeep"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Regression Shrinkage and Selection via the Lasso", "authors": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society,", "year": 1996}, {"title": "Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory - a Renaissance", "authors": ["Tropp", "Joel A"], "year": 2015}, {"title": "Estimation in High Dimensions: A geometric perspective", "authors": ["Vershynin", "Roman"], "venue": "In Sampling Theory, a Renaissance,", "year": 2015}, {"title": "Quantile Regression for Analyzing Heterogeneity in Ultra-high Dimension", "authors": ["Wang", "Lan", "Wu", "Yichao", "Li", "Runze"], "venue": "Journal of the American Statistical Association,", "year": 2012}, {"title": "Variable Selection in Quantile Regression", "authors": ["Y.C. Wu", "Y.F. Liu"], "venue": "Statistica Sinica,", "year": 2009}, {"title": "Composite Quantile Regression and the Oracle Model Selection Theory", "authors": ["H. Zou", "M. Yuan"], "venue": "The Annals of Statistics,", "year": 2008}], "id": "SP:9b8e2fa4e104cbe0c270eb0d01d7d06e2d479493", "authors": [{"name": "Vidyashankar Sivakumar", "affiliations": []}, {"name": "Arindam Banerjee", "affiliations": []}], "abstractText": "Quantile regression aims at modeling the conditional median and quantiles of a response variable given certain predictor variables. In this work we consider the problem of linear quantile regression in high dimensions where the number of predictor variables is much higher than the number of samples available for parameter estimation. We assume the true parameter to have some structure characterized as having a small value according to some atomic norm R(\u00b7) and consider the norm regularized quantile regression estimator. We characterize the sample complexity for consistent recovery and give non-asymptotic bounds on the estimation error. While this problem has been previously considered, our analysis reveals geometric and statistical characteristics of the problem not available in prior literature. We perform experiments on synthetic data which support the theoretical results.", "title": "High-Dimensional Structured Quantile Regression"}