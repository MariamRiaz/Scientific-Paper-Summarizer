0	79	Sentiment classification is an important task of natural language processing (NLP), aiming to classify the sentiment polarity of a given text as positive, negative, or more fine-grained classes.
2	11	Most existing studies set up sentiment classifiers using supervised machine learning approaches, such as support vector machine (SVM) (Pang et al., 2002), convolutional neural network (CNN) (Kim, 2014; Bonggun et al., 2017), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Qian et al., 2017), Tree-LSTM (Tai et al., 2015), and attention-based methods (Zhou et al., 2016; Yang et al., 2016; Lin et al., 2017; Du et al., 2017).
4	24	Sentiment resources including sentiment lexicon, negation words, intensity words play a crucial role in traditional sentiment classification approaches (Maks and Vossen, 2012; Duyu et al., 2014).
5	49	Despite its usefulness, to date, the sentiment linguistic knowledge has been underutilized in most recent deep neural network models (e.g., CNNs and LSTMs).
6	20	In this work, we propose a Multi-sentimentresource Enhanced Attention Network (MEAN) for sentence-level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi-path attention mechanism.
15	20	Our proposed MEAN model consists of three key components: coupled word embedding module, multi-sentiment-resource attention module, sentence classifier module.
16	10	In the rest of this section, we will elaborate these three parts in details.
17	10	To exploit the sentiment-related morphological information implied by some prefixes and suffixes of words (such as “Non-”, “In-”, “Im-”), we design a coupled word embedding learned from character-level embedding and word-level embedding.
21	3	For word-level embedding, we use pre-trained word vectors, GloVe (Pennington et al., 2014), to map each word to a lowdimensional vector space.
22	36	Finally, each word is represented as a concatenation of the characterlevel embedding and word-level embedding.
25	9	Each W is normalized to better calculate the following word correlation.
27	20	Concretely, we use the three kinds of sentiment resource words as attention sources to attend to the context words respectively, which is beneficial to capture different sentiment-relevant context words corresponding to different types of sentiment sources.
34	9	Mathematically, the detailed formulation is described as follows.
36	11	After obtaining the correlation matrices, we can compute the sentiment-resource-relevant context word representations Xcs , X c i , X c n by the dot products among the context words and different types of corresponding correlation matrices.
39	30	Formally, given the word embedding Xc, Xs, Xi, Xn, the hidden state matrices Hc, Hs, H i, Hn can be obtained as follows: Hc = GRU(Xc) (8) Hs = GRU(Xs) (9) H i = GRU(Xi) (10) Hn = GRU(Xn) (11) After obtaining the hidden state matrices, the sentiment-word-enhanced sentence representation o1 can be computed as: o1 = t∑ i=1 αih c i , q s = m∑ i=1 hsi/m (12) β([hci ; qs]) = u T s tanh(Ws[h c i ; qs]) (13) αi = exp(β([hci ; qs]))∑t i=1 exp(β([h c i ; qs])) (14) where qs denotes the mean-pooling operation towards Hs, β is the attention function that calculates the importance of the i-th word hci in the context and αi indicates the importance of the ith word in the context, us and Ws are learnable parameters.
40	29	Similarly, with the hidden states H i and Hn for the intensity words and the negation words as attention sources, we can obtain the intensityword-enhanced sentence representation o2 and the negation-word-enhanced sentence representation o3.
41	56	The final comprehensive sentiment-specific sentence representation õ is the composition of the above three sentiment-resource-specific sentence representations o1, o2, o3: õ = [o1, o2, o3] (15)
42	7	After obtaining the final sentence representation õ, we feed it to a softmax layer to predict the sentiment label distribution of a sentence: ŷ = exp(W̃o T õ + b̃o)∑C i=1 exp(W̃o T õ + b̃o) (16) where ŷ is the predicted sentiment distribution of the sentence, C is the number of sentiment labels, W̃o and b̃o are parameters to be learned.
43	3	For model training, our goal is to minimize the cross entropy between the ground truth and predicted results for all sentences.
46	18	Specifically, the final loss function is presented as follows: L(ŷ, y) =− N∑ i=1 C∑ j=1 yji log(ŷ j i ) + λ( ∑ θ∈Θ θ2) (17) + µ||ÕÕT − ψI||2F Õ =[o1; o2; o3] (18) where yji is the target sentiment distribution of the sentence, ŷji is the prediction probabilities, θ denotes each parameter to be regularized, Θ is parameter set, λ is the coefficient for L2 regularization, µ is a hyper-parameter to balance the three terms, ψ is the weight parameter, I denotes the the identity matrix and ||.||F denotes the Frobenius norm of a matrix.
47	27	Here, the first two terms of the loss function are cross-entropy function of the predicted and true distributions and L2 regularization respectively, and the final term is a penalization term to encourage the diversity of sentiment sources.
48	118	Movie Review (MR)2 and Stanford Sentiment Treebank (SST)3 are used to evaluate our model.
51	32	SST consists of 8,545 training samples, 1,101 validation samples, 2210 test samples.
53	22	Sentiment lexicon combines the sentiment words from both (Qian et al., 2017) and (Hu and Liu, 2004), resulting in 10,899 sentiment words in total.
54	74	We collect negation and intensity words manually as the number of these words is limited.
62	19	Self-attention: Lin et al. (2017) proposes a selfattention mechanism to learn structured sentence embedding.
76	20	For example, our model achieves 2.4% improvements over the MR dataset and 0.8% improvements over the SST dataset compared to LR-Bi-LSTM.
77	62	This is because that MEAN designs attention mechanisms to leverage sentiment resources efficiently, which utilizes the interactive information between context words and sentiment resource words.
78	6	In order to analyze the effectiveness of each component of MEAN, we also report the ablation test in terms of discarding character-level embedding (denoted as MEAN w/o CharCNN) and sentiment words/negation words/intensity words (denoted as MEAN w/o sentiment words/negation words/intensity words).
79	39	All the tested factors con- tribute greatly to the improvement of the MEAN.
