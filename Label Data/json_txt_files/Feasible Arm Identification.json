{"sections": [{"heading": "1. Introduction", "text": "Pure exploration multi-armed bandit (MAB) problems provide a framework for determining via a sequential experiment which of a set of distributions meet some criteria. In this setting, there are K distributions \u03bd1, . . . , \u03bdK and the agent sequentially chooses from which distribution to sample an observation. At the end of the sampling stage, the agent outputs the distributions which he believes meet the desired criteria and the performance of the agent is measured based on the quality of this decision. In the MAB literature, distributions are also referred to as arms, and sampling a realization from a distribution \u03bdi is referred to as pulling arm i. The most well-studied of these problems is top-k arm identification. In this problem, the goal is to find the k best arms, that is, k arms with the largest means. This problem and other pure exploration problems have applications in a wide range of areas, including crowdsourcing, A/B testing, and online advertising.\nIn many application domains, the arms and the criteria for\n1Department of Computer Science and Electrical Engineering, University of Michigan. Correspondence to: Julian Katz-Samuels <jkatzsam@umich.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\na good arm are multi-dimensional in nature. For example, in crowdsourcing it is important to distinguish good workers from bad workers. For a multilabel classification task (where examples are associated with multiple labels), a worker can be modeled as a multi-dimensional arm where each dimension corresponds to her accuracy at identifying a particular label, and a natural definition for a \u201cgood worker\u201d is that her accuracy is above some threshold for each label (e.g., 90%). A common approach for finding such workers is to use a collection of examples labeled by domain experts as a set of tests. Since workers are paid for each example that they label, often an organization is only willing to spend a limited number of queries to find good workers and an effective method under this budget constraint is needed.\nAs another example, consider A/B testing for designing products such as websites, ads, and video games. In this setting, there are several options for a product and a company diverts traffic to each of the options to determine which one to choose. Multi-dimensional criteria arise naturally in this domain, as well. For example, a company that wants to grow its user base for its website might desire the rate of new subscriptions to be above some level, while still maintaining a certain level of user retention among its current users. If the product is a video game, the company might also be interested in maintaining some metric of user engagement above a certain threshold.\nThe pure exploration MAB literature lacks (i) a simple framework for describing problems where the arms and criteria are multi-dimensional and (ii) practical algorithms for addressing these problems. In this paper, we aim to address this gap. We introduce the feasible arm identification problem in which arms are associated with multi-dimensional distributions and the goal is to find arms whose means belong to a given polyhedron1 P \u201c tx : Ax \u010f bu. Polyhedra encompass a large class of regions that can model common user-defined constraints, including thresholds or ranges on individual dimensions and linear constraints involving multiple dimensions. We propose several algorithms for the fixed budget setting and provide upper and lower bounds. Finally, we demonstrate through experiments on synthetic and real-world datasets that by leveraging the geometry of the\n1There are several conflicting definitions of polyhedra. We define a polyhedron as the intersection of a finite number of closed halfspaces (Boyd and Vandenberghe, 2004).\nproblem, our methods significantly outperform a uniform allocation strategy. Indeed, in several of our experiments, our methods find the feasible arms with a probability that is a factor of 10 better than that of a uniform allocation strategy. All proofs are contained in the supplementary material."}, {"heading": "2. Related Work", "text": "MABs have received a significant amount of attention. Most work considers minimizing the cumulative regret instead of a pure exploration objective. There have been relatively few works on multi-dimensional arms and criteria in this regime (Drugan and Now, 2013; Busa-Fekete et al., 2017; Tekin and Turgay, 2017). Drugan and Now (2013) modify a UCB algorithm to find all arms on the Pareto front. Busa-Fekete et al. (2017) use the Generalized Gini Index to optimize all objectives in a fair way. Tekin and Turgay (2017) consider a contextual MAB setting where the goal is to maximize the total reward in a non-dominant objective, subject to the constraint that the total reward in a dominant objective is maximized. These works differ from our work in that (i) they consider the cumulative regret setting, which is fundamentally different from the pure exploration setting (Bubeck et al., 2009), and (ii) they aim to either balance multiple objective functions or find arms on the Pareto front, whereas we aim to find feasible arms, where feasibility is defined by membership in a given polyhedron.\nIn recent years, there have been many advances in pure exploration MABs in the fixed confidence and fixed budget settings (Mannor and Tistisklis, 2004; Gabillon et al., 2012; Bubeck et al., 2013; Chen et al., 2014; Jamieson et al., 2014). A limited number of works have considered multi-dimensional feedback. Auer et al. (2016) considered a variant of the top arm identification problem where arms are multi-dimensional with each dimension corresponding to a distinct objective that an agent wishes to optimize, and the goal is to identify the Pareto front of the arms. In contrast to our work, they consider the fixed confidence setting. More importantly, Pareto front identification and feasible arm identification are mathematically very different problems and apply to distinct situations. Whereas Pareto front identification is relevant to multi-objective optimization, the feasible arm identification problem is useful for situations where there are user-defined criteria for what qualifies as a good arm.\nChen et al. (2017) recently proposed the general sampling problem, which can model a setting where arms are multidimensional and the goal is to find arms with means belonging to a given polyhedron. There are several major differences with our work. First, Chen et al. (2017) do not consider multi-dimensional feedback as the agent can sample from one dimension of one arm at a time. Second, whereas they study the fixed confidence setting, we\nstudy the fixed budget setting. Third, they assume isotropic Gaussian arms, whereas we assume each arm is associated with a multi-dimensional sub-Gaussian distribution. Finally, their proposed algorithm (see their Algorithm 7) is sampleinefficient and impractical since in its first stage, it employs a uniform allocation strategy until the confidence bounds (defined with \u03b4 \u201c 0.01) of all of the means either intersect with the given polyhedron or do not intersect with the given polyhedron.\nLocatelli et al. (2016) introduced the thresholding bandit problem (TBP), which is essentially the scalar version of the feasible arm identification problem, and the algorithm APT. In TBP, there are K scalar-valued distributions, a threshold \u03c4 , and a budget T . The goal is to identify all of the distributions with means above \u03c4 . Our work significantly generalizes TBP by considering multi-dimensional arms and the problem of identifying those arms with means belonging to a given polyhedron. Unlike Locatelli et al. (2016) who only analyze APT, we provide an unified analysis of three algorithms for the feasible arm identification problem. One of our algorithm, MD-APT, reduces to APT in the onedimensional thresholding case and our upper bound also reduces to the upper bound of APT (up to constant factors). To deal with this general setting, we introduce a novel complexity measure that characterizes the hardness of determining whether an arm is in P . This measure is essentially the distance of the mean of an arm to the boundary of the polyhedron. In addition, our general setting introduces technical challenges for establishing upper and lower bounds. We overcome these by using tools from convex analysis, properties of multi-dimensional sub-Gaussian distributions, and change-of-measure arguments involving multi-dimensional distributions.\nRecently, Zheng et al. (2017) considered a problem with a polyhedral constraint, but their setup is very different from our own. In their setting, the goal is to solve a linear program where either the constraints are not fully known or the cost function is not fully known but can be estimated by adaptive sampling. In our work, the constraints are known and we wish to learn which out of a collection of distributions have feasible means."}, {"heading": "3. Setup", "text": "In this section, we formalize the feasible arm identification problem. To begin, we define some notation. For all n P N, let rns \u201c t1, . . . , nu. For any x P RD and A \u0102 RD, let distpx, Aq \u201c infyPA }x\u00b4 y}2. Let 1 \u201c p1, . . . , 1qt P RD and 1t\u00a8u denote the indicator function. Define SD\u00b41 \u201c tx P RD : }x}2 \u201c 1u.\nSuppose we are given K stochastic arms. When the ith arm is pulled, a reward is drawn i.i.d. from a D-dimensional\ndistribution \u03bdi. Denote \u00b5i \u201c EX\u201e\u03bdiX . We assume that the agent is given a polyhedron P \u201c tx : Ax \u010f bu where A P RM\u02c6D and b P RM . Let atj denote the jth row of A. By dividing each constraint by }aj}2, we can assume without loss of generality that }aj}2 \u201c 1 for all j P rM s. Let BP denote the boundary of P , i.e., BP \u201c sP zP \u02dd where sP denotes the closure of P and P \u02dd denotes the interior of P . For simplicity, we assume that P has positive volume.\nWe consider the fixed budget setting. The game is as follows: there are T rounds and at each round t, the agent chooses an arm It P rKs and observes a realization Xt \u201e \u03bdIt . The goal is to identify all of the arms whose means belong to the polyhedron. To define a performance measure, let \u0105 0 denote the tolerance, and define SintP, \u2013 ti P rKs : \u00b5i P P and distp\u00b5i, BP q \u011b u and SoutP, \u2013 ti P rKs : distp\u00b5i, P q \u0105 u. SintP, is the set of arms that lie in the interior of P by at least and SoutP, is the set of arms that lie outside of P by at least . Let pS \u0102 rKs denote the set of arms outputted by an algorithm. We define the following error measure:\nLT,P, ppSq \u2013 1tpS X SoutP, \u2030 H_ pSc X SintP, \u2030 Hu\nIn words, the goal is to identify all of the arms with means belonging to the polyhedron up to tolerance in the sense that an algorithm is successful if its output includes every arm i such that \u00b5i P P and distp\u00b5i, BP q \u011b and excludes every arm l such that distp\u00b5l, P q \u0105 .\nWe define the margin of arm i as\n\u2206 pP, q i \u2013 distp\u00b5i, BP q `\n\u201c \" minjPrMs distp\u00b5i, tx : atjx \u201c bjuq ` : \u00b5i P P distp\u00b5i, P q ` : \u00b5i R P\n(1)\n\u201c \" minjPrMs bj \u00b4 atj\u00b5i ` : \u00b5i P P distp\u00b5i, P q ` : \u00b5i R P\n(2)\nwhere line (1) follows by Lemma H.1 and line (2) follows by the closed form solution of the distance from a point to a hyperplane and }aj}2 \u201c 1 (Boyd and Vandenberghe, 2004).\nThe complexity of an instance of the feasible arm identification problem is defined to be:\nHP, \u2013 \u00ff iPrKs r\u2206pP, qi s \u00b42.\nIn words, an instance has low complexity if all of the arms are far from the boundary of the polyhedron and high complexity if some of the arms are very close to the boundary. The intuition behind this complexity measure is that for an algorithm to output the correct answer about arm i, it is sufficient to guarantee that an estimate p\u00b5i is within a ball centered at \u00b5i with radius \u2206 pP, q i\n2 (see Lemma 3). For\nthe sake of brevity, we usually write LT, ppSq, \u2206p qi , and H instead of LT,P, ppSq, \u2206pP, qi , and HP, , respectively.\nOur analysis assumes that each \u03bdi is a multi-dimensional sub-Gaussian distribution, which we now define (see Vershynin et al. (2017) for more details). Let X be a scalar random variable. We say that X is R-sub-Gaussian if E exppX 2\nR2 q \u010f 2. We define the sub-Gaussian norm of X as the smallest R that satisfies the above requirement:\n}X}\u03c82 \u201c inftR \u0105 0 : E expp X2\nR2 q \u010f 2u.\nA random vector X P RD is sub-Gaussian if Xta is subGaussian for all a P RD. The sub-Gaussian norm ofX is defined as\n}X}\u03c82 \u201c sup aPSD\u00b41\n\u203a \u203aXta \u203a \u203a\n\u03c82 .\nWe say that a random vector X is R-sub-Gaussian if }X}\u03c82 \u010f R. Henceforth, we assume that \u03bd1, . . . , \u03bdK are R-sub-Gaussian. See Vershynin (2012) for a discussion of sub-Gaussian distributions."}, {"heading": "4. Lower Bound", "text": "In this section, we establish a lower bound for the feasible arm identification problem. Our construction takes any polyhedron P and means \u00b51, . . . ,\u00b5K P P \u02dd and produces a collection of problems such that any algorithm makes a mistake on one of the problems with probability at least on the order of expp\u00b4c TH q (where c is a constant). In fact, this lower bound holds even when the algorithm is given the distance of each arm to the boundary of the polyhedron. If A \u0102 RD is closed and x P RD, let ProjApxq denote the projection of x onto A.\nTheorem 1. Let P \u201c tx P RD : Ax \u010f bu have positive volume and \u011b 0 such that P \u02dd \u2013 tx P P : distpx, BP q \u0105 qu is nonempty. Let\u00b51, . . . ,\u00b5K P P \u02dd , \u03c4i P ProjBP p\u00b5iq for all i P rKs, and \u00b51i \u201c \u00b5i ` 2p\u03c4i \u00b4 \u00b5iq for all i P rKs. Let \u03bdi denote the distribution Np\u00b5i, Iq and \u03bd1i the distribution Np\u00b51i, Iq. Let B0 denote the product distribution \u03bd1b . . .b \u03bdK and Bi denote the product distribution\n\u03bd1 b . . .b \u03bdi\u00b41 b \u03bd1i b \u03bdi`1 b . . .b \u03bdK .\nThen, B0, . . . ,BK have the same problem complexity\nH \u201c K \u00ff\ni\u201c1 r distp\u00b5i, BP q ` s\u00b42\nand for any algorithm,\nmax iPt0,...,Ku\nEBipLT, ppSqq\n\u011b expp\u00b413 T H \u00b4 25D logp48plogpT q ` 1qKDqqq.\nThis lower bound is equal to the lower bound of Locatelli et al. (2016) (see their Theorem 1) up to the factor of D and constants. Since D logpplogpT q ` 1qKDqq grows very slowly as a function of T in comparison with TH , the dependence on D is quite mild. We also note that the lower bound does not depend on the number of constraints M in the polyhedron P , which suggests that the number of constraints of P does not directly affect the statistical difficulty of the feasible arm identification problem. Since polyhedra approximate convex sets arbitrarily well, the independence of our lower bound from M enables us to derive a nearly identical lower bound for the setting where P is convex (see the supplementary material for details).\nThe proof of Theorem 1 is based on a novel lower bound construction with multidimensional distributions for MABs. Often, lower bounds in the bandit literature modify scalar distributions and the main idea is to perturb the mean of a scalar distribution by making it either larger or smaller. In the feasible arm identification problem, picking a direction to perturb the mean of a distribution is not so simple. Indeed, the direction depends on the polyhedron since for some polyhedra, changing the first coordinate does not produce points lying outside of the polyhedron. In our construction, we interchange a distribution \u03bdi with mean \u00b5i P P \u02dd with a distribution \u03bd1i with mean \u00b5 1 i that is shifted away from \u00b5i in the direction of its projection onto the boundary of P .\nTheorem 1 also implies the following non-asymptotic minimax bound.\nCorollary 1. Let P \u201c tx P RD : Ax \u010f bu have positive volume, \u011b 0 such that P \u02dd is nonempty, and R \u0105 0. Let H\u0303 \u0105 0 such that there exists \u00b51, . . . ,\u00b5K P P \u02dd with\nH\u0303 \u201c K \u00ff\ni\u201c1 r distp\u00b5i, BP q ` s\u00b42.\nLet BP, ,H\u0303,R denote the set of feasible arm identification problems on polyhedron P , with tolerance , and with K arms such that the distributions are R-sub-Gaussian and the problem complexity is less than H\u0303 . Then, T \u011b 25H\u0303R2D logp48plogpT q ` 1qKDqq implies that, for any algorithm,\nsup BPBP, ,H\u0303,R\nEBpLT, ppSqq \u011b expp\u00b414 T\nH\u0303R2 q.\nIn words, this result says essentially that for any polyhedron P and tolerance \u011b 0, the induced class of feasible arm identification problems with P and has a minimax lower bound on the order of expp\u00b4c THR2 q where c is a constant. Henceforth, we say that an algorithm is nearly optimal if for large enough T its expected loss decays as Opexpp\u00b4c THR2 qq where c is a constant."}, {"heading": "5. Algorithms", "text": "In this section, we extend three algorithms to the feasible arm identification problem, namely, an upper confidence bound based algorithm (UCBE) (Audibert and Bubeck, 2010), a successive accepts and rejects algorithm (SAR) (Bubeck et al., 2013; Chen et al., 2014), and the Anytime Parameter-free Thresholding algorithm (APT) (Locatelli et al., 2016). The main novelty of our approach is that our algorithms estimate the distance of the mean of each arm to the boundary of the polyhedron to decide which arm to pull. To begin, we introduce some notation. Let It denote the index of the arm chosen at time t. Let Xi,j,t denote the tth realization of the jth coordinate of \u03bdi, Tiptq \u201c \u0159t\u00b41 s\u201c1 1tIs \u201c iu denote the number of pulls of arm i at round t, and p\u00b5i,t denote the estimate of \u00b5i after t samples, i.e., p\u00b5i,t \u201c pp\u00b5i,1,t, . . . , p\u00b5i,D,tqt where p\u00b5i,j,t \u201c 1t \u0159t s\u201c1Xi,j,s.\nThe key quantity in each of these algorithms is the following empirical estimator of the margin of each arm:\np\u2206 p q i,t \u201c\n\"\nminjPrMs bj \u00b4 atj p\u00b5i,t ` : p\u00b5i,t P P distpp\u00b5i,t, P q ` : p\u00b5i,t R P\nGiven p\u00b5i,t, distpp\u00b5i,t, P q can be computed by solving a quadratic program and, thus, the interior point method can compute p\u2206p qi,t in runtime polynomial in M and D. Each of our algorithms updates one p\u2206p qi,t in each round, thus solving at most T quadratic programs. Therefore, each algorithm can be implemented efficiently.\nAlgorithm 1 MD-UCBE: Multi-dimensional Upper Confidence Bound Exploration algorithm\n1: Input: K arms, polyhedron P , tolerance , budget T , hyperparameter a 2: for t \u201c 1, . . . , T do 3: if t \u010f K then 4: SampleXt \u201e \u03bdt. 5: else 6: Choose It \u201c arg mini p\u2206 p q i,Tiptq\u00b4 b a Tiptq and sampleXt \u201e \u03bdIt . 7: end if 8: end for 9: Return: pS \u201c ti P rKs : p\u00b5i,TipT`1q P P u\nNext, we describe each of the algorithms and our results. Each algorithm outputs pS \u201c ti P rKs : p\u00b5i,TipT`1q P P u. The algorithms differ in how they decide which arm to pull. MD-UCBE (Algorithm 1) is a modification of the algorithm UCBE from Audibert and Bubeck (2010). At each time step t, it pulls an arm i that minimizes p\u2206p qi,Tiptq\u00b4 b a Tiptq breaking ties arbitrarily where a is a hyperparameter. Theorem 2 gives an upper bound on its expected loss.\nTheorem 2. Let K \u011b 0, T \u011b K and \u011b 0. Suppose 0 \u010f a \u010f 2536 T\u00b4K H . Then, the expected loss of MD-UCBE satisfies:\nErLT, ppSqs \u010f 2plogpT q ` 1qK5D expp\u00b4 a\n1600R2 q.\nParalleling our upper bounds for MD-SAR and MD-APT, this result says that the degree of difficulty of a problem for MD-UCBE depends on H , i.e., the distance of the arms to the boundary of the polyhedron and the tolerance parameter . Theorem 2 suggests setting a \u201c 2536 T\u00b4K H , in which case MD-UCBE is nearly optimal. One important shortcoming of this algorithm is that H is not known in practice, so it is unclear how to set the hyperparameter a. Indeed, in our experiments, we show that the performance of MD-UCBE is highly sensitive to the selection of a.\nAlgorithm 2 MD-SAR: Multi-dimensional Successive Accepts and Rejects algorithm\n1: Input: K arms, polyhedron P , tolerance , budget T 2: \u010elogpxq \u201c 12` \u0159x i\u201c2 1 i , n0 \u201c 0, nk \u201c Q T\u00b4K \u011alogpKqpK`1\u00b4kq U\npk \u0105 1q 3: Q \u201c rKs 4: for k \u201c 1, . . . ,K \u00b4 1 do 5: Query nk \u00b4 nk\u00b41 samples from all arms i P Q 6: Q\u00d0\u00dd Qz arg maxiPQ p\u2206 p q i,nk 7: end for 8: Return: pS \u201c ti P rKs : p\u00b5i,TipT`1q P P u\nMD-SAR (Algorithm 2) extends the SAR algorithm from Bubeck et al. (2013). It divides the budget T into K \u00b4 1 rounds. In each round, it samples all of the arms belonging to Q \u0102 rKs the same number of times. At the end of each round, it removes from Q an arm i that maximizes p\u2206 p q i,Tiptq. Intuitively, MD-SAR stops sampling from an arm i for which there is the least amount of uncertainty about whether \u00b5i P P . Theorem 3 provides an upper bound on the expected loss of MD-SAR. It depends on a different complexity term that is nevertheless related to H . Let piq denote the index of the arm with the ith smallest margin so that \u2206p qp1q \u010f \u2206 p q p2q \u010f . . . \u010f \u2206 p q pKq and define the complexity parameter\nH2 \u201c max iPrKs\nir\u2206p qpiqs \u00b42.\nThe analysis of Audibert and Bubeck (2010) of the analogous quantities immediately implies that H2 \u010f H \u010f logp2KqH2.\nTheorem 3. Let K \u011b 0, T \u011b K and \u011b 0. Then, the\nexpected loss of MD-SAR satisfies: ErLT, ppSqs \u010f\n2plogpT q ` 1qK5D expp\u00b4 T \u00b4K 1296 logp2KqH2 1 R2 q\n`4K35D expp\u00b4 T \u00b4K 512R2H2 q.\nSimilar to previous results on SAR-type algorithms in the fixed budget setting (Audibert and Bubeck, 2010; Chen et al., 2014), our upper bound on MD-SAR is loose by a factor of logpKq in the exponential. While the guarantee is not tight, it has the significant practical advantage over MD-UCBE that it does not involve a difficult-to-tune hyperparameter. On the other hand, MD-SAR has the limitation that it needs to know T in advance.\nAlgorithm 3 MD-APT: Multi-dimensional Anytime Parameter-Free Thresholding algorithm\n1: Input: K arms, polyhedron P , tolerance , budget T 2: for t \u201c 1, . . . , T do 3: if t \u010f K then 4: SampleXt \u201e \u03bdt. 5: else 6: Choose It \u201c arg mini p\u2206 p q i,Tiptq a\nTiptq and sampleXt \u201e \u03bdIt .\n7: end if 8: end for 9: Return: pS \u201c ti P rKs : p\u00b5i,Tipt`1q P P u\nMD-APT (Algorithm 3) is a modification of the APT algorithm in Locatelli et al. (2016). After an initialization phase in which it pulls each arm once, at each round t, it pulls an arm i that minimizes p\u2206p qi,Tiptq a\nTiptq. The intuition behind the algorithm is that if the margins \u2206p qi were known in advance, then a nearly optimal strategy would allocate samples to the arms proportionally to the r\u2206p qi s\u00b42s. For simplicity, let \u201c 0; the case \u0105 0 is not as clear since arms whose distance to the boundary is less than do not need to be sampled at all. Proposition 1. Let \u201c 0. A static allocation strategy with a total of T\nr\u2206p qi s2H pulls of the ith arm @i P rKs achieves\nErLT, ppSqs \u010f 2K5D expp\u00b4 1\n8\nT\nHR2 q.\nThus, such a static allocation is nearly optimal. Since the \u2206 p q i s are unknown, MD-APT samples the arms proportionally to the estimates rp\u2206p qi,Tiptqs \u00b42. Theorem 4 gives an upper bound on the expected loss of MD-APT. Theorem 4. Let K \u011b 0, T \u011b 2K, and \u011b 0. Then, the expected loss of MD-APT satisfies:\nErLT, ppSqs \u010f 2plogpT q ` 1qK5D expp\u00b4 T\n1296R2H q.\nThis Theorem implies that MD-APT is nearly optimal. Further, unlike MD-UCBE, it is parameter-free and, unlike MD-SAR, it is an anytime algorithm in the sense that MDAPT does not require knowledge of the budget T . These properties make MD-ADT practical for many applications (Jun and Nowak, 2016).\nWe note that although the runtime of our algorithms depends on M , our upper bounds on their statistical performance are independent of M . We leverage this result and the fact that one can approximate convex sets arbitrarily well with polyhedra to obtain a computationally inefficient algorithm with nearly the same guarantee as Theorem 4 for the setting where P is convex (see the supplementary material for details)."}, {"heading": "6. Analysis", "text": "Our analyses of the three algorithms are unified through a series of lemmas. The first key idea is a sufficient condition for p\u2206p qi,t to concentrate around \u2206 p q i . Lemma 1 shows that concentration of p\u00b5iptq around its mean in the norm sense is sufficient. Lemma 1. Let \u03b3 \u0105 0, i P rKs, and t P rT s. If }p\u00b5i,t \u00b4 \u00b5i}2 \u010f \u03b3, then\n|p\u2206p qi,t \u00b4\u2206 p q i | \u010f 2\u03b3.\nIn the scalar case, concentration of the empirical margin around the true margin often follows by the triangle inequality. In our setting, because of the more complicated relationship between p\u00b5i,t and p\u2206 p q i,t such an argument is not sufficient.\nThe second key idea is that with an appropriately high probability, p\u00b5i,t concentrates around its mean in the norm sense. The main tools are Hoeffding\u2019s maximal inequality (see Lemma H.2) and an -net, which we now define (Vershynin et al., 2017). Definition 1. Let A \u0102 RD and \u0105 0. N \u0102 A is an -net of A if @x P A, there exists y P N such that }x\u00b4 y}2 \u010f . Let N \u0102 A be an -net of A. We say that N is minimal if, for any other -net M of A, it holds that |M| \u011b |N |. Lemma 2. Let N be a minimal 12 -net on S\nD\u00b41. Let \u03c9 \u0105 0. Define the event\n\u039e \u201c t@i,@y P N ,@r P rT s : |ytpp\u00b5i.r \u00b4 \u00b5iq| \u010f c \u03c92\n4r u.\nThen, on \u039e, for all i P rKs and for all r P rT s,\n}p\u00b5i.r \u00b4 \u00b5i}2 \u010f c \u03c92\nr\nand\nPrp\u039eq \u011b 1\u00b4 2plogpT q ` 1qK5D expp\u00b4 \u03c9 2\n16R2 q.\nIn effect, Lemma 1 and Lemma 2 together imply that with high probability, (i) p\u00b5i,t concentrates around \u00b5i in the norm sense and (ii) p\u2206p qi,t concentrates around \u2206 p q i .\nFinally, the third idea is the simple observation that if for all i P rKs, p\u00b5i,t lies in a ball centered at \u00b5i with radius \u2206 p q i\n2 , then an algorithm does not make a mistake. Lemma 3. Fix t P rT s and i P rKs and suppose that }p\u00b5i,t \u00b4 \u00b5i}2 \u0103 1 2\u2206 p q i . Then, A\u00b5i \u010f b \u00b4 1 implies that Ap\u00b5i,t \u0103 b and distp\u00b5i, P q \u011b implies that p\u00b5i,t R P .\nThe analysis of each algorithm then proceeds as follows. First, suppose some appropriately defined variant of the event \u039e in Lemma 2. Second, by Lemmas 1 and 2, (i) p\u00b5i,t concentrates around \u00b5i in the norm sense and (ii) p\u2206 p q i,t concentrates around \u2206p qi . Given these concentration results, it is shown that each algorithm pulls each arm a sufficient number of times so that Lemma 3 can be applied."}, {"heading": "7. Experiments", "text": "In this section, we conduct experiments on synthetic and real-world datasets. In addition to the algorithms MDUCBE, MD-SAR, and MD-APT, we consider a uniform allocation algorithm (UA), which samples the arms in a cyclic fashion. We consider the performance of MD-UCBE under four hyperparameter settings ai \u201c i 2536 T\u00b4K H for i P t.1, 1, 10, 100u. Let MD-UCBE[i] denote MD-UCBE with hyperparameter ai. Note that the larger i is, the more MD-UCBE[i] explores and that our theoretical guarantee in Theorem 2 only covers i \u010f 1. To calculate p\u2206p qi,t , we use the quadratic programming solver in the CVXOPT package for python. We average all experiments over 2000 trials."}, {"heading": "7.1. Synthetic Experiments", "text": "Each experiment has 20 5-dimensional arms and is run for 2000 time steps. We use Gaussian distributions with variance 14 . For experiments 1, 2, and 3 we use a cube P \u201c tx P R5 : 0 \u010f xi \u010f 1u. In experiments 4 and 5, we use more complicated feasibility regions. In the following, we say an arm i is irrelevant if the error measure LT, p\u00a8q does not depend on how i is categorized.\nExperiment 1 (Four Groups with Irrelevant Arms): We set \u201c 0.075 and use \u00b50:1 \u201c p.8qb5, \u00b52:3 \u201c p.9qb5, \u00b54:5 \u201c p1.1qb5, \u00b56:7 \u201c p1.2qb5, \u00b58 \u201c p.975qb5, \u00b59 \u201c p1.025qb5, \u00b510:19 \u201c p.3qb5. Note that this problem has two irrelevant arms, \u00b58 and \u00b59.\nExperiment 2 (Four Groups with no Irrelevant Arms): We set \u201c 0 and use\u00b50:1 \u201c p.8qb5,\u00b52:3 \u201c p.9qb5,\u00b54:5 \u201c p1.1qb5, \u00b56:7 \u201c p1.2qb5, \u00b58 \u201c p.95qb5, \u00b59 \u201c p1.05qb5, \u00b510:19 \u201c p.3qb5. In comparison to experiment 1, we make it slightly easier to determine whether the arms \u00b58 and \u00b59\nbelong to the polyhedron because otherwise the difficulty of the problem prevents any algorithm from achieving substantial progress after 2000 time steps.\nExperiment 3 (Linear Progression with Irrelevant Arms): We set \u201c 0.075 and use \u00b50:3 \u201c p.75qb5 ` p0 : 3q \u02c6 .05, \u00b54 \u201c p.975qb5, \u00b55 \u201c p1.025qb5, \u00b56:9 \u201c p1.25qb5\u00b4p0 : 3q\u02c6 .05, \u00b510:19 \u201c p1.15qb5. Note that this problem has two irrelevant arms, \u00b54 and \u00b55.\nExperiment 4 (Four Groups on the Simplex): For this experiment, we use P \u201c tx P R5 : xi \u011b 0, \u0159\ni xi \u010f 2u. We set \u201c .1. Let c \u201c p.2qb5. We use \u00b50:4 \u201c c, \u00b55:9 \u201c 1.85 \u00a8 c, \u00b510:14 \u201c 2.25 \u00a8 c, and \u00b515:19 \u201c 1.95 \u00a8 c. \u00b50:9 are good arms, \u00b510:14 are bad arms, and \u00b515:19 are irrelevant.\nExperiment 5 (Ordered Polyhedron): For this experiment, we use P \u201c tx P R5 : xi \u010f xi`1@i P r4su and \u201c .1. We use \u00b50:3 \u201c p0, .2, .4, .6, .8qt, \u00b54:7 \u201c p.0, .15, .3, .45, .6qt, \u00b58:11 \u201c p0, .2, .15, .6, .8sqt, \u00b512:15 \u201c p0, .2, .05, .6, .8qt, and \u00b516:19 \u201c p0, .2, .4, .2, 0qt. The arms \u00b58:11 are irrelevant.\nThe performance of MD-UCBE is very sensitive to the selection of its hyperparameter. MD-UCBE[1] and MDUCBE[10] tend to do well, but MD-UCBE[100] explores too much so that it tends to perform only slightly better than UA and MD-UCBE[.1] does not explore enough. Although MD-UCBE[.1] has a theoretical guarantee, the constants are too large so that it never makes progress in solving the problems. MD-APT performs better than MD-SAR in experiments 1, 4, and 5 and worse than MD-SAR in experiments 2 and 3. In experiment 2, MD-APT pulls arm 8, which minimizes \u2206p qi , too frequently. It pulls arm 8 on average 904.8125 times, whereas MD-SAR more evenly spreads out its pulls, pulling arm 8 on average 317.751 times. We observe a similar phenomenon in a variant of experiment 3 where we set \u201c 0 and which we defer to the supplementary material due to lack of space. This suggests that in certain problems MD-APT focuses too much on specific arms with means near the boundary and does not allocate enough samples to other arms. On the other hand, MDSAR utilizes knowledge of the time horizon T to effectively spread out samples. MD-APT\u2019s agnosticism about T may put it a disadvantage in the regime where some of the \u2206p qi are very small and T is small relative to H . As suggested by experiment 1, the parameter can be used to counteract the sensitivity of MD-APT to arms with means near the boundary."}, {"heading": "7.2. Application 1: Dose-Finding", "text": "In clinical trials, an important challenge is determining the appropriate dosage of a drug. The main difficulty is the trade-off that as the dosage increases, the effectiveness of the drug tends to increase, but the likelihood of adverse\neffects also increases. Thus, one must find a dosage that is sufficiently effective, but does not have too many side effects. We assume a situation where the side effects are mild enough not to be a concern for clinical trials, but could nevertheless be unacceptable for a final commercial product.\nWe investigate this problem by considering the data in Genovese et al. (2013) (see ARCR20 in week 16 in Table 2 and Table 3). In this study, the authors examine the drug secukinumab for treating rheumatoid arthritis. They consider four dosage levels (25mg, 75mg, 150mg, 300mg) and a placebo. We design a simulation based on their data where each arm corresponds to a drug and has two attributes, the likelihood of being effective and the likelihood of causing an adverse effect. Let \u00b5i,1 denote the probability of being effective and \u00b5i,2 the probability of causing an adverse effect. Then, dosage levels 25mg, 75mg, 150mg, and 300mg have means \u00b51 \u201c p.34, .519qt,\u00b52 \u201c p.469, .612qt,\u00b53 \u201c p.465, .465qt,\u00b54 \u201c p.537, .61qt, respectively, and the placebo has mean \u00b55 \u201c p.36, .58qt. We suppose that a drug is considered good if the probability of success is above .4 and the probability of adverse effects is below .5 and we set \u201c 0. Thus, only arm 3 is good and all other arms are bad. We chose these thresholds so that one drug is good; we did not try other threshold settings. We run the experiment for 1000 time steps.\nFigure 6 gives the results of the experiment. MD-APT and MD-UCBE[10] perform better than the rest of the algorithms. MD-UCBE[1] performs slightly worse than UA, which may be because there are only 5 arms so that UA is not that bad of a strategy and MD-UCBE[1] does not explore sufficiently. MD-SAR only performs slightly better than UA. This may be because the time horizon is only 1000 time steps and there are only 5 arms."}, {"heading": "7.3. Application 2: Crowdsourcing", "text": "We use a real-world dataset for the natural language processing task of affective text analysis (Snow et al., 2008). In this task, workers are asked to rate a short headline on valence and six emotions: disgust, fear, joy, anger, sadness and surprise. A group of experts also provide such ratings for the headlines.\nWe consider the problem of finding workers that tend to agree with the expert views on each of the tasks. We examine the deviation of a worker\u2019s ratings with the experts ratings. We normalize this deviation onto a scale of r0, 1s. Let \u00b5i,j denote the mean of worker i on task j and let \u00b5\u0304j denote the mean of all of the workers on task j. We deem a worker i good if \u00b5i,j \u010f \u00b5\u0304j for all j P r7s. In words, a worker is good if for every task, he performs better than the average worker. To make this realistic, we assume that we are in a setting where the average worker performance on each task is known based on another pool of workers. We\n0 250 500 750 1000 1250 1500 1750 2000 horizon\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\nlo g(\nes t.\nfa ilu\nre p\nro ba\nbi lit\ny)\nFigure 1. Four Groups on Cube with Irrelevant Arms 0 250 500 750 1000 1250 1500 1750 2000 horizon\n\u22121.50\n\u22121.25\n\u22121.00\n\u22120.75\n\u22120.50\n\u22120.25\n0.00\nlo g(\nes t.\nfa ilu\nre p\nro ba\nbi lit\ny)\nFigure 2. Four Groups on Cube, no Irrelevant Arms 0 250 500 750 1000 1250 1500 1750 2000 horizon\n\u22126\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\nlo g(\nes t.\nfa ilu\nre p\nro ba\nbi lit\ny)\nFigure 3. Linear Progression on Cube with Irrelevant Arms 0 250 500 750 1000 1250 1500 1750 2000 horizon\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\nlo g(\nes t.\nfa ilu\nre p\nro ba\nbi lit\ny)\nFigure 5. Ordered polyhedron 0 200 400 600 800 1000 horizon\n\u22122.5\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n\u22123.0\n\u22122.5\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\nlo g(\nes t.\nfa ilu\nre p\nro ba\nbi lit\ny)\nFigure 7. Crowdsourcing Experiment\nuse a tolerance of \u201c 0.02. There is a total of 38 workers, where 30 workers are bad arms, 3 workers are good arms, and 5 workers are irrelevant. Because each worker only provides a small number (at least 20) of ratings, whenever an arm is pulled, the algorithm observes an observation chosen uniformly at random with replacement from the data associated with the arm. We run each algorithm for 4000 time steps and in each trial, we randomly permute the samples of each worker. In the supplementary material, we repeat this experiment, but we simulate each arm as a Gaussian distribution (see Section J); the results are very similar.\nFigure 7 gives the results of the experiment. Until roughly time step 3000, MD-APT and MD-UCBE[10] perform the best. Afterwards, MD-SAR does substantially better than MD-APT and MD-UCBE[10]. MD-UCBE[1] and MDUCBE[100] perform only marginally better than UA."}, {"heading": "7.4. Summary of Results", "text": "The experiments suggest that although MD-UCBE is a competitive algorithm, it is highly sensitive to hyperparameter selection, which limits its applicability in practice. MDSAR and MD-APT tend to perform dramatically better than UA. For example, in the crowdsourcing experiment, UA has a final error rate of roughly 52%, whereas MD-SAR has a final error rate of roughly 5%. Further, our algorithms can handle complicated polyhedra such as the polyhdron that requires that coordinates are sorted in ascending order (see experiment 5). These results suggest that MD-APT tends to perform better than MD-SAR, but in some settings\n(e.g., some arms with small \u2206p qi and H large relative to T ) MD-APT focuses too much on some of the arms with means near the boundary. Because MD-SAR more evenly spreads out its pulls among the arms, it performs better in this regime."}, {"heading": "8. Conclusion", "text": "In this paper, we introduced the feasible arm identification problem. This problem provides a flexible framework for settings where arms are multi-dimensional and it is of interest to determine whether each arm satisfies user-defined multi-dimensional criteria. We provided a characterization of the difficulty of these problems that yielded a lower bound and we provided a unified analysis of three algorithms MDUCBE, MD-SAR, and MD-APT. Our experiments suggest that by leveraging the geometry of the feasible arm identification problem, MD-SAR and MD-APT are able to dramatically outperform a uniform allocation approach.\nOur work also suggests several open directions for future research. For example, in many crowdsourcing problems, one does not ask workers to perform all tasks at once, but rather one task at a time and, yet, it may be of interest to find workers who excel at a collection of tasks. This suggests a variant of the feasible arm identification problem where the agent chooses one coordinate of one arm and observes a realization of the corresponding random variable in each round."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grant 1422157. We thank the anonymous reviewers for their very helpful comments and Aditya Modi for his useful feedback."}], "year": 2018, "references": [{"title": "Best arm identification in multi-armed bandits", "authors": ["J.-Y. Audibert", "S. Bubeck"], "venue": "Conference on Learning Theory,", "year": 2010}, {"title": "Pareto front identification from stochastic bandit feedback", "authors": ["P. Auer", "C.-K. Chiang", "R. Ortner", "M. Drugan"], "venue": "Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Pure exploration in multi-armed bandits problems", "authors": ["S. Bubeck", "R. Munos", "G. Stolz"], "venue": "Algorithmic Learning Theory, pages", "year": 2009}, {"title": "Multiple identifications in multi-armed bandits", "authors": ["S. Bubeck", "T. Wang", "N. Viswanathan"], "venue": "International Conference on Machine Learning,", "year": 2013}, {"title": "Multiobjective bandits: Optimizing the generalized gini index", "authors": ["R. Busa-Fekete", "B. Szorenyi", "P. Weng", "S. Mannor"], "year": 2017}, {"title": "Nearly optimal sampling algorithms for combinatorial pure exploration", "authors": ["L. Chen", "A. Gupta", "J. Li", "M. Qiao", "R. Wang"], "venue": "Proceedings of Machine Learning Research,", "year": 2017}, {"title": "Combinatorial pure exploration of multi-armed bandits", "authors": ["S. Chen", "T. Lin", "I. King", "M. Lyu", "W. Chen"], "venue": "Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Designing multi-objective multiarmed bandits algorithms: A study", "authors": ["M. Drugan", "A. Now"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on,", "year": 2013}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "authors": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits", "authors": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": "Conference on Learning Theory,", "year": 2014}, {"title": "Anytime exploration for multiarmed bandits using confidence information", "authors": ["K.-S. Jun", "R. Nowak"], "year": 2016}, {"title": "An optimal algorithm for the thresholding bandit problem", "authors": ["A. Locatelli", "M. Gutzeit", "A. Carpentier"], "venue": "Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "authors": ["S. Mannor", "J. Tistisklis"], "venue": "Journal of Machine Learning Research,", "year": 2004}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "authors": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A. Ng"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "year": 2008}, {"title": "Multi-objective contextual multiarmed bandit problem with a dominant objective", "authors": ["C. Tekin", "E. Turgay"], "venue": "arXiv preprint arXiv:1708.05655,", "year": 2017}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["R. Vershynin"], "venue": "Compressed Sensing: Theory and Applications,", "year": 2012}, {"title": "High-dimensional probability: An introduction with applications in data science", "authors": ["R. Vershynin", "P. Hsu", "C. Ma", "J. Nelson", "E. Schnoor", "D. Stoger", "T. Sullivan", "T. Tao"], "year": 2017}, {"title": "Active information acquisition for linear optimization", "authors": ["S. Zheng", "B. Waggoner", "Y. Liu", "Y. Chen"], "venue": "arXiv preprint arXiv:1709.10061,", "year": 2017}], "id": "SP:b2f9086e841a2151513f6647da11ca5665a9c63e", "authors": [{"name": "Julian Katz-Samuels", "affiliations": []}, {"name": "Clayton Scott", "affiliations": []}], "abstractText": "We introduce the feasible arm identification problem, a pure exploration multi-armed bandit problem where the agent is given a set of Ddimensional arms and a polyhedron P \u201c tx : Ax \u010f bu \u0102 R. Pulling an arm gives a random vector and the goal is to determine, using a fixed budget of T pulls, which of the arms have means belonging to P . We propose three algorithms MD-UCBE, MD-SAR, and MD-APT and provide a unified analysis establishing upper bounds for each of them. We also establish a lower bound that matches up to constants the upper bounds of MDUCBE and MD-APT. Finally, we demonstrate the effectiveness of our algorithms on synthetic and real-world datasets.", "title": "Feasible Arm Identification"}