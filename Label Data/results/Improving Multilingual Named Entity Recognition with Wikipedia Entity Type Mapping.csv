0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1829–1838 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1829",text,[0],[0]
"Networks are ubiquitous, with prominent examples including social networks (e.g., Facebook, Twitter) or citation networks of research papers (e.g., arXiv).",1 Introduction,[0],[0]
"When analyzing data from these real-world networks, traditional methods often represent vertices (nodes) as one-hot representations (containing the connectivity information of each vertex with respect to all other vertices), usually suffering from issues related to the inherent sparsity of large-scale networks.",1 Introduction,[0],[0]
"This results in models that are not able to fully capture the relationships between vertices of the network (Perozzi et al., 2014; Tu et al., 2016).",1 Introduction,[0],[0]
"Alternatively, network embedding (i.e., network representation learning) has been considered, representing each vertex of a network with a low-dimensional vector that preserves information on its similarity rel-
ative to other vertices.",1 Introduction,[0],[0]
"This approach has attracted considerable attention in recent years (Tang and Liu, 2009; Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Wang et al., 2016; Chen et al., 2016; Wang et al., 2017a; Zhang et al., 2018).
",1 Introduction,[0],[0]
"Traditional network embedding approaches focus primarily on learning representations of vertices that preserve local structure, as well as internal structural properties of the network.",1 Introduction,[0],[0]
"For instance, Isomap (Tenenbaum et al., 2000), LINE (Tang et al., 2015), and Grarep (Cao et al., 2015) were proposed to preserve first-, second-, and higher-order proximity between nodes, respectively.",1 Introduction,[0],[0]
"DeepWalk (Perozzi et al., 2014), which learns vertex representations from random-walk sequences, similarly, only takes into account structural information of the network.",1 Introduction,[0],[0]
"However, in realworld networks, vertices usually contain rich textual information (e.g., user profiles in Facebook, paper abstracts in arXiv, user-generated content on Twitter, etc.), which may be leveraged effectively for learning more informative embeddings.
",1 Introduction,[0],[0]
"To address this opportunity, Yang et al. (2015) proposed text-associated DeepWalk, to incorporate textual information into the vectorial representations of vertices (embeddings).",1 Introduction,[0],[0]
"Sun et al. (2016) employed deep recurrent neural networks to integrate the information from vertex-
associated text into network representations.",1 Introduction,[0],[0]
"Further, Tu et al. (2017) proposed to more effectively model the semantic relationships between vertices using a mutual attention mechanism.
",1 Introduction,[0],[0]
"Although these methods have demonstrated performance gains over structure-only network embeddings, the relationship between text sequences for a pair of vertices is accounted for solely by comparing their sentence embeddings.",1 Introduction,[0],[0]
"However, as shown in Figure 1, to assess the similarity between two research papers, a more effective strategy would compare and align (via localweighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (downweighted).",1 Introduction,[0],[0]
"This alignment mechanism is difficult to accomplish in models where text sequences are first embedded into a common space and then compared in pairs (He and Lin, 2016; Parikh et al., 2016; Wang and Jiang, 2017; Wang et al., 2017b; Shen et al., 2018a).
",1 Introduction,[0],[0]
We propose to learn a semantic-aware Network Embedding (NE) that incorporates wordlevel alignment features abstracted from text sequences associated with vertex pairs.,1 Introduction,[0],[0]
"Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors.",1 Introduction,[0],[0]
"These features are then accumulated via a simple but efficient aggregation function, obtaining the final representation for the sentence.",1 Introduction,[0],[0]
"As a result, the word-by-word alignment features (as illustrated in Figure 1) are explicitly and effectively captured by our model.",1 Introduction,[0],[0]
"Further, the learned network embeddings under our framework are adaptive to the specific (local) vertices that are considered, and thus are context-aware and especially suitable for downstream tasks, such as link prediction.",1 Introduction,[0],[0]
"Moreover, since the word-by-word matching procedure introduced here is highly parallelizable and does not require any complex encoding networks, such as Long Short-Term Memory (LSTM) or Convolutional Neural Networks (CNNs), our framework requires significantly less time for training, which is attractive for large-scale network applications.
",1 Introduction,[0],[0]
"We evaluate our approach on three real-world datasets spanning distinct network-embeddingbased applications: link prediction, vertex classi-
fication and visualization.",1 Introduction,[0],[0]
"We show that the proposed word-by-word alignment mechanism efficiently incorporates textual information into the network embedding, and consistently exhibits superior performance relative to several competitive baselines.",1 Introduction,[0],[0]
Analyses considering the extracted word-by-word pairs further validate the effectiveness of the proposed framework.,1 Introduction,[0],[0]
"A network (graph) is defined as G = {V ,E}, where V and E denote the set of N vertices (nodes) and edges, respectively, where elements of E are two-element subsets of V .",2.1 Problem Definition,[0],[0]
"Here we only consider undirected networks, however, our approach (introduced below) can be readily extended to the directed case.",2.1 Problem Definition,[0],[0]
"We also define W , the symmetric RN×N matrix whose elements,wij , denote the weights associated with edges in V , and T , the set of text sequences assigned to each vertex.",2.1 Problem Definition,[0],[0]
"Edges and weights contain the structural information of the network, while the text can be used to characterize the semantic properties of each vertex.",2.1 Problem Definition,[0],[0]
"Given network G, with the network embedding we seek to encode each vertex into a low-dimensional vector h (with dimension much smaller than N ), while preserving structural and semantic features of G.",2.1 Problem Definition,[0],[0]
"To incorporate both structural and semantic information into the network embeddings, we specify two types of (latent) embeddings: (i) hs, the structural embedding; and (ii) ht, the textual embedding.",2.2 Framework Overview,[0],[0]
"Specifically, each vertex in G is encoded into a low-dimensional embedding h =",2.2 Framework Overview,[0],[0]
[hs;ht].,2.2 Framework Overview,[0],[0]
"To learn these embeddings, we specify an objective that leverages the information from both W and T , denoted as
L = ∑ e∈E Lstruct(e) + Ltext(e) + Ljoint(e) , (1)
where Lstruct, Ltext and Ljoint denote structure, text, and joint structure-text training losses, respectively.",2.2 Framework Overview,[0],[0]
"For a vertex pair {vi, vj} weighted by wij , Lstruct(vi, vj) in (1) is defined as (Tang et al., 2015)
Lstruct(vi, vj) = wij log p(his|hjs) , (2)
where p(his|hjs) denotes the conditional probability between structural embeddings for vertices {vi, vj}.",2.2 Framework Overview,[0],[0]
"To leverage the textual information in T , similar text-specific and joint structure-text training objectives are also defined
Ltext(vi, vj) = wijα1 log p(hit|h j t ) , (3)
Ljoint(vi, vj) =",2.2 Framework Overview,[0],[0]
wijα2 log p(hit|hjs) (4) + wijα3 log p(h,2.2 Framework Overview,[0],[0]
"i s|h j t ) , (5)
where p(hit|h j t ) and p(h i t|hjs) (or p(his|h j t ))",2.2 Framework Overview,[0],[0]
"denote the conditional probability for a pair of text embeddings and text embedding given structure embedding (or vice versa), respectively, for vertices {vi, vj}.",2.2 Framework Overview,[0],[0]
"Further, α1, α2 and α3 are hyperparameters that balance the impact of the different training-loss components.",2.2 Framework Overview,[0],[0]
"Note that structural embeddings, hs, are treated directly as parameters, while the text embeddings ht are learned based on the text sequences associated with vertices.
",2.2 Framework Overview,[0],[0]
"For all conditional probability terms, we follow Tang et al. (2015) and consider the second-order proximity between vertex pairs.",2.2 Framework Overview,[0],[0]
"Thus, for vertices {vi, vj}, the probability of generating hi conditioned on hj may be written as
p(hi|hj) = exp
( hj T hi )
∑N k=1 exp ( hj T hk ) .",2.2 Framework Overview,[0],[0]
"(6)
Note that (6) can be applied to both structural and text embeddings in (2) and (3).
",2.2 Framework Overview,[0],[0]
"Inspired by Tu et al. (2017), we further assume that vertices in the network play different roles depending on the vertex with which they interact.",2.2 Framework Overview,[0],[0]
"Thus, for a given vertex, the text embedding, ht, is adaptive (specific) to the vertex it is being conditioned on.",2.2 Framework Overview,[0],[0]
"This type of contextaware textual embedding has demonstrated superior performance relative to context-free embeddings (Tu et al., 2017).",2.2 Framework Overview,[0],[0]
"In the following two sections, we describe our strategy for encoding the text sequence associated with an edge into its adaptive textual embedding, via word-by-context and word-by-word alignments.",2.2 Framework Overview,[0],[0]
"We first introduce our base model, which reweights the importance of individual words within a text sequence in the context of the edge being considered.",2.3 Word-by-Context Alignment,[0],[0]
"Consider text sequences associated with two vertices connected by an edge, de-
noted ta and tb and contained in T .",2.3 Word-by-Context Alignment,[0],[0]
"Text sequences ta and tb are of lengths Ma and Mb, respectively, and are represented by Xa ∈ Rd×Ma",2.3 Word-by-Context Alignment,[0],[0]
"and Xb ∈ Rd×Mb , respectively, where d is the dimension of the word embedding.",2.3 Word-by-Context Alignment,[0],[0]
"Further, x(i)a denotes the embedding of the i-th word in sequence ta.
",2.3 Word-by-Context Alignment,[0],[0]
Our goal is to encode text sequences ta and tb into counterpart-aware vectorial representations ha and hb.,2.3 Word-by-Context Alignment,[0],[0]
"Thus, while inferring the adaptive textual embedding for sentence ta, we propose reweighting the importance of each word in ta to explicitly account for its alignment with sentence tb.",2.3 Word-by-Context Alignment,[0],[0]
"The weight αi, corresponding to the i-th word in ta, is generated as:
αi = exp(tanh(W1cb",2.3 Word-by-Context Alignment,[0],[0]
+W2x (i) a )),2.3 Word-by-Context Alignment,[0],[0]
"∑Ma
j=1 exp(tanh(W1cb",2.3 Word-by-Context Alignment,[0],[0]
+,2.3 Word-by-Context Alignment,[0],[0]
"W2x (j) a ))
, (7)
where W1 and W2 are model parameters and cb = ∑Mb i=1",2.3 Word-by-Context Alignment,[0],[0]
"x b i is the context vector of sequence tb, obtained by simply averaging over all the word embeddings in the sequence, similar to fastText (Joulin et al., 2016).",2.3 Word-by-Context Alignment,[0],[0]
"Further, the word-by-context embedding for sequence ta is obtained by taking the weighted average over all word embeddings
ha = ∑Ma i=1αix (i) a .",2.3 Word-by-Context Alignment,[0],[0]
"(8)
Intuitively, αi may be understood as the relevance score between the ith word in ta and sequence tb.",2.3 Word-by-Context Alignment,[0],[0]
"Specifically, keywords within ta, in the context of tb, should be assigned larger weights, while less important words will be correspondingly downweighted.",2.3 Word-by-Context Alignment,[0],[0]
"Similarly, hb is encoded as a weighted embedding using (7) and (8).",2.3 Word-by-Context Alignment,[0],[0]
"With the alignment in the previous section, wordby-context matching features αi are modeled; however, the word-by-word alignment information (fine-grained), which is key to characterize the relationship between two vertices (as discussed in the above), is not explicitly captured.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"So motivated, we further propose an architecture to explicitly abstract word-by-word alignment information from ta and tb, to learn the relationship between the two vertices.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"This is inspired by the recent success of Relation Networks (RNs) for relational reasoning (Santoro et al., 2017).
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"As illustrated in Figure 2, given two input embedding matrices Xa and Xb, we first compute the affinity matrix A ∈ RMb×Ma , whose elements represent the affinity scores corresponding to all word pairs between sequences ta and tb
A = XTb Xa .",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"(9)
Subsequently, we compute the context-aware matrix for sequence tb as
Ab = softmax(A) , X̃b = XbAb , (10)
where the softmax(·) function is applied columnwise to A, and thus Ab contains the attention weights (importance scores) across sequence tb (columns), which account for each word in sequence ta (rows).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Thus, matrix X̃b ∈ Rd×Ma in (10) constitutes an attention-weighted embedding for Xb.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Specifically, the i-th column of X̃b, denoted as x̃(i)b , can be understood as a weighted average over all the words in tb, where higher attention weights indicate better alignment (match) with the i-th word in ta.
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"To abstract the word-by-word alignments, we compare x(i)a with x̃ (i) b , for i = 1, 2, ...,Ma, to obtain the corresponding matching vector
m(i)a = falign ( x(i)a , x̃ (i) b ) , (11)
where falign(·) represents the alignment function.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Inspired by the observation in Wang and Jiang (2017) that simple comparison/alignment functions based on element-wise operations exhibit excellent performance in matching text sequences, here we use a combination of element-wise subtraction and multiplication as
falign(x (i) a , x̃ (i) a ) =",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"[x (i) a − x̃ (i) a ;x (i) a x̃ (i) a ] ,
where denotes the element-wise Hadamard product, then these two operations are concatenated to produce the matching vector m(i)a .",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Note these operators may be used individually or combined as we will investigate in our experiments.
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Subsequently, matching vectors from (11) are aggregated to produce the final textual embedding hat for sequence ta as
hat = faggregate ( m(1)a ,m (2) a , ...,m (Ma) a ) , (12)
where faggregate denotes the aggregation function, which we specify as the max-pooling pooling operation.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Notably, other commutative operators, such as summation or average pooling, can be otherwise employed.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Although these aggregation functions are simple and invariant to the order of words in input sentences, they have been demonstrated to be highly effective in relational reasoning (Parikh et al., 2016; Santoro et al., 2017).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"To further explore this, in Section 5.3, we conduct an ablation study comparing different choices of alignment and aggregation functions.
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"The representation hb can be obtained in a similar manner through (9), (10), (11) and (12), but replacing (9) with A = XTaXb (its transpose).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Note that this word-by-word alignment is more computationally involved than word-by-context; however, the former has substantially fewer parameters to learn, provided we no longer have to estimate the parameters in (7).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"For large-scale networks, computing and optimizing the conditional probabilities in (1) using (6) is computationally prohibitive, since it requires the summation over all vertices V in G. To address this limitation, we leverage the negative sampling strategy introduced by Mikolov et al. (2013), i.e., we perform computations by sampling a subset of negative edges.",2.5 Training and Inference,[0],[0]
"As a result, the conditional in (6) can be rewritten as:
p(hi|hj) = log σ",2.5 Training and Inference,[0],[0]
"( hj T hi )
+ K∑ i=1",2.5 Training and Inference,[0],[0]
Ehi∼P,2.5 Training and Inference,[0],[0]
"(v) [ log σ ( −hjThi )] ,
where σ(x) = 1/(1 + exp(−x)) is the sigmoid function.",2.5 Training and Inference,[0],[0]
"Following Mikolov et al. (2013), we set the noise distribution P (v) ∝",2.5 Training and Inference,[0],[0]
"d3/4v , where dv is the out-degree of vertex v ∈ V .",2.5 Training and Inference,[0],[0]
The number of negative samples K is treated as a hyperparameter.,2.5 Training and Inference,[0],[0]
"We
use Adam (Kingma and Ba, 2014) to update the model parameters while minimizing the objective in (1).",2.5 Training and Inference,[0],[0]
"Network embedding methods can be divided into two categories: (i) methods that solely rely on the structure, e.g., vertex information; and (ii) methods that leverage both the structure the network and the information associated with its vertices.
",3 Related Work,[0],[0]
"For the first type of models, DeepWalk (Perozzi et al., 2014) has been proposed to learn node representations by generating node contexts via truncated random walks; it is similar to the concept of Skip-Gram (Mikolov et al., 2013), originally introduced for learning word embeddings.",3 Related Work,[0],[0]
"LINE (Tang et al., 2015) proposed a principled objective to explicitly capture first-order and second-order proximity information from the vertices of a network.",3 Related Work,[0],[0]
"Further, Grover and Leskovec (2016) introduced a biased random walk procedure to generate the neighborhood for a vertex, which infers the node representations by maximizing the likelihood of preserving the local context information of vertices.",3 Related Work,[0],[0]
"However, these algorithms generally ignore rich heterogeneous information associated with vertices.",3 Related Work,[0],[0]
"Here, we focus on incorporating textual information into network embeddings.
",3 Related Work,[0],[0]
"To learn semantic-aware network embeddings, Text-Associated DeepWalk (TADW) (Yang et al., 2015) proposed to integrate textual features into network representations with matrix factorization, by leveraging the equivalence between DeepWalk and matrix factorization.",3 Related Work,[0],[0]
"CENE (ContentEnhanced Network Embedding) (Sun et al., 2016) used bidirectional recurrent neural networks to abstract the semantic information associated with vertices, which further demonstrated the advantages of employing textual information.",3 Related Work,[0],[0]
"To capture the interaction between sentences of vertex pairs, Tu et al. (2017) further proposed ContextAware Network Embedding (CANE), that employs a mutual attention mechanism to adaptively account for the textual information from neighboring vertices.",3 Related Work,[0],[0]
"Despite showing improvement over structure-only models, these semantic-aware methods cannot capture word-level alignment information, which is important for inferring the relationship between node pairs, as previously discussed.",3 Related Work,[0],[0]
"In this work, we introduce a WordAlignment-based Network Embedding (WANE)
framework, which aligns and aggregates word-byword matching features in an explicit manner, to obtain more informative network representations.",3 Related Work,[0],[0]
"Datasets We investigate the effectiveness of the proposed WANE model on two standard networkembedding-based tasks, i.e., link prediction and multi-label vertex classification.",4 Experimental Setup,[0],[0]
"The following three real-world datasets are employed for quantitative evaluation: (i) Cora, a standard paper citation network that contains 2,277 machine learning papers (vertices) grouped into 7 categories and connected by 5,214 citations (edges) (ii) HepTh, another citation network of 1,038 papers with abstract information and 1,990 citations; (iii) Zhihu, a network of 10,000 active users from Zhihu, the largest Q&A website in China, where 43,894 vertices and descriptions of the Q&A topics are available.",4 Experimental Setup,[0],[0]
"The average lengths of the text in the three datasets are 90, 54, and 190, respectively.",4 Experimental Setup,[0],[0]
"To make direct comparison with existing work, we employed the same preprocessing procedure1 of Tu et al. (2017).
",4 Experimental Setup,[0],[0]
"Training Details For fair comparison with CANE (Tu et al., 2017), we set the dimension of network embedding for our model to 200.",4 Experimental Setup,[0],[0]
"The number of negative samples K is selected from {1, 3, 5} according to performance on the validation set.",4 Experimental Setup,[0],[0]
"We set the batch size as 128, and the model is trained using Adam (Kingma and Ba, 2014), with a learning rate of 1× 10−3 for all parameters.",4 Experimental Setup,[0],[0]
"Dropout regularization is employed on the word embedding layer, with rate selected from {0.5, 0.7, 0.9}, also on the validation set.",4 Experimental Setup,[0],[0]
"Our code will be released to encourage future research.
",4 Experimental Setup,[0],[0]
"Baselines To evaluate the effectiveness of our framework, we consider several strong baseline methods for comparisons, which can be categorized into two types: (i) models that only exploit structural information: MMB (Airoldi et al., 2008), DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), and node2vec (Grover and Leskovec, 2016).",4 Experimental Setup,[0],[0]
"(ii) Models that take both structural and textual information into account: Naive combination (which simply concatenates the structure-based embedding with CNN-based text embeddings, as explored in (Tu et al., 2017), TADW (Yang et al., 2015), CENE (Sun et al.,
1https://github.com/thunlp/CANE
2016), and CANE (Tu et al., 2017).",4 Experimental Setup,[0],[0]
"It is worth noting that unlike all these baselines, WANE explicitly captures word-by-word interactions between text sequence pairs.
",4 Experimental Setup,[0],[0]
Evaluation Metrics,4 Experimental Setup,[0],[0]
"We employ AUC (Hanley and McNeil, 1982) as the evaluation metric for link prediction, which measures the probability that vertices within an existing edge, randomly sampled from the test set, are more similar than those from a random pair of non-existing vertices, in terms of the inner product between their corresponding embeddings.
",4 Experimental Setup,[0],[0]
"For multi-label vertex classification and to ensure fair comparison, we follow Yang et al. (2015) and employ a linear SVM on top of the learned network representations, and evaluate classification accuracy with different training ratios (varying from 10% to 50%).",4 Experimental Setup,[0],[0]
The experiments for each setting are repeated 10 times and the average test accuracy is reported.,4 Experimental Setup,[0],[0]
"We experiment with three variants for our WANE model: (i) WANE: where the word embeddings of each text sequence are simply average to obtain the sentence representations, similar to (Joulin et al., 2016; Shen et al., 2018c).",5 Experimental Results,[0],[0]
"(ii) WANE-
wc: where the textual embeddings are inferred with word-by-context alignment.",5 Experimental Results,[0],[0]
(iii) WANE-ww: where the word-by-word alignment mechanism is leveraged to capture word-by-word matching features between available sequence pairs.,5 Experimental Results,[0],[0]
"Table 1 presents link prediction results for all models on Cora dataset, where different ratios of edges are used for training.",5.1 Link Prediction,[0],[0]
"It can be observed that when only a small number of edges are available, e.g., 15%, the performances of structure-only methods is much worse than semantic-aware models that have taken textual information into consideration The perfromance gap tends to be smaller when a larger proportion of edges are employed for training.",5.1 Link Prediction,[0],[0]
"This highlights the importance of incorporating associated text sequences into network embeddings, especially in the case of representing a relatively sparse network.",5.1 Link Prediction,[0],[0]
"More importantly, the proposed WANE-ww model consistently outperforms other semantic-aware NE models by a substantial margin, indicating that our model better abstracts word-by-word alignment features from the text sequences available, thus yields more informative network representations.
",5.1 Link Prediction,[0],[0]
"Further, WANE-ww also outperforms WANE or WANE-wc on a wide range of edge training pro-
portions.",5.1 Link Prediction,[0],[0]
This suggests that: (i) adaptively assigning different weights to each word within a text sequence (according to its paired sequence) tends to be a better strategy than treating each word equally (as in WANE).,5.1 Link Prediction,[0],[0]
(ii) Solely considering the context-by-word alignment features (as in WANE-wc) is not as efficient as abstracting word-by-word matching information from text sequences.,5.1 Link Prediction,[0],[0]
"We observe the same trend and the superiority of our WANE-ww models on the other two datasets, HepTh and Zhihu datasets, as shown in Table 2 and 3, respectively.",5.1 Link Prediction,[0],[0]
We further evaluate the effectiveness of proposed framework on vertex classification tasks with the Cora dataset.,5.2 Multi-label Vertex Classification,[0],[0]
"Similar to Tu et al. (2017), we generate the global embedding for each vertex by taking the average over its context-aware embeddings with all other connected vertices.",5.2 Multi-label Vertex Classification,[0],[0]
"As shown in Figure 3(c), semantic-aware NE methods (including naive combination, TADW, CENE, CANE) exhibit higher test accuracies than semantic-agnostic models, demonstrating the advantages of incorporating textual information.",5.2 Multi-label Vertex Classification,[0],[0]
"Moreover, WANEww consistently outperforms other competitive semantic-aware models on a wide range of labeled proportions, suggesting that explicitly capturing word-by-word alignment features is not only use-
ful for vertex-pair-based tasks, such as link prediction, but also results in better global embeddings which are required for vertex classification tasks.",5.2 Multi-label Vertex Classification,[0],[0]
"These observations further demonstrate that WANE-ww is an effective and robust framework to extract informative network representations.
",5.2 Multi-label Vertex Classification,[0],[0]
"Semi-supervised classification We further consider the case where the training ratio is less than 10%, and evaluate the learned network embedding with a semi-supervised classifier.",5.2 Multi-label Vertex Classification,[0],[0]
"Following Yang et al. (2015), we employ a Transductive SVM (TSVM) classifier with a linear kernel (Joachims, 1998) for fairness.",5.2 Multi-label Vertex Classification,[0],[0]
"As illustrated in Table 4, the proposed WANE-ww model exhibits superior performances in most cases.",5.2 Multi-label Vertex Classification,[0],[0]
"This may be due to the fact that WANE-ww extracts information from the vertices and text sequences jointly, thus the obtained vertex embeddings are less noisy and perform more consistently with relatively small training ratios (Yang et al., 2015).",5.2 Multi-label Vertex Classification,[0],[0]
"Motivated by the observation in Wang and Jiang (2017) that the advantages of different functions to match two vectors vary from task to task, we further explore the choice of alignment and aggregation functions in our WANE-ww model.",5.3 Ablation Study,[0],[0]
"To match the word pairs between two sequences, we experimented with three types of operations: sub-
traction, multiplication, and Sub & Multi (the concatenation of both approaches).",5.3 Ablation Study,[0],[0]
"As shown in Figure 3(a) and 3(b), element-wise subtraction tends to be the most effective operation performancewise on both Cora and Zhihu datasets, and performs comparably to Sub & Multi on the HepTh dataset.",5.3 Ablation Study,[0],[0]
"This finding is consistent with the results in Wang and Jiang (2017), where they found that simple comparison functions based on elementwise operations work very well on matching text sequences.
",5.3 Ablation Study,[0],[0]
"In terms of the aggregation functions, we compare (one-layer) CNN, mean-pooling, and maxpooling operations to accumulate the matching vectors.",5.3 Ablation Study,[0],[0]
"As shown in Figure 3(b), max-pooling has the best empirical results on all three datasets.",5.3 Ablation Study,[0],[0]
"This may be attributed to the fact that the maxpooling operation is better at selecting important word-by-word alignment features, among all matching vectors available, to infer the relationship between vertices.",5.3 Ablation Study,[0],[0]
"Embedding visualization To visualize the learned network representations, we further employ t-SNE to map the low-dimensional vectors of the vertices to a 2-D embedding space.",5.4 Qualitative Analysis,[0],[0]
"We use the Cora dataset because there are labels associated with each vertex and WANE-ww to obtain the network embeddings.
",5.4 Qualitative Analysis,[0],[0]
"As shown in Figure 4 where each point indicates one paper (vertex), and the color of each point indicates the category it belongs to, the embeddings of the same label are indeed very close in the 2-D plot, while those with different labels are relatively farther from each other.",5.4 Qualitative Analysis,[0],[0]
"Note that the model is not trained with any label information, indicating that WANE-ww has extracted meaningful patterns from the text and vertex information available.
",5.4 Qualitative Analysis,[0],[0]
Case study The proposed word-by-word alignment mechanism can be used to highlight the most informative words (and the corresponding matching features) wrt the relationship between vertices.,5.4 Qualitative Analysis,[0],[0]
We visualize the norm of matching vector obtained in (11) in Figure 5 for the Cora dataset.,5.4 Qualitative Analysis,[0],[0]
"It can be observed that matched key words, e.g., ‘MCMC’, ‘convergence’, between the text sequences are indeed assigned higher values in the matching vectors.",5.4 Qualitative Analysis,[0],[0]
These words would be selected preferentially by the final max-pooling aggregation operation.,5.4 Qualitative Analysis,[0],[0]
This indicates that WANEww is able to abstract important word-by-word alignment features from paired text sequences.,5.4 Qualitative Analysis,[0],[0]
We have presented a novel framework to incorporate the semantic information from vertexassociated text sequences into network embeddings.,6 Conclusions,[0],[0]
"An align-aggregate framework is introduced, which first aligns a sentence pair by capturing the word-by-word matching features, and then adaptively aggregating these word-level alignment
information with an efficient max-pooling function.",6 Conclusions,[0],[0]
"The semantic features abstracted are further encoded, along with the structural information, into a shared space to obtain the final network embedding.",6 Conclusions,[0],[0]
Compelling experimental results on several tasks demonstrated the advantages of our approach.,6 Conclusions,[0],[0]
"In future work, we aim to leverage abundant unlabeled text data to abstract more informative sentence representations (Dai and Le, 2015; Zhang et al., 2017; Shen et al., 2017; Tang and de Sa, 2018) .",6 Conclusions,[0],[0]
"Another interesting direction is to learn binary and compact network embedding, which could be more efficient in terms of both computation and memory, relative to its continuous counterpart (Shen et al., 2018b).
",6 Conclusions,[0],[0]
"Acknowledgments This research was supported in part by DARPA, DOE, NIH, ONR and NSF.",6 Conclusions,[0],[0]
"Network embeddings, which learn lowdimensional representations for each vertex in a large-scale network, have received considerable attention in recent years.",abstractText,[0],[0]
"For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc.",abstractText,[0],[0]
We propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices.,abstractText,[0],[0]
"We introduce a word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function.",abstractText,[0],[0]
"In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification.",abstractText,[0],[0]
Results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.,abstractText,[0],[0]
Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment,title,[0],[0]
At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse.,1 Introduction,[0],[0]
"Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable.",1 Introduction,[0],[0]
"Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).",1 Introduction,[0],[0]
"Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions.
",1 Introduction,[0],[0]
"The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides strong evidence regarding its grammatical role in morphologically rich languages (Ballesteros, 2013, inter alia), this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled.",1 Introduction,[0],[0]
"In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency.",1 Introduction,[0],[0]
"Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3).
",1 Introduction,[0],[0]
"Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems (§4).",1 Introduction,[0],[0]
"In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916).",1 Introduction,[0],[0]
"Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing.
",1 Introduction,[0],[0]
A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees.,1 Introduction,[0],[0]
"We do this by augmenting its transition operations
1Software for replicating the experiments is available from https://github.com/clab/lstm-parser.
",1 Introduction,[0],[0]
"ar X
iv :1
50 8.
00 65
7v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
1 A
ug 2
01 5
with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages.",1 Introduction,[0],[0]
"We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based.
",2 An LSTM Dependency Parser,[0],[0]
"Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser.",2 An LSTM Dependency Parser,[0],[0]
"In particular, the parser implements the arc-standard parsing algorithm (Nivre, 2004).
",2 An LSTM Dependency Parser,[0],[0]
"At each time step t, a transition action is applied that alters these data structures by pushing or popping words from the stack and the buffer; the operations are listed in Figure 1.
",2 An LSTM Dependency Parser,[0],[0]
"Along with the discrete transitions above, the parser calculates a vector representation of the states of B, S, and A; at time step t these are denoted by bt, st, and at, respectively.",2 An LSTM Dependency Parser,[0],[0]
"The total parser state at t is given by
pt = max {0,W[st;bt;at] + d} (1)
where the matrix W and the vector d are learned parameters.",2 An LSTM Dependency Parser,[0],[0]
"This continuous-state representation pt is used to decide which operation to apply next, updating B, S, and A (Figure 1).
",2 An LSTM Dependency Parser,[0],[0]
"We elaborate on the design of bt, st, and at using RNNs in §2.1, on the representation of partial parses in S in §2.2, and on the parser’s decision mechanism in §2.3.",2 An LSTM Dependency Parser,[0],[0]
We discuss the inclusion of SWAP in §2.4.,2 An LSTM Dependency Parser,[0],[0]
RNNs are functions that read a sequence of vectors incrementally; at time step t the vector xt is read in and the hidden state ht computed using xt and the previous hidden state ht−1.,2.1 Stack LSTMs,[0],[0]
"In principle, this allows retaining information from time steps in the distant past, but the nonlinear “squashing” functions applied in the calcluation of each ht result in a decay of the error signal used in training with backpropagation.",2.1 Stack LSTMs,[0],[0]
"LSTMs are a variant of RNNs designed to cope with this “vanishing gradient” problem using an extra memory “cell” (Hochreiter and Schmidhuber, 1997; Graves, 2013).
",2.1 Stack LSTMs,[0],[0]
Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it) or forget (ft).,2.1 Stack LSTMs,[0],[0]
"We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt, the previous hidden state ht−1, and the memory cell ct−1:
it = σ(Wixxt +Wihht−1 +Wicct−1 + bi)
ft = 1− it ct = ft ct−1+
it tanh(Wcxxt",2.1 Stack LSTMs,[0],[0]
+Wchht−1 + bc) ot = σ(Woxxt,2.1 Stack LSTMs,[0],[0]
"+Wohht−1 +Wocct + bo)
ht = ot tanh(ct),
where σ is the component-wise logistic sigmoid function and is the component-wise (Hadamard) product.",2.1 Stack LSTMs,[0],[0]
Parameters are all represented using W and b.,2.1 Stack LSTMs,[0],[0]
"This formulation differs slightly from the classic LSTM formulation in that it makes use of “peephole connections” (Gers et al., 2002) and defines the forget gate so that it sums with the input gate to 1 (Greff et al., 2015).",2.1 Stack LSTMs,[0],[0]
"To improve the representational capacity of LSTMs (and RNNs generally), they can be stacked in “layers.”",2.1 Stack LSTMs,[0],[0]
"In these architectures, the input LSTM at higher layers at time t is the value of ht computed by the lower layer (and xt is the input at the lowest layer).
",2.1 Stack LSTMs,[0],[0]
The stack LSTM augments the left-to-right sequential model of the conventional LSTM with a stack pointer.,2.1 Stack LSTMs,[0],[0]
"As in the LSTM, new inputs are added in the right-most position, but the stack pointer indicates which LSTM cell provides ct−1 and ht−1 for the computation of the next iterate.",2.1 Stack LSTMs,[0],[0]
"Further, the stack LSTM provides a pop operation that moves the stack pointer to the previous element.",2.1 Stack LSTMs,[0],[0]
"Hence each of the parser data structures (B, S, and A) is implemented with its own stack LSTM, each with its own parameters.",2.1 Stack LSTMs,[0],[0]
"The values of bt, st, and at are the ht vectors from their respective stack LSTMs.",2.1 Stack LSTMs,[0],[0]
"Whenever a REDUCE operation is selected, two tree fragments are popped off of S and combined to form a new tree fragment, which is then popped back onto S (see Figure 1).",2.2 Composition Functions,[0],[0]
"This tree must be embedded as an input vector xt.
",2.2 Composition Functions,[0],[0]
"To do this, Dyer et al. (2015) use a recursive neural network gr (for relation r) that composes
the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr(u,v) or gr(v,u), depending on the direction of attachment.",2.2 Composition Functions,[0],[0]
The resulting vector embeds the tree fragment in the same space as the words and other tree fragments.,2.2 Composition Functions,[0],[0]
"This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015).",2.2 Composition Functions,[0],[0]
"The parser uses a probabilistic model of parser decisions at each time step t. Letting A(S,B) denote the set of allowed transitions given the stack S and buffer S (i.e., those where preconditions are met; see Figure 1), the probability of action z ∈ A(S,B) defined using a log-linear distribution:
p(z | pt) = exp
( g>z pt + qz )∑ z′∈A(S,B) exp",2.3 Predicting Parser Decisions,[0],[0]
"( g>z′pt + qz′
) (2) (where gz and qz are parameters associated with each action type z).
",2.3 Predicting Parser Decisions,[0],[0]
"Parsing proceeds by always choosing the most probable action from A(S,B).",2.3 Predicting Parser Decisions,[0],[0]
"The probabilistic definition allows parameter estimation for all of the parameters (W∗, b∗ in all three stack LSTMs, as well as W, d, g∗, and q∗) by maximizing the conditional likelihood of each correct parser decisions given the state.",2.3 Predicting Parser Decisions,[0],[0]
"Dyer et al. (2015)’s parser implemented the most basic version of the arc-standard algorithm, which is capable of producing only projective parse trees.",2.4 Adding the SWAP Operation,[0],[0]
"In order to deal with nonprojective trees, we also add the SWAP operation which allows nonprojective trees to be produced.
",2.4 Adding the SWAP Operation,[0],[0]
"The SWAP operation, first introduced by Nivre (2009), allows a transition-based parser to produce
nonprojective trees.",2.4 Adding the SWAP Operation,[0],[0]
"Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack.",2.4 Adding the SWAP Operation,[0],[0]
This is easily handled with the stack LSTM.,2.4 Adding the SWAP Operation,[0],[0]
"Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words.",2.4 Adding the SWAP Operation,[0],[0]
"Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments.",2.4 Adding the SWAP Operation,[0],[0]
The main contribution of this paper is to change the word representations.,3 Word Representations,[0],[0]
"In this section, we present the standard word embeddings as in Dyer et al. (2015), and the improvements we made generating word embeddings designed to capture morphology based on orthographic strings.",3 Word Representations,[0],[0]
"Dyer et al.’s parser generates a word representation for each input token by concatenating two vectors: a vector representation for each word type (w) and a representation (t) of the POS tag of the token (if it is used), provided as auxiliary input to the parser.2 A linear map (V) is applied to the resulting vector and passed through a component-wise ReLU:
x = max {0,V[w; t] + b}
For out-of-vocabulary words, the parser uses an “UNK” token that is handled as a separate word during parsing time.",3.1 Baseline: Standard Word Embeddings,[0],[0]
"This mapping can be shown schematically as in Figure 2.
2Dyer et al. (2015), included a third input representation learned from a neural language model (w̃LM).",3.1 Baseline: Standard Word Embeddings,[0],[0]
"We do not include these pretrained representations in our experiments, focusing instead on character-based representations.",3.1 Baseline: Standard Word Embeddings,[0],[0]
"Following Ling et al. (2015), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber, 2005).",3.2 Character-Based Embeddings of Words,[0],[0]
"When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character sequence, which is the h vector of the LSTM; we denote it by → w.",3.2 Character-Based Embeddings of Words,[0],[0]
"The same process is also applied in reverse (albeit with different parameters), computing a similar continuous-space vector embedding starting from the last character and finishing at the first ( ← w); again each character is represented with an LSTM cell.",3.2 Character-Based Embeddings of Words,[0],[0]
"After that, we concatenate these vectors and a (learned) representation of their tag to produce the representation w.",3.2 Character-Based Embeddings of Words,[0],[0]
"As in §3.1, a linear map (V) is applied and passed through a component-wise ReLU.
",3.2 Character-Based Embeddings of Words,[0],[0]
"x = max { 0,V",3.2 Character-Based Embeddings of Words,[0],[0]
[ → w; ← w; t] + b },3.2 Character-Based Embeddings of Words,[0],[0]
"This process is shown schematically in Figure 3.
",3.2 Character-Based Embeddings of Words,[0],[0]
"Note that under this representation, out-ofvocabulary words are treated as bidirectional LSTM encodings and thus they will be “close” to other words that the parser has seen during training, ideally close to their more frequent, syntactically similar morphological relatives.",3.2 Character-Based Embeddings of Words,[0],[0]
"We conjecture that this will give a clear advantage over a single “UNK” token for all the words that the parser does not see during training, as done by Dyer et al. (2015) and other parsers without additional resources.",3.2 Character-Based Embeddings of Words,[0],[0]
In §4 we confirm this hypothesis.,3.2 Character-Based Embeddings of Words,[0],[0]
"We applied our parsing model and several variations of it to several parsing tasks and report re-
sults below.",4 Experiments,[0],[0]
"In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeillé et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (Świdziński and Woliński, 2010) and Swedish (Nivre et al., 2006b).",4.1 Data,[0],[0]
"For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation.
",4.1 Data,[0],[0]
"We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006).",4.1 Data,[0],[0]
"We used gold POS tags, as is common with the CoNLL-X data sets.
",4.1 Data,[0],[0]
"To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English
3The POS tags were calculated with the MarMot tagger (Müller et al., 2013) by the best performing system of the SPMRL Shared Task (Björkelund et al., 2013).",4.1 Data,[0],[0]
Arabic: 97.38.,4.1 Data,[0],[0]
Basque: 97.02.,4.1 Data,[0],[0]
French: 97.61.,4.1 Data,[0],[0]
German: 98.10.,4.1 Data,[0],[0]
Hebrew: 97.09.,4.1 Data,[0],[0]
Hungarian: 98.72.,4.1 Data,[0],[0]
Korean: 94.03.,4.1 Data,[0],[0]
Polish: 98.12.,4.1 Data,[0],[0]
"Swedish: 97.27.
4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set.
setups as Chen and Manning (2014) and Dyer et al. (2015).",4.1 Data,[0],[0]
"For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags.",4.1 Data,[0],[0]
"For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7.",4.1 Data,[0],[0]
"Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols.",4.1 Data,[0],[0]
"In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations:
• Words: words only, as in §3.1 (but without POS tags)
",4.2 Experimental Configurations,[0],[0]
"• Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags)
•",4.2 Experimental Configurations,[0],[0]
"Words + POS: words and POS tags (§3.1)
• Chars + POS: character-based representations of words with bidirectional LSTMs plus POS tags (§3.2)
",4.2 Experimental Configurations,[0],[0]
None of the experimental configurations include pretrained word-embeddings or any additional data resources.,4.2 Experimental Configurations,[0],[0]
"All experiments include the SWAP transition, meaning that nonprojective trees can be produced in any language.
Dimensionality.",4.2 Experimental Configurations,[0],[0]
The full version of our parsing model sets dimensionalities as follows.,4.2 Experimental Configurations,[0],[0]
"LSTM hidden states are of size 100, and we use two layers of LSTMs for each stack.",4.2 Experimental Configurations,[0],[0]
"Embeddings of the parser actions used in the composition functions have 20 dimensions, and the output embedding size is 20 dimensions.",4.2 Experimental Configurations,[0],[0]
"The learned word representations embeddings have 32 dimensions when used, while the character-based representations have 100 dimensions, when used.",4.2 Experimental Configurations,[0],[0]
Part of speech embeddings have 12 dimensions.,4.2 Experimental Configurations,[0],[0]
"These dimensionalities were chosen after running several tests with different values, but a more careful selection of these values would probably further improve results.
",4.2 Experimental Configurations,[0],[0]
"5Training: 001–815, 1001–1136.",4.2 Experimental Configurations,[0],[0]
"Development: 886– 931, 1148–1151.",4.2 Experimental Configurations,[0],[0]
"Test: 816–885, 1137–1147.
",4.2 Experimental Configurations,[0],[0]
6Training: 02–21.,4.2 Experimental Configurations,[0],[0]
Development: 22.,4.2 Experimental Configurations,[0],[0]
Test: 23.,4.2 Experimental Configurations,[0],[0]
"7The POS tags are predicted by using the Stanford Tagger
(Toutanova et al., 2003) with an accuracy of 97.3%.",4.2 Experimental Configurations,[0],[0]
Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation.,4.3 Training Procedure,[0],[0]
"Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set.",4.3 Training Procedure,[0],[0]
No pretraining of any parameters is done.,4.3 Training Procedure,[0],[0]
"Tables 1 and 2 show the results of the parsers for the development sets and the final test sets, respectively.",4.4 Results and Discussion,[0],[0]
"Most notable are improvements for agglutinative languages—Basque, Hungarian, Korean, and Turkish—both when POS tags are included and when they are not.",4.4 Results and Discussion,[0],[0]
"Consistently, across all languages, Chars outperforms Words, suggesting that the character-level LSTMs are learning representations that capture similar information to parts of speech.",4.4 Results and Discussion,[0],[0]
"On average, Chars is on par with Words + POS, and the best average of labeled attachment scores is achieved with Chars + POS.
",4.4 Results and Discussion,[0],[0]
"It is common practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense).",4.4 Results and Discussion,[0],[0]
"Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy.",4.4 Results and Discussion,[0],[0]
"However, the POS tags in treebanks for morphologically rich languages do not seem to be enough.
",4.4 Results and Discussion,[0],[0]
"Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case.",4.4 Results and Discussion,[0],[0]
"Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context.",4.4 Results and Discussion,[0],[0]
"Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context.",4.4 Results and Discussion,[0],[0]
"Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words.
",4.4 Results and Discussion,[0],[0]
"8Tense and number features provide little improvement in a transition-based parser, compared with other features such as case, when the POS tags are included (Ballesteros, 2013).",4.4 Results and Discussion,[0],[0]
Figure 4 visualizes a sample of the characterbased bidirectional LSTMs’s learned representations (Chars).,4.4.1 Learned Word Representations,[0],[0]
"Clear clusters of past tense verbs, gerunds, and other syntactic classes are visible.",4.4.1 Learned Word Representations,[0],[0]
The colors in the figure represent the most common POS tag for each word.,4.4.1 Learned Word Representations,[0],[0]
The character-based representation for words is notably beneficial for out-of-vocabulary (OOV) words.,4.4.2 Out-of-Vocabulary Words,[0],[0]
We tested this specifically by comparing Chars to a model in which all OOVs are replaced by the string “UNK” during parsing.,4.4.2 Out-of-Vocabulary Words,[0],[0]
"This always has a negative effect on LAS (average−4.5 points,
−2.8 UAS).",4.4.2 Out-of-Vocabulary Words,[0],[0]
"Figure 5 shows how this drop varies with the development OOV rate across treebanks; most extreme is Korean, which drops 15.5 LAS.",4.4.2 Out-of-Vocabulary Words,[0],[0]
"A similar, but less pronounced pattern, was observed for models that include POS.
",4.4.2 Out-of-Vocabulary Words,[0],[0]
"Interestingly, this artificially impoverished model is still consistently better than Words for all languages (e.g., for Korean, by 4 LAS).",4.4.2 Out-of-Vocabulary Words,[0],[0]
"This implies that not all of the improvement is due to OOV words; statistical sharing across orthographically close words is beneficial, as well.",4.4.2 Out-of-Vocabulary Words,[0],[0]
"The character-based representations make the parser slower, since they require composing the character-based bidirectional LSTMs for each
word of the input sentence; however, at test time these results could be cached.",4.4.3 Computational Requirements,[0],[0]
"On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons-
9We are using a machine with 32 Intel Xeon CPU E52650 at 2.00GHz; the parser runs on a single core.
tant, needing some hours to have a competitive model.",4.4.3 Computational Requirements,[0],[0]
"In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB.",4.4.3 Computational Requirements,[0],[0]
Table 3 shows a comparison with state-of-theart parsers.,4.4.4 Comparison with State-of-the-Art,[0],[0]
"We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013).",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection.",4.4.4 Comparison with State-of-the-Art,[0],[0]
Our parser outperforms these in most cases.,4.4.4 Comparison with State-of-the-Art,[0],[0]
"Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For English and Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings.
",4.4.4 Comparison with State-of-the-Art,[0],[0]
We also show the best reported results on these datasets.,4.4.4 Comparison with State-of-the-Art,[0],[0]
"For the SPMRL data sets, the best performing system of the shared task is either Björkelund et al. (2013) or Björkelund et al. (2014), which are consistently better than our sys-
tem for all languages.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimarães (2015)",5 Related Work,[0],[0]
"learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks.",5 Related Work,[0],[0]
Our approach is similar to theirs.,5 Related Work,[0],[0]
Others have used character-based models as features to improve existing models.,5 Related Work,[0],[0]
"For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets.
",5 Related Work,[0],[0]
"Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer.",5 Related Work,[0],[0]
"That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer.",5 Related Work,[0],[0]
"Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information.
",5 Related Work,[0],[0]
Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011).,5 Related Work,[0],[0]
"More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian.",5 Related Work,[0],[0]
"Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words.
",5 Related Work,[0],[0]
"Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a).
",5 Related Work,[0],[0]
"To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper.",5 Related Work,[0],[0]
We have presented several interesting findings.,6 Conclusion,[0],[0]
"First, we add new evidence that character-based representations are useful for NLP tasks.",6 Conclusion,[0],[0]
"In this paper, we demonstrate that they are useful for transition-based dependency parsing, since they are capable of capturing morphological information crucial for analyzing syntax.
",6 Conclusion,[0],[0]
"The improvements provided by the characterbased representations using bidirectional LSTMs are strong for agglutinative languages, such as
Basque, Hungarian, Korean, and Turkish, comparing favorably to POS tags as encoded in those languages’ currently available treebanks.",6 Conclusion,[0],[0]
"This outcome is important, since annotating morphological information for a treebank is expensive.",6 Conclusion,[0],[0]
"Our finding suggests that the best investment of annotation effort may be in dependencies, leaving morphological features to be learned implicitly from strings.
",6 Conclusion,[0],[0]
"The character-based representations are also a way of overcoming the out-of-vocabulary problem; without any additional resources, they enable the parser to substantially improve the performance when OOV rates are high.",6 Conclusion,[0],[0]
"We expect that, in conjunction with a pretraing regime, or in conjunction with distributional word embeddings, further improvements could be realized.",6 Conclusion,[0],[0]
MB was supported by the European Commission under the contract numbers FP7-ICT610411 (project MULTISENSOR) and H2020RIA-645012 (project KRISTINA).,Acknowledgments,[0],[0]
This research was supported by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533 and NSF IIS-1054319.,Acknowledgments,[0],[0]
This work was completed while NAS was at CMU.,Acknowledgments,[0],[0]
"Thanks to Joakim Nivre, Bernd Bohnet, Fei Liu and Swabha Swayamdipta for useful comments.",Acknowledgments,[0],[0]
We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages.,abstractText,[0],[0]
"Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs.",abstractText,[0],[0]
This allows statistical sharing across word forms that are similar on the surface.,abstractText,[0],[0]
Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.,abstractText,[0],[0]
Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs,title,[0],[0]
"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",text,[0],[0]
"Generative models play an important role in NLP, both in their use as language models and because of their ability to effectively learn from unlabeled data.",1. Introduction,[0],[0]
"By parameterzing generative models using neural nets, recent work has proposed model classes that are particularly expressive and can pontentially model a wide range of phenomena in language and other modalities.",1. Introduction,[0],[0]
"We focus on a specific instance
1Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Zichao Yang <zichaoy@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
of this class: the variational autoencoder1 (VAE) (Kingma & Welling, 2013).
",1. Introduction,[0],[0]
"The generative story behind the VAE (to be described in detail in the next section) is simple: First, a continuous latent representation is sampled from a multivariate Gaussian.",1. Introduction,[0],[0]
"Then, an output is sampled from a distribution parameterized by a neural decoder, conditioned on the latent representation.",1. Introduction,[0],[0]
"The latent representation (treated as a latent variable during training) is intended to give the model more expressive capacity when compared with simpler neural generative models–for example, conditional language models.",1. Introduction,[0],[0]
"The choice of decoding architecture and final output distribution, which connect the latent representation to output, depends on the kind of data being modeled.",1. Introduction,[0],[0]
"The VAE owes its name to an accompanying variational technique (Kingma & Welling, 2013) that has been successfully used to train such models on image data (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).
",1. Introduction,[0],[0]
"The application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).",1. Introduction,[0],[0]
"The obvious choice for decoding architecture for a textual VAE is an LSTM, a typical workhorse in NLP.",1. Introduction,[0],[0]
"However, Bowman et al. (2015) found that using an LSTM-VAE for text modeling yields higher perplexity on held-out data than using an LSTM language model.",1. Introduction,[0],[0]
"In particular, they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and, as a result, VAE collapses into a simple language model.",1. Introduction,[0],[0]
"Related work (Miao et al., 2016; Larochelle & Lauly, 2012; Mnih & Gregor, 2014) has used simpler decoders that model text as a bag of words.",1. Introduction,[0],[0]
"Their results indicate better use of latent representations, but their decoders cannot effectively model longer-range dependencies in text and thus underperform in terms of final perplexity.
",1. Introduction,[0],[0]
"Motivated by these observations, we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data.",1. Introduction,[0],[0]
"We propose the use of a dilated CNN as a decoder in VAE, inspired by the recent success of using CNNs for audio, image and language
1The name VAE is often used to refer to both a model class and an associated inference procedure.
modeling (van den Oord et al., 2016a; Kalchbrenner et al., 2016a; van den Oord et al., 2016b).",1. Introduction,[0],[0]
"In contrast with prior work where extremely large CNNs are used, we exploit the dilated CNN for its flexibility in varying the amount of conditioning context.",1. Introduction,[0],[0]
"In the two extremes, depending on the choice of dilation, the CNN decoder can reproduce a simple MLP using a bags of words representation of text, or can reproduce the long-range dependence of recurrent architectures (like an LSTM) by conditioning on the entire history.",1. Introduction,[0],[0]
"Thus, by choosing a dilated CNN as the decoder, we are able to conduct experiments where we vary contextual capacity, finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation.
",1. Introduction,[0],[0]
"We demonstrate that when this trade-off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders (Chen et al., 2016).",1. Introduction,[0],[0]
"We go on to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering, outperforming several strong baselines (from (Dai & Le, 2015)) on both text categorization and sentiment analysis.
",1. Introduction,[0],[0]
"Our contributions are as follows: First, we propose the use of a dilated CNN as a new decoder for VAE.",1. Introduction,[0],[0]
"We then empirically evaluate several dilation architectures with different capacities, finding that reduced contextual capacity leads to stronger reliance on latent representations.",1. Introduction,[0],[0]
"By picking a decoder with suitable contextual capacity, we find our VAE performs better than LSTM language models on two data sets.",1. Introduction,[0],[0]
"We also explore the use of dilated CNN VAEs for semi-supervised classification and find they perform better than strong baselines from (Dai & Le, 2015).",1. Introduction,[0],[0]
"Finally, we verify that the same framework can be used effectively for unsupervised clustering.",1. Introduction,[0],[0]
"In this section, we begin by providing background on the use of variational autoencoders for language modeling.",2. Model,[0],[0]
Then we introduce the dilated CNN architecture that we will use as a new decoder for VAE in experiments.,2. Model,[0],[0]
"Finally, we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification.",2. Model,[0],[0]
"Neural language models (Mikolov et al., 2010) typically generate each token xt conditioned on the entire history of previously generated tokens:
p(x) = ∏ t p(xt|x1, x2, ..., xt−1).",2.1. Background on Variational Autoencoders,[0],[0]
"(1)
State-of-the-art language models often parametrize these conditional probabilities using RNNs, which compute an evolving hidden state over the text which is used to predict each xt.",2.1. Background on Variational Autoencoders,[0],[0]
"This approach, though effective in modeling text, does not explicitly model variance in higher-level properties of entire utterances (e.g. topic or style) and thus can have difficulty with heterogeneous datasets.
",2.1. Background on Variational Autoencoders,[0],[0]
"Bowman et al. (2015) propose a different approach to generative text modeling inspired by related work on vision (Kingma & Welling, 2013).",2.1. Background on Variational Autoencoders,[0],[0]
"Instead of directly modeling the joint probability p(x) as in Equation 1, we specify a generative process for which p(x) is a marginal distribution.",2.1. Background on Variational Autoencoders,[0],[0]
"Specifically, we first generate a continuous latent vector representation z from a multivariate Gaussian prior pθ(z), and then generate the text sequence x from a conditional distribution pθ(x|z) parameterized using a neural net (often called the generation model or decoder).",2.1. Background on Variational Autoencoders,[0],[0]
"Because this model incorporates a latent variable that modulates the entire generation of each whole utterance, it may be better able to capture high-level sources of variation in the data.",2.1. Background on Variational Autoencoders,[0],[0]
"Specifically, in contrast with Equation 1, this generating distribution conditions on latent vector representation z:
pθ(x|z) = ∏ t pθ(xt|x1, x2, ..., xt−1, z).",2.1. Background on Variational Autoencoders,[0],[0]
"(2)
To estimate model parameters θ we would ideally like to maximize the marginal probability pθ(x) =∫ pθ(z)pθ(x|z)dz.",2.1. Background on Variational Autoencoders,[0],[0]
"However, computing this marginal is intractable for many decoder choices.",2.1. Background on Variational Autoencoders,[0],[0]
"Thus, the following variational lower bound is often used as an objective (Kingma & Welling, 2013):
log pθ(x) =",2.1. Background on Variational Autoencoders,[0],[0]
"− log ∫ pθ(z)pθ(x|z)dz
≥ Eqφ(z|x)[log pθ(x|z)]− KL(qφ(z|x)||pθ(z)).
",2.1. Background on Variational Autoencoders,[0],[0]
"Here, qφ(z|x) is an approximation to the true posterior (often called the recognition model or encoder) and is parameterized by φ.",2.1. Background on Variational Autoencoders,[0],[0]
"Like the decoder, we have a choice of neural architecture to parameterize the encoder.",2.1. Background on Variational Autoencoders,[0],[0]
"However, unlike the decoder, the choice of encoder does not change the model class – it only changes the variational approximation used in training, which is a function of both the model parameters θ and the approximation parameters φ.",2.1. Background on Variational Autoencoders,[0],[0]
Training seeks to optimize these parameters jointly using stochastic gradient ascent.,2.1. Background on Variational Autoencoders,[0],[0]
A final wrinkle of the training procedure involves a stochastic approximation to the gradients of the variational objective (which is itself intractable).,2.1. Background on Variational Autoencoders,[0],[0]
"We omit details here, noting only that the final distribution of the posterior approximation qφ(z|x) is typically assumed to be Gaussian so that a re-parametrization trick can be used, and refer readers to (Kingma & Welling, 2013).",2.1. Background on Variational Autoencoders,[0],[0]
"Together, this combination of generative model and variational inference procedure are often referred to as a variational autoencoder (VAE).",2.2. Training Collapse with Textual VAEs,[0],[0]
We can also view the VAE as a regularized version of the autoencoder.,2.2. Training Collapse with Textual VAEs,[0],[0]
"Note, however, that while VAEs are valid probabilistic models whose likelihood can be evaluated on held-out data, autoencoders are not valid models.",2.2. Training Collapse with Textual VAEs,[0],[0]
"If only the first term of the VAE variational bound Eqφ(z|x)[log pθ(x|z)] is used as an objective, the variance of the posterior probability qφ(z|x) will become small and the training procedure reduces to an autoencoder.",2.2. Training Collapse with Textual VAEs,[0],[0]
"It is the KL-divergence term, KL(qφ(z|x)||pθ(z)), that discourages the VAE memorizing each x as a single latent point.
",2.2. Training Collapse with Textual VAEs,[0],[0]
"While the KL term is critical for training VAEs, historically, instability on text has been evidenced by the KL term becoming vanishingly small during training, as observed by Bowman et al. (2015).",2.2. Training Collapse with Textual VAEs,[0],[0]
"When the training procedure collapses in this way, the result is an encoder that has duplicated the Gaussian prior (instead of a more interesting posterior), a decoder that completely ignores the latent variable z, and a learned model that reduces to a simpler language model.",2.2. Training Collapse with Textual VAEs,[0],[0]
We hypothesize that this collapse condition is related to the contextual capacity of the decoder architecture.,2.2. Training Collapse with Textual VAEs,[0],[0]
The choice encoder and decoder depends on the type of data.,2.2. Training Collapse with Textual VAEs,[0],[0]
"For images, these are typically MLPs or CNNs.",2.2. Training Collapse with Textual VAEs,[0],[0]
"LSTMs have been used for text, but have resulted in training collapse as discussed above (Bowman et al., 2015).",2.2. Training Collapse with Textual VAEs,[0],[0]
"Here, we propose to use a dilated CNN as the decoder instead.",2.2. Training Collapse with Textual VAEs,[0],[0]
"In one extreme, when the effective contextual width of a CNN is very large, it resembles the behavior of LSTM.",2.2. Training Collapse with Textual VAEs,[0],[0]
"When the width is very small, it behaves like a bag-ofwords model.",2.2. Training Collapse with Textual VAEs,[0],[0]
The architectural flexibility of dilated CNNs allows us to change the contextual capacity and conduct experiments to validate our hypothesis: decoder contextual capacity and effective use of encoding information are directly related.,2.2. Training Collapse with Textual VAEs,[0],[0]
We next describe the details of our decoder.,2.2. Training Collapse with Textual VAEs,[0],[0]
"The typical approach to using CNNs used for text generation (Kalchbrenner et al., 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.",2.3. Dilated Convolutional Decoders,[0],[0]
"We take this approach here in defining our decoder.
",2.3. Dilated Convolutional Decoders,[0],[0]
"One dimensional convolution: For a CNN to serve as a decoder for text, generation of xt must only condition on past tokens x<t.",2.3. Dilated Convolutional Decoders,[0],[0]
Applying the traditional convolution will break this assumption and use tokens x≥t as inputs to predict xt.,2.3. Dilated Convolutional Decoders,[0],[0]
"In our decoder, we avoid this by simply shifting the input by several slots (van den Oord et al., 2016b).",2.3. Dilated Convolutional Decoders,[0],[0]
"With a convolution with filter size of k and using n layers, our effective filter size (the number of past tokens
to condition to in predicting xt) would be (k− 1)× n+ 1.",2.3. Dilated Convolutional Decoders,[0],[0]
"Hence, the filter size would grow linearly with the depth of the network.
",2.3. Dilated Convolutional Decoders,[0],[0]
"Dilation: Dilated convolution (Yu & Koltun, 2015) was introduced to greatly increase the effective receptive field size without increasing the computational cost.",2.3. Dilated Convolutional Decoders,[0],[0]
"With dilation d, the convolution is applied so that d − 1 inputs are skipped each step.",2.3. Dilated Convolutional Decoders,[0],[0]
Causal convolution can be seen a special case with d = 1.,2.3. Dilated Convolutional Decoders,[0],[0]
"With dilation, the effective receptive size grows exponentially with network depth.",2.3. Dilated Convolutional Decoders,[0],[0]
"In Figure 1b, we show dilation of sizes of 1 and 2 in the first and second layer, respectively.",2.3. Dilated Convolutional Decoders,[0],[0]
"Suppose the dilation size in the i-th layer is di and we use the same filter size k in all layers, then the effective filter size is (k − 1) ∑ i di + 1.",2.3. Dilated Convolutional Decoders,[0],[0]
The dilations are typically set to double every layer di+1,2.3. Dilated Convolutional Decoders,[0],[0]
=,2.3. Dilated Convolutional Decoders,[0],[0]
"2di, so the effective receptive field size can grow exponentially.",2.3. Dilated Convolutional Decoders,[0],[0]
"Hence, the contextual capacity of a CNN can be controlled across a greater range by manipulating the filter size, dilation size and network depth.",2.3. Dilated Convolutional Decoders,[0],[0]
"We use this approach in experiments.
",2.3. Dilated Convolutional Decoders,[0],[0]
"Residual connection: We use residual connection (He et al., 2016) in the decoder
ReLU 1x1, 512
ReLU 1xk, 512
conv
ReLU 1x1, 1024
+
conv
conv to speed up convergence and enable training of deeper models.",2.3. Dilated Convolutional Decoders,[0],[0]
"We use a residual block (shown to the right) similar to that of (Kalchbrenner et al., 2016a).",2.3. Dilated Convolutional Decoders,[0],[0]
"We use three convolutional layers with filter size 1×1, 1×k, 1×1, respectively, and ReLU activation be-
tween convolutional layers.
",2.3. Dilated Convolutional Decoders,[0],[0]
Overall architecture: Our VAE architecture is shown in Figure 1a.,2.3. Dilated Convolutional Decoders,[0],[0]
"We use LSTM as the encoder to get the posterior probability q(z|x), which we assume to be diagonal Gaussian.",2.3. Dilated Convolutional Decoders,[0],[0]
We parametrize the mean µ and variance σ with LSTM output.,2.3. Dilated Convolutional Decoders,[0],[0]
"We sample z from q(z|x), the decoder is conditioned on the sample by concatenating z with every word embedding of the decoder input.",2.3. Dilated Convolutional Decoders,[0],[0]
"In addition to conducting language modeling experiments, we will also conduct experiments on semi-supervised classification of text using our proposed decoder.",2.4. Semi-supervised VAE,[0],[0]
"In this section, we briefly review semi-supervised VAEs of (Kingma et al., 2014) that incorporate discrete labels as additional variables.",2.4. Semi-supervised VAE,[0],[0]
"Given the labeled set (x, y) ∼ DL and the unlabeled set x ∼ DU , (Kingma et al., 2014) proposed a model whose latent representation contains continuous vector z and discrete label y:
p(x,y, z) =",2.4. Semi-supervised VAE,[0],[0]
"p(y)p(z)p(x|y, z).",2.4. Semi-supervised VAE,[0],[0]
"(3)
The semi-supervised VAE fits a discriminative network q(y|x), an inference network q(z|x,y) and a generative network p(x|y, z) jointly as part of optimizing a variational lower bound similar that of basic VAE.",2.4. Semi-supervised VAE,[0],[0]
"For labeled data (x,y), this bound is:
log p(x,y) ≥Eq(z|x,y)[log p(x|y, z)]",2.4. Semi-supervised VAE,[0],[0]
"− KL(q(z|x,y)||p(z)) + log p(y)
=L(x,y) + log p(y).
",2.4. Semi-supervised VAE,[0],[0]
"For unlabeled data x, the label is treated as a latent variable, yielding:
log p(x) ≥U(x) =Eq(y|x)",2.4. Semi-supervised VAE,[0],[0]
"[ Eq(z|x,y)[log p(x|y, z)]
− KL(q(z|x,y)||p(z))",2.4. Semi-supervised VAE,[0],[0]
"+ log p(y)− log q(y|x) ]
",2.4. Semi-supervised VAE,[0],[0]
"= ∑ y q(y|x)L(x,y)− KL(q(y|x)||p(y)).
",2.4. Semi-supervised VAE,[0],[0]
"Combining the labeled and unlabeled data terms, we have the overall objective as:
J =E(x,y)∼DL",2.4. Semi-supervised VAE,[0],[0]
"[L(x,y)] + Ex∼DU",2.4. Semi-supervised VAE,[0],[0]
[U(x)],2.4. Semi-supervised VAE,[0],[0]
"+ αE(x,y)∼DL",2.4. Semi-supervised VAE,[0],[0]
"[log q(y|x)],
where α controls the trade off between generative and discriminative terms.
",2.4. Semi-supervised VAE,[0],[0]
Gumbel-softmax: Jang et al. (2016); Maddison et al. (2016) propose a continuous approximation to sampling from a categorical distribution.,2.4. Semi-supervised VAE,[0],[0]
"Let u be a categorical distribution with probabilities π1, π2, ..., πc.",2.4. Semi-supervised VAE,[0],[0]
"Samples from u
can be approximated using:
yi = exp((log(πi) +",2.4. Semi-supervised VAE,[0],[0]
"gi)/τ)∑c j=1 exp((log(πj) + gj)/τ) , (4)
where gi follows Gumbel(0, 1).",2.4. Semi-supervised VAE,[0],[0]
The approximation is accurate when τ → 0 and smooth when τ > 0.,2.4. Semi-supervised VAE,[0],[0]
"In experiments, we use Gumbel-Softmax to approximate the samples from p(y|x) to reduce the computational cost.",2.4. Semi-supervised VAE,[0],[0]
"As a result, we can directly back propagate the gradients of U(x) to the discriminator network.",2.4. Semi-supervised VAE,[0],[0]
We anneal τ,2.4. Semi-supervised VAE,[0],[0]
"so that sample variance is small when training starts and then gradually decrease τ .
",2.4. Semi-supervised VAE,[0],[0]
Unsupervised clustering: In this section we adapt the same framework for unsupervised clustering.,2.4. Semi-supervised VAE,[0],[0]
"We directly minimize the objective U(x), which is consisted of two parts: reconstruction loss and KL regularization on q(y|x).",2.4. Semi-supervised VAE,[0],[0]
The first part encourages the model to assign x to label y such that the reconstruction loss is low.,2.4. Semi-supervised VAE,[0],[0]
We find that the model can easily get stuck in two local optimum: the KL term is very small and q(y|x) is close to uniform distribution or the KL term is very large and all samples collapse to one class.,2.4. Semi-supervised VAE,[0],[0]
"In order to make the model more robust, we modify the KL term by:
KLy = max(γ,KL(q(y|x)|p(y)).",2.4. Semi-supervised VAE,[0],[0]
"(5)
That is, we only minimize the KL term when it is large enough.",2.4. Semi-supervised VAE,[0],[0]
"Since we would like to investigate VAEs for language modeling and semi-supervised classification, the data sets should be suitable for both purposes.",3.1. Data sets,[0],[0]
"We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).",3.1. Data sets,[0],[0]
"The original data sets contain millions of samples, of which we sample 100k as training and 10k as validation and test from the respective partitions.",3.1. Data sets,[0],[0]
The detailed statistics of both data sets are in Table 1.,3.1. Data sets,[0],[0]
"Yahoo Answer contains 10 topics including Society & Culture, Science & Mathematics etc.",3.1. Data sets,[0],[0]
"Yelp15 contains 5 level of rating, with higher rating better.",3.1. Data sets,[0],[0]
We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders.,3.2. Model configurations and Training details,[0],[0]
"For CNNs, we explore several different configurations.",3.2. Model configurations and Training details,[0],[0]
"We set the convolution filter size to be 3 and gradually increase the depth and dilation from [1, 2, 4], [1, 2, 4, 8, 16] to [1, 2, 4, 8, 16, 1, 2, 4, 8, 16].",3.2. Model configurations and Training details,[0],[0]
"They represent small, medium and large model and we name them as SCNN, MCNN and LCNN.",3.2. Model configurations and Training details,[0],[0]
"We also explore a very large model with dilations [1, 2, 4, 8, 16, 1, 2, 4, 8, 16, 1, 2, 4, 8, 16] and name it as VLCNN.",3.2. Model configurations and Training details,[0],[0]
"The effective filter size are 15, 63, 125 and 187 respectively.",3.2. Model configurations and Training details,[0],[0]
"We use the last hidden state of the encoder LSTM and feed it though an MLP to get the mean and variance of q(z|x), from which we sample z and then feed it through an MLP to get the starting state of decoder.",3.2. Model configurations and Training details,[0],[0]
"For the LSTM decoder, we follow (Bowman et al., 2015) to use it as the initial state of LSTM and feed it to every step of LSTM.",3.2. Model configurations and Training details,[0],[0]
"For the CNN decoder, we concatenate it with the word embedding of every decoder input.
",3.2. Model configurations and Training details,[0],[0]
The architecture of the Semi-supervised VAE basically follows that of the VAE.,3.2. Model configurations and Training details,[0],[0]
We feed the last hidden state of the encoder LSTM through a two layer MLP then a softmax to get q(y|x).,3.2. Model configurations and Training details,[0],[0]
We use Gumbel-softmax to sample y from q(y|x).,3.2. Model configurations and Training details,[0],[0]
"We then concatenate y with the last hidden state of encoder LSTM and feed them throught an MLP to get the mean and variance of q(z|y,x).",3.2. Model configurations and Training details,[0],[0]
"y and z together are used as the starting state of the decoder.
",3.2. Model configurations and Training details,[0],[0]
We use a vocabulary size of 20k for both data sets and set the word embedding dimension to be 512.,3.2. Model configurations and Training details,[0],[0]
The LSTM dimension is 1024.,3.2. Model configurations and Training details,[0],[0]
"The number of channels for convolutions
in CNN decoders is 512 internally and 1024 externally, as shown in Section 2.3.",3.2. Model configurations and Training details,[0],[0]
"We select the dimension of z from [32, 64].",3.2. Model configurations and Training details,[0],[0]
"We find our model is not sensitive to this parameter.
",3.2. Model configurations and Training details,[0],[0]
"We use Adam (Kingma & Ba, 2014) to optimize all models and the learning rate is selected from [2e-3, 1e-3, 7.5e-4] and β1 is selected from [0.5, 0.9].",3.2. Model configurations and Training details,[0],[0]
"Empirically, we find learning rate 1e-3 and β1 = 0.5 to perform the best.",3.2. Model configurations and Training details,[0],[0]
"We select drop out ratio of LSTMs (both encoder and decoder) from [0.3, 0.5].",3.2. Model configurations and Training details,[0],[0]
"Following (Bowman et al., 2015), we also use drop word for the LSTM decoder, the drop word ratio is selected from [0, 0.3, 0.5, 0.7].",3.2. Model configurations and Training details,[0],[0]
"For the CNN decoder, we use a drop out ratio of 0.1 at each layer.",3.2. Model configurations and Training details,[0],[0]
We do not use drop word for CNN decoders.,3.2. Model configurations and Training details,[0],[0]
We use batch size of 32 and all model are trained for 40 epochs.,3.2. Model configurations and Training details,[0],[0]
We start to half the learning rate every 2 epochs after epoch 30.,3.2. Model configurations and Training details,[0],[0]
"Following (Bowman et al., 2015), we use KL cost annealing strategy.",3.2. Model configurations and Training details,[0],[0]
We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .,3.2. Model configurations and Training details,[0],[0]
"We treat T as a hyper parameter and select it from [10k, 40k, 80k].",3.2. Model configurations and Training details,[0],[0]
The results for language modeling are shown in Table 2.,3.3. Language modeling results,[0],[0]
We report the negative log likelihood (NLL) and perplexity (PPL) of the test set.,3.3. Language modeling results,[0],[0]
"For the NLL of VAEs, we decompose it into reconstruction loss and KL divergence and report the KL divergence in the parenthesis.",3.3. Language modeling results,[0],[0]
"To better visualize these results, we plot the results of Yahoo data set (Table 2a) in Figure 2.
",3.3. Language modeling results,[0],[0]
We first look at the LM results for Yahoo data set.,3.3. Language modeling results,[0],[0]
"As we gradually increase the effective filter size of CNN from SCNN, MCNN to LCNN, the NLL decreases from 345.3, 338.3 to 335.4.",3.3. Language modeling results,[0],[0]
The NLL of LCNN-LM is very close to the NLL of LSTM-LM 334.9.,3.3. Language modeling results,[0],[0]
"But VLCNN-LM is a little bit worse than LCNN-LM, this indicates a little bit of over-fitting.
",3.3. Language modeling results,[0],[0]
"We can see that LSTM-VAE is worse than LSTM-LM in terms of NLL and the KL term is nearly zero, which verifies the finding of (Bowman et al., 2015).",3.3. Language modeling results,[0],[0]
"When we use CNNs as the decoders for VAEs, we can see improvement over pure CNN LMs.",3.3. Language modeling results,[0],[0]
"For SCNN, MCNN and LCNN, the VAE results improve over LM results from 345.3 to 337.8, 338.3 to 336.2, and 335.4 to 333.9 respectively.",3.3. Language modeling results,[0],[0]
The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity.,3.3. Language modeling results,[0],[0]
"When the model is as large as VLCNN, the improvement diminishes and the VAE result is almost the same with LM result.",3.3. Language modeling results,[0],[0]
"This is also reflected in the KL term, SCNN-VAE has the largest KL of 13.3 and VLCNN-VAE has the smallest KL of 0.7.",3.3. Language modeling results,[0],[0]
"When LCNN is used as the decoder, we obtain an optimal trade off between using contextual information and latent representation.",3.3. Language modeling results,[0],[0]
"LCNN-VAE achieves a NLL of 333.9, which improves over LSTM-LM with NLL of 334.9.
",3.3. Language modeling results,[0],[0]
"We find that if we initialize the parameters of LSTM encoder with parameters of LSTM language model, we can improve the VAE results further.",3.3. Language modeling results,[0],[0]
This indicates better encoder model is also a key factor for VAEs to work well.,3.3. Language modeling results,[0],[0]
"Combined with encoder initialization, LCNN-VAE improves over LSTM-LM from 334.9 to 332.1 in NLL and from 66.2 to 63.9 in PPL.",3.3. Language modeling results,[0],[0]
Similar results for the sentiment data set are shown in Table 2b.,3.3. Language modeling results,[0],[0]
"LCNN-VAE improves over LSTM-LM from 362.7 to 359.1 in NLL and from 42.6 to 41.1 in PPL.
",3.3. Language modeling results,[0],[0]
"Latent representation visualization: In order to visualize the latent representation, we set the dimension of z to be 2 and plot the mean of posterior probability q(z|x), as shown in Figure 3.",3.3. Language modeling results,[0],[0]
We can see distinct different characteristics of topic and sentiment representation.,3.3. Language modeling results,[0],[0]
"In Figure 3a, we can see that documents of different topics fall into different clusters, while in Figure 3b, documents of different ratings form a continuum, they lie continuously on the xaxis as the review rating increases.",3.3. Language modeling results,[0],[0]
"Motivated by the success of VAEs for language modeling, we continue to explore VAEs for semi-supervised learning.",3.4. Semi-supervised VAE results,[0],[0]
"Following that of (Kingma et al., 2014), we set the number of labeled samples to be 100, 500, 1000 and 2000 respectively.
",3.4. Semi-supervised VAE results,[0],[0]
Ablation Study:,3.4. Semi-supervised VAE results,[0],[0]
"At first, we would like to explore the effect of different decoders for semi-supervised classification.",3.4. Semi-supervised VAE results,[0],[0]
We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in Table.,3.4. Semi-supervised VAE results,[0],[0]
5.,3.4. Semi-supervised VAE results,[0],[0]
We can see that SCNN-VAESemi has the best classification accuracy of 65.5.,3.4. Semi-supervised VAE results,[0],[0]
The accuracy decreases as we gradually increase the decoder contextual capacity.,3.4. Semi-supervised VAE results,[0],[0]
"On the other hand, LCNN-VAE-Semi has the best NLL result.",3.4. Semi-supervised VAE results,[0],[0]
"This classification accuracy and NLL trade off once again verifies our conjecture: with small contextual window size, the decoder is forced to use the encoder information, hence the latent representation is better
learned.
",3.4. Semi-supervised VAE results,[0],[0]
"Comparing the NLL results of Table 5 with that of Table 2a, we can see the NLL improves.",3.4. Semi-supervised VAE results,[0],[0]
"The NLL of semisupervised VAE improves over simple VAE from 337.8 to 335.7 for SCNN, from 336.2 to 332.8 for MCNN, and from 333.9 to 332.8 for LCNN.",3.4. Semi-supervised VAE results,[0],[0]
"The improvement mainly comes from the KL divergence part, this indicates that better latent representations decrease the KL divergence, further improving the VAE results.
",3.4. Semi-supervised VAE results,[0],[0]
"Comparison with related methods: We compare Semisupervised VAE with the methods from (Dai & Le, 2015), which represent the previous state-of-the-art for semisupervised sequence learning.",3.4. Semi-supervised VAE results,[0],[0]
Dai & Le (2015) pre-trains a classifier by initializing the parameters of a classifier with that of a language model or a sequence autoencoder.,3.4. Semi-supervised VAE results,[0],[0]
They find it improves the classification accuracy significantly.,3.4. Semi-supervised VAE results,[0],[0]
"Since SCNN-VAE-Semi performs the best according to Table 5, we fix the decoder to be SCNN in this part.",3.4. Semi-supervised VAE results,[0],[0]
The detailed comparison is in Table 4.,3.4. Semi-supervised VAE results,[0],[0]
"We can see that semisupervised VAE performs better than LM-LSTM and LALSTM from (Dai & Le, 2015).",3.4. Semi-supervised VAE results,[0],[0]
We also initialize the encoder of the VAE with parameters from LM and find classification accuracy further improves.,3.4. Semi-supervised VAE results,[0],[0]
We also see the advantage of SCNN-VAE-Semi over LM-LSTM is greater when the number of labeled samples is smaller.,3.4. Semi-supervised VAE results,[0],[0]
The advantage decreases as we increase the number of labeled samples.,3.4. Semi-supervised VAE results,[0],[0]
"When we set the number of labeled samples to be 25k, the SCNN-VAE-Semi achieves an accuracy of 70.4, which is similar to LM-LSTM with an accuracy of 70.5.",3.4. Semi-supervised VAE results,[0],[0]
"Also, SCNN-VAE-Semi performs better on Yahoo data set than Yelp data set.",3.4. Semi-supervised VAE results,[0],[0]
"For Yelp, SCNN-VAE-Semi is a little bit worse than LM-LSTM if the number of labeled samples is greater than 100, but becomes better when we initialize the encoder.",3.4. Semi-supervised VAE results,[0],[0]
Figure 3b explains this observation.,3.4. Semi-supervised VAE results,[0],[0]
It shows the documents are coupled together and are harder to classify.,3.4. Semi-supervised VAE results,[0],[0]
"Also, the latent representation contains information other than sentiment, which may not be useful for classification.",3.4. Semi-supervised VAE results,[0],[0]
We also explored using the same framework for unsupervised clustering.,3.5. Unsupervised clustering results,[0],[0]
"We compare with the baselines that ex-
tract the feature with existing models and then run Gaussian Mixture Model (GMM) on these features.",3.5. Unsupervised clustering results,[0],[0]
We find empirically that simply using the features does not perform well since the features are high dimensional.,3.5. Unsupervised clustering results,[0],[0]
"We run a PCA on these features, the dimension of PCA is selected from [8, 16, 32].",3.5. Unsupervised clustering results,[0],[0]
"Since GMM can easily get stuck in poor local optimum, we run each model ten times and report the best result.",3.5. Unsupervised clustering results,[0],[0]
We find directly optimizing U(x) does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model.,3.5. Unsupervised clustering results,[0],[0]
The model only works well for Yahoo data set.,3.5. Unsupervised clustering results,[0],[0]
This is potentially because Figure 3b shows that sentiment latent representations does not fall into clusters.,3.5. Unsupervised clustering results,[0],[0]
"γ in Equation 5 is a sensitive parameter, we select it from the range between 0.5 and 1.5 with an interval of 0.1.",3.5. Unsupervised clustering results,[0],[0]
"We use the following evaluation protocol (Makhzani et al., 2015): after we finish training, for cluster i, we find out the validation sample xn from cluster i that has the best q(yi|x) and assign the label of xn to all samples in cluster i.",3.5. Unsupervised clustering results,[0],[0]
We then compute the test accuracy based on this assignment.,3.5. Unsupervised clustering results,[0],[0]
The detailed results are in Table 5.,3.5. Unsupervised clustering results,[0],[0]
We can see SCNN-VAE-Unsup + init performs better than other baselines.,3.5. Unsupervised clustering results,[0],[0]
"LSTM+GMM performs very bad probably because the feature dimension is 1024 and is too high for GMM, even though we already used PCA to reduce the dimension.
",3.5. Unsupervised clustering results,[0],[0]
"Conditional text generation With the semi-supervised VAE, we are able to generate text conditional on the label.",3.5. Unsupervised clustering results,[0],[0]
"Due to space limitation, we only show one example of
generated reviews conditioning on review rating in Table 6.",3.5. Unsupervised clustering results,[0],[0]
"For each group of generated text, we fix z and vary the label y, while picking x via beam search with a beam size of 10.",3.5. Unsupervised clustering results,[0],[0]
"Variational inference via the re-parameterization trick was initially proposed by (Kingma & Welling, 2013; Rezende et al., 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016; Hu et al., 2017b).
",4. Related work,[0],[0]
"Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016; Hu et al., 2017a).",4. Related work,[0],[0]
"(Bowman et al., 2015) is the first work to combine VAE with language model and they use LSTM as the decoder and find some negative results.",4. Related work,[0],[0]
"On the other hand, (Miao et al., 2016) models text as bag of words, though improvement has been found, the model can not be used to generate text.",4. Related work,[0],[0]
Our work fills the gaps between them.,4. Related work,[0],[0]
"(Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported.",4. Related work,[0],[0]
"(Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.
",4. Related work,[0],[0]
"Our use of CNN as decoder is inspired by recent success of PixelCNN model for images (van den Oord et al., 2016b), WaveNet for audios (van den Oord et al., 2016a), Video Pixel Network for video modeling (Kalchbrenner et al., 2016b) and ByteNet for machine translation (Kalchbrenner et al., 2016a).",4. Related work,[0],[0]
"But in contrast to those works showing using a very deep architecture leads to better performance, CNN as decoder is used in our model to control the contextual capacity, leading to better performance.
",4. Related work,[0],[0]
"Our work is closed related the recently proposed variational lossy autoencoder (Chen et al., 2016) which is used to pre-
dict image pixels.",4. Related work,[0],[0]
"They find that conditioning on a smaller window of a pixels leads to better results with VAE, which is similar to our finding.",4. Related work,[0],[0]
"Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows.",4. Related work,[0],[0]
We treat this as one of our future works.,4. Related work,[0],[0]
"This work is largely orthogonal and could be potentially combined with a more effective choice of decoder to yield additional gains.
",4. Related work,[0],[0]
"There is much previous work exploring unsupervised sentence encodings, for example skip-thought vectors (Kiros et al., 2015), paragraph vectors (Le & Mikolov, 2014), and sequence autoencoders (Dai & Le, 2015).",4. Related work,[0],[0]
"(Dai & Le, 2015) applies a pretrained model to semi-supervised classification and find significant gains, we use this as the baseline for our semi-supervised VAE.",4. Related work,[0],[0]
"We showed that by controlling the decoder’s contextual capacity in VAE, we can improve performance on both language modeling and semi-supervised classification tasks by preventing a degenerate collapse of the training procedure.",5. Conclusion,[0],[0]
These results indicate that more carefully characterizing decoder capacity and understanding how it relates to common variational training procedures may represent important avenues for unlocking future unsupervised problems.,5. Conclusion,[0],[0]
"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015).",abstractText,[0],[0]
"This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder.",abstractText,[0],[0]
"In this paper, we experiment with a new type of decoder for VAE: a dilated CNN.",abstractText,[0],[0]
"By changing the decoder’s dilation architecture, we control the size of context from previously generated words.",abstractText,[0],[0]
"In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information.",abstractText,[0],[0]
"We show that when carefully managed, VAEs can outperform LSTM language models.",abstractText,[0],[0]
"We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE.",abstractText,[0],[0]
"Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",abstractText,[0],[0]
Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1808–1817 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1808",text,[0],[0]
Text summarization concerns the task of compressing a long sequence of text into a more concise form.,1 Introduction,[0],[0]
"The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation.",1 Introduction,[0],[0]
We focus on developing a summarization model that produces an increased level of abstraction.,1 Introduction,[0],[0]
"That is, the model produces concise summaries without only copying long passages from the source document.
",1 Introduction,[0],[0]
"∗ Work performed while at Salesforce Research.
",1 Introduction,[0],[0]
"A high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct.",1 Introduction,[0],[0]
"Because it is difficult to gauge the correctness of the summary, evaluation metrics for summarization models use word overlap with the ground-truth summary in the form of ROUGE (Lin, 2004) scores.",1 Introduction,[0],[0]
"However, word overlap metrics do not capture the abstractive nature of high quality human-written summaries: the use of paraphrases with words that do not necessarily appear in the source document.
",1 Introduction,[0],[0]
"The state-of-the-art abstractive text summarization models have high word overlap performance, however they tend to copy long passages of the source document directly into the summary, thereby producing summaries that are not abstractive (See et al., 2017).
",1 Introduction,[0],[0]
We propose two general extensions to summarization models that improve the level of abstraction of the summary while preserving word overlap with the ground-truth summary.,1 Introduction,[0],[0]
Our first contribution decouples the extraction and generation responsibilities of the decoder by factoring it into a contextual network and a language model.,1 Introduction,[0],[0]
The contextual network has the sole responsibility of extracting and compacting the source document whereas the language model is responsible for the generation of concise paraphrases.,1 Introduction,[0],[0]
Our second contribution is a mixed objective that jointly optimizes the n-gram overlap with the ground-truth summary while encouraging abstraction.,1 Introduction,[0],[0]
This is done by combining maximum likelihood estimation with policy gradient.,1 Introduction,[0],[0]
"We reward the policy with the ROUGE metric, which measures word overlap with the ground-truth summary, as well as a novel abstraction reward that encourages the generation of words not in the source document.
",1 Introduction,[0],[0]
"We demonstrate the effectiveness of our contributions on a encoder-decoder summarization
Article
(cnn) to allay possible concerns, boston prosecutors released video friday of the shooting of a police officer last month that resulted in the killing of the gunman.",1 Introduction,[0],[0]
"the officer wounded, john moynihan, is white.",1 Introduction,[0],[0]
"angelo west, the gunman shot to death by officers, was black.",1 Introduction,[0],[0]
"after the shooting, community leaders in the predominantly african-american neighborhood of (...)
model.",1 Introduction,[0],[0]
"Our model obtains state-of-the-art ROUGE-L scores, and ROUGE-1 and ROUGE-2 performance comparable to state-of-the-art methods on the CNN/DailyMail dataset.",1 Introduction,[0],[0]
"Moreover, we significantly outperform all previous abstractive approaches in our abstraction metrics.",1 Introduction,[0],[0]
"Table 1 shows a comparison of summaries generated by our model and previous abstractive models, showing less copying and more abstraction in our model.",1 Introduction,[0],[0]
The base model follows the encoder-decoder architecture with temporal attention and intraattention proposed by Paulus et al. (2017).,2.1 Base Model and Training Objective,[0],[0]
Let E ∈ Rn×demb denote the matrix of demb dimensional word embeddings of the n words in the source document.,2.1 Base Model and Training Objective,[0],[0]
"The encoding of the source document henc is computed via a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) whose output has dimension dhid.
",2.1 Base Model and Training Objective,[0],[0]
"henc = BiLSTM (E) ∈ Rn×dhid (1)
",2.1 Base Model and Training Objective,[0],[0]
The decoder uses temporal attention over the encoded sequence that penalizes input tokens that previously had high attention scores.,2.1 Base Model and Training Objective,[0],[0]
"Let hdect denote the decoder state at time t. The temporal at-
tention context at time t, ctmpt , is computed as
stmpti = ( hdect )ᵀ",2.1 Base Model and Training Objective,[0],[0]
W tmphenci ∈ R (2) qtmpti = exp(stmpti )∑t−1,2.1 Base Model and Training Objective,[0],[0]
j=1 exp(s tmp ji ) ∈ R (3) αtmpti =,2.1 Base Model and Training Objective,[0],[0]
"qtmpti∑n j=1 q tmp tj ∈ R (4)
",2.1 Base Model and Training Objective,[0],[0]
ctmpt = n∑ i=1,2.1 Base Model and Training Objective,[0],[0]
αtmpti h enc,2.1 Base Model and Training Objective,[0],[0]
"i ∈ Rd hid (5)
where we set qtmpti to exp(s tmp ti ) for t = 1.
",2.1 Base Model and Training Objective,[0],[0]
The decoder also attends to its previous states via intra-attention over the decoded sequence.,2.1 Base Model and Training Objective,[0],[0]
"The intra-attention context at time t, cintt , is computed as
sintti = ( hdect )ᵀ",2.1 Base Model and Training Objective,[0],[0]
W inthdeci ∈ R (6) cintt = t−1∑ i=1,2.1 Base Model and Training Objective,[0],[0]
( sintti∑n j=1 s int tj ),2.1 Base Model and Training Objective,[0],[0]
"hdeci ∈ Rd hid (7)
",2.1 Base Model and Training Objective,[0],[0]
The decoder generates tokens by interpolating between selecting words from the source document via a pointer network as well as selecting words from a fixed output vocabulary.,2.1 Base Model and Training Objective,[0],[0]
"Let zt denote the ground-truth label as to whether the tth
output word should be generated by the selecting from the output vocabulary as opposed to from the source document.",2.1 Base Model and Training Objective,[0],[0]
"We compute p(zt), the probability that the decoder generates from the output vocabulary, as
rt",2.1 Base Model and Training Objective,[0],[0]
=,2.1 Base Model and Training Objective,[0],[0]
"[h dec t ; c tmp t ; c int t ] ∈ R3d hid (8)
p(zt) = sigmoid(W",2.1 Base Model and Training Objective,[0],[0]
zrt + b z) ∈ R,2.1 Base Model and Training Objective,[0],[0]
"(9)
",2.1 Base Model and Training Objective,[0],[0]
"The probability of selecting the word yt from a fixed vocabulary at time step t is defined as
pgen(yt) = softmax (W genrt + b gen) (10)
",2.1 Base Model and Training Objective,[0],[0]
"We set pcp(yt), the probability of copying the word yt from the source document, to the temporal attention distribution αtmpt .",2.1 Base Model and Training Objective,[0],[0]
"The joint probability of using the generator and generating the word yt at time step t, p(zt, yt), is then
p(zt, yt) =",2.1 Base Model and Training Objective,[0],[0]
"p(yt | zt)p(zt) (11)
the likelihood of which is
log p(zt, yt) = log p(yt | zt) + log p(zt) =",2.1 Base Model and Training Objective,[0],[0]
"zt log p
gen(yt)",2.1 Base Model and Training Objective,[0],[0]
+ (1− zt) log pcp(yt),2.1 Base Model and Training Objective,[0],[0]
+ zt log p(zt),2.1 Base Model and Training Objective,[0],[0]
"+ (1− zt) log (1− p (zt))
",2.1 Base Model and Training Objective,[0],[0]
"= zt (log p gen(yt) + log p(zt))
+ (1− zt) (log pcp(yt) + log (1− p (zt)))",2.1 Base Model and Training Objective,[0],[0]
"(12)
",2.1 Base Model and Training Objective,[0],[0]
The objective function combines maximum likelihood estimation with policy learning.,2.1 Base Model and Training Objective,[0],[0]
"Let m denote the length of the ground-truth summary, The maximum likelihood loss Lml is computed as
Lml = − m∑ t=1 log p(zt, yt) (13)
Policy learning uses ROUGE-L as its reward function and a self-critical baseline using the greedy decoding policy (Rennie et al., 2016).",2.1 Base Model and Training Objective,[0],[0]
"Let ysam denote the summary obtained by sampling from the current policy p, ygre and zgre the summary and generator choice obtained by greedily choosing from p(zt, yt), R(y) the ROUGE-L score of the summary y, and Θ the model parameters.",2.1 Base Model and Training Objective,[0],[0]
"The policy learning loss is
R̂ = R (ysam)−R (ygre)",2.1 Base Model and Training Objective,[0],[0]
"(14) Lpg = −E zsam ∼p(z),
ysam ∼p(y|z)",2.1 Base Model and Training Objective,[0],[0]
"[R̂] (15)
where we use greedy predictions by the model according to eq. (13) as a baseline for variance reduction.",2.1 Base Model and Training Objective,[0],[0]
"The policy gradient, as per Schulman et al. (2015), is
∇ΘLpg",2.1 Base Model and Training Objective,[0],[0]
≈ −R̂ m∑ t=1,2.1 Base Model and Training Objective,[0],[0]
"∇Θ log p (zsamt , ysamt ) (16)
",2.1 Base Model and Training Objective,[0],[0]
"The final loss is a mixture between the maximum likelihood loss and the policy learning loss, weighted by a hyperparameter γ.
",2.1 Base Model and Training Objective,[0],[0]
L = (1− γ)Lml + γLpg (17),2.1 Base Model and Training Objective,[0],[0]
The decoder is an essential component of the base model.,2.2 Language Model Fusion,[0],[0]
"Given the source document and the previously generated summary tokens, the decoder both extracts relevant parts of the source document through the pointer network as well as composes paraphrases from the fixed vocabulary.",2.2 Language Model Fusion,[0],[0]
We decouple these two responsibilities by augmenting the decoder with an external language model.,2.2 Language Model Fusion,[0],[0]
"The language model assumes responsibility of generating from the fixed vocabulary, and allows the decoder to focus on attention and extraction.",2.2 Language Model Fusion,[0],[0]
"This decomposition has the added benefit of easily incorporating external knowledge about fluency or domain specific styles via pre-training the language model on a large scale text corpora.
",2.2 Language Model Fusion,[0],[0]
The architecture of our language model is based on Merity et al. (2018).,2.2 Language Model Fusion,[0],[0]
"We use a 3-layer unidirectional LSTM with weight-dropped LSTM units.
",2.2 Language Model Fusion,[0],[0]
"Let et denote the embedding of the word generated during time step t. The hidden state of the language model at the l-th layer is
hlml,t = LSTM lm 3 ( et−1, h lm l,t−1 ) (18)
",2.2 Language Model Fusion,[0],[0]
"At each time step t, we combine the hidden state of the last language model LSTM layer, hlm3,t, with rt defined in eq.",2.2 Language Model Fusion,[0],[0]
(8) in a fashion similar to Sriram et al. (2017).,2.2 Language Model Fusion,[0],[0]
Let denote element-wise multiplication.,2.2 Language Model Fusion,[0],[0]
"We use a gating function whose output
gt filters the content of the language model hidden state.
ft = sigmoid ( W lm[rt;h lm 3,t] + b lm ) (19)
gt = W fuse([rt; gt hlm3,t]) + bfuse (20) hfuset = ReLU (gt) (21)
",2.2 Language Model Fusion,[0],[0]
"We then replace the output distribution of the language model pgen (yt) in eq. 10 with
pgen (yt) = softmax ( W genhfuset + b gen ) (22)",2.2 Language Model Fusion,[0],[0]
"In order to produce an abstractive summary, the model cannot exclusively copy from the source document.",2.3 Abstractive Reward,[0],[0]
"In particular, the model needs to parse large chunks of the source document and create concise summaries using phrases not in the source document.",2.3 Abstractive Reward,[0],[0]
"To encourage this behavior, we propose a novelty metric that promotes the generation of novel words.
",2.3 Abstractive Reward,[0],[0]
We define a novel phrase in the summary as one that is not in the source document.,2.3 Abstractive Reward,[0],[0]
"Let ng (x, n) denote the function that computes the set of unique n-grams in a document x, xgen the generated summary, xsrc the source document, and ‖s‖ the number of words in s. The unnormalized novelty metric N is defined as the fraction of unique n-grams in the summary that are novel.
",2.3 Abstractive Reward,[0],[0]
"N (xgen, n)",2.3 Abstractive Reward,[0],[0]
"= ‖ng (xgen, n)− ng (xsrc, n)‖
‖ng (xgen, n)‖ (23)
",2.3 Abstractive Reward,[0],[0]
"To prevent the model for receiving high novelty rewards by outputting very short summaries, we normalize the metric by the length ratio of the generated and ground-truth summaries.",2.3 Abstractive Reward,[0],[0]
Let xgt denote the ground-truth summary.,2.3 Abstractive Reward,[0],[0]
"We define the novelty metric as
Rnov (xgen, n)",2.3 Abstractive Reward,[0],[0]
"= N (xgen, n) ‖xgen‖ ‖xgt‖
(24)
",2.3 Abstractive Reward,[0],[0]
"We incorporate the novelty metric as a reward into the policy gradient objective in eq. (15), alongside the original ROUGE-L metric.",2.3 Abstractive Reward,[0],[0]
"In doing so, we encourage the model to generate summaries that both overlap with human written ground-truth summaries as well as incorporate novel words not in the source document:
R (y) =",2.3 Abstractive Reward,[0],[0]
λrouRrou (ysam) +,2.3 Abstractive Reward,[0],[0]
"λnovRnov (ysam) (25)
where λrou and λnov are hyperparameters that control the weighting of each reward.",2.3 Abstractive Reward,[0],[0]
"We train our model on the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016).",3.1 Datasets,[0],[0]
Previous works on abstractive summarization either use an anonymized version of this dataset or the original article and summary texts.,3.1 Datasets,[0],[0]
"Due to these different formats, it is difficult to compare the overall ROUGE scores and performance between each version.",3.1 Datasets,[0],[0]
"In order to compare against previous results, we train and evaluate on both versions of this dataset.",3.1 Datasets,[0],[0]
"For the anonymized version, we follow the pre-processing steps described in Nallapati et al. (2016), and the pre-processing steps of See et al. (2017) for the the full-text version.
",3.1 Datasets,[0],[0]
We use named entities and the source document to supervise the model regarding when to use the pointer and when to use the generator (e.g. zt in eq.,3.1 Datasets,[0],[0]
(13).,3.1 Datasets,[0],[0]
"Namely, during training, we teach the model to point from the source document if the word in the ground-truth summary is a named entity, an out-of-vocabulary word, or a numerical value that is in the source document.",3.1 Datasets,[0],[0]
We obtain the list of named entities from Hermann et al. (2015).,3.1 Datasets,[0],[0]
"For each dataset version, we train a language model consisting of a 400-dimensional word embedding layer and a 3-layer LSTM with each layer having a hidden size of 800 dimensions, except the last input layer which has an output size of 400.",3.2 Language Models,[0],[0]
"The final decoding layer shares weights with the embedding layer (Inan et al., 2017; Press and Wolf, 2016).",3.2 Language Models,[0],[0]
"We also use DropConnect (Wan et al., 2013) in the hidden-to-hidden connections, as well as the non-monotonically triggered asynchronous gradient descent optimizer from Merity et al. (2018).
",3.2 Language Models,[0],[0]
"We train this language model on the CNN/Daily Mail ground-truth summaries only, following the same training, validation, and test splits as our main experiments.",3.2 Language Models,[0],[0]
"The two LSTMs of our bidirectional encoder are 200-dimensional, and out decoder LSTM is 400- dimensional.",3.3 Training details,[0],[0]
"We restrict the input vocabulary for the embedding matrix to 150,000 tokens, and the output decoding layer to 50,000 tokens.",3.3 Training details,[0],[0]
"We limit the size of input articles to the first 400 tokens, and the summaries to 100 tokens.",3.3 Training details,[0],[0]
"We use scheduled sampling (Bengio et al., 2015) with a probability of 0.25 when calculating the maximum-likelihood training loss.",3.3 Training details,[0],[0]
"We also set n = 3 when computing our novelty reward Rnov(xgen, n).",3.3 Training details,[0],[0]
"For our final training loss using reinforcement learning, we set γ = 0.9984, λrou = 0.9, and λnov = 0.1.",3.3 Training details,[0],[0]
"Finally, we use the trigram repetition avoidance heuristic defined by Paulus et al. (2017) during beam search decoding to ensure that the model does not output twice the same trigram in a given summary, reducing the amount of repetitions.",3.3 Training details,[0],[0]
"We also create a novelty baseline by taking the outputs of our base model, without RL training and without the language model, and inserting random words not present in the article after each summary token with a probability r = 0.0005.",3.4 Novelty baseline,[0],[0]
"This baseline will intuitively have a higher percentage of novel n-grams than our base model outputs while being very similar to these original outputs, hence keeping the ROUGE score difference relatively small.",3.4 Novelty baseline,[0],[0]
"We obtain a validation and test perplexity of 65.80 and 66.61 respectively on the anonymized dataset, and 81.13 and 82.98 on the full-text dataset with the language models described in Section 3.2.
",4.1 Quantitative analysis,[0],[0]
The ROUGE scores and novelty scores of our final summarization model on both versions of the CNN/Daily Mail dataset are shown in Table 2.,4.1 Quantitative analysis,[0],[0]
"We report the ROUGE-1, ROUGE-2, and ROUGEL F-scores as well as the percentage of novel ngrams, marked NN-n, in the generated summaries, with n from 1 to 4.",4.1 Quantitative analysis,[0],[0]
Results are omitted in cases where they have not been made available by previous authors.,4.1 Quantitative analysis,[0],[0]
"We also include the novel n-gram scores for the ground-truth summaries as a comparison to indicate the level of abstraction of human written summaries.
",4.1 Quantitative analysis,[0],[0]
"Even though our model outputs significantly fewer novel n-grams than human written summaries, it has a much higher percentage of novel n-grams than all the previous abstractive approaches.",4.1 Quantitative analysis,[0],[0]
"It also achieves state-of-the-art ROUGE-L performance on both dataset versions, and obtains ROUGE-1 and ROUGE-2 scores close to state-of-the-art results.",4.1 Quantitative analysis,[0],[0]
"In order to evaluate the relative impact of each of our individual contributions, we run ablation studies comparing our model ablations against each other and against the novelty baseline.",4.2 Ablation study,[0],[0]
The results of these different models on the validation set of the anonymized CNN/Daily Mail dataset are shown in Table 3.,4.2 Ablation study,[0],[0]
"Results show that our base model trained with the maximum-likelihood loss only and using the language model in the decoder (ML, with LM) has higher ROUGE scores, novel unigrams, and novel bigrams scores than our base model without the language model (ML).",4.2 Ablation study,[0],[0]
ML with LM also beats the novelty baseline for these metrics.,4.2 Ablation study,[0],[0]
"When training these models with reinforcement learning using the ROUGE reward (ML+RL ROUGE and ML+RL ROUGE with LM), the model with language model obtains higher ROUGE-1 and ROUGE-2 scores.",4.2 Ablation study,[0],[0]
"However, it also loses its novel unigrams and bigrams advantage.",4.2 Ablation study,[0],[0]
"Finally, using the mixed ROUGE and novelty rewards (ML+RL ROUGE+Novel) produces both higher ROUGE scores and more novel unigrams with the language model than without
it.",4.2 Ablation study,[0],[0]
This indicates that the combination of the language model in the decoder and the novelty reward during training makes our model produce more novel unigrams while maintaining high ROUGE scores.,4.2 Ablation study,[0],[0]
"In order to understand the correlation between ROUGE and novel n-gram scores across different architectures, and to find the model type that gives the best trade-off between each of these metrics, we plot the ROUGE-1 and novel unigram scores for the five best iterations of each model type on the anonymized dataset, as well as the ROUGE-2 and novel bigram scores on a separate plot.",4.3 ROUGE vs novelty trade-off,[0],[0]
We also include the novelty baseline described in Section 4.2 for values of r between 0.005 and 0.035.,4.3 ROUGE vs novelty trade-off,[0],[0]
"For each model type, we indicate the Pareto frontier by a line plot (Ben-Tal, 1980), illustrating which models of a given type give the best combination of ROUGE and novelty scores.",4.3 ROUGE vs novelty trade-off,[0],[0]
"These plots are shown in Figure 2.
",4.3 ROUGE vs novelty trade-off,[0],[0]
"These plots show that there exist an inverse correlation between ROUGE and novelty scores in all model types, illustrating the challenge of choosing a model that performs well in both.",4.3 ROUGE vs novelty trade-off,[0],[0]
"Given that, our final model (ML+RL ROUGE+Novel, with LM) provides the best trade-off of ROUGE-1 scores compared to novel unigrams, indicated by the higher Pareto frontier in the first plot.",4.3 ROUGE vs novelty trade-off,[0],[0]
"Similarly, our final model gives one of the best trade-offs of ROUGE-2 scores to novel bigrams, even though the same model without LM produces more novel
bigrams with a lower ROUGE-2 score.",4.3 ROUGE vs novelty trade-off,[0],[0]
"In order to ensure the quality of our model outputs, we ask 5 human evaluators to rate 100 randomly selected full-text test summaries, giving them two scores from 1 to 10 respectively for readability and relevance given the original article.",4.4 Qualitative evaluation,[0],[0]
We also include the full-text test outputs from See et al. (2017) and Liu et al. (2018) for comparison.,4.4 Qualitative evaluation,[0],[0]
Evaluators are shown different summaries corresponding to the same article side by side without being told which models have generated them.,4.4 Qualitative evaluation,[0],[0]
The mean score and confidence interval at 95% for each model and each evaluation criterion are reported in Table 4.,4.4 Qualitative evaluation,[0],[0]
"These results show that our model matches the relevance score of See et al. (2017) and Liu et al. (2018), but is slightly inferior to them in terms of readability.",4.4 Qualitative evaluation,[0],[0]
Text summarization.,5 Related work,[0],[0]
"Existing summarization approaches are usually either extractive or abstrac-
tive.",5 Related work,[0],[0]
"In extractive summarization, the model selects passages from the input document and combines them to form a shorter summary, sometimes with a post-processing step to ensure final coherence of the output (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015; Nallapati et al., 2017).",5 Related work,[0],[0]
"While extractive models are usually robust and produce coherent summaries, they cannot create concise summaries that paraphrase the source document using new phrases.
",5 Related work,[0],[0]
Abstractive summarization allows the model to paraphrase the source document and create concise summaries with phrases not in the source document.,5 Related work,[0],[0]
"The state-of-the-art abstractive summarization models are based on sequence-tosequence models with attention (Bahdanau et al., 2015).",5 Related work,[0],[0]
"Extensions to this model include a selfattention mechanism (Paulus et al., 2017) and an article coverage vector (See et al., 2017) to prevent repeated phrases in the output summary.",5 Related work,[0],[0]
"Different training procedures have also been used improve the ROUGE score (Paulus et al., 2017) or textual
entailment (Pasunuru and Bansal, 2018) with reinforcement learning; as well as generative adversarial networks to generate more natural summaries (Liu et al., 2018).
",5 Related work,[0],[0]
Several datasets have been used to train and evaluate summarization models.,5 Related work,[0],[0]
"The Gigaword (Graff and Cieri, 2003) and some DUC datasets (Over et al., 2007) have been used for headline generation models (Rush et al., 2015; Nallapati et al., 2016), where the generated summary is shorter than 75 characters.",5 Related work,[0],[0]
"However, generating longer summaries is a more challenging task, especially for abstractive models.",5 Related work,[0],[0]
"Nallapati et al. (2016) have proposed using the CNN/Daily Mail dataset (Hermann et al., 2015) to train models for generating longer, multi-sentence summaries up to 100 words.",5 Related work,[0],[0]
"The New York Times dataset (Sandhaus, 2008) has also been used as a benchmark for the generation of long summaries (Durrett et al., 2016; Paulus et al., 2017).
",5 Related work,[0],[0]
Training strategies for sequential models.,5 Related work,[0],[0]
The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing.,5 Related work,[0],[0]
"At each time step, the model is given the previous ground-truth output and predicts the current output.",5 Related work,[0],[0]
"The sequence objective is the accumulation of cross entropy losses from each time step.
",5 Related work,[0],[0]
"Despite its popularity, this approach for sequence generation is suboptimal due to exposure bias (Huszar, 2015) and loss-evaluation mismatch (Wiseman and Rush, 2016).",5 Related work,[0],[0]
Goyal et al. (2016) propose one way to reduce exposure bias by explicitly forcing the hidden representations of the model to be similar during training and inference.,5 Related work,[0],[0]
Bengio et al. (2015) and Wiseman and Rush (2016) propose an alternate method that exposes the network to the test dynamics during training.,5 Related work,[0],[0]
"Reinforcement learning methods (Sutton and Barto, 1998), such as policy learning (Sutton
et al., 1999), mitigate the mismatch between the optimization objective and the evaluation metrics by directly optimizing evaluation metrics.",5 Related work,[0],[0]
"This approach has led to consistent improvements in domains such as image captioning (Zhang et al., 2017) and abstractive text summarization (Paulus et al., 2017).
",5 Related work,[0],[0]
"A recent approach to training sequential models utilizes generative adversarial networks to improving the human perceived quality of generated outputs (Fedus et al., 2018; Guimaraes et al., 2017; Liu et al., 2018).",5 Related work,[0],[0]
Such models use an additional discriminator network that distinguishes between natural and generated output to guide the generative model towards outputs akin to human-written text.,5 Related work,[0],[0]
"We introduced a new abstractive summarization model which uses an external language model in the decoder, as well as a new reinforcement learning reward to encourage summary abstraction.",6 Conclusions,[0],[0]
"Experiments on the CNN/Daily Mail dataset show that our model generates summaries that are much more abstractive that previous approaches, while maintaining high ROUGE scores close to or above the state of the art.",6 Conclusions,[0],[0]
"Future work could be done on closing the gap to match human levels of abstraction, which is still very far ahead from our model in terms of novel n-grams.",6 Conclusions,[0],[0]
Including mechanisms to promote paraphrase generation in the summary generator could be an interesting direction.,6 Conclusions,[0],[0]
Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document.,abstractText,[0],[0]
"However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches.",abstractText,[0],[0]
We propose two techniques to improve the level of abstraction of generated summaries.,abstractText,[0],[0]
"First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation.",abstractText,[0],[0]
"Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases.",abstractText,[0],[0]
"Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.ive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document.",abstractText,[0],[0]
"However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches.",abstractText,[0],[0]
We propose two techniques to improve the level of abstraction of generated summaries.,abstractText,[0],[0]
"First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation.",abstractText,[0],[0]
"Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases.",abstractText,[0],[0]
"Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.",abstractText,[0],[0]
Improving Abstraction in Text Summarization,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 58–68 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Morphologically complex words (MCWs) are multi-layer structures which consist of different subunits, each of which carries semantic information and has a specific syntactic role.",1 Introduction,[0],[0]
Table 1 gives a Turkish example to show this type of complexity.,1 Introduction,[0],[0]
This example is a clear indication that word-based models are not suitable to process such complex languages.,1 Introduction,[0],[0]
"Accordingly, when translating MRLs, it might not be a good idea to treat words as atomic units as it demands a large vocabulary that im-
poses extra overhead.",1 Introduction,[0],[0]
"Since MCWs can appear in various forms we require a very large vocabulary to i) cover as many morphological forms and words as we can, and ii) reduce the number of OOVs.",1 Introduction,[0],[0]
"Neural models by their nature are complex, and we do not want to make them more complicated by working with large vocabularies.",1 Introduction,[0],[0]
"Furthermore, even if we have quite a large vocabulary set, clearly some words would remain uncovered by that.",1 Introduction,[0],[0]
"This means that a large vocabulary not only complicates the entire process, but also does not necessarily mitigate the OOV problem.",1 Introduction,[0],[0]
"For these reasons we propose an NMT engine which works at the character level.
",1 Introduction,[0],[0]
"In this paper, we focus on translating into MRLs and issues associated with word formation on the target side.",1 Introduction,[0],[0]
"To provide a better translation we do not necessarily need a large target lexicon, as an MCW can be gradually formed during decoding by means of its subunits, similar to the solution proposed in character-based decoding models (Chung et al., 2016).",1 Introduction,[0],[0]
"Generating a complex word character-by-character is a better approach compared to word-level sampling, but it has other disadvantages.
",1 Introduction,[0],[0]
"One character can co-occur with another with almost no constraint, but a particular word or morpheme can only collocate with a very limited number of other constituents.",1 Introduction,[0],[0]
"Unlike words, characters are not meaning-bearing units and do not preserve syntactic information, so (in the extreme case) the
58
chance of sampling each character by the decoder is almost equal to the others, but this situation is less likely for words.",1 Introduction,[0],[0]
"The only constraint that prioritize which character should be sampled is information stored in the decoder, which we believe is insufficient to cope with all ambiguities.",1 Introduction,[0],[0]
"Furthermore, when everything is segmented into characters the target sentence with a limited number of words is changed to a very long sequence of characters, which clearly makes it harder for the decoder to remember such a long history.",1 Introduction,[0],[0]
"Accordingly, character-based information flows in the decoder may not be as informative as wordor morpheme-based information.
",1 Introduction,[0],[0]
In the character-based NMT model everything is almost the same as its word-based counterpart except the target vocabulary whose size is considerably reduced from thousands of words to just hundreds of characters.,1 Introduction,[0],[0]
"If we consider the decoder as a classifier, it should in principle be able to perform much better over hundreds of classes (characters) rather than thousands (words), but the performance of character-based models is almost the same as or slightly better than their wordbased versions.",1 Introduction,[0],[0]
"This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models.
",1 Introduction,[0],[0]
"Character-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same time widens the search space by working with characters whose sampling seems to be harder than words.",1 Introduction,[0],[0]
"The freedom in selection and sampling of characters can mislead the decoder, which prevents us from taking the maximum advantages of character-level decoding.",1 Introduction,[0],[0]
"If we can control the selection process with other constraints, we may obtain further benefit from restricting the vocabulary set, which is the main goal followed in this paper.
",1 Introduction,[0],[0]
In order to address the aforementioned problems we redesign the neural decoder in three different scenarios.,1 Introduction,[0],[0]
In the first scenario we equip the decoder with an additional morphology table including target-side affixes.,1 Introduction,[0],[0]
We place an attention module on top of the table which is controlled by the decoder.,1 Introduction,[0],[0]
"At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state.",1 Introduction,[0],[0]
Signals sent from the table can be interpreted as additional constraints.,1 Introduction,[0],[0]
"In the second scenario we share
the decoder between two output channels.",1 Introduction,[0],[0]
The first one samples the target character and the other one predicts the morphological annotation of the character.,1 Introduction,[0],[0]
This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions.,1 Introduction,[0],[0]
In the third scenario we combine these two models.,1 Introduction,[0],[0]
"Section 3 provides more details on our models.
",1 Introduction,[0],[0]
"Together with different findings that will be discussed in the next sections, there are two main contributions in this paper.",1 Introduction,[0],[0]
We redesigned and tuned the NMT framework for translating into MRLs.,1 Introduction,[0],[0]
It is quite challenging to show the impact of external knowledge such as morphological information in neural models especially in the presence of large parallel corpora.,1 Introduction,[0],[0]
"However, our models are able to incorporate morphological information into decoding and boost its quality.",1 Introduction,[0],[0]
We inject the decoder with morphological properties of the target language.,1 Introduction,[0],[0]
"Furthermore, the novel architecture proposed here is not limited to morphological information alone and is flexible enough to provide other types of information for the decoder.",1 Introduction,[0],[0]
There are several models for NMT of MRLs which are designed to deal with morphological complexities.,2 NMT for MRLs,[0],[0]
García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models.,2 NMT for MRLs,[0],[0]
Morphological annotations can be treated as extra factors in such models.,2 NMT for MRLs,[0],[0]
Jean et al. (2015) proposed a model to handle very large vocabularies.,2 NMT for MRLs,[0],[0]
Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations.,2 NMT for MRLs,[0],[0]
Sennrich et al. (2016) used subword units for NMT.,2 NMT for MRLs,[0],[0]
The model relies on frequent subword units instead of words.,2 NMT for MRLs,[0],[0]
Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs.,2 NMT for MRLs,[0],[0]
The model encodes source words with a convolutional module proposed by Kim et al. (2016).,2 NMT for MRLs,[0],[0]
"Each word is represented by a convolutional combination of its characters.
",2 NMT for MRLs,[0],[0]
Luong and Manning (2016) used a hybrid model for representing words.,2 NMT for MRLs,[0],[0]
"In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings.",2 NMT for MRLs,[0],[0]
"Vylomova et al. (2016) compared differ-
ent representation models (word-, morpheme, and character-level models) which try to capture complexities on the source side, for the task of translating from MRLs.
",2 NMT for MRLs,[0],[0]
Chung et al. (2016) proposed an architecture which benefits from different segmentation schemes.,2 NMT for MRLs,[0],[0]
"On the encoder side, words are segmented into subunits with the byte-pair segmentation model (bpe) (Sennrich et al., 2016), and on the decoder side, one target character is produced at each time step.",2 NMT for MRLs,[0],[0]
"Accordingly, the target sequence is treated as a long chain of characters without explicit segmentation.",2 NMT for MRLs,[0],[0]
Grönroos et al. (2017) focused on translating from English into Finnish and implicitly incorporated morphological information into NMT through multi-task learning.,2 NMT for MRLs,[0],[0]
"Passban (2018) comprehensively studied the problem of translating MRLs and addressed potential challenges in the field.
",2 NMT for MRLs,[0],[0]
"Among all the models reviewed in this section, the network proposed by Chung et al. (2016) could be seen as the best alternative for translating into MRLs as it works at the character level on the decoder side and it was evaluated in different settings on different languages.",2 NMT for MRLs,[0],[0]
"Consequently, we consider it as a baseline model in our experiments.",2 NMT for MRLs,[0],[0]
We propose a compatible neural architecture for translating into MRLs.,3 Proposed Architecture,[0],[0]
The model benefits from subword- and character-level information and improves upon the state-of-the-art model of Chung et al. (2016).,3 Proposed Architecture,[0],[0]
"We manipulated the model to incorporate morphological information and developed three new extensions, which are discussed in Sections 3.1, 3.2, and 3.3.",3 Proposed Architecture,[0],[0]
In the first extension an additional table containing the morphological information of the target language is plugged into the decoder to assist with word formation.,3.1 The Embedded Morphology Table,[0],[0]
"Each time the decoder samples from the target vocabulary, it searches the morphology table to find the most relevant affixes given its current state.",3.1 The Embedded Morphology Table,[0],[0]
"Items selected from the table act as guiding signals to help the decoder sample a better character.
",3.1 The Embedded Morphology Table,[0],[0]
"Our base model is an encoder-decoder model with attention (Bahdanau et al., 2014), implemented using gated recurrent units (GRUs) (Cho et al., 2014).",3.1 The Embedded Morphology Table,[0],[0]
"We use a four-layer model in our
experiments.",3.1 The Embedded Morphology Table,[0],[0]
"Similar to Chung et al. (2016) and Wu et al. (2016), we use bidirectional units to encode the source sequence.",3.1 The Embedded Morphology Table,[0],[0]
Bidirectional GRUs are placed only at the input layer.,3.1 The Embedded Morphology Table,[0],[0]
The forward GRU reads the input sequence in its original order and the backward GRU reads the input in the reverse order.,3.1 The Embedded Morphology Table,[0],[0]
Each hidden state of the encoder in one time step is a concatenation of the forward and backward states at the same time step.,3.1 The Embedded Morphology Table,[0],[0]
"This type of bidirectional processing provides a richer representation of the input sequence.
",3.1 The Embedded Morphology Table,[0],[0]
"On the decoder side, one target character is sampled from a target vocabulary at each time step.",3.1 The Embedded Morphology Table,[0],[0]
"In the original encoder-decoder model, the probability of predicting the next token yi is estimated based on i) the current hidden state of the decoder, ii) the last predicted token, and iii) the context vector.",3.1 The Embedded Morphology Table,[0],[0]
"This process can be formulated as p(yi|y1, ..., yi−1,x) = g(hi, yi−1, ci), where g(.) is a softmax function, yi is the target token (to be predicted), x is the representation of the input sequence, hi is the decoder’s hidden state at the i-th time step, and ci indicates the context vector which is a weighted summary of the input sequence generated by the attention module.",3.1 The Embedded Morphology Table,[0],[0]
"ci is generated via the procedure shown in (1):
ci = n∑
j=1
αijsj
αij = exp (eij)∑ n k=1 exp (eik) ; eij = a(sj , hi−1)
(1)
where αij denotes the weight of the j-th hidden state of the encoder (sj) when the decoder predicts the i-th target token, and a() shows a combinatorial function which can be modeled through a simple feed-forward connection.",3.1 The Embedded Morphology Table,[0],[0]
"n is the length of the input sequence.
",3.1 The Embedded Morphology Table,[0],[0]
"In our first extension, the prediction probability is conditioned on one more constraint in addition to those three existing ones, as in p(yi|y1, ..., yi−1,x) = g(hi, yi−1, ci, cmi ), where cmi is the morphological context vector and carries information from those useful affixes which can enrich the decoder’s information.",3.1 The Embedded Morphology Table,[0],[0]
cmi is generated via an attention module over the morphology table which works in a similar manner to wordbased attention model.,3.1 The Embedded Morphology Table,[0],[0]
"The attention procedure for
generating cmi is formulated as in (2):
cmi =
|A|∑
u=1
βiufu
βiu = exp (emiu)∑ |A| v=1 exp (eiv) ; emiu = a m(fu, hi−1)
(2) where fu represents the embedding of the u-th affix (u-th column) in the morphology/affix tableA, βiu is the weight assigned to fu when predicting the i-th target token, and am is a feed-forward connection between the morphology table and the decoder.
",3.1 The Embedded Morphology Table,[0],[0]
"The attention module in general can be considered as a search mechanism, e.g. in the original encoder-decoder architecture the basic attention module finds the most relevant input words to make the prediction.",3.1 The Embedded Morphology Table,[0],[0]
"In multi-modal NMT (Huang et al., 2016; Calixto et al., 2017) an extra attention module is added to the basic one in order to search the image input to find the most relevant image segments.",3.1 The Embedded Morphology Table,[0],[0]
"In our case we have a similar additional attention module which searches the morphology table.
",3.1 The Embedded Morphology Table,[0],[0]
"In this scenario, the morphology table including the target language’s affixes can be considered as an external knowledge repository that sends auxiliary signals which accompany the main input sequence at all time steps.",3.1 The Embedded Morphology Table,[0],[0]
Such a table certainly includes useful information for the decoder.,3.1 The Embedded Morphology Table,[0],[0]
"As we are not sure which affix preserves those pieces of useful information, we use an attention module to search for the best match.",3.1 The Embedded Morphology Table,[0],[0]
The attention module over the table works as a filter which excludes irrelevant affixes and amplifies the impact of relevant ones by assigning different weights (β values).,3.1 The Embedded Morphology Table,[0],[0]
"In the first scenario, we embedded a morphology table into the decoder in the hope that it can enrich sampling information.",3.2 The Auxiliary Output Channel,[0],[0]
"Mathematically speaking, such an architecture establishes an extra constraint
for sampling and can control the decoder’s predictions.",3.2 The Auxiliary Output Channel,[0],[0]
"However, this is not the only way of constraining the decoder.",3.2 The Auxiliary Output Channel,[0],[0]
"In the second scenario, we define extra supervision to the network via another predictor (output channel).",3.2 The Auxiliary Output Channel,[0],[0]
"The first channel is responsible for generating translations and predicts one character at each time step, and the other one tries to understand the morphological status of the decoder by predicting the morphological annotation (li) of the target character.
",3.2 The Auxiliary Output Channel,[0],[0]
"The approach in the second scenario proposes a multi-task learning architecture, by which in one task we learn translations and in the other one morphological annotations.",3.2 The Auxiliary Output Channel,[0],[0]
"Therefore, all network modules –especially the last hidden layer just before the predictors– should provide information which is useful enough to make correct predictions in both channels, i.e. the decoder should preserve translation as well as morphological knowledge.",3.2 The Auxiliary Output Channel,[0],[0]
"Since we are translating into MRLs this type of mixed information (morphology+translation) can be quite useful.
",3.2 The Auxiliary Output Channel,[0],[0]
"In our setting, the morphological annotation li predicted via the second channel shows to which part of the word or morpheme the target character belongs, i.e. the label for the character is the morpheme that includes it.",3.2 The Auxiliary Output Channel,[0],[0]
We clarify the prediction procedure via an example from our training set (see Section 4).,3.2 The Auxiliary Output Channel,[0],[0]
"When the Turkish word ‘terbiyesizlik’ is generated, the first channel is supposed to predict t, e, r, up to k, one after another.",3.2 The Auxiliary Output Channel,[0],[0]
"For the same word, the second channel is supposed to predict stem-C for the fist 7 steps as the first 7 characters ‘terbiye’ belong to the stem of the word.",3.2 The Auxiliary Output Channel,[0],[0]
The C sign indicates that stem-C is a class label.,3.2 The Auxiliary Output Channel,[0],[0]
"The second channel should also predict siz-C when the first channel predicts s (eighth character), i (ninth character), and z (tenth character), and lik-C when the first channel samples the last three characters.",3.2 The Auxiliary Output Channel,[0],[0]
"Clearly, the second channel is a classifier which works over the {stem-C, siz-C, lik-C, ...} classes.",3.2 The Auxiliary Output Channel,[0],[0]
"Figure 1 illustrates a segment of a sentence including this Turkish word and explains which class
tags should be predicted by each channel.",3.2 The Auxiliary Output Channel,[0],[0]
To implement the second scenario we require a single-source double-target training corpus: [source sentence]→ [sequence of target characters & sequence of morphological annotations] (see Section 4).,3.2 The Auxiliary Output Channel,[0],[0]
The objective function should also be manipulated accordingly.,3.2 The Auxiliary Output Channel,[0],[0]
"Given a training set {xt,yt,mt}Tt=1 the goal is to maximize the joint loss function shown in (3):
λ T∑
t=1
logP (yt|xt; θ)+(1−λ) T∑
t=1
logP (mt|xt; θ)
(3) where xt is the t-th input sentence whose translation is a sequence of target characters shown by yt.",3.2 The Auxiliary Output Channel,[0],[0]
mt is the sequence of morphological annotations and T is the size of the training set.,3.2 The Auxiliary Output Channel,[0],[0]
θ is the set of network parameters and λ is a scalar to balance the contribution of each cost function.,3.2 The Auxiliary Output Channel,[0],[0]
λ is adjusted on the development set during training.,3.2 The Auxiliary Output Channel,[0],[0]
"In the first scenario, we aim to provide the decoder with useful information about morphological properties of the target language, but we are not sure whether signals sent from the table are what we really need.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"They might be helpful or even harmful, so there should be a mechanism to control their quality.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"In the second scenario we also have a similar problem as the last layer requires some information to predict the correct morphological class through the second channel, but there is no guarantee to ensure that information in the decoder is sufficient for this sort of prediction.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"In order to address these problems, in the third extension we combine both scenarios as they are complementary and can potentially help each other.
",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"The morphology table acts as an additional useful source of knowledge as it already consists of affixes, but its content should be adapted according to the decoder and its actual needs.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"Accordingly, we need a trainer to update the table properly.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
The extra prediction channel plays this role for us as it forces the network to predict the target language’s affixes at the output layer.,3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
The error computed in the second channel is backpropagated to the network including the morphology table and updates its affix information into what the decoder actually needs for its prediction.,3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"Therefore, the second output channel helps us train better affix embeddings.
",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
The morphology table also helps the second predictor.,3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"Without considering the table, the last layer only includes information about the input sequence and previously predicted outputs, which is not directly related to morphological information.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"The second attention module retrieves useful affixes from the morphology table and concatenates to the last layer, which means the decoder is explicitly fed with morphological information.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"Therefore, these two modules mutually help each other.",3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
The external channel helps update the morphology table with high-quality affixes (backward pass) and the table sends its high-quality signals to the prediction layer (forward pass).,3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
The relation between these modules and the NMT architecture is illustrated in Figure 2.,3.3 Combining the Extended Output Layer and the Embedded Morphology Table,[0],[0]
"As previously reviewed, different models try to capture complexities on the encoder side, but to the best of our knowledge the only model which proposes a technique to deal with complex constituents on the decoder side is that of Chung et al. (2016), which should be an appropriate baseline for our comparisons.",4 Experimental Study,[0],[0]
"Moreover, it outperforms other existing NMT models, so we prefer to compare our network to the best existing model.",4 Experimental Study,[0],[0]
This model is referred to as CDNMT in our experiments.,4 Experimental Study,[0],[0]
"In the next sections first we explain our experimental setting, corpora, and how we build the morphology table (Section 4.1), and then report experimental results (Section 4.2).",4 Experimental Study,[0],[0]
"In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20.",4.1 Experimental Setting,[0],[0]
"Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015).",4.1 Experimental Setting,[0],[0]
"Chung et al. (2016) and Sennrich et al. (2016) demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe.",4.1 Experimental Setting,[0],[0]
"We use WMT-15 corpora1 to train the models, newstest-2013 for tuning and newstest-2015 as the test sets.",4.1 Experimental Setting,[0],[0]
"For English–Turkish (En–Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).",4.1 Experimental Setting,[0],[0]
"The training side of the English–German (En–De), English–Russian (En– Ru), and En–Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively.",4.1 Experimental Setting,[0],[0]
We randomly select 3K sentences for each of the development and test sets for En–Tr.,4.1 Experimental Setting,[0],[0]
"For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character.
",4.1 Experimental Setting,[0],[0]
One of the key modules in our architecture is the morphology table.,4.1 Experimental Setting,[0],[0]
In order to implement it we use a look-up table whose columns include embeddings for the target language’s affixes (each column represents one affix) which are updated during training.,4.1 Experimental Setting,[0],[0]
"As previously mentioned, the table is intended to provide useful, morphological information so it should be initialized properly, for which we use a morphology-aware embeddinglearning model.",4.1 Experimental Setting,[0],[0]
"To this end, we use the neural language model of Botha and Blunsom (2014) in which each word is represented via a linear combination of the embeddings of its surface form and subunits, e.g. −−−−−−−−−→ terbiyesizlik =
−−−−−−−−−→ terbiyesizlik +−−−−→
terbiye",4.1 Experimental Setting,[0],[0]
+ −→ siz,4.1 Experimental Setting,[0],[0]
+,4.1 Experimental Setting,[0],[0]
−→ lik.,4.1 Experimental Setting,[0],[0]
"Given a sequence of words, the neural language model tries to predict the next word, so it learns sentence-level dependencies as well as intra-word relations.",4.1 Experimental Setting,[0],[0]
"The model trains surface form and subword-level embeddings which provides us with high-quality affix embeddings.
",4.1 Experimental Setting,[0],[0]
"Our neural language model is a recurrent network with a single 1000-dimensional GRU layer, which is trained on the target sides of our parallel corpora.",4.1 Experimental Setting,[0],[0]
The embedding size is 512 and we use a batch size of 100 to train the model.,4.1 Experimental Setting,[0],[0]
"Before training the neural language model, we need
1http://www.statmt.org/wmt15/
to manipulate the training corpus to decompose words into morphemes for which we use Morfessor (Smit et al., 2014), an unsupervised morphological analyzer.",4.1 Experimental Setting,[0],[0]
"Using Morfessor each word is segmented into different subunits where we consider the longest part as the stem of each word; what appears before the stem is taken as a member of the set of prefixes (there might be one or more prefixes) and what follows the stem is considered as a member of the set of suffixes.
",4.1 Experimental Setting,[0],[0]
"Since Morfessor is an unsupervised analyzer, in order to minimize segmentation errors and avoid noisy results we filter its output and exclude subunits which occur fewer than 500 times.2 After decomposing, filtering, and separating stems from affixes, we extracted several affixes which are reported in Table 2.",4.1 Experimental Setting,[0],[0]
"We emphasize that there might be wrong segmentations in Morfessor’s output, e.g. Turkish is a suffix-based language, so there are no prefixes in this language, but based on what Morfessor generated we extracted 11 different types of prefixes.",4.1 Experimental Setting,[0],[0]
"We do not post-process Morfessor’s outputs.
",4.1 Experimental Setting,[0],[0]
"Using the neural language model we train word, stem, and affix embeddings, and initialize the look-up table (but not other parts) of the decoder using those affixes.",4.1 Experimental Setting,[0],[0]
The look-up table includes high-quality affixes trained on the target side of the parallel corpus by which we train the translation model.,4.1 Experimental Setting,[0],[0]
"Clearly, such an affix table is an additional knowledge source for the decoder.",4.1 Experimental Setting,[0],[0]
It preserves information which is very close to what the decoder actually needs.,4.1 Experimental Setting,[0],[0]
"However, there might be some missing pieces of information or some incompatibility between the decoder and the table, so we do not freeze the morphology table during training, but let the decoder update it with respect to its needs in the forward and backward passes.
",4.1 Experimental Setting,[0],[0]
"2The number may seem a little high, but for a corpus with more than 115M words this is not a strict threshold in practice.",4.1 Experimental Setting,[0],[0]
Table 3 summarizes our experimental results.,4.2 Experimental Results,[0],[0]
"We report results for the bpe→char setting, which means the source token is a bpe unit and the decoder samples a character at each time step.",4.2 Experimental Results,[0],[0]
CDNMT is the baseline model.,4.2 Experimental Results,[0],[0]
"Table 3 includes scores reported from the original CDNMT model (Chung et al., 2016) as well as the scores from our reimplementation.",4.2 Experimental Results,[0],[0]
"To make our work comparable and show the impact of the new architecture, we tried to replicate CDNMT’s results in our experimental setting, we kept everything (parameters, iterations, epochs etc.) unchanged and evaluated the extended model in the same setting.",4.2 Experimental Results,[0],[0]
"Table 3 reports BLEU scores (Papineni et al., 2002) of our NMT models.
",4.2 Experimental Results,[0],[0]
"Table 3 can be interpreted from different perspectives but the main findings are summarized as follows:
•",4.2 Experimental Results,[0],[0]
"The morphology table yields significant improvements for all languages and settings.
",4.2 Experimental Results,[0],[0]
•,4.2 Experimental Results,[0],[0]
The morphology table boosts the En–Tr engine more than others and we think this is because of the nature of the language.,4.2 Experimental Results,[0],[0]
"Turkish is an agglutinative language in which morphemes are clearly separable from each other, but in German and Russian morphological transformations rely more on fusional operations rather than agglutination.
",4.2 Experimental Results,[0],[0]
"• It seems that there is a direct relation between the size of the morphology table and the gain provided for the decoder, because Russian and Turkish have bigger tables and benefit from the table more than German which has fewer affixes.
",4.2 Experimental Results,[0],[0]
"• The auxiliary output channel is even more useful than the morphology table for all settings but En–Ru, and we think this is because of the morpheme-per-word ratio in Russian.",4.2 Experimental Results,[0],[0]
"The number of morphemes attached to a Russian word is usually more than those of German and Turkish words in our corpora, and it makes the prediction harder for the classifier (the more the number of suffixes attached to a word, the harder the classification task).
",4.2 Experimental Results,[0],[0]
"• The combination of the morphology table and the extra output channel provides the best result for all languages.
",4.2 Experimental Results,[0],[0]
"Figure 3 depicts the impact of the morphology table and the extra output channel for each language.
",4.2 Experimental Results,[0],[0]
To further study our models’ behaviour and ensure that our extensions do not generate random improvements we visualized some attention weights when generating ‘terbiyesizlik’.,4.2 Experimental Results,[0],[0]
"In Figure 4, the upper figure shows attention weights for all Turkish affixes, where the y axis shows different time steps and the x axis includes attention weights of all affixes (304 columns) for those time steps, e.g. the first row and the first column represents the attention weight assigned to the first Turkish affix when sampling t in ‘terbiyesizlik’.",4.2 Experimental Results,[0],[0]
"While at the first glance the figure may appear to be somewhat confusing, but it provides some interesting insights which we elaborate next.
",4.2 Experimental Results,[0],[0]
In addition to the whole attention matrix we also visualized a subset of weights to show how the morphology table provides useful information.,4.2 Experimental Results,[0],[0]
"In the second figure we study the behaviour of the morphology table for the first (t1), fifth (i5), ninth
(i9), and twelfth (i12) time steps when generating the same Turkish word ‘t1erbi5yesi9zli12k’.",4.2 Experimental Results,[0],[0]
t1 is the first character of the word.,4.2 Experimental Results,[0],[0]
"We also have three i characters from different morphemes, where the first one is part of the stem, the second one belongs to the suffix ‘siz’, and the third one to ‘lik’.",4.2 Experimental Results,[0],[0]
It is interesting to see how the table reacts to the same character from different parts.,4.2 Experimental Results,[0],[0]
For each time step we selected the top-10 affixes which have the highest attention weights.,4.2 Experimental Results,[0],[0]
"The set of top-10 affixes can be different for each step, so we made a union of those sets which gives us 22 affixes.",4.2 Experimental Results,[0],[0]
"The bottom part of Figure 4 shows the attention weights for those 22 affixes at each time step.
",4.2 Experimental Results,[0],[0]
After analyzing the weights we observed interesting properties about the morphology table and the auxiliary attention module.3,4.2 Experimental Results,[0],[0]
"The main findings about the behaviour of the table are as follows:
•",4.2 Experimental Results,[0],[0]
The model assigns high attention weights to stem-C for almost all time steps.,4.2 Experimental Results,[0],[0]
"However, the weights assigned to this class for t1 and i5 are much higher than those of affix characters (as they are part of the stem).",4.2 Experimental Results,[0],[0]
"The vertical lines in both figures approve this feature (bad behaviour).
",4.2 Experimental Results,[0],[0]
"• For some unknown reasons there are some affixes which have no direct relation to that particulate time step but they receive a high attention, such as maz in t12 (bad behaviour).
",4.2 Experimental Results,[0],[0]
"• For almost all time steps the highest attention weight belongs to the class which is expected
3Our observations are not based on this example alone as we studied other random examples, and the table shows consistent behaviour for all examples.
to be selected, e.g. weights for (i5,stem-C) or (i9,siz-C) (good behaviour).
",4.2 Experimental Results,[0],[0]
•,4.2 Experimental Results,[0],[0]
"The morphology table may send bad or good signals but it is consistent for similar or cooccurring characters, e.g. for the last three time steps l11, i12, and k13, almost the same set of affixes receives the highest attention weights.",4.2 Experimental Results,[0],[0]
"This consistency is exactly what we are looking for, as it can define a reliable external constraint for the decoder to guide it.",4.2 Experimental Results,[0],[0]
Vertical lines on the figure also confirm this fact.,4.2 Experimental Results,[0],[0]
"They show that for a set of consecutive characters which belong to the same morpheme the attention module sends a signal from a particular affix (good behaviour).
",4.2 Experimental Results,[0],[0]
• There are some affixes which might not be directly related to that time step but receive high attention weights.,4.2 Experimental Results,[0],[0]
"This is because those affixes either include the same character which the decoder tries to predict (e.g. i-C for i4 or t-C and tin-C for t1), or frequently appear with that part of the word which includes the target character (e.g. mi-C has a high weight when predicting t1 because t1 belongs to terbiye which frequently collocates with mi-C: terbiye+mi) (good behaviour).
",4.2 Experimental Results,[0],[0]
"Finally, in order to complete our evaluation study we feed the English-to-German NMT model with the sentence ‘Terms and conditions for sending contributions to the BBC’, to show how the model behaves differently and generates a better target sentence.",4.2 Experimental Results,[0],[0]
"Translations generated by our models are illustrated in Table 4.
",4.2 Experimental Results,[0],[0]
"The table demonstrates that our architecture is able to control the decoder and limit its selections, e.g. the word ‘allgemeinen’ generated by the baseline model is redundant.",4.2 Experimental Results,[0],[0]
"There is no constraint to inform the baseline model that this word should not be generated, whereas our proposed architecture controls the decoder in such situations.",4.2 Experimental Results,[0],[0]
"After analyzing our model, we realized that there are strong attention weights assigned to the w-space (indicating white space characters) and BOS (beginning of the sequence) columns of the affix table while sampling the first character of the word ‘Geschäft’, which shows that the decoder is informed about the start point of the sequence.",4.2 Experimental Results,[0],[0]
"Similar to the baseline model’s decoder, our decoder can sample any character including ‘a’ of ‘allgemeinen’ or ‘G’ of ‘Geschäft’.",4.2 Experimental Results,[0],[0]
"Translation information stored in the baseline decoder is not sufficient for selecting the right character ‘G’, so the decoder wrongly starts with ‘i’ and continues along a wrong path up to generating the whole word.",4.2 Experimental Results,[0],[0]
"However, our decoder’s information is accompanied with signals from the affix table which force it to start with a better initial character, whose sampling leads to generating the correct target word.
",4.2 Experimental Results,[0],[0]
Another interesting feature about the table is the new structure ‘Geschäft s bedingungen’ generated by the improved model.,4.2 Experimental Results,[0],[0]
"As the reference translation shows, in the correct form these two structures should be glued together via ‘s’, which can be considered as an infix.",4.2 Experimental Results,[0],[0]
"As our model is supposed to detect this sort of intra-word relation, it treats the whole structure as two compounds which are connected to one another via an infix.",4.2 Experimental Results,[0],[0]
"Although this is not a correct translation and it would be trivial to post-edit into the correct output form, it is interesting to see how our mechanism forces the decoder to pay attention to intra-word relations.
",4.2 Experimental Results,[0],[0]
"Apart from these two interesting findings, the number of wrong character selections in the baseline model is considerably reduced in the improved model because of our enhanced architecture.",4.2 Experimental Results,[0],[0]
In this paper we proposed a new architecture to incorporate morphological information into the NMT pipeline.,5 Conclusion and Future Work,[0],[0]
"We extended the state-of-the-art NMT model (Chung et al., 2016) with a morphology table.",5 Conclusion and Future Work,[0],[0]
The table could be considered as an external knowledge source which is helpful as it increases the capacity of the model by increasing the number of network parameters.,5 Conclusion and Future Work,[0],[0]
We tried to benefit from this advantage.,5 Conclusion and Future Work,[0],[0]
"Moreover, we managed to fill the table with morphological information to further boost the NMT model when translating into MRLs.",5 Conclusion and Future Work,[0],[0]
Apart from the table we also designed an additional output channel which forces the decoder to predict morphological annotations.,5 Conclusion and Future Work,[0],[0]
The error signals coming from the second channel during training inform the decoder with morphological properties of the target language.,5 Conclusion and Future Work,[0],[0]
"Experimental results show that our techniques were useful for NMT of MRLs.
",5 Conclusion and Future Work,[0],[0]
As our future work we follow three main ideas.,5 Conclusion and Future Work,[0],[0]
i),5 Conclusion and Future Work,[0],[0]
We try to find more efficient ways to supply morphological information for both the encoder and decoder. ii),5 Conclusion and Future Work,[0],[0]
"We plan to benefit from other types of information such as syntactic and semantic annotations to boost the decoder, as the table is not limited to morphological information alone and can preserve other sorts of information.",5 Conclusion and Future Work,[0],[0]
iii),5 Conclusion and Future Work,[0],[0]
"Finally, we target sequence generation for fusional languages.",5 Conclusion and Future Work,[0],[0]
"Although our model showed significant improvements for both German and Russian, the proposed model is more suitable for generating sequences in agglutinative languages.",5 Conclusion and Future Work,[0],[0]
"We thank our anonymous reviewers for their valuable feedback, as well as the Irish centre for highend computing (www.ichec.ie) for providing computational infrastructures.",Acknowledgments,[0],[0]
This work has been supported by the ADAPT Centre for Digital Content Technology which is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund.,Acknowledgments,[0],[0]
"Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches.",abstractText,[0],[0]
"However, its performance drops considerably in the presence of morphologically rich languages (MRLs).",abstractText,[0],[0]
Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs.,abstractText,[0],[0]
"Therefore, it is not suitable to exploit existing word-based models to translate this set of languages.",abstractText,[0],[0]
"In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information.",abstractText,[0],[0]
"In our architecture, an additional morphology table is plugged into the model.",abstractText,[0],[0]
"Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder’s current state and constrain it to provide better predictions.",abstractText,[0],[0]
"We evaluated our model to translate English into German, Russian, and Turkish as three MRLs and observed significant improvements.",abstractText,[0],[0]
Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation,title,[0],[0]
"The Gibbs sampler of Geman & Geman (1984), also known as the Glauber dynamics or the heat-bath algorithm, is a leading Markov chain Monte Carlo (MCMC) method for approximating expectations unavailable in closed form.",1. Introduction,[0],[0]
"First detailed as a technique for restoring degraded images (Geman & Geman, 1984), Gibbs sampling has since found diverse applications in statistical physics (Janke, 2008), stochastic optimization and parameter estimation (Geyer, 1991), and Bayesian inference (Lunn et al., 2000).
",1. Introduction,[0],[0]
"The hallmark of any Gibbs sampler is conditional simulation: individual variables are successively simulated from the univariate conditionals of a multivariate target distribu-
1Department of Computer Science, Stanford University, Stanford, CA 94305 USA 2Microsoft Research New England, One Memorial Drive, Cambridge, MA 02142 USA.",1. Introduction,[0],[0]
"Correspondence to: Ioannis Mitliagkas <imit@stanford.edu>, Lester Mackey <lmackey@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tion.",1. Introduction,[0],[0]
"The principal degree of freedom is the scan, the order in which variables are sampled (He et al., 2016).",1. Introduction,[0],[0]
"While it is common to employ a systematic scan, sweeping through each variable in turn, or a uniform random scan, sampling each variable with equal frequency, it is known that nonuniform scans can lead to more accurate inferences both in theory and in practice (Liu et al., 1995; Levine & Casella, 2006).",1. Introduction,[0],[0]
This effect is particularly pronounced when certain variables are of greater inferential interest.,1. Introduction,[0],[0]
"Past approaches to optimizing Gibbs sampler scans were based on asymptotic quality measures approximated with the output of a Markov chain (Levine et al., 2005; Levine & Casella, 2006).
",1. Introduction,[0],[0]
"In this work, we propose a computable non-asymptotic scan quality measure for discrete target distributions based on Dobrushin’s notion of variable influence (Dobrushin & Shlosman, 1985).",1. Introduction,[0],[0]
"We show that for a given subset of variables, this Dobrushin variation (DV) bounds the marginal total variation between a target distribution and T steps of Gibbs sampling with a specified scan.",1. Introduction,[0],[0]
"More generally, Dobrushin variation bounds a weighted total variation based on userinputted importance weights for each variable.",1. Introduction,[0],[0]
We couple this quality measure with an efficient procedure for optimizing scan quality by minimizing Dobrushin variation.,1. Introduction,[0],[0]
"Our Dobrushin-optimized Gibbs samplers (DoGS) come equipped with a guaranteed bound on scan quality, are never worse than the standard uniform random and systematic scans, and can be tailored to a target number of sampling steps and a subset of target variables.",1. Introduction,[0],[0]
"Moreover, Dobrushin variation can be used to evaluate and compare the quality of any user-specified set of scans prior to running any expensive simulations.
",1. Introduction,[0],[0]
"The improvements achieved by DoGS are driven by an inputted matrix, ¯C, of pairwise variable influence bounds discussed in more detail in Section 3.",1. Introduction,[0],[0]
"While DoGS can be used with any discrete distribution, it was designed for targets with total influence k ¯Ck < 1, measured in any matrix norm.",1. Introduction,[0],[0]
"This criterion is known to hold for a variety of distributions, including Ising models with sufficiently high temperatures, hard-core lattice gas models, random graph colorings (Hayes, 2006), and classes of weighted constraint satisfaction problems (Feng et al., 2017).",1. Introduction,[0],[0]
"Moreover, as we will see in Section 4.1, suitable variable influence bounds are readily available for pairwise and binary Markov random fields.",1. Introduction,[0],[0]
"These user-friendly bounds give rise to total
influence k ¯Ck < 1 in all of our experiments and thereby enable improvements in both inferential speed and accuracy over standard scans.
",1. Introduction,[0],[0]
The remainder of the paper is organized as follows.,1. Introduction,[0],[0]
Section 2 reviews Gibbs sampling and standard but computationally intractable measures of Gibbs sampler quality.,1. Introduction,[0],[0]
"In Section 3, we introduce our scan quality measure and its relationship to (weighted) total variation.",1. Introduction,[0],[0]
We describe our procedures for selecting high-quality Gibbs sampler scans in Section 4.,1. Introduction,[0],[0]
"In Section 5, we apply our techniques to three popular applications of the Gibbs sampler: joint image segmentation and object recognition, MCMC maximum likelihood estimation with intractable gradients, and inference in the Ising model.",1. Introduction,[0],[0]
"In each case, we observe substantial improvements in full or marginal total variation over standard scans.",1. Introduction,[0],[0]
"Section 6 presents our conclusions and discussion of future work.
",1. Introduction,[0],[0]
Notation For any vector v and index,1. Introduction,[0],[0]
"i, we let v",1. Introduction,[0],[0]
"i represent the subvector of v with entry v
i removed.",1. Introduction,[0],[0]
We use diag(v) for a square diagonal matrix with v on the diagonal and for element-wise multiplication.,1. Introduction,[0],[0]
"The i-th standard basis vector is denoted by e
i , I represents an identity matrix, 1 signifies a vector of ones, and kCk is the spectral norm of matrix C. We use the shorthand",1. Introduction,[0],[0]
"[p] , {1, . . .",1. Introduction,[0],[0]
", p}.",1. Introduction,[0],[0]
"Consider a target distribution ⇡ on a finite p-dimensional state space, X p.",2. Gibbs sampling and total variation,[0],[0]
"Our inferential goal is to approximate expectations – means, moments, marginals, and more complex function averages, E
⇡
[f(X)]",2. Gibbs sampling and total variation,[0],[0]
"= P
x2Xp ⇡(x)f(x) – under ⇡, but we assume that both exact computation and direct sampling from ⇡ are prohibitive due to the large number of states, |X |p.",2. Gibbs sampling and total variation,[0],[0]
"Markov chain Monte Carlo (MCMC) algorithms attempt to skirt this intractability by simulating a sequence of random vectors X0, X1, . . .",2. Gibbs sampling and total variation,[0],[0]
", XT 2 X p from tractable distributions such that expectations over XT are close to expectations under ⇡.",2. Gibbs sampling and total variation,[0],[0]
"Algorithm 1 summarizes the specific recipe employed by the Gibbs sampler (Geman & Geman, 1984), a leading MCMC algorithm which successively simulates single variables from their tractable conditional distributions.",2.1. Gibbs sampling,[0],[0]
"The principal degree of freedom in a Gibbs sampler is the scan, the sequence of p-dimensional probability vectors q
1 , . . .",2.1. Gibbs sampling,[0],[0]
", q T
determining the probability of resampling each variable on each round of Gibbs sampling.",2.1. Gibbs sampling,[0],[0]
"Typically one selects between the uniform random scan, q
t = (1/p, . . .",2.1. Gibbs sampling,[0],[0]
", 1/p) for all t, where variable indices are selected uniformly at random on each round and the systematic scan, q
t = e (t mod p)+1
for each t, which repeatedly cycles through each variable
Algorithm 1 Gibbs sampling (",2.1. Gibbs sampling,[0],[0]
"Geman & Geman, 1984) input Scan (q
t
)
T t=1 ; starting distribution µ; single-variable conditionals of target distribution, ⇡(·|X i)
",2.1. Gibbs sampling,[0],[0]
"Sample from starting distribution: X0 ⇠ µ for t in 1, 2, . . .",2.1. Gibbs sampling,[0],[0]
", T do
Sample variable index to update using scan: i t ⇠ q t",2.1. Gibbs sampling,[0],[0]
Sample Xt,2.1. Gibbs sampling,[0],[0]
it ⇠ ⇡(·|Xt 1 it ) from its conditional Copy remaining variables: Xt it = X t 1,2.1. Gibbs sampling,[0],[0]
"it
end for
output Sample sequence (Xt)T t=0
in turn.",2.1. Gibbs sampling,[0],[0]
"However, non-uniform scans are known to lead to better approximations (Liu et al., 1995; Levine & Casella, 2006), motivating the need for practical procedures for evaluating and improving Gibbs sampler scans.",2.1. Gibbs sampling,[0],[0]
"Let ⇡ t represent the distribution of the t-th step, Xt, of a Gibbs sampler.",2.2. Total variation,[0],[0]
The quality of a T -step,2.2. Total variation,[0],[0]
"Gibbs sampler and its scan is typically measured in terms of total variation (TV) distance between ⇡
T
and the target distribution ⇡:
Definition 1.",2.2. Total variation,[0],[0]
"The total variation distance between probability measures µ and ⌫ is the maximum difference in expectations over all [0, 1]-valued functions,
kµ ⌫k TV , sup f",2.2. Total variation,[0],[0]
":Xp![0,1] |E µ",2.2. Total variation,[0],[0]
[f(X)],2.2. Total variation,[0],[0]
E ⌫,2.2. Total variation,[0],[0]
[f(Y )],2.2. Total variation,[0],[0]
"|.
We view TV as providing a bound on the bias of a large class of Gibbs sampler expectations; note, however, that TV does not control the variance of these expectations.",2.2. Total variation,[0],[0]
"While we typically sample all p variables in the process of Gibbs sampling, it is common for some variables to be of greater interest than others.",2.3. Marginal and weighted total variation,[0],[0]
"For example, when modeling a large particle system, we may be interested principally in the behavior in local region of the system; likewise, when segmenting an image into its component parts, a particular region, like the area surrounding a face, is often of primary interest.",2.3. Marginal and weighted total variation,[0],[0]
"In these cases, it is more natural to consider a marginal total variation that measures the discrepancy in expectation over only those variables of interest.
",2.3. Marginal and weighted total variation,[0],[0]
Definition 2 (Marginal total variation).,2.3. Marginal and weighted total variation,[0],[0]
The marginal total variation between probability measures µ and ⌫ on a subset of variables S 2,2.3. Marginal and weighted total variation,[0],[0]
"[p] is the maximum difference in expectations over all [0, 1]-valued functions of X
S , the restriction of X to the coordinates in S:
kµ ⌫k S,TV , sup f :X",2.3. Marginal and weighted total variation,[0],[0]
"|S|![0,1] E µ
⇥",2.3. Marginal and weighted total variation,[0],[0]
f X S ⇤ E ⌫ ⇥,2.3. Marginal and weighted total variation,[0],[0]
"f Y S ⇤ .
",2.3. Marginal and weighted total variation,[0],[0]
"More generally, we will seek to control an arbitrary userdefined weighted total variation that assigns an independent non-negative weight to each variable and hence controls the approximation error for functions with varying sensitivities in each variable.",2.3. Marginal and weighted total variation,[0],[0]
Definition 3 (d-bounded differences).,2.3. Marginal and weighted total variation,[0],[0]
We say f : X p !,2.3. Marginal and weighted total variation,[0],[0]
"R has d-bounded differences for d 2 Rd if, for all X,Y 2 X p,
|f(X) f(Y )",2.3. Marginal and weighted total variation,[0],[0]
"|  pX
i=1
d i I[X",2.3. Marginal and weighted total variation,[0],[0]
i 6=,2.3. Marginal and weighted total variation,[0],[0]
"Y i ].
",2.3. Marginal and weighted total variation,[0],[0]
"For example, every function with range [0, 1] is a 1- Lipschitz feature, and the value of the first variable, x 7!",2.3. Marginal and weighted total variation,[0],[0]
"x
1 , is an e
1 -Lipschitz feature.",2.3. Marginal and weighted total variation,[0],[0]
This definition leads to a measure of sample quality tailored to d-bounded difference functions.,2.3. Marginal and weighted total variation,[0],[0]
Definition 4 (d-weighted total variation).,2.3. Marginal and weighted total variation,[0],[0]
"The d-weighted total variation between probability measures µ and ⌫ is the maximum difference in expectations across d-bounded difference functions:
",2.3. Marginal and weighted total variation,[0],[0]
"kµ ⌫k d,TV , sup d bounded di↵erencef |E µ",2.3. Marginal and weighted total variation,[0],[0]
[f(X)],2.3. Marginal and weighted total variation,[0],[0]
E ⌫,2.3. Marginal and weighted total variation,[0],[0]
[f(Y )],2.3. Marginal and weighted total variation,[0],[0]
|,2.3. Marginal and weighted total variation,[0],[0]
"variation
Since the direct computation of total variation measures is typically prohibitive, we will define an efficiently computable upper bound on the weighted total variation of Definition 4.",3. Measuring scan quality with Dobrushin,[0],[0]
"Our construction is inspired by the Gibbs sampler convergence analysis of Dobrushin & Shlosman (1985).
",3. Measuring scan quality with Dobrushin,[0],[0]
"The first step in Dobrushin’s approach is to control total variation in terms of coupled random vectors, (X
t , Y t ) T
t=0 , where Xt has the distribution, ⇡
t , of the t-th step of the Gibbs sampler and Y t follows the target distribution ⇡.",3. Measuring scan quality with Dobrushin,[0],[0]
"For any such coupling, we can define the marginal coupling probability p
t,i , P(Xt i 6=",3. Measuring scan quality with Dobrushin,[0],[0]
Y t i ).,3. Measuring scan quality with Dobrushin,[0],[0]
"The following lemma, a generalization of results in (Dobrushin & Shlosman, 1985; Hayes, 2006), shows that weighted total variation is controlled by these marginal coupling probabilities.",3. Measuring scan quality with Dobrushin,[0],[0]
"The proof is given in Appendix A.1, and similar arguments can be found in Rebeschini & van Handel (2014).",3. Measuring scan quality with Dobrushin,[0],[0]
Lemma 5 (Marginal coupling controls weighted TV).,3. Measuring scan quality with Dobrushin,[0],[0]
"For any joint distribution (X, Y ) such that X ⇠ µ and Y ⇠ ⌫ for probability measures µ and ⌫ on X p and any nonnegative weight vector d 2 Rp,
kµ ⌫k d,TV
 X
i
d i P(X i 6=",3. Measuring scan quality with Dobrushin,[0],[0]
"Y i ).
",3. Measuring scan quality with Dobrushin,[0],[0]
"Dobrushin’s second step is to control the marginal coupling probabilities p
t in terms of influence, a measure of how much a change in variable j affects the conditional distribution of variable i.
Definition 6 (Dobrushin influence matrix).",3. Measuring scan quality with Dobrushin,[0],[0]
"The Dobrushin influence of variable j on variable i is given by
C ij , max (X,Y )",3. Measuring scan quality with Dobrushin,[0],[0]
"2Nj k⇡(·|X i) ⇡(·|Y i)kTV (1)
where (X, Y ) 2 N j signifies X l = Y l for all l 6=",3. Measuring scan quality with Dobrushin,[0],[0]
"j.
This influence matrix is at the heart of our efficiently computable measure of scan quality, Dobrushin variation.",3. Measuring scan quality with Dobrushin,[0],[0]
Definition 7 (Dobrushin variation).,3. Measuring scan quality with Dobrushin,[0],[0]
"For any nonnegative weight vector d 2 Rp and entrywise upper bound ¯C on the Dobrushin influence (1), we define the Dobrushin variation of a scan (q
t
)
T t=1
as
V(q 1 , . .",3. Measuring scan quality with Dobrushin,[0],[0]
.,3. Measuring scan quality with Dobrushin,[0],[0]
", q T ; d, ¯C) , d>B(q T ) · · · B(q 1 )1
for B(q) , (I diag(q)(I ¯C)).
",3. Measuring scan quality with Dobrushin,[0],[0]
Theorem 8 shows that Dobrushin variation dominates weighted TV and thereby provides target- and scan-specific guarantees on the weighted TV quality of a Gibbs sampler.,3. Measuring scan quality with Dobrushin,[0],[0]
"The proof in Appendix A.2 rests on the fact that, for each t, b
t , B(q t ) · · · B(q 1 )1 provides an elementwise upper bound on the vector of marginal coupling probabilities, p
t .",3. Measuring scan quality with Dobrushin,[0],[0]
Theorem 8 (Dobrushin variation controls weighted TV).,3. Measuring scan quality with Dobrushin,[0],[0]
"Suppose that ⇡
T is the distribution of the T -th step of a Gibbs sampler with scan (q
t
)
T t=1 .",3. Measuring scan quality with Dobrushin,[0],[0]
"Then, for any nonnegative weight vector d 2 Rp and entrywise upper bound ¯C on the Dobrushin influence (1),
k⇡ T ⇡k d,TV  V (q t ) T t=1 ; d, ¯C .",3. Measuring scan quality with Dobrushin,[0],[0]
We next present an efficient algorithm for improving the quality of any Gibbs sampler scan by minimizing Dobrushin variation.,4. Improving scan quality with DoGS,[0],[0]
We will refer to the resulting customized Gibbs samplers as Dobrushin-optimized Gibbs samplers or DoGS for short.,4. Improving scan quality with DoGS,[0],[0]
"Algorithm 2 optimizes Dobrushin variation using coordinate descent, with the selection distribution q
t for each time step serving as a coordinate.",4. Improving scan quality with DoGS,[0],[0]
"Since Dobrushin variation is linear in each q
t , each coordinate optimization (in the absence of ties) selects a degenerate distribution, a single coordinate, yielding a fully deterministic scan.",4. Improving scan quality with DoGS,[0],[0]
"If m  p is a bound on the size of the Markov blanket of each variable, then our forward-backward algorithm runs in time O(kdk
0 + min(m log p + m2, p)T ) with O(p + T ) storage for deterministic input scans.",4. Improving scan quality with DoGS,[0],[0]
"The T (m log p + m2) term arises from maintaining the derivative vector, w, in an efficient sorting structure, like a max-heap.
",4. Improving scan quality with DoGS,[0],[0]
"A user can initialize DoGS with any baseline scan, including a systematic or uniform random scan, and the resulting customized scan is guaranteed to have the same or better Dobrushin variation.",4. Improving scan quality with DoGS,[0],[0]
"Moreover, DoGS scans will always
Algorithm 2 DoGS: Scan selection via coordinate descent input Scan (q
⌧
)
T ⌧=1 ; variable weights d; influence entrywise upper bound ¯C; (optional) target accuracy ✏.
//",4. Improving scan quality with DoGS,[0],[0]
Forward:,4. Improving scan quality with DoGS,[0],[0]
"Precompute coupling bounds of Section 3, // b
t = B(q t ) · · · B(q 1 )1 = B(q t )b t 1 with b0 = 1.
//",4. Improving scan quality with DoGS,[0],[0]
Only store b = b T and sequence of changes ( b t ) T 1 t=0 .,4. Improving scan quality with DoGS,[0],[0]
//,4. Improving scan quality with DoGS,[0],[0]
"Also precompute Dobrushin variation V = d>b
T
// and derivatives w = @V/@q T = d",4. Improving scan quality with DoGS,[0],[0]
(I ¯C)b T .,4. Improving scan quality with DoGS,[0],[0]
"b 1, V d>b, w d",4. Improving scan quality with DoGS,[0],[0]
"(I ¯C)b for t in 1, 2, . . .",4. Improving scan quality with DoGS,[0],[0]
"T do
b t 1 diag (qt)(I ¯C)b",4. Improving scan quality with DoGS,[0],[0]
"b b b
t 1 // bt = bt 1 bt 1 V V d> b
t 1 // V = d>bt w w + d (I ¯C)",4. Improving scan quality with DoGS,[0],[0]
"b
t 1 //",4. Improving scan quality with DoGS,[0],[0]
"w = d (I ¯C)bt end for
",4. Improving scan quality with DoGS,[0],[0]
"// Backward: Optimize scan one step, q⇤ t , at a time.",4. Improving scan quality with DoGS,[0],[0]
"for t in T, T 1, . . .",4. Improving scan quality with DoGS,[0],[0]
", 1 do
If V  ✏, then q⇤ t
q t ; break // early stopping b b + b
t 1 // bt 1 = bt + bt 1 //",4. Improving scan quality with DoGS,[0],[0]
"Update w = @V/@q
t = d t (I ¯C)b t 1
// for d> t , d>B(q⇤ T ) · · · B(q⇤ t+1 ) and d> T , d> w w d",4. Improving scan quality with DoGS,[0],[0]
(I ¯C),4. Improving scan quality with DoGS,[0],[0]
b t 1 //,4. Improving scan quality with DoGS,[0],[0]
"Pick probability vector q⇤
t minimizing d> t B(q t )b",4. Improving scan quality with DoGS,[0],[0]
"t 1
q⇤ t
e argmini wi
V V + d>",4. Improving scan quality with DoGS,[0],[0]
diag(q⇤ t q t )b,4. Improving scan quality with DoGS,[0],[0]
"// V = d> t 1bt 1
d >",4. Improving scan quality with DoGS,[0],[0]
d>,4. Improving scan quality with DoGS,[0],[0]
"diag(q⇤ t
)",4. Improving scan quality with DoGS,[0],[0]
(I ¯C),4. Improving scan quality with DoGS,[0],[0]
d> d> d> //,4. Improving scan quality with DoGS,[0],[0]
"d>
t 1 = d > t B(q⇤ t )
w",4. Improving scan quality with DoGS,[0],[0]
w+ d (I ¯C)b //,4. Improving scan quality with DoGS,[0],[0]
"w = d t 1 (I ¯C)bt 1
end for
output Optimized scan (q ⌧ ) t 1 ⌧=1 , (q⇤ ⌧ ) T ⌧=t
be d-ergodic (i.e., k⇡ T ⇡k d,TV ! 0",4. Improving scan quality with DoGS,[0],[0]
"as T ! 1) when initialized with a systematic or uniform random scan and ¯C
< 1.",4. Improving scan quality with DoGS,[0],[0]
"This follows from the following proposition, which shows that Dobrushin variation—and hence the dweighted total variation by Theorem 8—goes to 0 under these conditions and standard scans.",4. Improving scan quality with DoGS,[0],[0]
"The proof relies on arguments in (Hayes, 2006) and is outlined in Appendix A.3.",4. Improving scan quality with DoGS,[0],[0]
Proposition 9.,4. Improving scan quality with DoGS,[0],[0]
"Suppose that ¯C is an entrywise upper bound on the Dobrushin influence matrix (1) and that (q
t
)
T t=1 is a systematic or uniform random scan.",4. Improving scan quality with DoGS,[0],[0]
"If ¯C < 1, then, for any nonnegative weight vector d, the Dobrushin variation vanishes as the chain length T increases.",4. Improving scan quality with DoGS,[0],[0]
"That is,
lim T!1 V(q 1 , . . .",4. Improving scan quality with DoGS,[0],[0]
", q T ; d, ¯C) = 0.",4. Improving scan quality with DoGS,[0],[0]
An essential input to our algorithms is the entrywise upper bound ¯C on the influence matrix (1).,4.1. Bounding influence,[0],[0]
"Fortunately, Liu &
Domke (2014) showed that useful influence bounds are particularly straightforward to compute for any pairwise Markov random field (MRF) target,
⇡(X) / exp( P
i,j P a,b2X ✓ ij
ab
I[X i = a, X j = b]).",4.1. Bounding influence,[0],[0]
"(2)
Theorem 10 (Pairwise MRF influence (Liu & Domke, 2014, Lems. 10, 11)).",4.1. Bounding influence,[0],[0]
"Using the shorthand (s) , 1
1+e s , the influence (1) of the target ⇡ in (2) satisfies
C ij  max xj ,yj |2 ( 1 2 max a,b (✓ij axj ✓ij ayj ) (✓ij bxj ✓ij byj ))",4.1. Bounding influence,[0],[0]
"1|.
",4.1. Bounding influence,[0],[0]
Pairwise MRFs with binary variables X,4.1. Bounding influence,[0],[0]
"i 2 { 1, 1} are especially common in statistical physics and computer vision.",4.1. Bounding influence,[0],[0]
"A general parameterization for binary pairwise MRFs is given by
⇡(X) / exp( P i 6=j ✓ijXiXj + P i",4.1. Bounding influence,[0],[0]
✓,4.1. Bounding influence,[0],[0]
"i X i ), (3)
and our next theorem, proved in Appendix A.4, leverages the strength of the singleton parameters ✓
i to provide a tighter bound on the influence of these targets.",4.1. Bounding influence,[0],[0]
Theorem 11 (Binary pairwise influence).,4.1. Bounding influence,[0],[0]
"The influence (1) of the target ⇡ in (3) satisfies
C ij
 |exp(2✓ij) exp( 2✓ij)| b ⇤
(1 + b⇤ exp(2✓ ij ))",4.1. Bounding influence,[0],[0]
"(1 + b⇤ exp( 2✓ ij ))
for b⇤ = max(e 2 P k 6=j |✓ik| 2✓i , min[e2 P k 6=j |✓ik| 2✓i , 1]).
",4.1. Bounding influence,[0],[0]
"Theorem 11 in fact provides an exact computation of the Dobrushin influence C
ij whenever b⇤ 6= 1.",4.1. Bounding influence,[0],[0]
"The only approximation comes from the fact that the value b⇤ = 1 may not belong to the set B = {e2 P k 6=j ✓ikXk 2✓i | X 2 { 1, 1}p}.",4.1. Bounding influence,[0],[0]
"An exact computation of C ij
would replace the cutoff of 1 with its closest approximation in B.
So far, we have focused on bounding influence in pairwise MRFs, as these bounds are most relevant to our experiments; indeed, in Section 5, we will use DoGS in conjunction with the bounds of Theorems 10 and 11 to improve scan quality for a variety of inferential tasks.",4.1. Bounding influence,[0],[0]
"However, user-friendly bounds are also available for non-pairwise MRFs (note that any discrete distribution can be represented as an MRF with parameters in the extended reals), and we include a simple extension of Theorem 11 that applies to binary MRFs with higher-order interactions.",4.1. Bounding influence,[0],[0]
Its proof is in Appendix A.5 Theorem 12 (Binary higher-order influence).,4.1. Bounding influence,[0],[0]
"The target
⇡(X) / exp( P S2S ✓S Q k2S",4.1. Bounding influence,[0],[0]
Xk + P,4.1. Bounding influence,[0],[0]
"i ✓ i X i ),
for X 2 { 1, 1}d and S a set of non-singleton subsets of [p], has influence (1) satisfying
C",4.1. Bounding influence,[0],[0]
"ij
 |",4.1. Bounding influence,[0],[0]
"exp(2 P S2S:i,j2S |✓S |) exp( 2 P S2S:i,j2S |✓S |)| b ⇤
(1+b ⇤ ) 2
for b⇤ = max(exp( 2 P
S2S:i2S,j /2S |",4.1. Bounding influence,[0],[0]
"✓S | 2✓
i
), min(exp(2 P
S2S:i2S,j /2S |",4.1. Bounding influence,[0],[0]
"✓S | 2✓i), 1)).",4.1. Bounding influence,[0],[0]
"In related work, Latuszynski et al. (2013) recently analyzed an abstract class of adaptive Gibbs samplers parameterized by an arbitrary scan selection rule.",4.2. Related Work,[0],[0]
"However, as noted in their Rem. 5.13, no explicit scan selection rules were provided in that paper.",4.2. Related Work,[0],[0]
"The only prior concrete scan selection rules of which we are aware are the Minimax Adaptive Scans with asymptotic variance or convergence rate objective functions (Levine & Casella, 2006).",4.2. Related Work,[0],[0]
"Unless some substantial approximation is made, it is unclear how to implement these procedures when the target distribution of interest is not Gaussian.
",4.2. Related Work,[0],[0]
"Levine & Casella (2006) approximate these Minimax Adaptive Scans for specific mixture models by considering single ad hoc features of interest; the approach has many hyperparameters to tune including the order of the Taylor expansion approximation, which sample points are used to approximate asymptotic quantities online, and the frequency of adaptive updating.",4.2. Related Work,[0],[0]
"Our proposed quality measure, Dobrushin variation, requires no approximation or tuning and can be viewed as a practical non-asymptotic objective function for the abstract scan selection framework of Levine and Casella.",4.2. Related Work,[0],[0]
"In the spirit of (Lacoste-Julien et al., 2011), DoGS can also be viewed as an approximate inference scheme calibrated for downstream inferential tasks depending only on subsets of variables.
",4.2. Related Work,[0],[0]
Levine et al. (2005) employ the Minimax Adaptive Scans of Levine and Casella by finding the mode of their target distribution using EM and then approximating the distribution by a Gaussian.,4.2. Related Work,[0],[0]
They report that this approach to scan selection introduces substantial computational overhead (10 minutes of computation for an Ising model with 64 variables).,4.2. Related Work,[0],[0]
"As we will see in Section 5, the overhead of DoGS scan selection is manageable (15 seconds of computation for an Ising model with 1 million variables) and outweighed by the increase in scan quality and sampling speed.",4.2. Related Work,[0],[0]
"In this section, we demonstrate how our proposed scan quality measure and efficient optimization schemes can be used to both evaluate and improve Gibbs sampler scans when either the full distribution or a marginal distribution is of principal interest.",5. Experiments,[0],[0]
"For all experiments with binary MRFs, we adopt the model parameterization of (3) (with no additional temperature parameter) and use Theorem 11 to produce the Dobrushin influence bound ¯C. On all ensuing plots, the numbers in the legend state the best guarantee achieved for each algorithm plotted.",5. Experiments,[0],[0]
"Due to space constraints, we display only one representative plot per experiment; the analogous plots from independent replicates of each experiment can be found in Appendix B.
2",5. Experiments,[0],[0]
"In our first experiment, we illustrate how Dobrushin variation can be used to select between standard scans and how DoGS can be used to efficiently improve upon standard scan quality when total variation quality is of interest.",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
We remind the reader that both scan evaluation and scan selection are performed offline prior to any expensive simulation from the Gibbs sampler.,5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"Our target is a 10 ⇥ 10 Ising model arranged in a two-dimensional lattice, a standard model of ferromagnetism in statistical physics.",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"In the notation of (3), we draw the unary parameters ✓
i uniformly at random from {0, 1}, and the interaction parameters uniformly at random: ✓
ij
⇠ Uniform([0, 0.25]).
",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"Figure 1 compares, as a function of the number of steps T , the total variation guarantee provided by Dobrushin variation (see Theorem 8) for the standard systematic and uniform random scans.",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"We see that the systematic scan, which traverses variables in row major order, obtains a significantly better TV guarantee than its uniform random counterpart for all sampling budgets T .",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"Hence, the systematic scan would be our standard scan of choice for this target.",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
DoGS (Algorithm 2) initialized with d = 1 and the systematic scan further improves the systematic scan guarantee by two orders of magnitude.,5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
Iterating Algorithm 2 on its own scan output until convergence (“Iterated DoGS” in Figure 1) provides additional improvement.,5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"However, since we consistently find that the bulk of the improvement is obtained with a single run of Algorithm 2, non-iterated DoGS remains our recommended recipe for quickly improving scan quality.
",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"Note that since our TV guarantee is an upper bound provided by the exact computation of Dobrushin variation, the actual gains in TV may differ from the gains in Dobrushin variation.",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"In practice and as evidenced in Section 5.4, we find that the actual gains in (marginal) TV over standard scans are typically larger than the Dobrushin variation gains.",5.1. Evaluating and optimizing Gibbs sampler scans,[0],[0]
"In this experiment, we demonstrate that using DoGS to optimize a scan can result in dramatic inferential speed-ups.",5.2. End-to-end wall clock time performance,[0],[0]
This effect is particularly pronounced for targets with a large number of variables and in settings that require repeated sampling from a low-bias Gibbs sampler.,5.2. End-to-end wall clock time performance,[0],[0]
"The setting is the exactly same as in the previous experiment, with the exception of model size: here we simulate a 103⇥103 Ising model, with 1 million variables in total.",5.2. End-to-end wall clock time performance,[0],[0]
"We target a single marginal X
1 with d = e 1 and take a systematic scan of length T = 2 ⇥ 106 as our input scan.",5.2. End-to-end wall clock time performance,[0],[0]
"After measuring the Dobrushin variation ✏ of the systematic scan, we use an efficient length-doubling scheme to select a DoGS scan: (0) initialize ˜T = 2; (1) run Algorithm 2 with the first ˜T steps of the systematic scan as input; (2) if the resulting DoGS scan has Dobrushin variation less than ✏, we keep it; otherwise we double ˜T and return to step (1).",5.2. End-to-end wall clock time performance,[0],[0]
"The resulting DoGS scan has length ˜T = 16.
0
We repeatedly draw independent sample points from either the length T systematic scan Gibbs sampler or the length ˜T DoGS scan Gibbs sampler.",5.2. End-to-end wall clock time performance,[0],[0]
"Figure 2 evaluates the bias of the resulting Monte Carlo estimates of E
⇡",5.2. End-to-end wall clock time performance,[0],[0]
"[X 1 ] as a function of time, including the 15s of setup time for DoGS on this 1 million variable model.",5.2. End-to-end wall clock time performance,[0],[0]
"In comparison, Levine et al. (2005) report 10 minutes of setup time for their adaptive Gibbs scans when processing a 64 variable Ising model.",5.2. End-to-end wall clock time performance,[0],[0]
"The bottom plot of Figure 2 uses the average measured time for a single step1, the measured setup time for DoGS and the size of the two scan sequences to give an estimate of the speedup as a function of the number of sample points drawn.",5.2. End-to-end wall clock time performance,[0],[0]
Additional timing experiments are deferred to Appendix B.2.,5.2. End-to-end wall clock time performance,[0],[0]
"estimation
We next illustrate how DoGS can be used to accelerate MCMC maximum likelihood estimation, while providing guarantees on parameter estimation quality.",5.3. Accelerated MCMC maximum likelihood,[0],[0]
"We replicate
1Each Gibbs step took 12.65µs on a 2015 Macbook Pro.
the Ising model maximum likelihood estimation experiment of (Domke, 2015, Sec. 6) and show how we can provide the same level of accuracy faster.",5.3. Accelerated MCMC maximum likelihood,[0],[0]
Our aim is to learn the parameters of binary MRFs based on training samples with independent Rademacher entries.,5.3. Accelerated MCMC maximum likelihood,[0],[0]
"On each step of MCMCMLE, Domke uses Gibbs sampling with a uniform random scan to produce an estimate of the gradient of the log likelihood.",5.3. Accelerated MCMC maximum likelihood,[0],[0]
"Our DoGS variant employs Algorithm 2 with d = 1, early stopping parameter ✏ = 0.01, and a Dobrushin influence bound constructed from the latest parameter estimate ˆ✓ using Theorem 11.",5.3. Accelerated MCMC maximum likelihood,[0],[0]
"We set the number of gradient steps, MC steps per gradient, and independent runs of Gibbs sampling to the suggested values in (Domke, 2015).",5.3. Accelerated MCMC maximum likelihood,[0],[0]
"After each gradient update, we record the distance between the optimal and estimated parameters.",5.3. Accelerated MCMC maximum likelihood,[0],[0]
Figure 3 displays the estimation error of five independent replicates of this experiment using each of two scans (uniform or DoGS) for two models (a 3⇥ 3 and a 4⇥ 4 Ising model).,5.3. Accelerated MCMC maximum likelihood,[0],[0]
The results show that DoGS consistently achieves the desired parameter accuracy much more quickly than standard Gibbs.,5.3. Accelerated MCMC maximum likelihood,[0],[0]
In this section we demonstrate how DoGS can be used to dramatically speed up marginal inference while providing target-dependent guarantees.,5.4. Customized scans for fast marginal mixing,[0],[0]
"We use a 40⇥ 40 non-toroidal Ising model and set our feature to be the top left variable with d = e
1 .",5.4. Customized scans for fast marginal mixing,[0],[0]
Figure 4 compares guarantees for a uniform random scan and a systematic scan; we also see how we can further improve the total variation guarantees by feeding a systematic scan into Algorithm 2.,5.4. Customized scans for fast marginal mixing,[0],[0]
"Again we see that a single run of Algorithm 2 yields the bulk of the improvement, and iterated applications only provide small further benefits.",5.4. Customized scans for fast marginal mixing,[0],[0]
"For the DoGS sequence, the figure also shows a histogram of the distance of sampled variables from the target variable, X
1
, at the top left corner of the grid.
",5.4. Customized scans for fast marginal mixing,[0],[0]
Figure 5 shows that optimizing our objective actually improves performance by reducing the marginal bias much more quickly than systematic scan.,5.4. Customized scans for fast marginal mixing,[0],[0]
"For completeness, we include additional experiments on a toroidal Ising model in Appendix B.3.",5.4. Customized scans for fast marginal mixing,[0],[0]
"recognition
The Markov field aspect model (MFAM) of Verbeek & Triggs (2007) is a generative model for images designed to automatically divide an image into its constituent parts (image segmentation) and label each part with its semantic object class (object recognition).",5.5. Targeted image segmentation and object,[0],[0]
"For each test image k, the MFAM extracts a discrete feature descriptor from each image patch i, assigns a latent object class label X
i
2 X to
each patch, and induces the posterior distribution
⇡(X|y; k) / exp( P
(i,j) spatial neighbors I{Xi = Xj} (4) +",5.5. Targeted image segmentation and object,[0],[0]
"P i log( P a2X ✓k,a a,yiI{Xi = a})),
over the configuration of patch levels X .",5.5. Targeted image segmentation and object,[0],[0]
"When the Potts parameter = 0, this model reduces to probabilistic latent semantic analysis (PLSA) (Hofmann, 2001), while a positive value of encourages nearby patches to belong to similar classes.",5.5. Targeted image segmentation and object,[0],[0]
"Using the Microsoft Research Cambridge (MSRC) pixel-wise labeled image database v12, we follow the weakly supervised setup of Verbeek & Triggs (2007) to fit the PLSA parameters ✓ and to a training set of images and then, for each test image k, use Gibbs sampling to generate patch label configurations X targeting the MFAM posterior (4) with = 0.48.",5.5. Targeted image segmentation and object,[0],[0]
"We generate a segmentation by assigning each patch the most frequent label encountered during Gibbs sampling and evaluate the accuracy of this labeling using the Hamming error described in (Verbeek & Triggs, 2007).",5.5. Targeted image segmentation and object,[0],[0]
"This experiment is repeated over 20 indepen-
2http://research.microsoft.com/vision/cambridge/recognition/
dently generated 90% training / 10% test partitions of the 240 image dataset.
",5.5. Targeted image segmentation and object,[0],[0]
"We select our DoGS scan to target a 12⇥ 8 marginal patch rectangle at the center of each image (the {0,1} entries of d indicate whether a patch is in the marginal rectangle highlighted in Figure 6) and compare its segmentation accuracy and efficiency with that of a standard systematic scan of length T = 620.",5.5. Targeted image segmentation and object,[0],[0]
"We initialize DoGS with the systematic scan, the influence bound ¯C of Theorem 10, and a target accuracy ✏ equal to the marginal Dobrushin variation guarantee of the systematic scan.",5.5. Targeted image segmentation and object,[0],[0]
"In 11.5ms, the doubling scheme described in Section 5.2 produced a DoGS sequence of length 110 achieving the Dobrushin variation guarantee ✏ on marginal TV.",5.5. Targeted image segmentation and object,[0],[0]
Figure 7 shows that DoGS achieves a slightly better average Hamming error than systematic scan using a 5.5⇥ shorter sequence.,5.5. Targeted image segmentation and object,[0],[0]
"Systematic scan takes 1.2s to resample each variable of interest, while DoGS consumes 0.37s.",5.5. Targeted image segmentation and object,[0],[0]
"Moreover, the 11.5ms DoGS scan selection was performed only once and then used to segment all test images.",5.5. Targeted image segmentation and object,[0],[0]
"For
each chain, X0 was initialized to the maximum a posteriori patch labeling under the PLSA model (obtained by setting = 0",5.5. Targeted image segmentation and object,[0],[0]
in the MFAM).,5.5. Targeted image segmentation and object,[0],[0]
We introduced a practical quality measure – Dobrushin variation – for evaluating and comparing existing Gibbs sampler scans and efficient procedures – DoGS – for developing customized fast-mixing scans tailored to marginals or distributional features of interest.,6. Discussion,[0],[0]
"We deployed DoGS for three common Gibbs sampler applications – joint image segmentation and object recognition, MCMC maximum likelihood estimation, and Ising model inference – and in each case achieved higher quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.",6. Discussion,[0],[0]
"In the future, we aim to enlist DoGS for additional applications in computer vision and natural language processing, extend the reach of DoGS to models containing continuous variables, and integrate DoGS into large inference engines built atop Gibbs sampling.",6. Discussion,[0],[0]
The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling.,abstractText,[0],[0]
"In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a discrete Gibbs sampler.",abstractText,[0],[0]
"Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers.",abstractText,[0],[0]
"In our experiments with joint image segmentation and object recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.",abstractText,[0],[0]
Improving Gibbs Sampler Scan Quality with DoGS,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2355–2365, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"In many realistic domains, information extraction (IE) systems require exceedingly large amounts of annotated data to deliver high performance.",1 Introduction,[0],[0]
Increases in training data size enable models to handle robustly the multitude of linguistic expressions that convey the same semantic relation.,1 Introduction,[0],[0]
"Consider, for instance, an IE system that aims to identify entities such as the perpetrator and the number of vic-
1Code is available at http://people.csail.mit.",1 Introduction,[0],[0]
"edu/karthikn/rl-ie/
tims in a shooting incident (Figure 1).",1 Introduction,[0],[0]
"The document does not explicitly mention the shooter (Scott Westerhuis), but instead refers to him as a suicide victim.",1 Introduction,[0],[0]
"Extraction of the number of fatally shot victims is similarly difficult, as the system needs to infer that ""A couple and four children"" means six people.",1 Introduction,[0],[0]
"Even a large annotated training set may not provide sufficient coverage to capture such challenging cases.
",1 Introduction,[0],[0]
"In this paper, we explore an alternative approach for boosting extraction accuracy, when a large training corpus is not available.",1 Introduction,[0],[0]
"Instead, the proposed method utilizes external information sources to resolve ambiguities inherent in text interpretation.",1 Introduction,[0],[0]
"Specifically, our strategy is to find other documents that contain the information sought, expressed in a form that a basic extractor can ""understand"".",1 Introduction,[0],[0]
"For instance, Figure 2 shows two other articles describing the same event, wherein the entities of interest
2355
– the number of people killed and the name of the shooter – are expressed explicitly.",1 Introduction,[0],[0]
"Processing such stereotypical phrasing is easier for most extraction systems, compared to analyzing the original source document.",1 Introduction,[0],[0]
"This approach is particularly suitable for extracting information from news where a typical event is covered by multiple news outlets.
",1 Introduction,[0],[0]
"The challenges, however, lie in (1) performing event coreference (i.e. retrieving suitable articles describing the same incident) and (2) reconciling the entities extracted from these different documents.",1 Introduction,[0],[0]
Querying the web (using the source article’s title for instance) often retrieves documents about other incidents with a tangential relation to the original story.,1 Introduction,[0],[0]
"For example, the query “4 adults, 1 teenager shot in west Baltimore 3 april 2015” yields only two relevant articles among the top twenty results on Bing search, while returning other shooting events at the same location.",1 Introduction,[0],[0]
"Moreover, the values extracted from these different sources require resolution since some of them might be inaccurate.
",1 Introduction,[0],[0]
"One solution to this problem would be to perform a single search to retrieve articles on the same event and then reconcile values extracted from them (say, using a meta-classifier).",1 Introduction,[0],[0]
"However, if the confidence of the new set of values is still low, we might wish to perform further queries.",1 Introduction,[0],[0]
"Thus, the problem is inherently sequential, requiring alternating phases of querying to retrieve articles and integrating the extracted values.
",1 Introduction,[0],[0]
"We address these challenges using a Reinforcement Learning (RL) approach that combines query formulation, extraction from new sources, and value
reconciliation.",1 Introduction,[0],[0]
"To effectively select among possible actions, our state representation encodes information about the current and new entity values along with the similarity between the source article and the newly retrieved document.",1 Introduction,[0],[0]
"The model learns to select good actions for both article retrieval and value reconciliation in order to optimize the reward function, which reflects extraction accuracy and includes penalties for extra moves.",1 Introduction,[0],[0]
"We train the RL agent using a Deep Q-Network (DQN) (Mnih et al., 2015) that is used to predict both querying and reconciliation choices simultaneously.",1 Introduction,[0],[0]
"While we use a maximum entropy model as the base extractor, this framework can be inherently applied to other extraction algorithms.
",1 Introduction,[0],[0]
We evaluate our system on two datasets where available training data is inherently limited.,1 Introduction,[0],[0]
The first dataset is constructed from a publicly available database of mass shootings in the United States.,1 Introduction,[0],[0]
The database is populated by volunteers and includes the source articles.,1 Introduction,[0],[0]
The second dataset is derived from a FoodShield database of illegal food adulterations.,1 Introduction,[0],[0]
Our experiments demonstrate that the final RL model outperforms basic extractors as well as a meta-classifier baseline in both domains.,1 Introduction,[0],[0]
"For instance, in the Shootings domain, the average accuracy improvement over the meta-classifier is 7%.",1 Introduction,[0],[0]
"Open Information Extraction Existing work in open IE has used external sources from the web to improve extraction accuracy and coverage (Agichtein and Gravano, 2000; Etzioni et al., 2011; Fader et al., 2011; Wu and Weld, 2010).",2 Related Work,[0],[0]
"Such research has focused on identifying multiple instances of the same relation, independent of the context in which this information appears.",2 Related Work,[0],[0]
"In contrast, our goal is to extract information from additional sources about a specific event described in a source article.",2 Related Work,[0],[0]
"Therefore, the novel challenge of our task resides in performing event coreference (Lee et al., 2012; Bejan and Harabagiu, 2014) (i.e identifying other sources describing the same event) while simultaneously reconciling extracted information.",2 Related Work,[0],[0]
"Moreover, relations typically considered by open IE systems have significantly higher coverage in online documents than a specific incident described in
a few news sources.",2 Related Work,[0],[0]
"Hence, we require a different mechanism for finding and reconciling online information.
",2 Related Work,[0],[0]
"Entity linking, multi-document extraction and event coreference Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input documents (Mann and Yarowsky, 2005; Han et al., 2011; Durrett and Klein, 2014).",2 Related Work,[0],[0]
"Since this setup already includes multiple input documents, the model is not required to look for additional sources or decide on their relevance.",2 Related Work,[0],[0]
"Also, while the set of input documents overlap in terms of entities mentioned, they do not necessarily describe the same event.",2 Related Work,[0],[0]
"Given these differences in setup, the challenges and opportunities of the two tasks are distinct.
",2 Related Work,[0],[0]
"Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015).",2 Related Work,[0],[0]
"Though our work also aims at increasing extraction recall for a database, traditional KBC approaches do not require searching for additional sources of information.",2 Related Work,[0],[0]
West et al. (2014) explore query reformulation in the context of KBC.,2 Related Work,[0],[0]
"Using existing search logs, they learn how to formulate effective queries for different types of database entries.",2 Related Work,[0],[0]
"Once query learning is completed, the model employs several selected queries, and then aggregates the results based on retrieval ranking.",2 Related Work,[0],[0]
"This approach is complementary to the proposed method, and can be combined with our approach if search logs are available.
",2 Related Work,[0],[0]
Kanani and McCallum (2012) also combine search and information extraction.,2 Related Work,[0],[0]
"In their task of faculty directory completion, the system has to find documents from which to extract desired information.",2 Related Work,[0],[0]
"They employ reinforcement learning to address computational bottlenecks, by minimizing the number of queries, document downloads and extraction action.",2 Related Work,[0],[0]
"The extraction accuracy is not part of this optimization, since the baseline IE system achieves high performance on the relations of interest.",2 Related Work,[0],[0]
"Hence, given different design goals, the two RL formulations are very different.",2 Related Work,[0],[0]
"Our approach is also close
in spirit to the AskMSR system (Banko et al., 2002) which aims at using information redundancy on the web to better answer questions.",2 Related Work,[0],[0]
"Though our goal is similar, we learn to query and consolidate the different sources of information instead of using predefined rules.",2 Related Work,[0],[0]
"Several slot-filling methods have experimented with query formulation over web-based corpora to populate knowledge bases (Surdeanu et al., 2010; Ji and Grishman, 2011).",2 Related Work,[0],[0]
"We model the information extraction task as a markov decision process (MDP), where the model learns to utilize external sources to improve upon extractions from a source article (see Figure 3).",3 Framework,[0],[0]
The MDP framework allows us to dynamically incorporate entity predictions while also providing flexibility to choose the type of articles to extract from.,3 Framework,[0],[0]
"At each step, the system has to reconcile extracted values from a related article (enew) with the current set of values (ecur), and decide on the next query for retrieving more articles.
",3 Framework,[0],[0]
"We represent the MDP as a tuple 〈S,A, T,R〉, where S = {s} is the space of all possible states, A = {a = (d, q)} is the set of all actions, R(s, a) is the reward function, and T (s′|s, a) is the transition function.",3 Framework,[0],[0]
"We describe these in detail below.
",3 Framework,[0],[0]
States,3 Framework,[0],[0]
"The state s in our MDP consists of the extractor’s confidence in predicted entity values, the context from which the values are extracted and the similarity between the new document and the original one.",3 Framework,[0],[0]
We represent the state as a continuous realvalued vector (Figure 3) incorporating these pieces of information: 1.,3 Framework,[0],[0]
"Confidence scores of current and newly extracted
entity values.",3 Framework,[0],[0]
2.,3 Framework,[0],[0]
"One-hot encoding of matches between current
and new values.",3 Framework,[0],[0]
3. Unigram/tf-idf counts2 of context words.,3 Framework,[0],[0]
"These
are words that occur in the neighborhood of the entity values in a document (e.g. the words which, left, people and wounded in the phrase “which left 5 people wounded”).",3 Framework,[0],[0]
4. tf-idf similarity between the original article and the new article.,3 Framework,[0],[0]
"2Counts are computed on the documents used to train the basic extraction system.
",3 Framework,[0],[0]
"Actions At each step, the agent is required to take two actions - a reconciliation decision d and a query choice q.",3 Framework,[0],[0]
"The decision d on the newly extracted values can be one of the following types: (1) accept a specific entity’s value (one action per entity)3, (2) accept all entity values, (3) reject all values or (4) stop.",3 Framework,[0],[0]
"In cases 1-3, the agent continues to inspect more articles, while the episode ends if a stop action (4) is chosen.",3 Framework,[0],[0]
"The current values and confidence scores are simply updated with the accepted values and the corresponding confidences.4 The choice q is used to choose the next query from a set of automatically generated alternatives (details below) in order to retrieve the next article.
",3 Framework,[0],[0]
Rewards The reward function is chosen to maximize the final extraction accuracy while minimizing the number of queries.,3 Framework,[0],[0]
"The accuracy component is calculated using the difference between the accuracy of the current and the previous set of entity values:
R(s, a) = ∑
entity j
Acc(ejcur)− Acc(ejprev)
",3 Framework,[0],[0]
"There is a negative reward per step to penalize the agent for longer episodes.
",3 Framework,[0],[0]
3No entity-specific features are used for action selection.,3 Framework,[0],[0]
"4We also experiment with other forms of value reconcilia-
tion.",3 Framework,[0],[0]
"See Section 5 for details.
",3 Framework,[0],[0]
"Queries The queries are based on automatically generated templates, created using the title of an article along with words5 most likely to co-occur with each entity type in the training data.",3 Framework,[0],[0]
"Table 1 provides some examples – for instance, the second template contains words such as arrested and identified which often appear around the name of the shooter.
",3 Framework,[0],[0]
"We use a search engine to query the web for articles on the same event as the source article and retrieve the top k links per query.6 Documents that are more than a month older than the original article are filtered out of the search results.
",3 Framework,[0],[0]
"Transitions Each episode starts off with a single source article xi from which an initial set of entity
5Stop words, numeric terms and proper nouns are filtered.",3 Framework,[0],[0]
"6We use k=20 in our experiments.
",3 Framework,[0],[0]
values are extracted.,3 Framework,[0],[0]
"The subsequent steps in the episode involve the extra articles, downloaded using different types of query formulations based on the source article.",3 Framework,[0],[0]
"A single transition in the episode consists of the agent being given the state s containing information about the current and new set of values (extracted from a single article) using which the next action a = (d, q) is chosen.",3 Framework,[0],[0]
"The transition function T (s′|s, a) incorporates the reconciliation decision d from the agent in state s along with the values from the next article retrieved using query q and produces the next state s′.",3 Framework,[0],[0]
"The episode stops whenever d is a stop decision.
",3 Framework,[0],[0]
Algorithm 1 details the entire MDP framework for the training phase.,3 Framework,[0],[0]
"During the test phase, each source article is handled only once in a single episode (lines 8-23).
",3 Framework,[0],[0]
"Algorithm 1 MDP framework for Information Extraction (Training Phase)
1: Initialize set of original articles X 2: for xi ∈ X do 3: for each query template T q do 4: Download articles with query T q(xi) 5:",3 Framework,[0],[0]
"Queue retrieved articles in Y qi 6: for epoch = 1,M do 7: for i = 1, |X|",3 Framework,[0],[0]
do //episode 8,3 Framework,[0],[0]
": Extract entities e0 from xi 9: ecur ← e0
10: q← 0, r← 0 //query",3 Framework,[0],[0]
"type, reward 11: while Y qi not empty do 12:",3 Framework,[0],[0]
"Pop next article y from Y qi 13: Extract entities enew from y 14: Compute tf-idf similarity Z(xi, y) 15: Compute context vector C(y) 16:",3 Framework,[0],[0]
"Form state s using ecur, enew, Z(xi, y)
and C(y) 17: Send (s, r) to agent 18: Get decision d, query q from agent 19: if q == “end_episode” then break 20: eprev ← ecur 21: ecur ← Reconcile(ecur, enew, d) 22: r ←∑entity j Acc(ejcur)− Acc(ejprev) 23: Send (send, r) to agent",3 Framework,[0],[0]
"In order to learn a good policy for an agent, we utilize the paradigm of reinforcement learning (RL).
",4 Reinforcement Learning for Information Extraction,[0],[0]
"The MDP described in the previous section can be viewed in terms of a sequence of transitions (s, a, r, s′).",4 Reinforcement Learning for Information Extraction,[0],[0]
"The agent typically utilizes a stateaction value function Q(s, a) to determine which action a to perform in state s. A commonly used technique for learning an optimal value function is Q-learning (Watkins and Dayan, 1992), in which the agent iteratively updates Q(s, a) using the rewards obtained from episodes.",4 Reinforcement Learning for Information Extraction,[0],[0]
"The updates are derived from the recursive Bellman equation (Sutton and Barto, 1998) for the optimal Q:
Qi+1(s, a) =",4 Reinforcement Learning for Information Extraction,[0],[0]
"E[r + γmax a′
Qi(s ′, a′) | s, a]
Here, r =",4 Reinforcement Learning for Information Extraction,[0],[0]
"R(s, a) is the reward and γ is a factor discounting the value of future rewards and the expectation is taken over all transitions involving state s and action a.
Since our problem involves a continuous state space S, we use a deep Q-network (DQN) (Mnih et al., 2015) as a function approximator Q(s, a)",4 Reinforcement Learning for Information Extraction,[0],[0]
"≈ Q(s, a; θ).",4 Reinforcement Learning for Information Extraction,[0],[0]
"The DQN, in which the Q-function is approximated using a deep neural network, has been shown to learn better value functions than linear approximators (Narasimhan et al., 2015; He et al., 2015) and can capture non-linear interactions between the different pieces of information in our state.
",4 Reinforcement Learning for Information Extraction,[0],[0]
"We use a DQN consisting of two linear layers (20 hidden units each) followed by rectified linear units (ReLU), along with two separate output layers.7",4 Reinforcement Learning for Information Extraction,[0],[0]
"The network takes the continuous state vector s as input and predicts Q(s, d) and Q(s, q) for reconciliation decisions d and query choices q simultaneously using the different output layers (see Supplementary material for the model architecture).
",4 Reinforcement Learning for Information Extraction,[0],[0]
Parameter Learning,4 Reinforcement Learning for Information Extraction,[0],[0]
"The parameters θ of the DQN are learnt using stochastic gradient descent with RMSprop (Tieleman and Hinton, 2012).",4 Reinforcement Learning for Information Extraction,[0],[0]
"Each parameter update aims to close the gap between the Q(st, at; θ) predicted by the DQN and the expected Q-value from the Bellman equation, rt + γmaxaQ(st+1, a; θ).",4 Reinforcement Learning for Information Extraction,[0],[0]
"Following Mnih et al. (2015), we make use of a (separate) target Qnetwork to calculate the expected Q-value, in order
7We did not observe significant differences with additional linear layers or the choice of non-linearity (Sigmoid/ReLU).
",4 Reinforcement Learning for Information Extraction,[0],[0]
"Algorithm 2 Training Procedure for DQN agent with -greedy exploration
1: Initialize experience memory D 2: Initialize parameters θ randomly 3: for episode = 1,M do 4: Initialize environment and get start state s1 5: for t = 1, N do 6: if random() < then 7: Select a random action at 8: else 9: Compute Q(st, a) for all actions a
10: Select at = argmax Q(st, a) 11: Execute action at and observe reward rt and
new state st+1 12:",4 Reinforcement Learning for Information Extraction,[0],[0]
"Store transition (st, at, rt, st+1) in D 13:",4 Reinforcement Learning for Information Extraction,[0],[0]
"Sample random mini batch of transitions
(sj , aj , rj , sj+1) from D 14: yj = { rj , if sj+1 is terminal rj + γ maxa′ Q(sj+1, a′; θt), else 15:",4 Reinforcement Learning for Information Extraction,[0],[0]
Perform gradient descent step on the loss L(θ) =,4 Reinforcement Learning for Information Extraction,[0],[0]
"(yj −Q(sj , aj ; θ))2 16: if st+1 == send then break
to have ‘stable updates’.",4 Reinforcement Learning for Information Extraction,[0],[0]
The target Q-network is periodically updated with the current parameters θ.,4 Reinforcement Learning for Information Extraction,[0],[0]
We also make use of an experience replay memory D to store transitions.,4 Reinforcement Learning for Information Extraction,[0],[0]
"To perform updates, we sample a batch of transitions (ŝ, â, ŝ′, r) at random from D and minimize the loss function8:
L(θ) = Eŝ,â[(y −Q(ŝ, â; θ))2]
where y = r + γmaxa′",4 Reinforcement Learning for Information Extraction,[0],[0]
"Q(ŝ′, a′; θt) is the target Qvalue.",4 Reinforcement Learning for Information Extraction,[0],[0]
"The learning updates are made every training step using the following gradients:
∇θL(θ) =",4 Reinforcement Learning for Information Extraction,[0],[0]
"Eŝ,â[2(y −Q(ŝ, â; θ))∇θQ(ŝ, â; θ)]
Algorithm 2 details the DQN training procedure.",4 Reinforcement Learning for Information Extraction,[0],[0]
Data We perform experiments on two different datasets.,5 Experimental Setup,[0],[0]
"For the first set, we collected data from the Gun Violence archive,9 a website tracking shootings in the United States.",5 Experimental Setup,[0],[0]
"The data contains a news article on each shooting and annotations for (1) the name of the shooter, (2) the number of people killed, (3) the number of people wounded, and (4) the city where
8The expectation is over the transitions sampled uniformly at random from D.
9www.shootingtracker.com/Main_Page
the incident took place.",5 Experimental Setup,[0],[0]
"We consider these as the entities of interest, to be extracted from the articles.",5 Experimental Setup,[0],[0]
The second dataset we use is the Foodshield EMA database10 documenting adulteration incidents since 1980.,5 Experimental Setup,[0],[0]
"This data contains annotations for (1) the affected food product, (2) the adulterant and (3) the location of the incident.",5 Experimental Setup,[0],[0]
"Both datasets are classic examples where the number of recorded incidents is insufficient for large-scale IE systems to leverage.
",5 Experimental Setup,[0],[0]
"For each source article in the above databases, we download extra articles (top 20 links) using the Bing Search API11 with different automatically generated queries.",5 Experimental Setup,[0],[0]
We use only the source articles from the train portion to learn the parameters of the base extractor.,5 Experimental Setup,[0],[0]
The entire train set with downloaded articles is used to train the DQN agent and the metaclassifier baseline (described below).,5 Experimental Setup,[0],[0]
All parameters are tuned on the dev set.,5 Experimental Setup,[0],[0]
"For the final results, we train the models on the combined train and dev sets and use the entire test set (source + downloaded articles) to evaluate.",5 Experimental Setup,[0],[0]
"Table 2 provides data statistics.
",5 Experimental Setup,[0],[0]
"Extraction model We use a maximum entropy classifier as the base extraction system, since it provides flexibility to capture various local context features and has been shown to perform well for information extraction (Chieu and Ng, 2002).",5 Experimental Setup,[0],[0]
"The classifier is used to tag each word in a document as one of the entity types or not (e.g. {ShooterName, NumKilled, NumWounded, City, Other} in the Shootings domain).",5 Experimental Setup,[0],[0]
"Then, for each tag except Other, we choose the mode of the values to obtain the set of entity extractions from the article.12 Features used in the classifier are provided in the Supplementary material.
",5 Experimental Setup,[0],[0]
The features and context window c = 4 of neighboring words are tuned to maximize performance on a dev set.,5 Experimental Setup,[0],[0]
"We also experimented with a conditional random field (CRF) (with the same features) for the sequence tagging (Culotta and McCallum, 2004)
10www.foodshield.org/member/login/",5 Experimental Setup,[0],[0]
"11www.bing.com/toolbox/bingsearchapi 12We normalize numerical words (e.g. ""one"" to ""1"") before
taking the mode.
",5 Experimental Setup,[0],[0]
but obtained worse empirical performance (see Section 6).,5 Experimental Setup,[0],[0]
"The parameters of the base extraction model are not changed during training of the RL model.
",5 Experimental Setup,[0],[0]
Evaluation We evaluate the extracted entity values against the gold annotations and report the corpus-level average accuracy on each entity type.,5 Experimental Setup,[0],[0]
"For entities like ShooterName, the annotations (and the news articles) often contain multiple names (first and last) in various combinations, so we consider retrieving either name as a successful extraction.",5 Experimental Setup,[0],[0]
"For all other entities, we look for exact matches.
",5 Experimental Setup,[0],[0]
Baselines We explore 4 types of baselines: Basic extractors: We use the CRF and the Maxent classifier mentioned previously.,5 Experimental Setup,[0],[0]
Aggregation systems: We examine two systems that perform different types of value reconciliation.,5 Experimental Setup,[0],[0]
The first model (Confidence) chooses entity values with the highest confidence score assigned by the base extractor.,5 Experimental Setup,[0],[0]
The second system (Majority) takes a majority vote over all values extracted from these articles.,5 Experimental Setup,[0],[0]
"Both methods filter new entity values using a threshold τ on the cosine similarity over the tf-idf representations of the source and new articles.
",5 Experimental Setup,[0],[0]
"Meta-classifer: To demonstrate the importance of modeling the problem in the RL framework, we consider a meta-classifier baseline.",5 Experimental Setup,[0],[0]
The classifier operates over the same input state space and produces the same set of reconciliation decisions {d} as the DQN.,5 Experimental Setup,[0],[0]
"For training, we use the original source article for each event along with a related downloaded article to compute the state.",5 Experimental Setup,[0],[0]
"If the downloaded article has the correct value and the original one does not, we label it as a positive example for that entity class.",5 Experimental Setup,[0],[0]
"If multiple such entity classes exist, we create several training instances with appropriate labels, and if none exist, we use the label corresponding to the reject all action.",5 Experimental Setup,[0],[0]
"For each test event, the classifier is used to provide decisions for all the downloaded articles and the final extraction is performed by aggregating the value predictions using the Confidence-based scheme described above.
",5 Experimental Setup,[0],[0]
"Oracle: Finally, we also have an ORACLE score which is computed assuming perfect reconciliation and querying decisions on top of the Maxent base extractor.",5 Experimental Setup,[0],[0]
"This helps us analyze the contribution of the RL system in isolation of the inherent limitations of the base extractor.
RL models We perform experiments using three variants of RL agents – (1) RL-Basic, which performs only reconciliation decisions13, (2) RL-Query, which takes only query decisions with the reconciliation strategy fixed (similar to Kanani and McCallum (2012)), and (3) RL-Extract, our full system incorporating both reconciliation and query decisions.
",5 Experimental Setup,[0],[0]
"We train the models for 10000 steps every epoch using the Maxent classifier as the base extractor, and evaluate on the entire test set every epoch.",5 Experimental Setup,[0],[0]
The final accuracies reported are averaged over 3 independent runs; each run’s score is averaged over 20 epochs after 100 epochs of training.,5 Experimental Setup,[0],[0]
The penalty per step is set to -0.001.,5 Experimental Setup,[0],[0]
"For the DQN, we use the dev set to tune all parameters.",5 Experimental Setup,[0],[0]
"We used a replay memory D of size 500k, and a discount (γ) of 0.8.",5 Experimental Setup,[0],[0]
We set the learning rate to 2.5E−5.,5 Experimental Setup,[0],[0]
The in -greedy exploration is annealed from 1 to 0.1 over 500k transitions.,5 Experimental Setup,[0],[0]
The target-Q network is updated every 5k steps.,5 Experimental Setup,[0],[0]
Performance Table 3 demonstrates that our system (RL-Extract) obtains a substantial gain in accuracy over the basic extractors on all entity types over both domains.,6 Results,[0],[0]
"For instance, RL-Extract is 11.4% more accurate than the basic Maxent extractor on City and 7.1% better on NumKilled, while also achieving gains of more than 5% on the other entities on the Shootings domain.",6 Results,[0],[0]
"The gains on the Adulteration dataset are also significant, up to a 11.5% increase on the Location entity.
",6 Results,[0],[0]
We can also observe that simple aggregation schemes like the Confidence and Majority baselines don’t handle the complexity of the task well.,6 Results,[0],[0]
RL-Extract outperforms these by 7.2% on Shootings and 5% on Adulteration averaged over all entities.,6 Results,[0],[0]
"Further, the importance of sequential decisionmaking is established by RL-Extract performing significantly better than the meta-classifier (7.0% on Shootings over all entities).",6 Results,[0],[0]
"This is also due to the fact that the meta-classifier aggregates over the entire set of extra documents, including the long tail of noisy, irrelevant documents.",6 Results,[0],[0]
"Finally, we see the advantage of enabling the RL system to select queries as our full model RL-Extract obtains significant im-
13Articles are presented to the agent in a round-robin fashion from the different query lists.
provements over RL-Basic on both domains.",6 Results,[0],[0]
"The full model also outperforms RL-Query, demonstrating the importance of performing both query selection and reconciliation in a joint fashion.
",6 Results,[0],[0]
Figure 4 shows the learning curve of the agent by measuring reward on the test set after each training epoch.,6 Results,[0],[0]
The reward improves gradually and the accuracy on each entity increases simultaneously.,6 Results,[0],[0]
Table 4 provides some examples where our model is able to extract the right values when the baseline fails.,6 Results,[0],[0]
"One can see that in most cases this is due to the model making use of articles with prototypical language or articles containing the entities in readily extractable form.
",6 Results,[0],[0]
"Analysis We also analyze the importance of different reconciliation schemes, rewards and contextvectors in RL-Extract on the Shootings domain (Table 5).",6 Results,[0],[0]
"In addition to simple replacement (Re-
place), we also experiment with using Confidence and Majority-based reconciliation schemes for RLExtract.",6 Results,[0],[0]
"We observe that the Replace scheme performs much better than the others (2-6% on all entities) and believe this is because it provides the agent with more flexibility in choosing the final values.
",6 Results,[0],[0]
"From the same table, we see that using the tfidf counts of context words as part of the state provides better performance than using no context or using simple unigram counts.",6 Results,[0],[0]
"In terms of reward structure, providing rewards after each step is empirically found to be significantly better (>10% on average) compared to a single delayed reward per episode.",6 Results,[0],[0]
The last column shows the average number of steps per episode – the values range from 6.8 to 10.0 steps for the different schemes.,6 Results,[0],[0]
"The best system (RL-Extract with Replace, tf-idf and step-based rewards) uses 9.4 steps per episode.",6 Results,[0],[0]
"In this paper, we explore the task of acquiring and incorporating external evidence to improve information extraction accuracy for domains with limited access to training data.",7 Conclusions,[0],[0]
"This process comprises issuing search queries, extraction from new sources and reconciliation of extracted values, repeated until sufficient evidence is obtained.",7 Conclusions,[0],[0]
We use a reinforcement learning framework and learn optimal action sequences to maximize extraction accuracy while penalizing extra effort.,7 Conclusions,[0],[0]
"We show that our model, trained as a deep Q-network, outperforms traditional extractors by 7.2% and 5% on average on two different domains, respectively.",7 Conclusions,[0],[0]
"We also demonstrate the
importance of sequential decision-making by comparing our model to a meta-classifier operating on the same space, obtaining up to a 7% gain.",7 Conclusions,[0],[0]
"We thank David Alvarez, Tao Lei and Ramya Ramakrishnan for helpful discussions and feedback, and the members of the MIT NLP group and the anonymous reviewers for their insightful comments.",Acknowledgements,[0],[0]
We also gratefully acknowledge support from a Google faculty award.,Acknowledgements,[0],[0]
Most successful information extraction systems operate with access to a large collection of documents.,abstractText,[0],[0]
"In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce.",abstractText,[0],[0]
"This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected.",abstractText,[0],[0]
We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information.,abstractText,[0],[0]
"We employ a deep Qnetwork, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort.",abstractText,[0],[0]
"Our experiments on two databases – of shooting incidents, and food adulteration cases – demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.1",abstractText,[0],[0]
Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 110–121 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
110",text,[0],[0]
"The past decade has witnessed great achievements in building web-scale knowledge graphs (KGs), e.g., Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), and Google’s Knowledge
∗Corresponding author: Quan Wang.
",1 Introduction,[0],[0]
"Vault (Dong et al., 2014).",1 Introduction,[0],[0]
"A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity).",1 Introduction,[0],[0]
"Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks (Wasserman-Pritsker et al., 2015; Hoffmann et al., 2011; Yang and Mitchell, 2017).
",1 Introduction,[0],[0]
"Recently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic.",1 Introduction,[0],[0]
"The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG.",1 Introduction,[0],[0]
"Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011).",1 Introduction,[0],[0]
"Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017).",1 Introduction,[0],[0]
"See (Wang et al., 2017) for a thorough review.
",1 Introduction,[0],[0]
"This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task.",1 Introduction,[0],[0]
"Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations.",1 Introduction,[0],[0]
"By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012).",1 Introduction,[0],[0]
"By using the latter, we further encode regularities of logical entailment between relations into their
distributed representations, which might be advantageous to downstream tasks like link prediction and relation extraction (Rocktäschel et al., 2015; Guo et al., 2016).",1 Introduction,[0],[0]
"These constraints impose prior beliefs upon the structure of the embedding space, and will help us to learn more predictive embeddings, without significantly increasing the space or time complexity.
",1 Introduction,[0],[0]
"Our work has some similarities to those which integrate logical background knowledge into KG embedding (Rocktäschel et al., 2015; Wang et al., 2015; Guo et al., 2016, 2018).",1 Introduction,[0],[0]
"Most of such works, however, need grounding of first-order logic rules.",1 Introduction,[0],[0]
The grounding process could be time and space inefficient especially for complicated rules.,1 Introduction,[0],[0]
"To avoid grounding, Demeester et al. (2016) tried to model rules using only relation representations.",1 Introduction,[0],[0]
"But their work creates vector representations for entity pairs rather than individual entities, and hence fails to handle unpaired entities.",1 Introduction,[0],[0]
"Moreover, it can only incorporate strict, hard rules which usually require extensive manual effort to create.",1 Introduction,[0],[0]
Minervini et al. (2017b) proposed adversarial training which can integrate first-order logic rules without grounding.,1 Introduction,[0],[0]
"But their work, again, focuses on strict, hard rules.",1 Introduction,[0],[0]
Minervini et al. (2017a) tried to handle uncertainty of rules.,1 Introduction,[0],[0]
"But their work assigns to different rules a same confidence level, and considers only equivalence and inversion of relations, which might not always be available in a given KG.
",1 Introduction,[0],[0]
"Our approach differs from the aforementioned works in that: (i) it imposes constraints directly on entity and relation representations without grounding, and can easily scale up to large KGs; (ii) the constraints, i.e., non-negativity and approximate entailment derived automatically from statistical properties, are quite universal, requiring no manual effort and applicable to almost all KGs; (iii) it learns an individual representation for each entity, and can successfully make predictions between unpaired entities.
",1 Introduction,[0],[0]
"We evaluate our approach on publicly available KGs of WordNet, Freebase, and DBpedia as well.",1 Introduction,[0],[0]
"Experimental results indicate that our approach is simple yet surprisingly effective, achieving significant and consistent improvements over competitive baselines, but without negative impacts on efficiency or scalability.",1 Introduction,[0],[0]
"The non-negativity and approximate entailment constraints indeed improve model interpretability, resulting in a substantially increased structuring of the embedding space.
",1 Introduction,[0],[0]
The remainder of this paper is organized as follows.,1 Introduction,[0],[0]
"We first review related work in Section 2, and then detail our approach in Section 3.",1 Introduction,[0],[0]
"Experiments and results are reported in Section 4, followed by concluding remarks in Section 5.",1 Introduction,[0],[0]
"Recent years have seen growing interest in learning distributed representations for entities and relations in KGs, a.k.a. KG embedding.",2 Related Work,[0],[0]
"Early works on this topic devised very simple models to learn such distributed representations, solely on the basis of triples observed in a given KG, e.g., TransE which takes relations as translating operations between head and tail entities (Bordes et al., 2013), and RESCAL which models triples through bilinear operations over entity and relation representations (Nickel et al., 2011).",2 Related Work,[0],[0]
"Later attempts roughly fell into two groups: (i) those which tried to design more complicated triple scoring models, e.g., the TransE extensions (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015), the RESCAL extensions (Yang et al., 2015; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), and the (deep) neural network models (Socher et al., 2013; Bordes et al., 2014; Shi and Weninger, 2017; Schlichtkrull et al., 2017; Dettmers et al., 2018); (ii) those which tried to integrate extra information beyond triples, e.g., entity types (Guo et al., 2015; Xie et al., 2016b), relation paths (Neelakantan et al., 2015; Lin et al., 2015a), and textual descriptions (Xie et al., 2016a; Xiao et al., 2017).",2 Related Work,[0],[0]
"Please refer to (Nickel et al., 2016a; Wang et al., 2017) for a thorough review of these techniques.",2 Related Work,[0],[0]
"In this paper, we show the potential of using very simple constraints (i.e., nonnegativity constraints and approximate entailment constraints) to improve KG embedding, without significantly increasing the model complexity.
",2 Related Work,[0],[0]
"A line of research related to ours is KG embedding with logical background knowledge incorporated (Rocktäschel et al., 2015; Wang et al., 2015; Guo et al., 2016, 2018).",2 Related Work,[0],[0]
"But most of such works require grounding of first-order logic rules, which is time and space inefficient especially for complicated rules.",2 Related Work,[0],[0]
"To avoid grounding, Demeester et al. (2016) proposed lifted rule injection, and Minervini et al. (2017b) investigated adversarial training.",2 Related Work,[0],[0]
"Both works, however, can only handle strict, hard rules which usually require extensive effort to create.",2 Related Work,[0],[0]
Minervini et al. (2017a) tried to handle uncertainty of background knowledge.,2 Related Work,[0],[0]
"But their work
considers only equivalence and inversion between relations, which might not always be available in a given KG.",2 Related Work,[0],[0]
"Our approach, in contrast, imposes constraints directly on entity and relation representations without grounding.",2 Related Work,[0],[0]
"And the constraints used are quite universal, requiring no manual effort and applicable to almost all KGs.
",2 Related Work,[0],[0]
Non-negativity has long been a subject studied in various research fields.,2 Related Work,[0],[0]
"Previous studies reveal that non-negativity could naturally induce sparsity and, in most cases, better interpretability (Lee and Seung, 1999).",2 Related Work,[0],[0]
"In many NLP-related tasks, nonnegativity constraints are introduced to learn more interpretable word representations, which capture the notion of semantic composition (Murphy et al., 2012; Luo et al., 2015a; Fyshe et al., 2015).",2 Related Work,[0],[0]
"In this paper, we investigate the ability of non-negativity constraints to learn more accurate KG embeddings with good interpretability.",2 Related Work,[0],[0]
This section presents our approach.,3 Our Approach,[0],[0]
We first introduce a basic embedding technique to model triples in a given KG (§ 3.1).,3 Our Approach,[0],[0]
Then we discuss the nonnegativity constraints over entity representations (§ 3.2) and the approximate entailment constraints over relation representations (§ 3.3).,3 Our Approach,[0],[0]
And finally we present the overall model (§ 3.4).,3 Our Approach,[0],[0]
"We choose ComplEx (Trouillon et al., 2016) as our basic embedding model, since it is simple and efficient, achieving state-of-the-art predictive performance.",3.1 A Basic Embedding Model,[0],[0]
"Specifically, suppose we are given a KG containing a set of triples O = {(ei, rk, ej)}, with each triple composed of two entities ei, ej ∈ E and their relation rk ∈",3.1 A Basic Embedding Model,[0],[0]
R.,3.1 A Basic Embedding Model,[0],[0]
Here E is the set of entities and R the set of relations.,3.1 A Basic Embedding Model,[0],[0]
"ComplEx then represents each entity e ∈ E as a complex-valued vector e∈ Cd, and each relation r ∈ R",3.1 A Basic Embedding Model,[0],[0]
"a complex-valued vector r ∈ Cd, where d is the dimensionality of the embedding space.",3.1 A Basic Embedding Model,[0],[0]
Each x ∈,3.1 A Basic Embedding Model,[0],[0]
"Cd consists of a real vector component Re(x) and an imaginary vector component Im(x), i.e., x = Re(x) + iIm(x).",3.1 A Basic Embedding Model,[0],[0]
"For any given triple (ei, rk, ej) ∈ E ×R× E , a multilinear dot product is used to score that triple, i.e.,
φ(ei, rk, ej) , Re(〈ei, rk, ēj〉) , Re( ∑
` [ei]`[rk]`[ēj ]`), (1)
where ei, rk, ej ∈",3.1 A Basic Embedding Model,[0],[0]
"Cd are the vectorial representations associated with ei, rk, ej , respectively; ēj is
the conjugate of ej ;",3.1 A Basic Embedding Model,[0],[0]
[·]` is the `-th entry of a vector; and Re(·) means taking the real part of a complex value.,3.1 A Basic Embedding Model,[0],[0]
"Triples with higher φ(·, ·, ·) scores are more likely to be true.",3.1 A Basic Embedding Model,[0],[0]
"Owing to the asymmetry of this scoring function, i.e., φ(ei, rk, ej) 6=",3.1 A Basic Embedding Model,[0],[0]
"φ(ej , rk, ei), ComplEx can effectively handle asymmetric relations (Trouillon et al., 2016).",3.1 A Basic Embedding Model,[0],[0]
"On top of the basic ComplEx model, we further require entities to have non-negative (and bounded) vectorial representations.",3.2 Non-negativity of Entity Representations,[0],[0]
"In fact, these distributed representations can be taken as feature vectors for entities, with latent semantics encoded in different dimensions.",3.2 Non-negativity of Entity Representations,[0],[0]
"In ComplEx, as well as most (if not all) previous approaches, there is no limitation on the range of such feature values, which means that both positive and negative properties of an entity can be encoded in its representation.",3.2 Non-negativity of Entity Representations,[0],[0]
"However, as pointed out by Murphy et al. (2012), it would be uneconomical to store all negative properties of an entity or a concept.",3.2 Non-negativity of Entity Representations,[0],[0]
"For instance, to describe cats (a concept), people usually use positive properties such as cats are mammals, cats eat fishes, and cats have four legs, but hardly ever negative properties like cats are not vehicles, cats do not have wheels, or cats are not used for communication.
",3.2 Non-negativity of Entity Representations,[0],[0]
"Based on such intuition, this paper proposes to impose non-negativity constraints on entity representations, by using which only positive properties will be stored in these representations.",3.2 Non-negativity of Entity Representations,[0],[0]
"To better compare different entities on the same scale, we further require entity representations to stay within the hypercube of [0, 1]d, as approximately Boolean embeddings (Kruszewski et al., 2015), i.e.,
0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E , (2)
where e ∈ Cd is the representation for entity e ∈ E , with its real and imaginary components denoted by Re(e), Im(e) ∈ Rd; 0 and 1 are d-dimensional vectors with all their entries being 0 or 1; and≥,≤ ,= denote the entry-wise comparisons throughout the paper whenever applicable.",3.2 Non-negativity of Entity Representations,[0],[0]
"As shown by Lee and Seung (1999), non-negativity, in most cases, will further induce sparsity and interpretability.",3.2 Non-negativity of Entity Representations,[0],[0]
"Besides the non-negativity constraints over entity representations, we also study approximate entailment constraints over relation representations.",3.3 Approximate Entailment for Relations,[0],[0]
"By approximate entailment, we mean an ordered pair
of relations that the former approximately entails the latter, e.g., BornInCountry and Nationality, stating that a person born in a country is very likely, but not necessarily, to have a nationality of that country.",3.3 Approximate Entailment for Relations,[0],[0]
Each such relation pair is associated with a weight to indicate the confidence level of entailment.,3.3 Approximate Entailment for Relations,[0],[0]
A larger weight stands for a higher level of confidence.,3.3 Approximate Entailment for Relations,[0],[0]
"We denote by rp
λ−→ rq the approximate entailment between relations rp and rq, with confidence level λ.",3.3 Approximate Entailment for Relations,[0],[0]
"This kind of entailment can be derived automatically from a KG by modern rule mining systems (Galárraga et al., 2015).",3.3 Approximate Entailment for Relations,[0],[0]
"Let T denote the set of all such approximate entailments derived beforehand.
",3.3 Approximate Entailment for Relations,[0],[0]
"Before diving into approximate entailment, we first explore the modeling of strict entailment, i.e., entailment with infinite confidence level λ = +∞. The strict entailment rp → rq states that if relation rp holds then relation rq must also hold.",3.3 Approximate Entailment for Relations,[0],[0]
"This entailment can be roughly modelled by requiring
φ(ei, rp, ej) ≤ φ(ei, rq, ej), ∀ei, ej ∈ E , (3)
where φ(·, ·, ·) is the score for a triple predicted by the embedding model, defined by Eq.",3.3 Approximate Entailment for Relations,[0],[0]
(1).,3.3 Approximate Entailment for Relations,[0],[0]
"Eq. (3) can be interpreted as follows: for any two entities ei and ej , if (ei, rp, ej) is a true fact with a high score φ(ei, rp, ej), then the triple (ei, rq, ej) with an even higher score should also be predicted as a true fact by the embedding model.",3.3 Approximate Entailment for Relations,[0],[0]
"Note that given the non-negativity constraints defined by Eq. (2), a sufficient condition for Eq.",3.3 Approximate Entailment for Relations,[0],[0]
"(3) to hold, is to further impose
Re(rp) ≤ Re(rq), Im(rp)",3.3 Approximate Entailment for Relations,[0],[0]
"= Im(rq), (4)
where rp and rq are the complex-valued representations for rp and rq respectively, with the real and imaginary components denoted by Re(·), Im(·) ∈ Rd.",3.3 Approximate Entailment for Relations,[0],[0]
"That means, when the constraints of Eq. (4) (along with those of Eq. (2)) are satisfied, the requirement of Eq.",3.3 Approximate Entailment for Relations,[0],[0]
(3) (or in other words rp → rq) will always hold.,3.3 Approximate Entailment for Relations,[0],[0]
"We provide a proof of sufficiency as supplementary material.
",3.3 Approximate Entailment for Relations,[0],[0]
Next we examine the modeling of approximate entailment.,3.3 Approximate Entailment for Relations,[0],[0]
"To this end, we further introduce the confidence level λ and allow slackness in Eq.",3.3 Approximate Entailment for Relations,[0],[0]
"(4), which yields
λ (",3.3 Approximate Entailment for Relations,[0],[0]
Re(rp)− Re(rq) ),3.3 Approximate Entailment for Relations,[0],[0]
"≤ α, (5)
λ ( Im(rp)− Im(rq) )",3.3 Approximate Entailment for Relations,[0],[0]
2 ≤ β.,3.3 Approximate Entailment for Relations,[0],[0]
"(6) Here α,β ≥ 0 are slack variables, and (·)2 means an entry-wise operation.",3.3 Approximate Entailment for Relations,[0],[0]
"Entailments with higher
confidence levels show less tolerance for violating the constraints.",3.3 Approximate Entailment for Relations,[0],[0]
"When λ = +∞, Eqs. (5) – (6) degenerate to Eq.",3.3 Approximate Entailment for Relations,[0],[0]
(4).,3.3 Approximate Entailment for Relations,[0],[0]
"The above analysis indicates that our approach can model entailment simply by imposing constraints over relation representations, without traversing all possible (ei, ej) entity pairs (i.e., grounding).",3.3 Approximate Entailment for Relations,[0],[0]
"In addition, different confidence levels are encoded in the constraints, making our approach moderately tolerant of uncertainty.",3.3 Approximate Entailment for Relations,[0],[0]
"Finally, we combine together the basic embedding model of ComplEx, the non-negativity constraints on entity representations, and the approximate entailment constraints over relation representations.",3.4 The Overall Model,[0],[0]
"The overall model is presented as follows:
min Θ,{α,β} ∑ D+∪D− log ( 1 + exp(−yijkφ(ei, rk, ej)) )",3.4 The Overall Model,[0],[0]
"+ µ ∑ T 1>(α + β) + η‖Θ‖22,
s.t. λ",3.4 The Overall Model,[0],[0]
( Re(rp)− Re(rq) ),3.4 The Overall Model,[0],[0]
"≤ α,
λ ( Im(rp)− Im(rq) )",3.4 The Overall Model,[0],[0]
"2 ≤ β, α,β ≥ 0, ∀rp
λ−→ rq ∈ T , 0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E .",3.4 The Overall Model,[0],[0]
"(7)
Here, Θ , {e : e ∈ E} ∪ {r : r ∈ R} is the set of all entity and relation representations;D+ andD− are the sets of positive and negative training triples respectively; a positive triple is directly observed in the KG, i.e., (ei, rk, ej) ∈ O; a negative triple can be generated by randomly corrupting the head or the tail entity of a positive triple, i.e., (e′i, rk, ej) or (ei, rk, e′j); yijk = ±1 is the label (positive or negative) of triple (ei, rk, ej).",3.4 The Overall Model,[0],[0]
"In this optimization, the first term of the objective function is a typical logistic loss, which enforces triples to have scores close to their labels.",3.4 The Overall Model,[0],[0]
"The second term is the sum of slack variables in the approximate entailment constraints, with a penalty coefficient µ ≥ 0.",3.4 The Overall Model,[0],[0]
"The motivation is, although we allow slackness in those constraints we hope the total slackness to be small, so that the constraints can be better satisfied.",3.4 The Overall Model,[0],[0]
"The last term is L2 regularization to avoid over-fitting, and η ≥ 0 is the regularization coefficient.
",3.4 The Overall Model,[0],[0]
"To solve this optimization problem, the approximate entailment constraints (as well as the corresponding slack variables) are converted into penalty terms and added to the objective function, while the non-negativity constraints remain as they are.",3.4 The Overall Model,[0],[0]
"As such, the optimization problem of Eq. (7) can
be rewritten as:
min Θ ∑ D+∪D− log ( 1 + exp(−yijkφ(ei, rk, ej)) )",3.4 The Overall Model,[0],[0]
+ µ ∑ T λ1> [ Re(rp)−Re(rq) ],3.4 The Overall Model,[0],[0]
"+
+ µ ∑ T λ1> ( Im(rp)−Im(rq) )",3.4 The Overall Model,[0],[0]
"2 + η‖Θ‖22,
s.t. 0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E , (8)
where [x]+ = max(0,x) with max(·, ·) being an entry-wise operation.",3.4 The Overall Model,[0],[0]
The equivalence between Eq. (7) and Eq.,3.4 The Overall Model,[0],[0]
(8) is shown in the supplementary material.,3.4 The Overall Model,[0],[0]
"We use SGD in mini-batch mode as our optimizer, with AdaGrad (Duchi et al., 2011) to tune the learning rate.",3.4 The Overall Model,[0],[0]
"After each gradient descent step, we project (by truncation) real and imaginary components of entity representations into the hypercube of [0, 1]d, to satisfy the non-negativity constraints.
",3.4 The Overall Model,[0],[0]
"While favouring a better structuring of the embedding space, imposing the additional constraints will not substantially increase model complexity.",3.4 The Overall Model,[0],[0]
"Our approach has a space complexity of O(nd + md), which is the same as that of ComplEx.",3.4 The Overall Model,[0],[0]
"Here, n is the number of entities, m the number of relations, and O(nd+md) to store a d-dimensional complex-valued vector for each entity and each relation.",3.4 The Overall Model,[0],[0]
"The time complexity (per iteration) of our approach isO(sd+td+n̄d), where s is the average number of triples in a mini-batch, n̄ the average number of entities in a mini-batch, and t the total number of approximate entailments in T .",3.4 The Overall Model,[0],[0]
"O(sd) is to handle triples in a mini-batch, O(td) penalty terms introduced by the approximate entailments, and O(n̄d) further the non-negativity constraints on entity representations.",3.4 The Overall Model,[0],[0]
"Usually there are much fewer entailments than triples, i.e., t s, and also n̄ ≤ 2s.1",3.4 The Overall Model,[0],[0]
"So the time complexity of our approach is on a par withO(sd), i.e., the time complexity of ComplEx.",3.4 The Overall Model,[0],[0]
This section presents our experiments and results.,4 Experiments and Results,[0],[0]
We first introduce the datasets used in our experiments (§ 4.1).,4 Experiments and Results,[0],[0]
Then we empirically evaluate our approach in the link prediction task (§ 4.2).,4 Experiments and Results,[0],[0]
"After that, we conduct extensive analysis on both entity representations (§ 4.3) and relation representations (§ 4.4) to show the interpretability of our model.
",4 Experiments and Results,[0],[0]
"1There will be at most 2s entities contained in s triples.
",4 Experiments and Results,[0],[0]
Code and data used in the experiments are available at https://github.com/iieir-km/ ComplEx-NNE_AER.,4 Experiments and Results,[0],[0]
"The first two datasets we used are WN18 and FB15K, released by Bordes et al. (2013).2 WN18 is a subset of WordNet containing 18 relations and 40,943 entities, and FB15K a subset of Freebase containing 1,345 relations and 14,951 entities.",4.1 Datasets,[0],[0]
"We create our third dataset from the mapping-based objects of core DBpedia.3 We eliminate relations not included within the DBpedia ontology such as HomePage and Logo, and discard entities appearing less than 20 times.",4.1 Datasets,[0],[0]
"The final dataset, referred to as DB100K, is composed of 470 relations and 99,604 entities.",4.1 Datasets,[0],[0]
"Triples on each datasets are further divided into training, validation, and test sets, used for model training, hyperparameter tuning, and evaluation respectively.",4.1 Datasets,[0],[0]
"We follow the original split for WN18 and FB15K, and draw a split of 597,572/ 50,000/50,000 triples for DB100K.
We further use AMIE+ (Galárraga et al., 2015)4 to extract approximate entailments automatically from the training set of each dataset.",4.1 Datasets,[0],[0]
"As suggested by Guo et al. (2018), we consider entailments with PCA confidence higher than 0.8.5",4.1 Datasets,[0],[0]
"As such, we extract 17 approximate entailments from WN18, 535 from FB15K, and 56 from DB100K. Table 1 gives some examples of these approximate entailments, along with their confidence levels.",4.1 Datasets,[0],[0]
Table 2 further summarizes the statistics of the datasets.,4.1 Datasets,[0],[0]
"We first evaluate our approach in the link prediction task, which aims to predict a triple (ei, rk, ej) with ei or ej missing, i.e., predict ei given (rk, ej) or predict ej given (ei, rk).
",4.2 Link Prediction,[0],[0]
Evaluation Protocol: We follow the protocol introduced by Bordes et al. (2013).,4.2 Link Prediction,[0],[0]
"For each test triple (ei, rk, ej), we replace its head entity ei with every entity e′i ∈ E , and calculate a score for the corrupted triple (e′i, rk, ej), e.g., φ(e ′",4.2 Link Prediction,[0],[0]
"i, rk, ej) defined by Eq. (1).",4.2 Link Prediction,[0],[0]
Then we sort these scores in de2https://everest.hds.utc.fr/doku.php?,4.2 Link Prediction,[0],[0]
"id=en:smemlj12 3http://downloads.dbpedia.org/2016-10/ core/ 4https://www.mpi-inf.mpg.de/departmen ts/databases-and-information-systems/res earch/yago-naga/amie/
5PCA confidence is the confidence under the partial completeness assumption.",4.2 Link Prediction,[0],[0]
"See (Galárraga et al., 2015) for details.
scending order, and get the rank of the correct entity ei.",4.2 Link Prediction,[0],[0]
"During ranking, we remove corrupted triples that already exist in either the training, validation, or test set, i.e., the filtered setting as described in (Bordes et al., 2013).",4.2 Link Prediction,[0],[0]
This whole procedure is repeated while replacing the tail entity ej .,4.2 Link Prediction,[0],[0]
"We report on the test set the mean reciprocal rank (MRR) and the proportion of correct entities ranked in the top n (HITS@N), with n = 1, 3, 10.
",4.2 Link Prediction,[0],[0]
Comparison Settings: We compare the performance of our approach against a variety of KG embedding models developed in recent years.,4.2 Link Prediction,[0],[0]
"These models can be categorized into three groups:
• Simple embedding models that utilize triples alone without integrating extra information, including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), HolE (Nickel et al., 2016b), ComplEx (Trouillon et al., 2016), and ANALOGY (Liu et al., 2017).",4.2 Link Prediction,[0],[0]
"Our approach is developed on the basis of ComplEx.
•",4.2 Link Prediction,[0],[0]
"Other extensions of ComplEx that integrate logical background knowledge in addition to triples, including RUGE (Guo et al., 2018) and ComplExR (Minervini et al., 2017a).",4.2 Link Prediction,[0],[0]
The former requires grounding of first-order logic rules.,4.2 Link Prediction,[0],[0]
"The latter is restricted to relation equiv-
alence and inversion, and assigns an identical confidence level to all different rules.
",4.2 Link Prediction,[0],[0]
"• Latest developments or implementations that achieve current state-of-the-art performance reported on the benchmarks of WN18 and FB15K, including R-GCN (Schlichtkrull et al., 2017), ConvE (Dettmers et al., 2018), and Single DistMult (Kadlec et al., 2017).6 The first two are built based on neural network architectures, which are, by nature, more complicated than the simple models.",4.2 Link Prediction,[0],[0]
"The last one is a re-implementation of DistMult, generating 1000 to 2000 negative training examples per positive one, which leads to better performance but requires significantly longer training time.
",4.2 Link Prediction,[0],[0]
"We further evaluate our approach in two different settings: (i) ComplEx-NNE that imposes only the Non-Negativity constraints on Entity representations, i.e., optimization Eq.",4.2 Link Prediction,[0],[0]
"(8) with µ = 0; and (ii) ComplEx-NNE+AER that further imposes the Approximate Entailment constraints over Relation representations besides those non-negativity ones, i.e., optimization Eq. (8) with µ > 0.
",4.2 Link Prediction,[0],[0]
Implementation Details: We compare our approach against all the three groups of baselines on the benchmarks of WN18 and FB15K. We directly report their original results on these two datasets to avoid re-implementation bias.,4.2 Link Prediction,[0],[0]
"On DB100K, the newly created dataset, we take the first two groups of baselines, i.e., those simple embedding models and ComplEx extensions with logical background knowledge incorporated.",4.2 Link Prediction,[0],[0]
We do not use the third group of baselines due to efficiency and complexity issues.,4.2 Link Prediction,[0],[0]
"We use the code provided by Trouillon et al. (2016)7 for TransE, DistMult, and ComplEx, and the code released by their authors for ANALOGY8 and RUGE9.",4.2 Link Prediction,[0],[0]
"We re-implement HolE and ComplExR so that all the baselines (as well as our approach) share the same optimization mode, i.e., SGD with AdaGrad and gradient normalization, to facilitate a fair comparison.10 We follow Trouillon et al. (2016) to adopt a ranking loss for TransE and a logistic loss for all the other methods.
",4.2 Link Prediction,[0],[0]
"6We do not consider Ensemble DistMult (Dettmers et al., 2018) which combines several different models together, to facilitate a fair comparison.
",4.2 Link Prediction,[0],[0]
"7https://github.com/ttrouill/complex 8https://github.com/quark0/ANALOGY 9https://github.com/iieir-km/RUGE
10An exception here is that ANALOGY uses asynchronous SGD with AdaGrad (Liu et al., 2017).
",4.2 Link Prediction,[0],[0]
"Among those baselines, RUGE and ComplExR require additional logical background knowledge.",4.2 Link Prediction,[0],[0]
"RUGE makes use of soft rules, which are extracted by AMIE+ from the training sets.",4.2 Link Prediction,[0],[0]
"As suggested by Guo et al. (2018), length-1 and length-2 rules with PCA confidence higher than 0.8 are utilized.",4.2 Link Prediction,[0],[0]
Note that our approach also makes use of AMIE+ rules with PCA confidence higher than 0.8.,4.2 Link Prediction,[0],[0]
"But it only considers entailments between a pair of relations, i.e., length-1 rules.",4.2 Link Prediction,[0],[0]
ComplExR takes into account equivalence and inversion between relations.,4.2 Link Prediction,[0],[0]
We derive such axioms directly from our approximate entailments.,4.2 Link Prediction,[0],[0]
"If rp λ1−→ rq and rq λ2−→ rp with λ1, λ2 > 0.8, we think relations rp and rq are equivalent.",4.2 Link Prediction,[0],[0]
"And similarly, if r−1p λ1−→ rq and r−1q λ2−→",4.2 Link Prediction,[0],[0]
"rp with
λ1, λ2 > 0.8, we consider rp as an inverse of rq.",4.2 Link Prediction,[0],[0]
"For all the methods, we create 100 mini-batches on each dataset, and conduct a grid search to find hyperparameters that maximize MRR on the validation set, with at most 1000 iterations over the training set.",4.2 Link Prediction,[0],[0]
"Specifically, we tune the embedding size d ∈ {100, 150, 200}, the L2 regularization coefficient η ∈ {0.001, 0.003, 0.01, 0.03, 0.1}, the ratio of negative over positive training examples α ∈ {2, 10}, and the initial learning rate γ ∈ {0.01, 0.05, 0.1, 0.5, 1.0}.",4.2 Link Prediction,[0],[0]
"For TransE, we tune the margin of the ranking loss δ ∈ {0.1, 0.2, 0.5, 1, 2, 5, 10}.",4.2 Link Prediction,[0],[0]
"Other hyperparameters of ANALOGY and RUGE are set or tuned according to the default settings suggested by their authors (Liu et al., 2017; Guo et al., 2018).",4.2 Link Prediction,[0],[0]
"After getting the best ComplEx model, we tune the relation constraint penalty of our approach ComplEx-NNE+AER (µ in Eq.",4.2 Link Prediction,[0],[0]
"(8)) in the range of {10−5, 10−4, · · · , 104, 105}, with all its other hyperparameters fixed to their optimal configurations.",4.2 Link Prediction,[0],[0]
We then directly set µ = 0 to get the optimal ComplEx-NNE model.,4.2 Link Prediction,[0],[0]
"The weight of soft constraints in ComplExR is tuned in the same range as µ. The optimal configurations for our approach are: d = 200, η = 0.03, α = 10, γ = 1.0, µ = 10 on WN18; d = 200, η=0.01, α=10, γ = 0.5, µ = 10−3 on FB15K; and d = 150, η = 0.03, α = 10, γ = 0.1, µ = 10−5 on DB100K.
Experimental Results: Table 3 presents the results on the test sets of WN18 and FB15K, where the results for the baselines are taken directly from
previous literature.",4.2 Link Prediction,[0],[0]
"Table 4 further provides the results on the test set of DB100K, with all the methods tuned and tested in (almost) the same setting.",4.2 Link Prediction,[0],[0]
"On all the datasets, we test statistical significance of the improvements achieved by ComplEx-NNE/ ComplEx-NNE+AER over ComplEx, by using a paired t-test.",4.2 Link Prediction,[0],[0]
"The reciprocal rank or HITS@N value with n = 1, 3, 10 for each test triple is used as paired data.",4.2 Link Prediction,[0],[0]
"The symbol “∗” indicates a significance level of p < 0.05.
",4.2 Link Prediction,[0],[0]
The results demonstrate that imposing the nonnegativity and approximate entailment constraints indeed improves KG embedding.,4.2 Link Prediction,[0],[0]
"ComplEx-NNE and ComplEx-NNE+AER perform better than (or at least equally well as) ComplEx in almost all the metrics on all the three datasets, and most of the improvements are statistically significant (except those on WN18).",4.2 Link Prediction,[0],[0]
"More interestingly, just by introducing these simple constraints, ComplEx-NNE+ AER can beat very strong baselines, including the best performing basic models like ANALOGY, those previous extensions of ComplEx like RUGE or ComplExR, and even the complicated developments or implementations like ConvE or Single DistMult.",4.2 Link Prediction,[0],[0]
This demonstrates the superiority of our approach.,4.2 Link Prediction,[0],[0]
This section inspects how the structure of the entity embedding space changes when the constraints are imposed.,4.3 Analysis on Entity Representations,[0],[0]
"We first provide the visualization of entity representations on DB100K. On this dataset each entity is associated with a single type label.11 We pick 4 types reptile, wine region, species, and programming language, and randomly select 30 entities from each type.",4.3 Analysis on Entity Representations,[0],[0]
"Figure 1 visualizes the representations of these entities learned by ComplEx and ComplEx-NNE+AER (real components only), with the optimal configurations determined by link prediction (see § 4.2 for details, applicable to all analysis hereafter).",4.3 Analysis on Entity Representations,[0],[0]
"During the visualization, we normalize the real component of each entity by [x̃]`= [x]`−min(x) max(x)−min(x) , where min(x) or max(x) is the minimum or maximum entry of x respectively.",4.3 Analysis on Entity Representations,[0],[0]
"We observe that after imposing the non-negativity constraints, ComplEx-NNE+AER indeed obtains compact and interpretable representations for entities.",4.3 Analysis on Entity Representations,[0],[0]
Each entity is represented by only a relatively small number of “active” dimensions.,4.3 Analysis on Entity Representations,[0],[0]
"And entities
11http://downloads.dbpedia.org/2016-10/ core-i18n/en/instance_types_wkd_uris_en.",4.3 Analysis on Entity Representations,[0],[0]
"ttl.bz2
with the same type tend to activate the same set of dimensions, while entities with different types often get clearly different dimensions activated.
",4.3 Analysis on Entity Representations,[0],[0]
Then we investigate the semantic purity of these dimensions.,4.3 Analysis on Entity Representations,[0],[0]
"Specifically, we collect the representations of all the entities on DB100K (real components only).",4.3 Analysis on Entity Representations,[0],[0]
"For each dimension of these representations, top K percent of entities with the highest activation values on this dimension are picked.",4.3 Analysis on Entity Representations,[0],[0]
We can calculate the entropy of the type distribution of the entities selected.,4.3 Analysis on Entity Representations,[0],[0]
"This entropy reflects diversity of entity types, or in other words, semantic purity.",4.3 Analysis on Entity Representations,[0],[0]
"If all the K percent of entities have the same type, we will get the lowest entropy of zero (the highest semantic purity).",4.3 Analysis on Entity Representations,[0],[0]
"On the contrary, if each of them has a distinct type, we will get the highest entropy (the lowest semantic purity).",4.3 Analysis on Entity Representations,[0],[0]
"Figure 2 shows the average entropy over all dimensions of entity representations (real components only) learned by ComplEx, ComplEx-NNE, and ComplEx-NNE+
AER, as K varies.",4.3 Analysis on Entity Representations,[0],[0]
"We can see that after imposing the non-negativity constraints, ComplEx-NNE and ComplEx-NNE+AER can learn entity representations with latent dimensions of consistently higher semantic purity.",4.3 Analysis on Entity Representations,[0],[0]
"We have conducted the same analyses on imaginary components of entity representations, and observed similar phenomena.",4.3 Analysis on Entity Representations,[0],[0]
The results are given as supplementary material.,4.3 Analysis on Entity Representations,[0],[0]
This section further provides a visual inspection of the relation embedding space when the constraints are imposed.,4.4 Analysis on Relation Representations,[0],[0]
"To this end, we group relation pairs involved in the DB100K entailment constraints into 3 classes: equivalence, inversion, and others.12 We choose 2 pairs of relations from each class, and visualize these relation representations learned by ComplEx-NNE+AER in Figure 3, where for each relation we randomly pick 5 dimensions from both its real and imaginary components.",4.4 Analysis on Relation Representations,[0],[0]
"By imposing the approximate entailment constraints, these relation representations can encode logical regularities quite well.",4.4 Analysis on Relation Representations,[0],[0]
Pairs of relations from the first class (equivalence) tend to have identical representations,4.4 Analysis on Relation Representations,[0],[0]
"rp ≈ rq, those from the second class (inversion) complex conjugate representations",4.4 Analysis on Relation Representations,[0],[0]
rp ≈ r̄q; and the others representations that Re(rp) ≤ Re(rq) and Im(rp),4.4 Analysis on Relation Representations,[0],[0]
"≈ Im(rq).
",4.4 Analysis on Relation Representations,[0],[0]
12Equivalence and inversion are detected using heuristics introduced in § 4.2 (implementation details).,4.4 Analysis on Relation Representations,[0],[0]
See the supplementary material for detailed properties of these three classes.,4.4 Analysis on Relation Representations,[0],[0]
This paper investigates the potential of using very simple constraints to improve KG embedding.,5 Conclusion,[0],[0]
"Two types of constraints have been studied: (i) the non-negativity constraints to learn compact, interpretable entity representations, and (ii) the approximate entailment constraints to further encode logical regularities into relation representations.",5 Conclusion,[0],[0]
"Such constraints impose prior beliefs upon the structure of the embedding space, and will not significantly increase the space or time complexity.",5 Conclusion,[0],[0]
"Experimental results on benchmark KGs demonstrate that our method is simple yet surprisingly effective, showing significant and consistent improvements over strong baselines.",5 Conclusion,[0],[0]
"The constraints indeed improve model interpretability, yielding a substantially increased structuring of the embedding space.",5 Conclusion,[0],[0]
"We would like to thank all the anonymous reviewers for their insightful and valuable suggestions, which help to improve the quality of this paper.",Acknowledgments,[0],[0]
"This work is supported by the National Key Research and Development Program of China (No. 2016QY03D0503) and the Fundamental Theory and Cutting Edge Technology Research Program of the Institute of Information Engineering, Chinese Academy of Sciences (No. Y7Z0261101).",Acknowledgments,[0],[0]
Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research.,abstractText,[0],[0]
Early works performed this task via simple models developed over KG triples.,abstractText,[0],[0]
"Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples.",abstractText,[0],[0]
"This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding.",abstractText,[0],[0]
We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations.,abstractText,[0],[0]
The former help to learn compact and interpretable representations for entities.,abstractText,[0],[0]
The latter further encode regularities of logical entailment between relations into their distributed representations.,abstractText,[0],[0]
"These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability.",abstractText,[0],[0]
"Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines.",abstractText,[0],[0]
"The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space.",abstractText,[0],[0]
Code and data are available at https://github.com/i ieir-km/ComplEx-NNE_AER.,abstractText,[0],[0]
Improving Knowledge Graph Embedding Using Simple Constraints,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 334–343 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealing for their single-model, end-to-end training process, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al., 2007; Junczys-Dowmunt et al., 2016).",1 Introduction,[0],[0]
"However, there are still many open problems in NMT (Koehn and Knowles, 2017).",1 Introduction,[0],[0]
One particular issue is mistranslation of rare words.,1 Introduction,[0],[0]
"For example, consider the Uzbek sentence:
Source: Ammo muammolar hali ko’p, deydi amerikalik olim Entoni Fauchi.",1 Introduction,[0],[0]
Reference:,1 Introduction,[0],[0]
"But still there are many problems, says American scientist Anthony Fauci.",1 Introduction,[0],[0]
Baseline NMT:,1 Introduction,[0],[0]
"But there is still a lot of problems, says James Chan.
",1 Introduction,[0],[0]
"At the position where the output should be Fauci, the NMT model’s top three candidates are Chan,
1The code for this work can be found at https://github.com/tnq177/improving_lexical_",1 Introduction,[0],[0]
"choice_in_nmt
Fauci, and Jenner.",1 Introduction,[0],[0]
"All three surnames occur in the training data with reference to immunologists: Fauci is the director of the National Institute of Allergy and Infectious Diseases, Margaret (not James) Chan is the former director of the World Health Organization, and Edward Jenner invented smallpox vaccine.",1 Introduction,[0],[0]
"But Chan is more frequent in the training data than Fauci, and James is more frequent than either Anthony or Margaret.
",1 Introduction,[0],[0]
"Because NMT learns word representations in continuous space, it tends to translate words that “seem natural in the context, but do not reflect the content of the source sentence” (Arthur et al., 2016).",1 Introduction,[0],[0]
"This coincides with other observations that NMT’s translations are often fluent but lack accuracy (Wang et al., 2017b; Wu et al., 2016).
",1 Introduction,[0],[0]
Why does this happen?,1 Introduction,[0],[0]
"At each time step, the model’s distribution over output words e is
p(e) ∝",1 Introduction,[0],[0]
"exp ( We · h̃ + be )
where We and be are a vector and a scalar depending only on e, and h̃ is a vector depending only on the source sentence and previous output words.",1 Introduction,[0],[0]
We propose two modifications to this layer.,1 Introduction,[0],[0]
"First, we argue that the term We · h̃, which measures how well e fits into the context h̃, favors common words disproportionately, and show that it helps to fix the norm of both vectors to a constant.",1 Introduction,[0],[0]
"Second, we add a new term representing a more direct connection from the source sentence, which allows the model to better memorize translations of rare words.
",1 Introduction,[0],[0]
"Below, we describe our models in more detail.",1 Introduction,[0],[0]
"Then we evaluate our approaches on eight language pairs, with training data sizes ranging from 100k words to 8M words, and show improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings.",1 Introduction,[0],[0]
"Finally, we provide some analysis to better understand why our modifications work well.
334",1 Introduction,[0],[0]
"Given a source sequence f = f1 f2 · · · fm, the goal of NMT is to find the target sequence e = e1e2 · · · en that maximizes the objective function:
log p(e | f )",2 Neural Machine Translation,[0],[0]
"= n∑
t=1
log p(et | e<t, f ).
",2 Neural Machine Translation,[0],[0]
We use the global attentional model with general scoring function and input feeding by Luong et al. (2015a).,2 Neural Machine Translation,[0],[0]
We provide only a very brief overview of this model here.,2 Neural Machine Translation,[0],[0]
"It has an encoder, an attention, and a decoder.",2 Neural Machine Translation,[0],[0]
"The encoder converts the words of the source sentence into word embeddings, then into a sequence of hidden states.",2 Neural Machine Translation,[0],[0]
The decoder generates the target sentence word by word with the help of the attention.,2 Neural Machine Translation,[0],[0]
"At each time step t, the attention calculates a set of attention weights at(s).",2 Neural Machine Translation,[0],[0]
These attention weights are used to form a weighted average of the encoder hidden states to form a context vector ct.,2 Neural Machine Translation,[0],[0]
From ct and the hidden state of the decoder are computed the attentional hidden state h̃t.,2 Neural Machine Translation,[0],[0]
"Finally, the predicted probability distribution of the t’th target word is:
p(et | e<t, f ) = softmax(Woh̃t + bo).",2 Neural Machine Translation,[0],[0]
"(1)
The rows of the output layer’s weight matrix",2 Neural Machine Translation,[0],[0]
"Wo can be thought of as embeddings of the output vocabulary, and sometimes are in fact tied to the embeddings in the input layer, reducing model size while often achieving similar performance (Inan et al., 2017; Press and Wolf, 2017).",2 Neural Machine Translation,[0],[0]
"We verified this claim on some language pairs and found out that this approach usually performs better than without tying, as seen in Table 1.",2 Neural Machine Translation,[0],[0]
"For this reason, we always tie the target embeddings and Wo in all of our models.",2 Neural Machine Translation,[0],[0]
"The output word distribution (1) can be written as:
p(e) ∝ exp ( ‖We‖ ‖h̃‖ cos θWe,h̃ + be ) ,
where We is the embedding of e, be is the e’th component of the bias bo, and θWe,h̃ is the angle between We and h̃. We can intuitively interpret the terms as follows.",3 Normalization,[0],[0]
"The term ‖h̃‖ has the effect of sharpening or flattening the distribution, reflecting whether the model is more or less certain in a particular context.",3 Normalization,[0],[0]
"The cosine similarity cos θWe,h̃ measures how well e fits into the context.",3 Normalization,[0],[0]
"The bias be controls how much the word e is generated; it is analogous to the language model in a log-linear translation model (Och and Ney, 2002).
",3 Normalization,[0],[0]
"Finally, ‖We‖ also controls how much e is generated.",3 Normalization,[0],[0]
Figure 1 shows that it generally correlates with frequency.,3 Normalization,[0],[0]
"But because it is multiplied by cos θWe,h̃, it has a stronger effect on words whose embeddings have direction similar to h̃, and less effect or even a negative effect on words in other directions.",3 Normalization,[0],[0]
"We hypothesize that the result is that the model learns ‖We‖ that are disproportionately large.
",3 Normalization,[0],[0]
"For example, returning to the example from Section 1, these terms are:
e ‖We‖ ‖h̃‖ cos θWe,h̃ be logit Chan 5.25 19.5 0.144 −1.53 13.2 Fauci 4.69 19.5 0.154 −1.35 12.8 Jenner 5.23 19.5 0.120 −1.59 10.7
Observe that cos θWe,h̃ and even be both favor the correct output word Fauci, whereas ‖We‖ favors the more frequent, but incorrect, word Chan.",3 Normalization,[0],[0]
"The most frequently-mentioned immunologist trumps other immunologists.
",3 Normalization,[0],[0]
"To solve this issue, we propose to fix the norm of all target word embeddings to some value r. Followingthe weight normalization approach of Salimans and Kingma (2016), we reparameterize We as r
ve ‖ve‖ , but keep r fixed.
",3 Normalization,[0],[0]
"A similar argument could be made for ‖h̃t‖: because a large ‖h̃t‖ sharpens the distribution, causing frequent words to more strongly dominate rare words, we might want to limit it as well.",3 Normalization,[0],[0]
"We compared both approaches on a development set and found that replacing h̃t in equation (1) with r
h̃t ‖h̃t‖
indeed performs better, as shown in Table 1.",3 Normalization,[0],[0]
"The attentional hidden state h̃ contains information not only about the source word(s) corresponding to the current target word, but also the contexts of those source words and the preceding context of the target word.",4 Lexical Translation,[0],[0]
This could make the model prone to generate a target word that fits the context but doesn’t necessarily correspond to the source word(s).,4 Lexical Translation,[0],[0]
"Count-based statistical models, by contrast, don’t have this problem, because they simply don’t model any of this context.",4 Lexical Translation,[0],[0]
Arthur et al. (2016) try to alleviate this issue by integrating a count-based lexicon into an NMT system.,4 Lexical Translation,[0],[0]
"However, this lexicon must be trained separately using GIZA++",4 Lexical Translation,[0],[0]
"(Och and Ney, 2003), and its parameters form a large, sparse array, which can be difficult to store in GPU memory.
",4 Lexical Translation,[0],[0]
We propose instead to use a simple feedforward neural network (FFNN) that is trained jointly with the rest of the NMT model to generate a target word based directly on the source word(s).,4 Lexical Translation,[0],[0]
"Let fs (s = 1, . . .",4 Lexical Translation,[0],[0]
",m) be the embeddings of the source words.",4 Lexical Translation,[0],[0]
"We use the attention weights to form a
weighted average of the embeddings (not the hidden states, as in the main model) to give an average source-word embedding at each decoding time step t:
f `t = tanh ∑
s
at(s) fs.
",4 Lexical Translation,[0],[0]
"Then we use a one-hidden-layer FFNN with skip connections (He et al., 2016):
h`t = tanh(W f ` t ) + f ` t
and combine its output with the decoder output to get the predictive distribution over output words at time step t:
p(yt | y<t, x) = softmax(Woh̃t + bo + W`h`t + b`).
",4 Lexical Translation,[0],[0]
"For the same reasons that were given in Section 3 for normalizing h̃t and the rows of Wot , we normalize h`t and the rows of W
` as well.",4 Lexical Translation,[0],[0]
"Note, however, that we do not tie the rows of W` with the word embeddings; in preliminary experiments, we found this to yield worse results.",4 Lexical Translation,[0],[0]
We conducted experiments testing our normalization approach and our lexical model on eight language pairs using training data sets of various sizes.,5 Experiments,[0],[0]
This section describes the systems tested and our results.,5 Experiments,[0],[0]
"We evaluated our approaches on various language pairs and datasets:
• Tamil (ta), Urdu (ur), Hausa (ha), Turkish (tu), and Hungarian (hu) to English (en), using data from the LORELEI program.
",5.1 Data,[0],[0]
"• English to Vietnamese (vi), using data from the IWSLT 2015 shared task.2
• To compare our approach with that of Arthur et al. (2016), we also ran on their English to Japanese (ja) KFTT and BTEC",5.1 Data,[0],[0]
"datasets.3
We tokenized the LORELEI datasets using the default Moses tokenizer, except for Urdu-English, where the Urdu side happened to be tokenized using Morfessor FlatCat (w = 0.5).",5.1 Data,[0],[0]
"We used the preprocessed English-Vietnamese and EnglishJapanese datasets as distributed by Luong et al., and Arthur et al., respectively.",5.1 Data,[0],[0]
Statistics about our data sets are shown in Table 2.,5.1 Data,[0],[0]
"We compared our approaches against two baseline NMT systems:
untied, which does not tie the rows of Wo to the target word embeddings, and tied, which does.
",5.2 Systems,[0],[0]
"In addition, we compared against two other baseline systems:
Moses: The Moses phrase-based translation system (Koehn et al., 2007), trained on the same data as the NMT systems, with the same maximum sentence length of 50.",5.2 Systems,[0],[0]
No additional data was used for training the language model.,5.2 Systems,[0],[0]
"Unlike the NMT systems, Moses used the full vocabulary from the training data; unknown words were copied to the target sentence.",5.2 Systems,[0],[0]
Arthur:,5.2 Systems,[0],[0]
Our reimplementation of the discrete lexicon approach of Arthur et al. (2016).,5.2 Systems,[0],[0]
"We only tried their auto lexicon, using GIZA++",5.2 Systems,[0],[0]
"(Och and Ney, 2003), integrated using their bias approach.",5.2 Systems,[0],[0]
"Note that we also tied embedding as we found it also helped in this case.
",5.2 Systems,[0],[0]
"Against these baselines, we compared our new systems:
fixnorm:",5.2 Systems,[0],[0]
The normalization approach described in Section 3.,5.2 Systems,[0],[0]
fixnorm+lex:,5.2 Systems,[0],[0]
"The same, with the addition of the lexical translation module from Section 4.
2https://nlp.stanford.edu/projects/nmt/ 3http://isw3.naist.jp/~philip-a/emnlp2016/",5.2 Systems,[0],[0]
"Model For all NMT systems, we fed the source sentences to the encoder in reverse order during both training and testing, following Luong et al. (2015a).",5.3 Details,[0],[0]
Information about the number and size of hidden layers is shown in Table 2.,5.3 Details,[0],[0]
"The word embedding size is always equal to the hidden layer size.
",5.3 Details,[0],[0]
"Following common practice, we only trained on sentences of 50 tokens or less.",5.3 Details,[0],[0]
We limited the vocabulary to word types that appear no less than 5 times in the training data and map the rest to UNK.,5.3 Details,[0],[0]
"For the English-Japanese and English-Vietnamese datasets, we used the vocabulary sizes reported in their respective papers (Arthur et al., 2016; Luong and Manning, 2015).
",5.3 Details,[0],[0]
"For fixnorm, we tried r ∈ {3, 5, 7} and selected the best value based on the development set performance, which was r = 5 except for EnglishJapanese (BTEC), where r = 7.",5.3 Details,[0],[0]
"For fixnorm+lex, because Wsh̃t+W`h`t takes on values in [−2r2, 2r2], we reduced our candidate r values by roughly a factor of √ 2, to r ∈ {2, 3.5, 5}.",5.3 Details,[0],[0]
"A radius r = 3.5 seemed to work the best for all language pairs.
",5.3 Details,[0],[0]
"Training We trained all NMT systems with Adadelta (Zeiler, 2012).",5.3 Details,[0],[0]
"All parameters were initialized uniformly from [−0.01, 0.01].",5.3 Details,[0],[0]
"When a gradient’s norm exceeded 5, we normalized it to 5.",5.3 Details,[0],[0]
"We also used dropout on non-recurrent connections only (Zaremba et al., 2014), with probability 0.2.",5.3 Details,[0],[0]
We used minibatches of size 32.,5.3 Details,[0],[0]
"We trained for 50 epochs, validating on the development set after every epoch, except on English-Japanese, where we validated twice per epoch.",5.3 Details,[0],[0]
"We kept the best checkpoint according to its BLEU on the development set.
",5.3 Details,[0],[0]
Inference We used beam search with a beam size of 12 for translating both the development and test sets.,5.3 Details,[0],[0]
"Since NMT often favors short translations (Cho et al., 2014), we followed Wu et al. (2016) in using a modified score s(e | f ) in place of log-probability:
s(e | f ) = log p(e | f ) lp(e)
lp(e) =",5.3 Details,[0],[0]
"(5 + |e|)α (5 + 1)α
",5.3 Details,[0],[0]
We set α = 0.8 for all of our experiments.,5.3 Details,[0],[0]
"Finally, we applied a postprocessing step to replace each UNK in the target translation with the
source word with the highest attention score (Luong et al., 2015b).
",5.3 Details,[0],[0]
"Evaluation For translation into English, we report case-sensitive NIST BLEU against detokenized references.",5.3 Details,[0],[0]
"For English-Japanese and English-Vietnamese, we report tokenized, casesensitive BLEU following Arthur et al. (2016) and Luong and Manning (2015).",5.3 Details,[0],[0]
"We measure statistical significance using bootstrap resampling (Koehn, 2004).",5.3 Details,[0],[0]
Our results are shown in Table 3.,6.1 Overall,[0],[0]
"First, we observe, as has often been noted in the literature, that NMT tends to perform poorer than PBMT on low resource settings (note that the rows of this table are sorted by training data size).
",6.1 Overall,[0],[0]
Our fixnorm system alone shows large improvements (shown in parentheses) relative to tied.,6.1 Overall,[0],[0]
Integrating the lexical module (fixnorm+lex) adds in further gains.,6.1 Overall,[0],[0]
"Our fixnorm+lex models surpass Moses on all tasks except Urdu- and Hausa-English, where it is 1.6 and 0.7 BLEU short respectively.
",6.1 Overall,[0],[0]
"The method of Arthur et al. (2016) does improve over the baseline NMT on most language pairs, but not by as much and as consistently as our models, and often not as well as Moses.",6.1 Overall,[0],[0]
"Unfortunately, we could not replicate their approach for English-Japanese (KFTT) because the lexical table was too large to fit into the computational graph.
",6.1 Overall,[0],[0]
"For English-Japanese (BTEC), we note that, due to the small size of the test set, all systems except for Moses are in fact not significantly different from tied (p > 0.01).",6.1 Overall,[0],[0]
"On all other tasks, however, our systems significantly improve over tied (p < 0.01).",6.1 Overall,[0],[0]
"In Table 4, we show examples of typical translation mistakes made by the baseline NMT systems.",6.2 Impact on translation,[0],[0]
"In the Uzbek example (top), untied and tied have confused 34 with UNK and 700, while in the Turkish one (middle), they incorrectly output other proper names, Afghan and Myanmar, for the proper name Kenya.",6.2 Impact on translation,[0],[0]
"Our systems, on the other hand, translate these words correctly.
",6.2 Impact on translation,[0],[0]
The bottom example is the one introduced in Section 1.,6.2 Impact on translation,[0],[0]
"We can see that our fixnorm approach
does not completely solve the mistranslation issue, since it translates Entoni Fauchi to UNK UNK (which is arguably better than James Chan).",6.2 Impact on translation,[0],[0]
"On the other hand, fixnorm+lex gets this right.",6.2 Impact on translation,[0],[0]
"To better understand how the lexical module helps in this case, we look at the top five translations for the word Fauci in fixnorm+lex:
e cos θWe,h̃ cos θW le,hl be + b l e logit
Fauci 0.522 0.762 −8.71 7.0 UNK 0.566 −0.009 −1.25 5.6 Anthony 0.263 0.644 −8.70 2.4",6.2 Impact on translation,[0],[0]
"Ahmedova 0.555 0.173 −8.66 0.3 Chan 0.546 0.150 −8.73 −0.2
As we can see, while cos θWe,h̃ might still be confused between similar words, cos θW le,hl significantly favors Fauci.",6.2 Impact on translation,[0],[0]
Both our baseline NMT and fixnorm models suffer from the problem of shifted alignments noted by Koehn and Knowles (2017).,6.3 Alignment and unknown words,[0],[0]
"As seen in Figure 2a and 2b, the alignments for those two systems seem to shift by one word to the left (on the source side).",6.3 Alignment and unknown words,[0],[0]
"For example, nói should be aligned to said instead of Telekom, and so on.",6.3 Alignment and unknown words,[0],[0]
"Although this is not a problem per se, since the decoder can decide to attend to any position in the encoder states as long as the state at that position holds the information the decoder needs, this becomes a real issue when we need to make use of the alignment information, as in unknown word replacement (Luong et al., 2015b).",6.3 Alignment and unknown words,[0],[0]
"As we can see in Figure 2, because of the alignment shift, both tied and fixnorm incorrectly replace the two unknown words (in bold) with But Deutsche instead of Deutsche Telekom.",6.3 Alignment and unknown words,[0],[0]
"In contrast, under fixnorm+lex and the model of Arthur et al. (2016), the alignment is corrected, causing the UNKs to be replaced with the correct source words.",6.3 Alignment and unknown words,[0],[0]
"The single most important hyper-parameter in our models is r. Informally speaking, r controls how much surface area we have on the hypersphere to allocate to word embeddings.",6.4 Impact of r,[0],[0]
"To better understand its impact, we look at the training perplexity and dev BLEUs during training with different values of r. Table 6 shows the train perplexity and best tokenized dev BLEU on Turkish-English for fixnorm and fixnorm+lex with different values of r. As we can see, a smaller r results in
hu-en 244 244 (0.599) document (0.005) By (0.003) by (0.002) offices (0.001) befektetéseinek investments (0.151) investment (0.017) Investments (0.015) all (0.012) investing (0.003) kutatás-fejlesztésre research (0.227) Research (0.040) Development (0.014) researchers (0.008) development (0.007)
tu-en ifade expression (0.109) expressed (0.061) express (0.056) speech (0.024) expresses (0.020) cumhurbaşkanı President (0.573) president (0.030) Republic (0.027)",6.4 Impact of r,[0],[0]
"Vice (0.010) Abdullah (0.008) Göstericiler protesters (0.115) demonstrators (0.050) Protesters (0.033) UNK (0.004) police (0.003)
worse training perplexity, indicating underfitting, whereas if r is too large, the model achieves better training perplexity but decrased dev BLEU, indicating overfitting.",6.4 Impact of r,[0],[0]
"One byproduct of lex is the lexicon, which we can extract and examine simply by feeding each source word embedding to the FFNN module and calculating p`(y) = softmax(W`h`+b`).",6.5 Lexicon,[0],[0]
"In Table 5, we show the top translations for some entries in the lexicons extracted from fixnorm+lex for Hungarian, Turkish, and Hausa-English.",6.5 Lexicon,[0],[0]
"As expected, the lexical distribution is sparse, with a few top translations accounting for the most probability mass.",6.5 Lexicon,[0],[0]
"Byte-Pair-Encoding (BPE) (Sennrich et al., 2016) is commonly used in NMT to break words into word-pieces, improving the translation of rare words.",6.6 Byte Pair Encoding,[0],[0]
"For this reason, we reran our experiments using BPE on the LORELEI and EnglishVietnamese datasets.",6.6 Byte Pair Encoding,[0],[0]
"Additionally, to see if our proposed methods work in high-resource scenarios, we run on the WMT 2014 English-German (en-de) dataset,4 using newstest2013 as the development set and reporting tokenized, case-sensitive BLEU on newstest2014 and newstest2015.
",6.6 Byte Pair Encoding,[0],[0]
"We validate across different numbers of BPE operations; specifically, we try {1k, 2k, 3k} merge operations for ta-en and ur-en due to their small sizes, {10k, 12k, 15k} for the other LORELEI datasets and en-vi, and 32k for en-de.",6.6 Byte Pair Encoding,[0],[0]
"Using BPE results in much smaller vocabulary sizes, so we do not apply a vocabulary cut-off.",6.6 Byte Pair Encoding,[0],[0]
"Instead, we train on
4https://nlp.stanford.edu/projects/nmt/
an additional copy of the training data in which all types that appear once are replaced with UNK, and halve the number of epochs accordingly.",6.6 Byte Pair Encoding,[0],[0]
"Our models, training, and evaluation processes are largely the same, except that for en-de, we use a 4-layer decoder and 4-layer bidirectional encoder (2 layers for each direction).
",6.6 Byte Pair Encoding,[0],[0]
"Table 7 shows that our proposed methods also significantly improve the translation when used with BPE, for both high and low resource language pairs.",6.6 Byte Pair Encoding,[0],[0]
"With BPE, we are only behind Moses on Urdu-English.",6.6 Byte Pair Encoding,[0],[0]
"The closest work to our lex model is that of Arthur et al. (2016), which we have discussed already in Section 4.",7 Related Work,[0],[0]
Recent work by Liu et al. (2016) has very similar motivation to that of our fixnorm model.,7 Related Work,[0],[0]
"They reformulate the output layer in terms of directions and magnitudes, as we do here.",7 Related Work,[0],[0]
"Whereas we have focused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes’ directions with something like a margin.",7 Related Work,[0],[0]
"Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification.
",7 Related Work,[0],[0]
Handling rare words is an important problem for NMT that has been approached in various ways.,7 Related Work,[0],[0]
"Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b).",7 Related Work,[0],[0]
"However, these methods only help with unknown words, not rare words.",7 Related Work,[0],[0]
"An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016).",7 Related Work,[0],[0]
Our approach is different in that we try to identify and address the root of the rare word problem.,7 Related Work,[0],[0]
"We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well.
",7 Related Work,[0],[0]
"Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation.",7 Related Work,[0],[0]
"However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model.",7 Related Work,[0],[0]
"It would be interesting to see if our methods benefit the Trans-
former network and other models as well.",7 Related Work,[0],[0]
"In this paper, we have presented two simple yet effective changes to the output layer of a NMT model.",8 Conclusion,[0],[0]
Both of these changes improve translation quality substantially on low-resource language pairs.,8 Conclusion,[0],[0]
"In many of the language pairs we tested, the baseline NMT system performs poorly relative to phrase-based translation, but our system surpasses it (when both are trained on the same data).",8 Conclusion,[0],[0]
"We conclude that NMT, equipped with the methods demonstrated here, is a more viable choice for low-resource translation than before, and are optimistic that NMT’s repertoire will continue to grow.",8 Conclusion,[0],[0]
This research was supported in part by University of Southern California subcontract 67108176 under DARPA contract HR0011-15-C-0115.,Acknowledgements,[0],[0]
Nguyen was supported in part by a fellowship from the Vietnam Education Foundation.,Acknowledgements,[0],[0]
"We would like to express our great appreciation to Sharon Hu for letting us use her group’s GPU cluster (supported by NSF award 1629914), and to NVIDIA corporation for the donation of a Titan X GPU.",Acknowledgements,[0],[0]
We also thank Tomer Levinboim for insightful discussions.,Acknowledgements,[0],[0]
We explore two solutions to the problem of mistranslating rare words in neural machine translation.,abstractText,[0],[0]
"First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value.",abstractText,[0],[0]
"Second, we integrate a simple lexical module which is jointly trained with the rest of the model.",abstractText,[0],[0]
"We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings.1",abstractText,[0],[0]
Improving Lexical Choice in Neural Machine Translation,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003).",1 Introduction,[0],[0]
"Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012).",1 Introduction,[0],[0]
This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning.,1 Introduction,[0],[0]
"Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013).
",1 Introduction,[0],[0]
"Despite these results, the advantage of multimodal over linguistic-only models has only been
demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity.",1 Introduction,[0],[0]
"Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a).",1 Introduction,[0],[0]
"Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b).",1 Introduction,[0],[0]
"Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory (Paivio, 1990), posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded only in the linguistic modality.
",1 Introduction,[0],[0]
Existing multi-modal architectures generally extract and process all the information from their specified sources of perceptual input.,1 Introduction,[0],[0]
"Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types.",1 Introduction,[0],[0]
The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts.,1 Introduction,[0],[0]
"For instance, 72% of word tokens in the British National Corpus (Leech et al., 1994) were rated by contributors to the University of South Florida dataset (USF) (Nelson et al., 2004) as more abstract than the noun war, a concept that many would consider quite abstract.
",1 Introduction,[0],[0]
"In light of these considerations, we propose a novel algorithm for approximating conceptual concreteness.",1 Introduction,[0],[0]
"Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in captur-
835
ing the semantic similarity of concepts.",1 Introduction,[0],[0]
"Further, our algorithm constitutes the first means of quantifying conceptual concreteness that does not rely on labor-intensive experimental studies or annotators.",1 Introduction,[0],[0]
"Finally, we demonstrate the application of this unsupervised concreteness metric to the semantic classification of adjective-noun pairs, an existing NLP task to which concreteness data has proved valuable previously.",1 Introduction,[0],[0]
Our experiments focus on multi-modal models that extract their perceptual input automatically from images.,2 Experimental Approach,[0],[0]
Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation.,2 Experimental Approach,[0],[0]
"They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets.
",2 Experimental Approach,[0],[0]
"We use Google Images as our image source, and extract the first n image results for each concept word.",2 Experimental Approach,[0],[0]
"It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011).",2 Experimental Approach,[0],[0]
"Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets.",2 Experimental Approach,[0],[0]
"Following the motivation outlined in Section 1, we aim to distinguish visual input corresponding to concrete concepts from visual input corresponding to abstract concepts.",2.1 Image Dispersion-Based Filtering,[0],[0]
Our algorithm is motivated by the intuition that the diversity of images returned for a particular concept depends on its concreteness (see Figure 1).,2.1 Image Dispersion-Based Filtering,[0],[0]
"Specifically, we anticipate greater congruence or similarity among a set of images for, say, elephant than among images for happiness.",2.1 Image Dispersion-Based Filtering,[0],[0]
"By exploiting this connection, the method approximates the concreteness of concepts, and provides a basis to filter the corresponding perceptual information.
",2.1 Image Dispersion-Based Filtering,[0],[0]
"Formally, we propose a measure, image dispersion d of a concept word w, defined as the average pairwise cosine distance between all the image representations { ~w1 . . .",2.1 Image Dispersion-Based Filtering,[0],[0]
~wn},2.1 Image Dispersion-Based Filtering,[0],[0]
"in the set of images for that concept:
d(w) = 1 2n(n− 1) ∑ i<j≤n 1− ~wi · ~wj| ~wi||",2.1 Image Dispersion-Based Filtering,[0],[0]
~wj,2.1 Image Dispersion-Based Filtering,[0],[0]
"| (1)
We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid.",2.1 Image Dispersion-Based Filtering,[0],[0]
"In all experiments we set n = 50.
",2.1 Image Dispersion-Based Filtering,[0],[0]
"Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003).",2.1 Image Dispersion-Based Filtering,[0],[0]
"BoVW obtains a vector representation for an
image by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means.
",2.1 Image Dispersion-Based Filtering,[0],[0]
"Previous NLP-related work uses SIFT (Feng and Lapata, 2010; Bruni et al., 2012) or SURF (Roller and Schulte im Walde, 2013) descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors.",2.1 Image Dispersion-Based Filtering,[0],[0]
"We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007).",2.1 Image Dispersion-Based Filtering,[0],[0]
"PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007).",2.1 Image Dispersion-Based Filtering,[0],[0]
"We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008).
",2.1 Image Dispersion-Based Filtering,[0],[0]
"The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images.
",2.1 Image Dispersion-Based Filtering,[0],[0]
"Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994).",2.1 Image Dispersion-Based Filtering,[0],[0]
"This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b).",2.1 Image Dispersion-Based Filtering,[0],[0]
"We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)).
",2.2 Evaluation Gold-standards,[0],[0]
"To test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (Finkelstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators.",2.2 Evaluation Gold-standards,[0],[0]
"WordSim has been used as a benchmark for distributional semantic models in numerous studies (see
e.g. (Huang et al., 2012; Bruni et al., 2012)).",2.2 Evaluation Gold-standards,[0],[0]
"As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004).",2.2 Evaluation Gold-standards,[0],[0]
"This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs.",2.2 Evaluation Gold-standards,[0],[0]
"The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013).",2.2 Evaluation Gold-standards,[0],[0]
"The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators.
",2.2 Evaluation Gold-standards,[0],[0]
We create a representative evaluation set of USF pairs as follows.,2.2 Evaluation Gold-standards,[0],[0]
We randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness.,2.2 Evaluation Gold-standards,[0],[0]
"We denote these sets C, for concrete, and A for abstract respectively.",2.2 Evaluation Gold-standards,[0],[0]
"We then extract all pairs (w1, w2) in the USF dataset such that bothw1 andw2 are inA∪C. This yields an evaluation set of 903 pairs, of which 304 are such that w1, w2 ∈ C and 317 are such that w1, w2 ∈ A.
",2.2 Evaluation Gold-standards,[0],[0]
The images used in our experiments and the evaluation gold-standards can be downloaded from http://www.cl.cam.ac.uk/ ˜dk427/dispersion.html.,2.2 Evaluation Gold-standards,[0],[0]
"We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included.",3 Improving Multi-Modal Representations,[0],[0]
"If not, in accordance with the Dual Coding Theory of human concept processing (Paivio, 1990), only the linguistic representation is used.",3 Improving Multi-Modal Representations,[0],[0]
"For both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter.",3 Improving Multi-Modal Representations,[0],[0]
"We compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations).",3 Improving Multi-Modal Representations,[0],[0]
"Similarity between concept pairs is calculated using cosine similarity.
",3 Improving Multi-Modal Representations,[0],[0]
"As Figure 3 shows, dispersion-filtered multimodal representations significantly outperform
standard multi-modal representations on both evaluation datasets.",3 Improving Multi-Modal Representations,[0],[0]
We observe a 17% increase in Spearman correlation on WordSim353 and a 22% increase on the USF norms.,3 Improving Multi-Modal Representations,[0],[0]
"Based on the correlation comparison method of Steiger (1980), both represent significant improvements (WordSim353, t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1).",3 Improving Multi-Modal Representations,[0],[0]
"In both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g. Bruni et al. (2012)).",3 Improving Multi-Modal Representations,[0],[0]
The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts.,4 Concreteness and Image Dispersion,[0],[0]
"Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail.",4 Concreteness and Image Dispersion,[0],[0]
To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2.,4.1 Quantifying Concreteness,[0],[0]
"By classifying con-
cepts with image dispersion below the median as concrete and concepts above this threshold as abstract we achieved an abstract-concrete prediction accuracy of 81%.
",4.1 Quantifying Concreteness,[0],[0]
"While well-understood intuitively, concreteness is not a formally defined notion.",4.1 Quantifying Concreteness,[0],[0]
Quantities such as the USF concreteness score depend on the subjective judgement of raters and the particular annotation guidelines.,4.1 Quantifying Concreteness,[0],[0]
"According to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation.",4.1 Quantifying Concreteness,[0],[0]
"As illustrated in Figure 4, our binary classification conforms to this characterization.",4.1 Quantifying Concreteness,[0],[0]
"The importance of the visual modality is significantly greater when evaluating on pairs for which both concepts are classified as concrete than on pairs of two abstract concepts.
",4.1 Quantifying Concreteness,[0],[0]
Image dispersion is also an effective predictor of concreteness on samples for which the abstract/concrete distinction is less clear.,4.1 Quantifying Concreteness,[0],[0]
"On a different set of 200 concepts extracted by random sampling from the USF dataset stratified by concreteness rating (including concepts across the concreteness spectrum), we observed a high correlation between abstractness and dispersion (Spearman ρ = 0.61, p < 0.001).",4.1 Quantifying Concreteness,[0],[0]
"On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying
the very abstract or very concrete concepts.",4.1 Quantifying Concreteness,[0],[0]
"As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract.
",4.1 Quantifying Concreteness,[0],[0]
"It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources.",4.1 Quantifying Concreteness,[0],[0]
Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept.,4.1 Quantifying Concreteness,[0],[0]
"By contrast, Sánchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999).",4.1 Quantifying Concreteness,[0],[0]
Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset.,4.1 Quantifying Concreteness,[0],[0]
The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space.,4.1 Quantifying Concreteness,[0],[0]
"In contrast to each of these approaches, the image dispersion approach requires no hand-coded resources.",4.1 Quantifying Concreteness,[0],[0]
"It is therefore more scalable, and instantly applicable to a wide range of languages.",4.1 Quantifying Concreteness,[0],[0]
"Finally, we explored whether image dispersion can be applied to specific NLP tasks as an effective proxy for concreteness.",4.2 Classifying Adjective-Noun Pairs,[0],[0]
Turney et al. (2011) showed that concreteness is applicable to the classification of adjective-noun modification as either literal or non-literal.,4.2 Classifying Adjective-Noun Pairs,[0],[0]
"By applying a logistic regression with noun concreteness as the predictor variable, Turney et al. achieved a classification accu-
racy of 79% on this task.",4.2 Classifying Adjective-Noun Pairs,[0],[0]
"This model relies on significant supervision in the form of over 4,000 human lexical concreteness ratings.1 Applying image dispersion in place of concreteness in an identical classifier on the same dataset, our entirely unsupervised approach achieves an accuracy of 63%.",4.2 Classifying Adjective-Noun Pairs,[0],[0]
This is a notable improvement on the largest-class baseline of 55%.,4.2 Classifying Adjective-Noun Pairs,[0],[0]
"We presented a novel method, image dispersionbased filtering, that improves multi-modal representations by approximating conceptual concreteness from images and filtering model input.",5 Conclusions,[0],[0]
The results clearly show that including more perceptual input in multi-modal models is not always better.,5 Conclusions,[0],[0]
"Motivated by this fact, our approach provides an intuitive and straightforward metric to determine whether or not to include such information.
",5 Conclusions,[0],[0]
"In addition to improving multi-modal representations, we have shown the applicability of the image dispersion metric to several other tasks.",5 Conclusions,[0],[0]
"To our knowledge, our algorithm constitutes the first unsupervised method for quantifying conceptual concreteness as applied to NLP, although it does, of course, rely on the Google Images retrieval algorithm.",5 Conclusions,[0],[0]
"Moreover, we presented a method to classify adjective-noun pairs according to modification type that exploits the link between image dispersion and concreteness.",5 Conclusions,[0],[0]
"It is striking that this apparently linguistic problem can be addressed solely using the raw data encoded in images.
",5 Conclusions,[0],[0]
"In future work, we will investigate the precise quantity of perceptual information to be included for best performance, as well as the optimal filtering threshold.",5 Conclusions,[0],[0]
"In addition, we will explore whether the application of image data, and the interaction between images and language, can yield improvements on other tasks in semantic processing and representation.",5 Conclusions,[0],[0]
DK is supported by EPSRC grant EP/I037512/1.,Acknowledgments,[0],[0]
"FH is supported by St John’s College, Cambridge.",Acknowledgments,[0],[0]
AK is supported by The Royal Society.,Acknowledgments,[0],[0]
SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1.,Acknowledgments,[0],[0]
"We thank the anonymous reviewers for their helpful comments.
",Acknowledgments,[0],[0]
"1The MRC Psycholinguistics concreteness ratings (Coltheart, 1981) used by Turney et al. (2011) are a subset of those included in the USF dataset.",Acknowledgments,[0],[0]
Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition.,abstractText,[0],[0]
"However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others.",abstractText,[0],[0]
"We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings.",abstractText,[0],[0]
"The method relies solely on image data, and can be applied to a variety of other NLP tasks.",abstractText,[0],[0]
Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More,title,[0],[0]
"ar X
iv :1
70 7.
02 45
9v 1
[ cs
.C L
] 8
J ul
(NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, highcoverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement).",text,[0],[0]
"Named entity recognition (NER) is an important NLP task that automatically detects entities in text and classifies them into pre-defined entity types such as persons, organizations, geopolitical entities, locations, events, etc.",1 Introduction,[1.0],"['Named entity recognition (NER) is an important NLP task that automatically detects entities in text and classifies them into pre-defined entity types such as persons, organizations, geopolitical entities, locations, events, etc.']"
"NER is a fundamental component of many information extraction and knowledge
discovery applications, including relation extraction, entity linking, question answering and data mining.
",1 Introduction,[0.9999999729876771],"['NER is a fundamental component of many information extraction and knowledge discovery applications, including relation extraction, entity linking, question answering and data mining.']"
The state-of-the-art NER systems are usually statistical machine learning models that are trained with human-annotated data.,1 Introduction,[0],[0]
"Popular models include maximum entropy Markov models (MEMM) (McCallum et al., 2000), conditional random fields (CRF) (Lafferty et al., 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016).",1 Introduction,[0],[0]
Such models have strong generalization capability to recognize unseen entities1 based on lexical and contextual information (features).,1 Introduction,[0],[0]
"However, a model could still make mistakes if its features favor a wrong entity type, which happens more frequently for unseen entities as we have observed in our experiments.
",1 Introduction,[0.9999999565042131],"['However, a model could still make mistakes if its features favor a wrong entity type, which happens more frequently for unseen entities as we have observed in our experiments.']"
"Wikipedia is an open-access, free-content Internet encyclopedia, which has become the de facto on-line source for general reference.",1 Introduction,[0],[0]
"A Wikipedia page about an entity normally includes both structured information and unstructured text information, and such information can be used to help determine the entity type of the referred entity.
",1 Introduction,[0],[0]
So far there are two classes of approaches that exploit Wikipedia to improve NER.,1 Introduction,[0],[0]
"The first class of approaches use Wikipedia to generate features for NER systems, e.g., (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015).",1 Introduction,[0],[0]
Kazama and Torisawa (2007) try to find the Wikipedia entity for each candidate word sequence and then extract a category label from the first sentence of the Wikipedia entity page.,1 Introduction,[0],[0]
"A part-of-speech (POS) tagger is used to extract the
1An entity is an unseen entity if it does not appear in the
training data used to train the NER model.
category label features in the training and decoding phase.",1 Introduction,[0],[0]
Ratinov and Roth (2009) aggregate several Wikipedia categories into higher-level concept and build a gazetteer on top of it.,1 Introduction,[0],[0]
The two approaches were shown to be able to improve an English NER system.,1 Introduction,[0],[0]
"Both approaches, however, are languagedependent because (Kazama and Torisawa, 2007) requires a POS tagger and (Ratinov and Roth, 2009) requires manual category aggregation by inspection of the annotation guidelines and the training set.",1 Introduction,[0],[0]
"Radford et al. (2015) assume that document-specific knowledge base (e.g., Wikipedia) tags for each document are provided, and they use those tags to build gazetteer type features for improving an English NER system.
",1 Introduction,[0],[0]
"The second class of approaches use Wikipedia to generate weakly annotated data for training multilingual NER systems, e.g., (Richman and Schone, 2008; Nothman et al., 2013).",1 Introduction,[1.0],"['The second class of approaches use Wikipedia to generate weakly annotated data for training multilingual NER systems, e.g., (Richman and Schone, 2008; Nothman et al., 2013).']"
The motivation is that annotating multilingual NER data by human is both expensive and time-consuming.,1 Introduction,[0],[0]
"Richman and Schone (2008) utilize the category information of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.g., category phrase “Living People” is mapped to entity type PERSON).",1 Introduction,[0],[0]
"Such a rule-based entity type mapping is limited both in accuracy and coverage, e.g., (Toral and Muoz, 2006).",1 Introduction,[0],[0]
Nothman et al. (2013) train a Wikipedia entity type classifier using human-annotated Wikipedia pages.,1 Introduction,[0],[0]
"Such a supervised-learning based approach has better accuracy and coverage, e.g., (Dakka and Cucerzan, 2008).",1 Introduction,[0],[0]
A number of heuristic rules are developed in both works to label the Wikipedia text to create weakly annotated NER training data.,1 Introduction,[0],[0]
"The NER systems trained with the weakly annotated data may achieve similar accuracy compared with systems trained with little human-annotated data (e.g., up to 40K tokens as in (Richman and Schone, 2008)), but they are still significantly worse than well-trained systems (e.g., a drop of 23.9 F1 score on the CoNLL data and a drop of 19.6 F1 score on the BBN data as in (Nothman et al., 2013)).
",1 Introduction,[0],[0]
"In this paper, we propose a new class of approaches that utilize Wikipedia to improve multilingual NER systems.",1 Introduction,[1.0],"['In this paper, we propose a new class of approaches that utilize Wikipedia to improve multilingual NER systems.']"
"Central to our approaches is the
construction of high-accuracy, high-coverage multilingual Wikipedia entity type mappings.",1 Introduction,[0],[0]
"We use weakly annotated data to train an English Wikipedia entity type classifier, as opposed to using humanannotated data as in (Dakka and Cucerzan, 2008; Nothman et al., 2013).",1 Introduction,[0],[0]
The accuracy of the classifier is further improved via self-training.,1 Introduction,[0],[0]
We apply the classifier on all the English Wikipedia pages and construct an English Wikipedia entity type mapping that includes entities with high classification confidence scores.,1 Introduction,[0],[0]
"To build multilingual Wikipedia entity type mappings, we generate weakly annotated classifier training data for another language via projection using the inter-language links of Wikipedia.",1 Introduction,[0],[0]
"This approach requires no human annotation or language-dependent knowledge, and thus can be easily applied to new languages.
",1 Introduction,[0],[0]
Our goal is to utilize the Wikipedia entity type mappings to improve NER systems.,1 Introduction,[0],[0]
A natural approach is to use a mapping to create dictionary type features for training an NER system.,1 Introduction,[0],[0]
"In addition, we develop several other approaches.",1 Introduction,[0],[0]
The first approach applies an entity type mapping as a decoding constraint for an NER system.,1 Introduction,[0],[0]
The second approach uses a mapping to post-process the output of an NER system.,1 Introduction,[0],[0]
We also design a robust joint approach that combines the decoding constraint approach and the post-processing approach in a smart way.,1 Introduction,[0],[0]
We evaluate the performance of the Wikipediabased approaches on NER systems trained for 6 languages.,1 Introduction,[0],[0]
"We find that when a system is well trained (e.g., with 200K to 300K tokens of human-annotated data), the dictionary feature approach achieves the best improvement over the baseline system; while when a system is trained with little human-annotated training data (e.g., 20K to 30K tokens), a more aggressive decoding constraint approach achieves the best improvement.",1 Introduction,[0],[0]
"In both scenarios, the Wikipediabased approaches are effective in improving the accuracy on unseen entities, especially when a system is applied to a new domain (3.6 F1 score improvement on political party articles/English NER) or it is trained with little training data (18.3 F1 score improvement on Japanese NER).
",1 Introduction,[0],[0]
We organize the paper as follows.,1 Introduction,[0],[0]
We describe how to build English Wikipedia entity type mapping in Section 2 and extend it to multilingual mappings in Section 3.,1 Introduction,[0],[0]
"We present several Wikipedia-based
approaches for improving NER systems in Section 4 and evaluate their performance in Section 5.",1 Introduction,[0],[0]
We conclude the paper in Section 6.,1 Introduction,[0],[0]
"In this section, we focus on English Wikipedia.",2 English Wikipedia Entity Type Mapping,[0],[0]
"We divide Wikipedia pages into two types:
• Entity pages that describe an entity or object, either a named entity such as “Michael Jordan”
or a common entity such as “Basketball.”
• Non-entity pages that do not describe a certain entity, including disambiguation pages, redi-
rection pages, list pages, etc.
",2 English Wikipedia Entity Type Mapping,[0],[0]
"We have developed an in-house English NER system (Florian et al., 2004).",2 English Wikipedia Entity Type Mapping,[0],[0]
"The system has 51 entity types, and the main motivation of deploying such a fine-grained entity type set is to build cognitive question answering applications on top of the NER system.",2 English Wikipedia Entity Type Mapping,[1.0],"['The system has 51 entity types, and the main motivation of deploying such a fine-grained entity type set is to build cognitive question answering applications on top of the NER system.']"
An important check for a question answering system is the capability to detect whether a particular answer matches the expected type derived from the question.,2 English Wikipedia Entity Type Mapping,[0],[0]
"The entity type system used in this paper has been engineered to cover many of the frequent types that are targeted by naturallyphrased questions (such as PERSON, ORGANIZATION, GPE, TITLEWORK, FACILITY, EVENT, DATE, TIME, LOCATION, etc), and it was created over a long period of time, being updated as more types were found to be useful for question answering, and to improve inter-annotator consistency.
",2 English Wikipedia Entity Type Mapping,[0],[0]
We want to classify Wikipedia pages into one of the entity types used in the NER system.,2 English Wikipedia Entity Type Mapping,[0],[0]
"For nonentity pages and entity pages describing common entities, we assign them with a new type OTHER.",2 English Wikipedia Entity Type Mapping,[0],[0]
"We build maximum entropy classifiers (Nigam et al., 1999) for Wikipedia entity type classification.",2.1.1 Features,[0],[0]
"We use both structured information and unstructured information of a Wikipedia page as features.
",2.1.1 Features,[0],[0]
Each Wikipedia page has a unique title.,2.1.1 Features,[0],[0]
"The title of an entity page is usually the name of the entity, and may include auxiliary information in a bracket
to distinguish entities with the same name.",2.1.1 Features,[0],[0]
We use both the entity name and auxiliary information in a bracket (if any) of a Wikipedia title as features because each could provide useful information for entity type classification.,2.1.1 Features,[0],[0]
"For example, based on the word “Prize” in the title “Nobel Prize” or the word “Awards” in the title “Academy Awards”, one can infer that the entity type is AWARD.",2.1.1 Features,[0],[0]
"Likewise, the auxiliary information “company” in the title “Jordan (company)” indicates that the entity is an ORGANIZATION, and the auxiliary information “film” in the title “Alien (film)” indicates that the entity is a TITLEWORK.
",2.1.1 Features,[0],[0]
The text in a Wikipedia page of an entity provides rich information about the entity.,2.1.1 Features,[0],[0]
A person can usually correctly infer the entity type by reading the first few sentences of the text in a Wikipedia page.,2.1.1 Features,[0],[0]
"Using more sentences provides additional information about the entity which might be helpful, but it is also more likely to introduce noisy information which could affect the classification accuracy adversely.",2.1.1 Features,[0],[0]
"Therefore, we use the first 200 tokens of the text in a Wikipedia page and create n-gram word features out of them.",2.1.1 Features,[0],[0]
"We have also found that including additional n-gram word features of the first sentence in a Wikipedia page results in a better classification accuracy.
",2.1.1 Features,[0],[0]
"Most Wikipedia pages also have a structured table called infobox, which is placed on the right top of a page.",2.1.1 Features,[0],[0]
"An infobox contains attribute-value pairs, often providing summary information about an entity.",2.1.1 Features,[0],[0]
The attributes in an infobox could be particularly useful for entity type classification.,2.1.1 Features,[0],[0]
"For example, the attribute “Born” in an infobox provides strong evidence that the corresponding entity is a PERSON; and the attribute “Headquarters” implies that the corresponding entity is an ORGANIZATION.",2.1.1 Features,[0],[0]
We include the infobox attributes as classifier features.,2.1.1 Features,[0],[0]
"Entity linking (EL) or entity disambiguation is the task of determining the identities of entities mentioned in text, by linking each entity to an entry (if exists) in an open knowledge base such as Wikipedia (Han et al., 2011; Hoffart et al., 2011).",2.1.2 Training and Test Data,[0],[0]
"We apply an EL system (Sil and Florian, 2014) to generate training data for Wikipedia entity type classification as follows: if a named entity in our NER training data
with entity type T is linked to a Wikipedia page, that page will be labeled with entity type T .",2.1.2 Training and Test Data,[0],[0]
"Similarly, we apply the EL system to generate a set of test data by linking named entities in our NER test data to Wikipedia pages.",2.1.2 Training and Test Data,[0],[0]
The English Wikipedia snapshot was dumped in April 2014 which contains around 4.6M pages.,2.1.2 Training and Test Data,[0],[0]
"Using this method we generate a training data set with 4,699 English Wikipedia pages and a test set of 415 English Wikipedia pages.
",2.1.2 Training and Test Data,[0],[0]
Notice that the automatically generated classifier training and test data are weakly labeled since the EL system may link an entity to a wrong Wikipedia page and thus the entity type assigned to that page could be wrong.,2.1.2 Training and Test Data,[1.0],['Notice that the automatically generated classifier training and test data are weakly labeled since the EL system may link an entity to a wrong Wikipedia page and thus the entity type assigned to that page could be wrong.']
"Since the test data is crucial for evaluating the classification accuracy, we manually corrected the output.",2.1.2 Training and Test Data,[0],[0]
"To evaluate the prediction power of different types of features, we train a number of classifiers using only title features, only infobox features, only text features, and all features respectively.",2.1.3 Classifier Performance,[1.0],"['To evaluate the prediction power of different types of features, we train a number of classifiers using only title features, only infobox features, only text features, and all features respectively.']"
We show the F1 score of the classifiers on different entity types in Table 1.,2.1.3 Classifier Performance,[0],[0]
"ALL is the overall performance, and PER (PERSON), ORG (ORGANIZATION), GPE, TITL (TITLEWORK), FAC (FACILITY) are the top five most frequently entity types in the test data.
",2.1.3 Classifier Performance,[0],[0]
"From Table 1, we can see that text features are the most important features for classifying Wikipedia pages, since the classifier trained with only text features achieves an overall F1 score of 87.2, which is better than the classifier trained with either title or infobox features alone.",2.1.3 Classifier Performance,[1.0],"['From Table 1, we can see that text features are the most important features for classifying Wikipedia pages, since the classifier trained with only text features achieves an overall F1 score of 87.2, which is better than the classifier trained with either title or infobox features alone.']"
"Nevertheless, both infobox and title features provide additional useful information for entity type classification, and the classifier trained with all the features achieves an overall F1 score of 90.1.",2.1.3 Classifier Performance,[0],[0]
"Self-training is a semi-supervised learning technique that can be used in applications where there
is only a small number of labeled training examples but a large number of unlabeled examples.",2.1.4 Improvement via Self-Training,[0],[0]
"Since our weakly annotated classifier training data only covers around 1% of all the Wikipedia pages, we are motivated to use self-training to further improve the classification accuracy.
",2.1.4 Improvement via Self-Training,[0],[0]
We first apply a standard self-training approach.,2.1.4 Improvement via Self-Training,[0],[0]
"The classifier trained with the initial training data is used to decode (i.e., classify) all the unlabeled Wikipedia pages to predict their entity types with confidence scores.",2.1.4 Improvement via Self-Training,[0],[0]
We add the self-decoded Wikipedia pages with high confidence scores to the training data and train a new classifier.,2.1.4 Improvement via Self-Training,[1.0],['We add the self-decoded Wikipedia pages with high confidence scores to the training data and train a new classifier.']
Via experiments a threshold of 0.9 is used to sort out highconfident self-decoded examples.,2.1.4 Improvement via Self-Training,[0],[0]
"The F1 score of the new classifier is improved to 91.1, as shown in Table 2.
",2.1.4 Improvement via Self-Training,[0],[0]
"Under the standard approach, about 2.3M selfdecoded examples are added, the size of which is about 500 times of the size of the original training data.",2.1.4 Improvement via Self-Training,[0],[0]
"The errors of the original classifier could be amplified with such a big increase of the training size with so many self-decoded examples.
",2.1.4 Improvement via Self-Training,[0],[0]
"To address this issue, we have developed a sampling-based self-training approach.",2.1.4 Improvement via Self-Training,[0],[0]
"Instead of adding all the self-decoded examples with confidence scores greater than or equal to 0.9, we do a random sampling of those high-confident examples.",2.1.4 Improvement via Self-Training,[0],[0]
"We use a sampling probability p(e) = q ·c(e), where q is a sampling ratio parameter and c(e) is the confidence score of example e.",2.1.4 Improvement via Self-Training,[0],[0]
"Under this approach, examples with higher confidence scores are more likely to be selected, while the total number of selected examples is controlled by the sampling ratio q. Via experiments we found that a small sampling ratio like q = 0.01 yields good improvement (although the improvement is not sensitive to q).",2.1.4 Improvement via Self-Training,[0],[0]
"As shown in Table 2, the classification accuracy under the sampling-based approach is further improved to 91.8 F1 score (the improvement is calculated by averaging over 5 random samples with q = 0.01).",2.1.4 Improvement via Self-Training,[1.0],"['As shown in Table 2, the classification accuracy under the sampling-based approach is further improved to 91.8 F1 score (the improvement is calculated by averaging over 5 random samples with q = 0.01).']"
We construct an English Wikipedia entity type mapping by applying the English Wikipedia entity type classifier on all the English Wikipedia pages (∼4.6M).,2.2 Wikipedia Entity Type Mapping,[1.0],['We construct an English Wikipedia entity type mapping by applying the English Wikipedia entity type classifier on all the English Wikipedia pages (∼4.6M).']
Each entry of the mapping includes an entity name (which is extracted from the title of a Wikipedia page) and the associated entity type with confidence score (which is determined by the classifier).,2.2 Wikipedia Entity Type Mapping,[0],[0]
"We denote the English Wikipedia entity type mapping that includes all the pages by English-WikiMapping.
To build a high-accuracy mapping, one may want to include only entities with confidence scores greater than or equal to a threshold t in the mapping, and we denote such a mapping by English-WikiMapping(t).",2.2 Wikipedia Entity Type Mapping,[0],[0]
"Notice that a mapping with a higher t will have more accurate entity types for its entities, but it will include fewer entities.",2.2 Wikipedia Entity Type Mapping,[0],[0]
"Therefore, there is a trade-off between accuracy and coverage of the mapping, which can be tuned by the confidence threshold t. There are about 2.9M entities in English-Wiki-Mapping(0.9), which covers about 63% of all the English Wikipedia pages.
",2.2 Wikipedia Entity Type Mapping,[0],[0]
"We have also found that the length of an entity name (i.e., number of words in an entity name) also plays an important role for determining which entities should be included in the mapping for improving an NER system.",2.2 Wikipedia Entity Type Mapping,[0],[0]
"Therefore, we use EnglishWiki-Mapping(t, i) to denote the English Wikipedia entity type mapping that includes all the entities with confidence scores greater than or equal to t and at least i words in their names.",2.2 Wikipedia Entity Type Mapping,[1.0],"['Therefore, we use EnglishWiki-Mapping(t, i) to denote the English Wikipedia entity type mapping that includes all the entities with confidence scores greater than or equal to t and at least i words in their names.']"
"English-WikiMapping(0.9,2) covers about 55% of all the English Wikipedia pages, and English-Wiki-Mapping(0.9,3) covers about 25% of all the English Wikipedia pages.",2.2 Wikipedia Entity Type Mapping,[0],[0]
"Based on the English Wikipedia entity type mapping, we want to build high-accuracy, high-coverage Wikipedia entity type mappings for other languages with minimum human annotation and languagedependent knowledge involved.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"We utilize the interlanguage links of Wikipedia, which are the links between one entity’s pages in different languages.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"The inter-language links between English Wikipedia
pages and Wikipedia pages of another language provide useful information for this task.
",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"Suppose we want to build a Wikipedia entity type mapping for a new language, and we use Portuguese as an example.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"A direct approach is projection using the inter-language links between English and Portuguese Wikipedia pages: for each Portuguese Wikipedia page that has an inter-language link to an English Wikipedia page, we project the entity type of the English Wikipedia page (determined by the English entity type mapping) to the Portuguese Wikipedia page.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"The rationale is that both the English and Portuguese pages are describing the same entity, even probably with different spelling (e.g., United States in English vs. Estados Unidos in Portuguese), the entity type of that entity does not change from one language to another.
",3 Multilingual Wikipedia Entity Type Mapping,[0.9999999268848682],"['The rationale is that both the English and Portuguese pages are describing the same entity, even probably with different spelling (e.g., United States in English vs. Estados Unidos in Portuguese), the entity type of that entity does not change from one language to another.']"
"However, the main limitation of the direct projection approach is coverage.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"Only a fraction of all the Portuguese Wikipedia pages have inter-language links to English Wikipedia pages, and among those pages only a subset of them have classified entity types with confidence scores high enough (e.g., at least 0.9).",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"For example, projecting EnglishWiki-Mapping(0.9) to Portuguese Wikipedia returns 143K pages, which covers only 15% of all the Portuguese Wikipedia pages (around 920K in total).
",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"We apply an alternative approach, which uses the 143K Portuguese Wikipedia pages (acquired by projection from English-Wiki-Mapping(0.9))",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
as weakly annotated training data to train a Portuguese Wikipedia entity type classifier.,3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"For feature engineering purpose, we also project the English Wikipedia entity type classifier training and test data (as described in Section 2.1.2) to Portuguese Wikipedia pages via inter-language links, and this produces 1,190 Portuguese Wikipedia pages which are used as the test data.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"Pages in the test data set are excluded from the 143K training data set.
",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"We use similar features (title, infobox and text) as for the English classifiers to train the Portuguese classifiers.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
Again we find that the classifier trained with all the features achieves the best accuracy of 86.3 F1 score.,3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"Notice that this is an approximated evaluation because the pages in the test data set are labeled via projection and not by human.
",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"We build Portuguese Wikipedia entity type mappings by applying the Portuguese Wikipedia en-
tity type classifier on all the Portuguese Wikipedia pages.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"We use Portuguese-Wiki-Mapping(t) to denote the mapping that includes entities with confidence scores greater than or equal to a threshold t. There are 525K entities in Portuguese-WikiMapping(0.9), which covers about 57% of all the Portuguese Wikipedia pages, a significant improvement of coverage compared to the direct projection approach (15%).
",3 Multilingual Wikipedia Entity Type Mapping,[0.9987412233353588],"['We use Portuguese-Wiki-Mapping(t) to denote the mapping that includes entities with confidence scores greater than or equal to a thresh- old t. There are 525K entities in Portuguese-WikiMapping(0.9), which covers about 57% of all the Portuguese Wikipedia pages, a significant improvement of coverage compared to the direct projection approach (15%).']"
"The main advantage of our approach is that no human annotation or language-dependent knowledge is required, so it can be easily applied to a new language.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
"We have applied this approach to build high-accuracy, high-coverage Wikipedia entity type mappings for several new languages including Portuguese, Japanese, Spanish, Dutch and German.",3 Multilingual Wikipedia Entity Type Mapping,[0],[0]
We have developed several approaches that utilize the Wikipedia entity type mappings to improve NER systems.,4 Improving NER Systems,[0],[0]
Let M be a Wikipedia entity type mapping.,4 Improving NER Systems,[0],[0]
"For an entity name x, let M(x) denote the set of possible entity types for x determined by the mapping.",4 Improving NER Systems,[0],[0]
"If an entity name x is in the mapping, then M(x) includes at least one entity type, i.e., |M(x)| ≥ 1, where |M(x)| is the cardinality of M(x).",4 Improving NER Systems,[0],[0]
"Otherwise if an entity name x is not in the mapping, then M(x) = ∅ is the empty set and |M(x)| = 0.
",4 Improving NER Systems,[0],[0]
The first approach is to use a Wikipedia entity type mapping M as a decoding constraint for an NER system.,4 Improving NER Systems,[0],[0]
"Under this approach, the mapping is applied as a constraint during the decoding procedure: if a sequence of words in the text form an entity name x that is included in the mapping, i.e., |M(x)| ≥ 1, then the sequence of words will be identified as an entity, and its entity type is determined by the decoding algorithm while being constrained to one of the entity types in M(x).
",4 Improving NER Systems,[0.9993881177536615],"['Under this approach, the mapping is applied as a constraint during the decoding procedure: if a sequence of words in the text form an entity name x that is included in the mapping, i.e., |M(x)| ≥ 1, then the sequence of words will be identified as an entity, and its entity type is determined by the decoding algorithm while being constrained to one of the entity types inM(x).']"
The second approach is to use a Wikipedia entity type mapping M to post-process the output of an NER system.,4 Improving NER Systems,[1.0],['The second approach is to use a Wikipedia entity type mapping M to post-process the output of an NER system.']
"Under this approach, the mapping is applied after the decoding procedure: if the name of a system entity x is in the mapping and the entity type for that entity name is unique based on the mapping, i.e., |M(x)| = 1, then its entity type will be determined by the unique entity type in M(x).
",4 Improving NER Systems,[0],[0]
"The decoding constraint approach is more aggressive than the post-processing approach, because it may create new entities and change entity boundaries.",4 Improving NER Systems,[0],[0]
This approach is more reliable for entities with longer names.,4 Improving NER Systems,[0],[0]
"Via experiments we find that using Wiki-Mapping(0.9,2) or Wiki-Mapping(0.9,3) achieves the best improvement under the decoding constraint approach.",4 Improving NER Systems,[0.9964418725188773],"['Via experiments we find that using Wiki-Mapping(0.9,1) or Wiki-Mapping(0.9,2) achieves the best improvement under the dictionary feature approach.']"
"Remember Wiki-Mapping(t, i) includes all the entities with confidence scores at least t and at least i words in their names.
",4 Improving NER Systems,[0],[0]
"In contrast, the post-processing approach is a more conservative approach since it relies on the system entity boundaries and only changes their entity types if determined by the mapping, so it will not create new entities.",4 Improving NER Systems,[1.0],"['In contrast, the post-processing approach is a more conservative approach since it relies on the system entity boundaries and only changes their entity types if determined by the mapping, so it will not create new entities.']"
"Via experiments we find that using Wiki-Mapping(0.9,2) achieves the best improvement under the post-processing approach.
",4 Improving NER Systems,[0],[0]
"Based on the observation that the decoding constraint approach is more reliable for longer entities while the post-processing approach can better handle short entities, we have designed a joint approach that combines the two approaches as follows: it first applies Wiki-Mapping(0.9,3) as a decoding constraint for an NER system to produce system entities, and then applies Wiki-Mapping(0.9,2) to post-process the system output.",4 Improving NER Systems,[1.0],"['Based on the observation that the decoding constraint approach is more reliable for longer entities while the post-processing approach can better handle short entities, we have designed a joint approach that combines the two approaches as follows: it first applies Wiki-Mapping(0.9,3) as a decoding constraint for an NER system to produce system entities, and then applies Wiki-Mapping(0.9,2) to post-process the system output.']"
"The joint approach combines the advantages of both approaches and achieves robust performance in our experiments.
",4 Improving NER Systems,[0],[0]
"Finally, we can use a Wikipedia entity type mapping to create dictionary features for training an NER system.",4 Improving NER Systems,[0],[0]
"The idea of using Wikipedia to create training features was explored before, e.g., (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015).",4 Improving NER Systems,[0],[0]
"The difference between our approach and the previous approaches is how the features are created: we first build high-accuracy, high-coverage multilingual Wikipedia entity type mappings and then use the mappings to generate dictionary features.",4 Improving NER Systems,[0],[0]
"Via experiments we find that using Wiki-Mapping(0.9,1) or Wiki-Mapping(0.9,2) achieves the best improvement under the dictionary feature approach.",4 Improving NER Systems,[0],[0]
"In this section, we evaluate the effectiveness of the proposed Wikipedia-based approaches via experiments on NER systems trained for 6 languages:
English, Portuguese, Japanese, Spanish, Dutch and German.",5 Experiments,[0],[0]
"For each language, we compare the baseline NER system with the following approaches:
• DC(i): the decoding constraint approach with mapping Language-Wiki-Mapping(0.9,i).
",5 Experiments,[0.99999996605367],"['For each language, we compare the baseline NER system with the following approaches: • DC(i): the decoding constraint approach with mapping Language-Wiki-Mapping(0.9,i).']"
"• PP(i): the post-processing approach with mapping Language-Wiki-Mapping(0.9,i).
",5 Experiments,[0],[0]
"• Joint: the joint approach that combines DC(3) and PP(2).
",5 Experiments,[0],[0]
"• DF(i): the dictionary feature approach with mapping Language-Wiki-Mapping(0.9,i).
",5 Experiments,[0],[0]
"To evaluate the generalization capability of an NER system, we compute the F1 score on the unseen entities (Unseen) as well as on all the entities (All) in a test data set.",5 Experiments,[1.0],"['To evaluate the generalization capability of an NER system, we compute the F1 score on the unseen entities (Unseen) as well as on all the entities (All) in a test data set.']"
The baseline English NER system is a CRF model trained with 328K tokens of human-annotated news articles.,5.1 English,[0],[0]
"It uses standard NER features in the literature including n-gram word features, word type features, prefix and suffix features, Brown cluster type features, gazetteer features, document-level cache features, etc.
",5.1 English,[0],[0]
"We have two human-annotated test data sets: the first set, Test (News), consists of 40K tokens of human-annotated news articles; and the second set, Test (Political), consists of 77K tokens of humanannotated political party articles from Wikipedia.",5.1 English,[1.0],"['We have two human-annotated test data sets: the first set, Test (News), consists of 40K tokens of human-annotated news articles; and the second set, Test (Political), consists of 77K tokens of humanannotated political party articles from Wikipedia.']"
"The results are shown in Table 3.
",5.1 English,[0],[0]
"For Test (News) which is in the same domain as the training data, the baseline system achieves 88.2 F1 score on all the entities, and a relatively low F1 score of 78.7 on the unseen entities (38% of all the entities are unseen entities).",5.1 English,[1.0],"['For Test (News) which is in the same domain as the training data, the baseline system achieves 88.2 F1 score on all the entities, and a relatively low F1 score of 78.7 on the unseen entities (38% of all the entities are unseen entities).']"
The dictionary feature approach DF(2) achieves the highest F1 scores among the Wikipedia-based approaches.,5.1 English,[0],[0]
It improves the baseline system by 1.2 F1 score on all the entities and by 3.1 F1 score on the unseen entities.,5.1 English,[0],[0]
The joint approach achieves the second highest F1 scores.,5.1 English,[0],[0]
"It improves the baseline by 0.7 F1 score on all the entities and by 2.0 F1 score on the unseen entities.
",5.1 English,[0],[0]
"For Test (Political) which is in a different domain from the training data, the fraction of unseen entities increases to 84%.",5.1 English,[0],[0]
"In this case, the F1 score of the
baseline system drops to 64.1, and the Wikipediabased approaches demonstrate larger improvements.",5.1 English,[0],[0]
"For example, DF(2) improves the baseline system by 2.7 F1 score on all the entities and by 3.6 F1 score on the unseen entities.",5.1 English,[0],[0]
"For Portuguese, we have applied a semi-supervised learning approach to build the baseline NER system.",5.2 Portuguese,[0],[0]
"The training data set includes 31K tokens of humanannotated news articles, and 2M tokens of weakly annotated data.",5.2 Portuguese,[0],[0]
The weakly annotated data is generated as follows.,5.2 Portuguese,[0],[0]
We have a large number of parallel sentences between English and Portuguese news articles.,5.2 Portuguese,[0],[0]
"We apply the English NER system on the English sentences and project the entity type tags to the Portuguese sentences via alignments between the English and Portuguese sentences.
",5.2 Portuguese,[0],[0]
"The baseline NER system is an MEMM model (CRF cannot handle such a big size of training data, since our NER system has 51 entity types, and the number of features and training time of CRF grow at least quadratically in the number of entity types).",5.2 Portuguese,[1.0],"['The baseline NER system is an MEMM model (CRF cannot handle such a big size of training data, since our NER system has 51 entity types, and the number of features and training time of CRF grow at least quadratically in the number of entity types).']"
"The test data set consists of 34K tokens of humanannotated Portuguese news articles.
",5.2 Portuguese,[0],[0]
The results are shown in Table 4.,5.2 Portuguese,[0],[0]
"Because the system is trained with little human-annotated training data, the performance of the baseline system achieves only 60.1 F1 score on all the entities and 50.2 F1 score on the unseen entities (80% of all the entities).",5.2 Portuguese,[0],[0]
"In this case, the more aggressive decoding constraint approach DC(2) achieves the best improvement among the Wikipedia-based approaches, which improves the baseline by 5.9 F1 score on all
the entities and by 8.6 F1 score on the unseen entities.",5.2 Portuguese,[1.0000000112927512],"['In this case, the more aggressive decoding constraint approach DC(2) achieves the best improvement among the Wikipedia-based approaches, which improves the baseline by 5.9 F1 score on all the entities and by 8.6 F1 score on the unseen entities.']"
The joint approach improves the baseline by 3.0 F1 score on all the entities and by 4.3 F1 score on the unseen entities.,5.2 Portuguese,[0],[0]
"For Japanese, the baseline NER system is an MEMM model trained with 20K tokens of humanannotated news articles and 2.1M tokens of weakly annotated data.",5.3 Japanese,[0],[0]
The weakly annotated data was generated using similar steps as for the Portuguese NER system.,5.3 Japanese,[0],[0]
"The test data set consists of 22K tokens of human-annotated Japanese news articles.
",5.3 Japanese,[0],[0]
The results are shown in Table 5.,5.3 Japanese,[0],[0]
"Again, in this low-resource case, DC(2) achieves the best improvement among the Wikipedia-based approaches.",5.3 Japanese,[0],[0]
It improves the baseline by 9.0 F1 score on all the entities and by 18.3 F1 score on the unseen entities (59% of all the entities).,5.3 Japanese,[0],[0]
The joint approach improves the baseline by 4.8 F1 score on all the entities and by 9.6 F1 score on the unseen entities.,5.3 Japanese,[0],[0]
"We also evaluate the Wikipedia-based approaches on Spanish, Dutch and German NER systems trained with the CoNLL data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).","5.4 Spanish, Dutch and German",[0],[0]
"There are only 4 entity types in the CoNLL data: PER (person), ORG (organization), LOC (location), MISC (miscellaneous names).","5.4 Spanish, Dutch and German",[0],[0]
"Accordingly, we have trained a CoNLL-style Wikipedia entity type classifier that produces the CoNLL entity types.","5.4 Spanish, Dutch and German",[0],[0]
"The training data for the classifier is generated by using the CoNLL English training data set and the AIDA-YAGO2 data set that provides the Wikipedia titles for the named entities in the CoNLL English data set (Hoffart et al., 2011).","5.4 Spanish, Dutch and German",[0],[0]
"Applying the classifier on all the English Wikipedia pages, we construct a CoNLL-style English Wikipedia entity type mapping.","5.4 Spanish, Dutch and German",[0],[0]
"We then build CoNLL-style Wikipedia entity type mappings for Spanish, Dutch and German using steps as described in Section 3.
","5.4 Spanish, Dutch and German",[0],[0]
"For each of the three languages, the baseline NER system is a CRF model trained with humanannotated news data (∼200K tokens), and there are two test data sets, TestA and TestB, that are also human-annotated news data (ranging from 40K to 70K tokens).","5.4 Spanish, Dutch and German",[0],[0]
The results are shown in Table 6.,"5.4 Spanish, Dutch and German",[0],[0]
"For Dutch and German, DF(1) achieves the best improvement among the Wikipedia-based approaches.","5.4 Spanish, Dutch and German",[0],[0]
"For Spanish, the joint approach achieves the best improvement among the Wikipedia-based approaches.","5.4 Spanish, Dutch and German",[0],[0]
"Again, in all cases, the Wikipedia-based approaches demonstrate larger improvements (ranging from 1.0 to 3.4 F1 score) on the unseen entities.","5.4 Spanish, Dutch and German",[0],[0]
"From the experimental results, we have the following observations:
• NER systems are more likely to make mistakes on unseen entities.",5.5 Discussion,[0],[0]
"In all cases, the F1 score
of an NER system on all the entities is always higher than the F1 score on the unseen entities.
",5.5 Discussion,[0],[0]
"• The Wikipedia-based approaches are effective in improving the generalization capability of
NER systems (i.e., improving the accuracy on unseen entities), especially when a system is
applied to a new domain (3.6 F1 score improvement on political party articles/English NER) or it is trained with little human-annotated training data (18.3 F1 score improvement on Japanese NER).
",5.5 Discussion,[0],[0]
"• In the low-resource scenario where an NER system is trained with little human-annotated
data (e.g., 20K-30K tokens of training data for the Portuguese and Japanese systems), the decoding constraint approach, which uses a highaccuracy, high-coverage Wikipedia entity type mapping to create constraints during the decoding phase, achieves the best improvement.
",5.5 Discussion,[0],[0]
"• In the rich-resource scenario where an NER system is well trained (e.g., 200K-300K tokens
of training data for the English, Dutch and German systems), the dictionary feature approach, which uses a Wikipedia entity type mapping to create dictionary type features, achieves the best improvement.
",5.5 Discussion,[0.9999999512314639],"['• In the rich-resource scenario where an NER system is well trained (e.g., 200K-300K tokens of training data for the English, Dutch and German systems), the dictionary feature approach, which uses a Wikipedia entity type mapping to create dictionary type features, achieves the best improvement.']"
"• In both scenarios, the joint approach, which combines the decoding constraint approach and
the post-processing approach in a smart way, achieves relatively robust performance among the Wikipedia-based approaches.",5.5 Discussion,[0.9999999524008172],"['• In both scenarios, the joint approach, which combines the decoding constraint approach and the post-processing approach in a smart way, achieves relatively robust performance among the Wikipedia-based approaches.']"
"In this paper, we proposed and evaluated several approaches that utilize high-accuracy, high-coverage Wikipedia entity type mappings to improve multilingual NER systems.",6 Conclusion,[0],[0]
"These mappings are built from weakly annotated data, and can be easily extended to new languages with no human annotation or language-dependent knowledge involved.
",6 Conclusion,[0],[0]
Experimental results show that the Wikipediabased approaches are effective in improving the generalization capability of NER systems.,6 Conclusion,[0],[0]
"When a system is well trained, the dictionary feature approach achieves the best improvement over the baseline system; while when a system is trained with little human-annotated training data, a more aggressive decoding constraint approach achieves the best improvement.",6 Conclusion,[1.0],"['When a system is well trained, the dictionary feature approach achieves the best improvement over the baseline system; while when a system is trained with little human-annotated training data, a more aggressive decoding constraint approach achieves the best improvement.']"
"The improvements are larger on unseen entities, and the approaches are especially useful when a system is applied to a new domain or it is trained with little training data.",6 Conclusion,[1.0],"['The improvements are larger on unseen entities, and the approaches are especially useful when a system is applied to a new domain or it is trained with little training data.']"
"We would like to thank Avirup Sil for helpful comments, and for collecting the Wikipedia data.",Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their suggestions.,Acknowledgments,[0],[0]
"The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information.",abstractText,[0],[0]
"However, such a model could still make mistakes if its features favor a wrong entity type.",abstractText,[0],[0]
"In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems.",abstractText,[0],[0]
"Central to our approach is the construction of high-accuracy, highcoverage multilingual Wikipedia entity type mappings.",abstractText,[0],[0]
These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved.,abstractText,[0],[0]
"Based on these mappings, we develop several approaches to improve an NER system.",abstractText,[0],[0]
We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages.,abstractText,[0],[0]
"Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement).",abstractText,[0],[0]
