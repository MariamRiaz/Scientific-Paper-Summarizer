1	51	Compared with traditional sentiment analysis tasks, which extract the overall sentiment of a document, a sentence or a tweet, targeted sentiment analysis extracts the sentiment over given targeted entities from a text, and therefore is practically more informative.
17	29	They model the joint task 612 as an extension to the NER task, where an extra sentiment label is assigned to each named entity, in addition to the entity label.
31	33	Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task.
33	29	First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features.
84	50	The links between labels and inputs represent output clique potentials: Ψ(~x, yi) = exp { ~θ · ~f(~x, yi) } , where ~f(~x, yi), is a discrete manual feature vector, and ~θ is the model parameter vector.
86	29	For both the pipeline and collapsed models, the conditional probability of a label sequence given an input sequence is: P (~y|~x) = |x|∏ i=1 Ψ(~x, yi) |x|∏ j=1 Φ(~x, yi, yi−1) Z(~x) , where Z(~x) is the partition function: Z(~x) = ∑ ~y′ ( |x|∏ i=1 Ψ(~x, y′i) |x|∏ j=1 Φ(~x, y′i, y ′ i−1) ) , For the joint model, we apply a multi-label CRF structure, where there are two separate sets of output clique potentials Ψ1(~x, yi) and Ψ2(~x, zi) and two separate sets of edge clique potentials Φ1(~x, yi, yi−1) and Φ2(~x, zi, zi−1) for the label sets {B, I,O} and {+,−, 0}, respectively.
98	35	Second, a hidden neural layer ~h is added between the input nodes ~x and the label nodes yi.
99	40	Formally, the links between the input nodes ~x and the hidden nodes ~hi for the node yi in Figure 4 represent a feature combination function: ~hi =tanh ( W.(e(~xi−2)⊕ e(~xi−1)⊕ e(~xi) ⊕ e(~xi+1)⊕ e(~xi+2)) +~b ) where e is the embedding lookup function, ⊕ is the vector concatenation function, the matrix W and vector ~b are model parameters and tanh is the activation function.
100	25	The output clique potential of yi becomes: Ψ(~x, yi) = exp { ~σ · ~hi } where ~σ is a model parameter, and the edge clique potentials remain the same as the baseline.
105	18	Gleaning different sources of information, neural features and discrete linear features comple- ments each other.
108	20	We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5.
121	40	Although giving comparable overall accuracies in both entity and sentiment labels, it suffers from unbalanced sentiment labels, assigning the neutral sentiment to most entities.
124	21	In contrast, max-margin training does not suffer from the label skew issue, thanks to the use of Hamming loss in the objective function.
130	54	Parameters: For all the neural models, we set the hidden layer size |~h| for neural features to 200, the hidden layer size |~g| for discrete features to 30, the initial learning rate for adagrad to 0.01 and the regularization parameter λ to 10−8.
131	18	English and Spanish word embeddings are trained using the word2vec tool4, with respective corpora of 20 minion random tweets crawled by tweet API5.
139	20	The main results on both the English and Spanish dataset are shown in Table 3, which are measured on the pipeline, the joint and the collapsed tasks, respectively.
141	21	The gains on English are mostly attributed to improved recalls, while the precision of the neural CRF models are relatively lower.
143	26	As a result, the neural model can better capture patterns that do not occur in the training data.
144	47	In contrast, the discrete model is based on manually defined binary features, which do not fire if not contained in the training data.
145	23	Because discrete feature instantiation is based on exact matching, the discrete model gives a relatively higher precision.
149	19	As shown in Table 3, the integrated model combines the relative advantages of both pure models, improving the recall over the discrete model and the precision over the neural model.
155	67	By fine-tuning, embeddings of in-vocabulary words are treated as model parameters, and updated with other parameters in supervised training.
156	34	This can improve the accuracy of the model by significantly enlarging the parameter space.
159	19	Similar findings apply to the English data.
161	127	On the other hand, thanks to the rich discrete features in parameter space, the integrated model does not rely on fine-tuning of word embeddings, which even caused slight overfitting and reduced the performances.
162	156	This makes the non-fine-tuned integrated model potentially advantageous in handling test data with many OOV words.
167	27	A more detail analysis, however, shows some relative strengths of the joint task.
172	25	When there is external resource for en- tity recognition, the pipeline can be a favorable choice.
176	53	Given complementary error distributions by the discrete and neural CRFs, we proposed a novel combination which significantly outperformed both models.
177	83	Under the neural setting, we find that it is preferable to solve open targeted sentiment as a pipeline or joint multi-label task, but not as a joint task with collapsed labels.
