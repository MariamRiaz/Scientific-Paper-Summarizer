0	32	Microblogging services, such as Twitter and Facebook, are today capturing the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic.
1	55	The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.1 From such micro-blogging services’ data streams, researchers have reported mining insights about a variety of domains, from election results (Tumasjan et al., 2010) and democracy movements (Starbird and Palen, 2012) to health issues and disease spreading (Paul and Dredze, 2011; Sadilek et al., 2012), as well as tracking product feedback and sentiment (Asur and Huberman, 2010).
14	112	Our primary contribution in this paper is a recasting and merging of the tasks of mention detection and entity disambiguation into a single endto-end entity linking task.
16	32	Treating detection and disambiguation as a single task also enables us to apply a large set of new features, conventionally used only for disambiguation, to the initial detection of mentions.
19	30	And don’t worry Ben, we already forgave you for Gigli.
20	93	Determining whether or not “The town” is a mention of a location or other specific entity based solely on lexical and syntactic features is challenging.
21	73	Knowing “The Town” is the name of a recent movie helps, and we can we be more confident if we know that Ben Affleck is an actor in the movie, and Gigli is another of his movies.
22	33	To train and evaluate our system, we created three separate annotated data sets of approximately 500 tweets each.
24	15	We evaluate our system by comparing its performance at detecting entities to the performance of two state-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1.
25	95	The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results.
67	13	In our framework, we use structural learning as a tool to capture the relationship between entities.
83	53	Link probability Pl(s(c)) is the probability that a phrase is used as an anchor in Wikipedia.
84	13	We also add a third feature that captures normalized link count.
89	64	Popularity Feature We have access to 300GBs of Wikipedia page view counts, representing one months worth of page view information, we use this as popularity data.3 As mentioned in Section 3, we find that the most often linked Wikipedia articles might not be the most popular ones on Twitter.
90	12	Using page view statistics helps our system correct this bias.
92	68	Entity Type and Tf-idf We use the procedure proposed in (Ratinov et al., 2011) to extract keyword phrases from categories for each Wikipedia page, and then build a rule-based system using keyword phrases to classify if each entity page belongs to one of the following entity types: Person, Location, Organization, TV Show, Book/Magazine and Movie.4 For a given candidate c and an entity e, the associated binary feature becomes active if the entity belongs to a specific entity type.
114	30	As contextual words are often shared within the same entity type (e.g. “watching” is likely to appear before a tv show), those words can potentially improve our final system.
117	32	The second rule filter outs noisy words (e.g., Twitter handles) in the unlabeled set.
119	15	To train the second end-to-end entity linking system, we add one additional feature for the contextual words.
120	18	For the feature vector Φ(t, ci, e), the context feature is active if the candidate ci is capitalized6 and the context words around ci belongs to Q(R), given R is the entity type for the entity e.
130	15	In order to train and test the SSVM model, one needs to solve both the inference problem Eq.
139	27	We next describe the two sets of Twitter data used as our training data and testing data.
140	15	In addition to these two datasets, we also randomly sampled another 200 tweets as our development set.
154	70	First, note that the average number of mentions per tweet is well below 1.
155	21	In fact, many tweets are personal conversations and do not carry any entities that can be linked to Wikipedia.
156	28	Still, many candidates are generated (such as “really”) for those tweets, given that those candidates can still potentially link to an entity (“really” could be a TV channel).
157	45	Therefore, it is very important to include tweets without entities in the training set because we do not want our system to create unnecessary links to entities.
158	61	Another interesting thing to note is the percentage of entity mentions that disambiguate directly to their most often linked entities in Wikipedia.
160	53	However, mention detection is a difficult problem as only about 3% of candidates are valid entity mentions.
166	18	We believe it is necessary to focus upon core entities, rather than considering all possible entities in Wikipedia.
