{"sections": [{"text": "\u221a \u03b5 rather than the eigengap."}, {"heading": "1 Introduction", "text": "The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics. Algorithms solving these problems are often used to extract features to compare large-scale datasets, as well as used for problems in regression (Kakade & Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis & Mineiro, 2014), word embeddings (Dhillon et al., 2011), and many others.\nGenEV. Given two symmetric matrices A,B \u2208 Rd\u00d7d whereB is positive definite. The GenEV problem is to find generalized eigenvectors v1, . . . , vd where each vi satisfies\nvi \u2208 arg max v\u2208Rd \u2223\u2223v>Av\u2223\u2223 s.t. { v>Bv = 1 v>Bvj = 0 \u2200j \u2208 [i\u2212 1]\nThe values \u03bbi def = v>i Avi are known as the generalized eigenvalues, and it satisfies |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 |\u03bbd|. Following the *Equal contribution . Future version of this paper shall be found at http://arxiv.org/abs/1607.06017. 1Microsoft Research 2Princeton University. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntradition of (Wang et al., 2016; Garber & Hazan, 2015), we\nassume without loss of generality that \u03bbi \u2208 [\u22121, 1].\nCCA. Given matrices X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy and denoting by Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY\n>Y , the CCA problem is to find canonical-correlation vectors {(\u03c6i, \u03c8i)}ri=1 where r = min{dx, dy} and each pair (\u03c6i, \u03c8i) \u2208 arg max\n\u03c6\u2208Rdx ,\u03c8\u2208Rdy\n{ \u03c6>Sxy\u03c8 } such that { \u03c6>Sxx\u03c6 = 1 \u2227 \u03c6>Sxx\u03c6j = 0 \u2200j \u2208 [i\u2212 1] \u03c8>Syy\u03c8 = 1 \u2227 \u03c8>Syy\u03c8j = 0 \u2200j \u2208 [i\u2212 1]\nThe values \u03c3i def = \u03c6>i Sxy\u03c8i \u2265 0 are known as the canonical-correlation coefficients, and\n1 \u2265 \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r \u2265 0 is always satisfied.\nIt is a fact that solving CCA exactly can be reduced to solving GenEV exactly, if one defines B = diag{Sxx, Syy} \u2208 Rd\u00d7d andA = [[0, Sxy]; [S>xy, 0]] \u2208 Rd\u00d7d for d def = dx+dy; see Lemma 2.3. (This reduction does not always hold if the generalized eigenvectors are computed only approximately.)\nDespite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently. In the breakthrough result of Ma, Lu and Foster (Ma et al., 2015), they proposed to study algorithms to find top k generalized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA). They designed an alternating minimization algorithm whose running time is only linear in the input matrix sparsity and nearly-linear in k. Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate. Unfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with \u03ba and 1/gap, where\n\u2022 \u03ba \u2265 1 is the condition number of matrix B in GenEV, or of matrices X>X,Y >Y in CCA; and \u2022 gap \u2208 [0, 1) is the eigengap \u03bbk\u2212\u03bbk+1\u03bbk in GenEV, or \u03c3k\u2212\u03c3k+1\n\u03c3k in CCA.\nThese parameters are usually not constants and scale with\nthe problem size.\nChallenge 1: Acceleration For many easier scientific computing problems, we are able to design algorithms that have accelerated dependencies on \u03ba and 1/gap. As two concrete examples, k-PCA can be solved with a running time linearly in 1/ \u221a gap as opposed to 1/gap (Golub & Van Loan, 2012); computing B\u22121w for a vector w can be solved in time linearly in \u221a \u03ba as opposed to \u03ba, where \u03ba is the condition number of matrix B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).\nTherefore, can we obtain doubly-accelerated methods for k-GenEV and k-CCA, meaning that the running times linearly scale with both \u221a \u03ba and 1/ \u221a gap? Before this paper, for the general case k > 1, the method of Ge et al. (Ge et al., 2016) made acceleration possible for parameter \u03ba, but not for parameter 1/gap (see Table 1).\nChallenge 2: Gap-Freeness Since gap can be even zero in the extreme case, can we design algorithms that do not scale with 1/gap? Recall that this is possible for the easier task of k-PCA. The block Krylov method (Musco & Musco, 2015) runs in time linear in 1/ \u221a \u03b5 as opposed to 1/ \u221a gap, where \u03b5 is the approximation ratio. There is no gap-free result previously known for k-GenEV or k-CCA even for k = 1.\nChallenge 3: Stochasticity For matrix-related problems, one can usually obtain stochastic running times which requires some notations to describe.\nConsider a simple task of computing B\u22121w for some vector w, where accelerated methods solve it in time linearly in \u221a \u03ba for \u03ba being the condition number of B. If B = 1 nX >X is given in the form of a covariance matrix where X \u2208 Rn\u00d7d, then (accelerated) stochastic methods compute B\u22121w in a time linearly in (1 + \u221a \u03ba\u2032/n) instead of \u221a \u03ba, where \u03ba\u2032 = maxi\u2208[n]{\u2016Xi\u2016 2} \u03bbmin(B) \u2208 [ \u03ba, n\u03ba ] and Xi is the i-th\nrow of X . (See Lemma 2.6.) Since 1 + \u221a \u03ba\u2032/n \u2264 O( \u221a \u03ba), stochastic methods are no slower than non-stochastic ones.\nSo, can we obtain a similar but doubly-accelerated stochastic method for k-CCA?1 Note that, if the doublyaccelerated requirement is dropped, this task is easier and indeed possible, see Ge et al. (Ge et al., 2016). However, since their stochastic method is not doubly-accelerated, in certain parameter regimes, it runs even slower than nonstochastic ones (even for k = 1, see Table 2).\nRemark. In general, if designed properly, for worst case running time:\n\u2022 Accelerated results are usually better because they are 1 Note that a similar problem can be also asked for k-GenEV when A and B are both given in their covariance matrix forms. We refrain from doing it in this paper for notational simplicity.\nno slower than non-accelerated ones in the worst-case.\n\u2022 Gap-free results are better because they imply gapdependent ones.2\n\u2022 Stochastic results are usually better because they are no slower than non-stochastic ones in the worst-case."}, {"heading": "1.1 Our Main Results", "text": "We provide algorithms LazyEV and LazyCCA that are doubly-accelerated, gap-free, and stochastic.3\nFor the general k-GenEV problem, our LazyEV can be implemented to run in time4\nO\u0303 (knnz(B)\u221a\u03ba\n\u221a gap\n+ knnz(A) + k2d \u221a gap\n) or\nO\u0303 (knnz(B)\u221a\u03ba\u221a\n\u03b5 + knnz(A) + k2d\u221a \u03b5 ) in the gap-dependent and gap-free cases respectively. Since our running time only linearly depends on \u221a \u03ba and \u221a gap (resp. \u221a \u03b5), our algorithm LazyEV is doubly-accelerated.\nFor the general k-CCA problem, our LazyCCA can be implemented to run in time\nO\u0303 (knnz(X,Y ) \u00b7 (1 +\u221a\u03ba\u2032/n)+ k2d\n\u221a gap\n) or\nO\u0303 (knnz(X,Y ) \u00b7 (1 +\u221a\u03ba\u2032/n)+ k2d\n\u221a \u03b5 ) in the gap-dependent and gap-free cases respectively. Here, nnz(X,Y ) = nnz(X) + nnz(Y ) and \u03ba\u2032 = 2 maxi{\u2016Xi\u20162,\u2016Yi\u20162} \u03bbmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector of X or Y . Therefore, our algorithm LazyCCA is doublyaccelerated and stochastic.\nWe fully compare our results with prior work in Table 2 (for k = 1) and Table 1 (for k \u2265 1), and summarize our main contributions:\n\u2022 For k > 1, we outperform all relevant prior works (see Table 1). Moreover, no known method was doublyaccelerated even in the non-stochastic setting.\n\u2022 For k \u2265 1, we obtain the first gap-free running time. \u2022 Even for k = 1, we outperform most of the state-of-\nthe-arts (see Table 2).\nNote that for CCA with k > 1, previous result CCALin only outputs the subspace spanned by the top k correlation vectors but does not identify which vector gives the highest correlation and so on. Our LazyCCA provides per-vector\n2If a method depends on 1/\u03b5 then one can choose \u03b5 = gap and this translates to a gap-dependent running time.\n3Recalling Footnote 1, for notational simplicity, we only state our k-GenEV result in non-stochastic running time.\n4Throughout the paper, we use the O\u0303 notation to hide polylogarithmic factors with respect to \u03ba, 1/gap, 1/\u03b5, d, n. We use nnz(M) to denote the time needed to multiply M to a vector.\n\u03bbk\n\u2208 [0, 1] and \u03baB = \u03bbmax(B)\u03bbmin(B) > 1.\nIn CCA, gap = \u03c3k\u2212\u03c3k+1 \u03c3k \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, \u03ba\u2032 = 2maxi{\u2016Xi\u2016 2,\u2016Yi\u20162} \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, 2n\u03ba], and \u03c3k \u2208 [0, 1].\nRemark 1. Stochastic methods depend on a modified condition number \u03ba\u2032. The reason \u03ba\u2032 \u2208 [\u03ba, 2n\u03ba] is in Fact 2.5. Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + \u221a \u03ba\u2032/n \u2264 O(\u03ba).\nRemark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.\nguarantees on all the top k correlation vectors."}, {"heading": "1.2 Our Side Results on Doubly-Stochastic Methods", "text": "Recall that when considering acceleration, there are two parameters \u03ba and 1/gap. One can also design stochastic methods with respect to both parameters \u03ba and 1/gap, meaning that\nwith a running time proportional to 1 + \u221a \u03ba\u2032/nc \u221a gap\ninstead of 1+ \u221a \u03ba\u2032/n\n\u221a gap (stochastic) or \u221a \u03ba\u221a gap (non-stochastic).\nThe constant c is usually 1/2. We call such methods doubly-stochastic.\nUnfortunately, doubly-stochastic methods are usually slower than stochastic ones. Take 1-CCA as an example. The best stochastic running time (obtained exclusively by\nus) for 1-CCA is nnz(X,Y ) \u00b7 O\u0303 ( 1+\u221a\u03ba\u2032/n \u221a gap ) . In contrast, if one uses a doubly-stochastic method \u2014either (Wang et al., 2016) or our LazyCCA\u2014 the running time becomes nnz(X,Y ) \u00b7 O\u0303 ( 1 +\n\u221a \u03ba\u2032/n1/4\u221a gap\u00b7\u03c31\n) . Therefore, for 1-CCA,\ndoubly-stochastic methods are faster than stochastic ones\nonly when \u03ba \u2032\n\u03c31 \u2264 o(n1/2) .\nThe above condition is usually not satisfied. For instance,\n\u2022 \u03ba\u2032 is usually around n for most interesting data-sets, cf.\nthe experiments of (Shalev-Shwartz & Zhang, 2014);\n\u2022 \u03ba\u2032 is between n1/2 and 100n in all the CCA experiments of (Wang et al., 2016); and\n\u2022 by Fact 2.5 it satisfies \u03ba\u2032 \u2265 d so \u03ba\u2032 cannot be smaller than o(n1/2) unless d n1/2.5 Even worse, parameter \u03c31 \u2208 [0, 1] is usually much smaller than 1. Note that \u03c31 is scaling invariant: even if one scales X and Y up by the same factor, \u03c31 remains unchanged.\nNevertheless, to compare our LazyCCA with all relevant prior works, we obtain doubly-stochastic running times for k-CCA as well. Our running time matches that of (Wang et al., 2016) when k = 1, and no doubly-stochastic running time for k > 1 was known before our work."}, {"heading": "1.3 Other Related Works", "text": "For the easier task of PCA and SVD, the first gap-free result was obtained by Musco and Musco (Musco & Musco, 2015), the first stochastic result was obtained by Shamir (Shamir, 2015), and the first accelerated stochastic result was obtained by Garber et al. (Garber & Hazan, 2015; Garber et al., 2016). The shift-and-invert preconditioning technique of Garber et al. is also used in this paper.\nFor another related problem PCR (principle compo-\n5Note that item (3) \u03ba\u2032 \u2265 d may not hold in the more general setting of CCA, see Remark A.1.\n\u03bb1\n\u2208 [0, 1] and \u03baB = \u03bbmax(B)\u03bbmin(B) > 1.\nIn CCA, gap = \u03c31\u2212\u03c32 \u03c31 \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, \u03ba\u2032 = 2maxi{\u2016Xi\u2016 2,\u2016Yi\u20162} \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, 2n\u03ba], and \u03c31 \u2208 [0, 1].\nRemark 1. Stochastic methods depend on modified condition number \u03ba\u2032; the reason \u03ba\u2032 \u2208 [\u03ba, 2n\u03ba] is in Def. 2.4. Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + \u221a \u03ba\u2032/n \u2264 O(\u03ba).\nRemark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.\nRemark 4. Some CCA methods have a running time dependency on \u03c31 \u2208 [0, 1], and this is intrinsic and cannot be removed. In particular, if we scale the data matrix X and Y , the value \u03c31 stays the same.\nRemark 5. The only (non-doubly-stochastic) doubly-accelerated method before our work is SI (Wang et al., 2016) (for 1-CCA only). Our LazyEV is faster than theirs by a factor \u2126( \u221a n\u03ba/\u03ba\u2032 \u00d7 \u221a 1/\u03c31). Here, n\u03ba/\u03ba\u2032 \u2265 1/2 and 1/\u03c31 \u2265 1 are two scaling-invariant quantities usually much greater than 1.\nnent regression), we recently obtained an accelerated method (Allen-Zhu & Li, 2017) as opposed the previously non-accelerated one (Frostig et al., 2016); however, the acceleration techniques there are not relevant to this paper.\nFor GenEV and CCA, many scalable algorithms have been designed recently (Ma et al., 2015; Wang & Livescu, 2015; Michaeli et al., 2015; Witten et al., 2009; Lu & Foster, 2014). However, as summarized by the authors of CCALin, these cited methods are more or less heuristics and do not have provable guarantees. Furthermore, for k > 1, the AppGrad method (Ma et al., 2015) only provides local convergence guarantees and thus requires a warm-start whose\ncomputational complexity is not discussed in their paper.\nFinally, our algorithms on GenEV and CCA are based on finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop the algorithm whenever the eigenvalues (or correlation values) are too small. Known approaches for k > 1 cases (such as GenELin, CCALin, AppGrad) find all k vectors at once, therefore requiring k to be known beforehand. As a separate note, these known approaches do not need the user to know the desired accuracy a priori but our LazyEV and LazyCCA algorithms do."}, {"heading": "2 Preliminaries", "text": "We denote by \u2016x\u2016 or \u2016x\u20162 the Euclidean norm of vector x. We denote by \u2016A\u20162, \u2016A\u2016F , and \u2016A\u2016Sq respectively the spectral, Frobenius, and Schatten q-norm of matrix A (for q \u2265 1). We write A B if A,B are symmetric and A\u2212B is positive semi-definite (PSD), and write A B if A,B are symmetric but A \u2212 B is positive definite (PD). We denote by \u03bbmax(M) and \u03bbmin(M) the largest and smallest eigenvalue of a symmetric matrix M , and by \u03baM the condition number \u03bbmax(M)/\u03bbmin(M) of a PSD matrix M .\nThroughout this paper, we use nnz(M) to denote the time to multiply matrix M to any arbitrary vector. For two matricesX,Y , we denote by nnz(X,Y ) = nnz(X)+nnz(Y ), and by Xi or Yi the i-th row vector of X or Y . We also use poly(x1, x2, . . . , xt) to represent a quantity that is asymptotically at most polynomial in terms of variables x1, . . . , xt. Given a column orthonormal matrix U \u2208 Rn\u00d7k, we denote by U\u22a5 \u2208 Rn\u00d7(n\u2212k) the column orthonormal matrix consisting of an arbitrary basis in the space orthogonal to the span of U \u2019s columns.\nGiven a PSD matrixB and a vector v, v>Bv is theB-seminorm of v. Two vectors v, w are B-orthogonal if v>Bw = 0. We denote by B\u22121 the Moore-Penrose pseudoinverse of B if B is not invertible, and by B1/2 the matrix square root of B (satisfying B1/2 0). All occurrences of B\u22121, B1/2 and B\u22121/2 are for analysis purpose only. Our final algorithms only require multiplications of B to vectors.\nDefinition 2.1 (GenEV). Given symmetric matrices A,B \u2208 Rd\u00d7d where B is positive definite. The generalized eigenvectors ofA with respect toB are v1, . . . , vd, where each vi is\nvi \u2208 arg max v\u2208Rd {\u2223\u2223v>Av\u2223\u2223 s.t. v>Bv = 1 v>Bvj = 0 \u2200j \u2208 [i\u2212 1] } The generalized eigenvalues \u03bb1, . . . , \u03bbd satisfy \u03bbi = v>i Avi which can be negative.\nFollowing (Wang et al., 2016; Garber & Hazan, 2015), we assume without loss of generality that \u03bbi \u2208 [\u22121, 1].\nDefinition 2.2 (CCA). Given X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy , letting Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY\n>Y , the canonical-correlation vectors are {(\u03c6i, \u03c8i)}ri=1 where r = min{dx, dy} and for all i \u2208 [r]:\n(\u03c6i, \u03c8i) \u2208 arg max \u03c6\u2208Rdx ,\u03c8\u2208Rdy\n{ \u03c6>Sxy\u03c8 such that\n{ \u03c6>Sxx\u03c6 = 1 \u2227 \u03c6>Sxx\u03c6j = 0 \u2200j \u2208 [i\u2212 1] \u03c8>Syy\u03c8 = 1 \u2227 \u03c8>Syy\u03c8j = 0 \u2200j \u2208 [i\u2212 1] }} The corresponding canonical-correlation coefficients \u03c31, . . . , \u03c3r satisfy \u03c3i = \u03c6>i Sxy\u03c8i \u2208 [0, 1].\nWe emphasize that \u03c3i always lies in [0, 1] and is scalinginvariant. When dealing with a CCA problem, we also denote by d = dx + dy .\nLemma 2.3 (CCA to GenEV). Given a CCA problem with matrices X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy , let the canonicalcorrelation vectors and coefficients be {(\u03c6i, \u03c8i, \u03c3i)}ri=1 where r = min{dx, dy}. Define A = ( 0 Sxy\nS>xy 0\n) and\nB = ( Sxx 0\n0 Syy\n) . Then, the GenEV problem of A with re-\nspect to B has 2r eigenvalues {\u00b1\u03c3i}ri=1 and corresponding generalized eigenvectors {( \u03c6i \u03c8i ) , ( \u2212\u03c6i \u03c8i )}n i=1 . The remaining dx + dy \u2212 2r eigenvalues are zeros.\nDefinition 2.4. In CCA, let A and B be as defined in Lemma 2.3. We define condition numbers\n\u03ba def = \u03baB = \u03bbmax(B) \u03bbmin(B) and \u03ba\u2032 def= 2 maxi{\u2016Xi\u2016 2,\u2016Yi\u20162} \u03bbmin(B) . Fact 2.5. \u03ba\u2032 \u2208 [\u03ba, 2n\u03ba] and \u03ba\u2032 \u2265 d. (See full version.)\nLemma 2.6. Given matrices X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy , let A and B be as defined in Lemma 2.3. For every w \u2208 Rd, the Katyusha method (Allen-Zhu, 2017) finds a vector w\u2032 \u2208 Rd satisfying \u2016w\u2032 \u2212B\u22121Aw\u2016 \u2264 \u03b5 in time\nO ( nnz(X,Y ) \u00b7 ( 1 + \u221a \u03ba\u2032/n ) \u00b7 log \u03ba\u2016w\u2016 2\n\u03b5\n) ."}, {"heading": "3 Leading Eigenvector via Two-Sided Shift-and-Invert", "text": "We introduce AppxPCA\u00b1, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix. AppxPCA\u00b1 uses the shift-and-invert framework (Garber & Hazan, 2015; Garber et al., 2016), and shall become our building block for the LazyEV and LazyCCA algorithms in the subsequent sections.\nOur pseudo-code Algorithm 1 is a modification of Algorithm 5 in (Garber & Hazan, 2015), and reduces the eigenvector problem to oracle calls to an arbitrary matrix inversion oracle A. The main differences between AppxPCA\u00b1 and (Garber & Hazan, 2015) are two-fold.\nFirst, given a symmetric matrix M , AppxPCA\u00b1 simultaneously considers an upper-bounding shift together with a lower-bounding shift, and try to perform power methods with respect to (\u03bbI \u2212 M)\u22121 and (\u03bbI + M)\u22121. This allows us to determine approximately how close \u03bb is to the largest and the smallest eigenvalues of M , and decrease \u03bb accordingly. In the end, AppxPCA\u00b1 outputs an approximate eigenvector of M that corresponds to a negative eigenvalue if needed. Second, we provide a multiplicative-error guarantee rather than additive as appeared in (Garber & Hazan, 2015). Without such guarantee, our final running time will depend on 1gap\u00b7\u03bbmax(M) rather than 1 gap . 6\n6This is why the SI method of (Wang et al., 2016) also uses\nAlgorithm 1 AppxPCA\u00b1(A,M, \u03b4\u00d7, \u03b5, p)\nInput: A, an approximate matrix inversion method; M \u2208 Rd\u00d7d, a symmetric matrix satisfying \u2212I M I; \u03b4\u00d7 \u2208 (0, 0.5], a multiplicative error; \u03b5 \u2208 (0, 1), a numerical accuracy parameter; and p \u2208 (0, 1), the confidence parameter.\n1: w\u03020 \u2190 RanInit(d); s\u2190 0; \u03bb(0) \u2190 1 + \u03b4\u00d7; w\u03020 is a random unit vector, see Def. 3.2 2: m1 \u2190 \u2308 4 log ( 288d\u03b8 p2 )\u2309 , m2 \u2190 \u2308 log ( 36d\u03b8 p2\u03b5 )\u2309 ; \u03b8 is the parameter of RanInit, see Def. 3.2\n3: \u03b5\u03031 \u2190 164m1 ( \u03b4\u00d7 48 )m1 and \u03b5\u03032 \u2190 \u03b58m2 ( \u03b4\u00d748 )m2 4: repeat m1 = TPM(8, 1/32, p) and m2 = TPM(2, \u03b5/4, p), see Lemma B.1 5: s\u2190 s+ 1; 6: for t = 1 to m1 do 7: Apply A to find w\u0302t satisfying\n\u2225\u2225w\u0302t \u2212 (\u03bb(s\u22121)I \u2212M)\u22121w\u0302t\u22121\u2225\u2225 \u2264 \u03b5\u03031; 8: wa \u2190 w\u0302m1/\u2016w\u0302m1\u2016; wa is roughly (\u03bb(s\u22121)I \u2212M)\u2212m1 w\u03020 then normalized 9: Apply A to find va satisfying\n\u2225\u2225va \u2212 (\u03bb(s\u22121)I \u2212M)\u22121wa\u2225\u2225 \u2264 \u03b5\u03031; 10: for t = 1 to m1 do 11: Apply A to find w\u0302t satisfying\n\u2225\u2225w\u0302t \u2212 (\u03bb(s\u22121)I +M)\u22121w\u0302t\u22121\u2225\u2225 \u2264 \u03b5\u03031; 12: wb \u2190 w\u0302m1/\u2016w\u0302m1\u2016; wb is roughly (\u03bb(s\u22121)I +M)\u2212m1 w\u03020 then normalized 13: Apply A to find vb satisfying\n\u2225\u2225vb \u2212 (\u03bb(s\u22121)I +M)\u22121wb\u2225\u2225 \u2264 \u03b5\u03031; 14: \u2206(s) \u2190 12 \u00b7\n1 max{w>a va,w>b vb}\u2212\u03b5\u03031\nand \u03bb(s) \u2190 \u03bb(s\u22121) \u2212 \u2206 (s)\n2 ;\n15: until \u2206(s) \u2264 \u03b4\u00d7\u03bb (s) 12 16: f \u2190 s; 17: if the last w>a va \u2265 w>b vb then 18: for t = 1 to m2 do 19: Apply A to find w\u0302t satisfying\n\u2225\u2225w\u0302t \u2212 (\u03bb(f)I \u2212M)\u22121w\u0302t\u22121\u2225\u2225 \u2264 \u03b5\u03032; 20: return (+, w) where w def= w\u0302m2/\u2016w\u0302m2\u2016. 21: else 22: for t = 1 to m2 do 23: Apply A to find w\u0302t satisfying\n\u2225\u2225w\u0302t \u2212 (\u03bb(f)I +M)\u22121w\u0302t\u22121\u2225\u2225 \u2264 \u03b5\u03032; 24: return (\u2212, w) where w def= w\u0302m2/\u2016w\u0302m2\u2016. 25: end if\nWe prove in full version the following theorem:\nTheorem 3.1 (AppxPCA\u00b1, informal). Let M \u2208 Rd\u00d7d be a symmetric matrix with eigenvalues 1 \u2265 \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd \u2265 \u22121 and eigenvectors u1, . . . , ud. Let \u03bb\u2217 = max{\u03bb1,\u2212\u03bbd}. With probability at least 1\u2212 p, AppxPCA\u00b1 produces a pair (sgn,w) satisfying\n\u2022 if sgn = +, then w is an approx. positive eigenvector: w>Mw \u2265 (\n1\u2212 \u03b4\u00d7 2\n) \u03bb\u2217 \u2227 \u2211\ni\u2208[d] \u03bbi\u2264(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n(w>ui) 2 \u2264 \u03b5\n\u2022 if sgn = \u2212, then w is an approx. negative eigenvector: w>Mw \u2264 \u2212 (\n1\u2212\u03b4\u00d7 2\n) \u03bb\u2217 \u2227 \u2211\ni\u2208[d] \u03bbi\u2265\u2212(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n(w>ui) 2 \u2264 \u03b5\nThe number of oracle calls toA is O\u0303(log(1/\u03b4\u00d7)), and each time we call A it satisfies\nshift-and-invert but depends on 1 gap\u00b7\u03c31 in Table 2.\n\u2022 \u03bbmax(\u03bb (s)I\u2212M) \u03bbmin(\u03bb(s)I\u2212M) , \u03bbmax(\u03bb (s)I+M) \u03bbmin(\u03bb(s)I+M) \u2208 [1, 96\u03b4\u00d7 ] and\n\u2022 1 \u03bbmin(\u03bb(s)I\u2212M) , 1 \u03bbmin(\u03bb(s)I+M) \u2264 48\u03b4\u00d7\u03bb\u2217 .\nWe remark here that, unlike the original shift-and-invert method which chooses a random (Gaussian) unit vector in Line 1 of AppxPCA\u00b1, we have allowed this initial vector to be generated from an arbitrary \u03b8-conditioned random vector generator (for later use), defined as follows:\nDefinition 3.2. An algorithm RanInit(d) is a \u03b8conditioned random vector generator if w = RanInit(d) is a d-dimensional unit vector and, for every p \u2208 (0, 1), every unit vector u \u2208 Rd, with probability at least 1\u2212 p, it satisfies (u>w)2 \u2264 p\n2\u03b8 9d .\nThis modification is needed in order to obtain our efficient implementations of GenEV and CCA. One can construct a \u03b8-conditioned random vector generator as follows:\nProposition 3.3. Given a PSD matrix B \u2208 Rd\u00d7d, if we set RanInit(d) def = B\n1/2v (v>Bv)0.5\nwhere v is a random Gaussian vector, then RanInit(d) is a \u03b8-conditioned random vector\ngenerator for \u03b8 = \u03baB ."}, {"heading": "4 LazyEV: Generalized Eigendecomposition", "text": "In this section, we construct an algorithm LazyEV that, given symmetric matrix M \u2208 Rd\u00d7d, computes approximately the k leading eigenvectors ofM that have the largest absolute eigenvalues. Then, for the original k-GenEV problem, we set M = B\u22121/2AB\u22121/2 and run LazyEV. This is our plan to find the top k leading generalized eigenvectors of A with respect to B.\nOur algorithm LazyEV is formally stated in Algorithm 2. The algorithm applies k times AppxPCA\u00b1, each time computing an approximate leading eigenvector of M with a multiplicative error \u03b4\u00d7/2, and projects the matrix M into the orthogonal space with respect to the obtained leading eigenvector. We state our main approximation theorem below.\nTheorem 4.1 (informal). Let M \u2208 Rd\u00d7d be a symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbd \u2208 [\u22121, 1] and corresponding eigenvectors u1, . . . , ud, and |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|. If \u03b5pca is sufficiently small,7 LazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) \u2208 Rd\u00d7k which, with probability at least 1\u2212 p, satisfies: (a) \u2016V >k U\u20162 \u2264 \u03b5 where U = (uj , . . . , ud) and j is the\nsmallest index satisfying |\u03bbj | \u2264 (1\u2212 \u03b4\u00d7)\u03bbk. (b) For every i \u2208 [k], (1\u2212\u03b4\u00d7)|\u03bbi| \u2264 |v>i Mvi| \u2264 11\u2212\u03b4\u00d7 |\u03bbi|.\nAbove, property (a) ensures the k columns of Vk have negligible correlation with the eigenvectors of M whose absolute eigenvalues are \u2264 (1\u2212 \u03b4\u00d7)\u03bbk; property (b) ensures the Rayleigh quotients v>i Mvi are all correct up to a 1\u00b1\u03b4\u00d7 error. We in fact have shown two more useful properties in the full version that may be of independent interest.\nThe next theorem states that, if M = B\u22121/2AB\u22121/2, our LazyEV can be implemented without the necessity to compute B1/2 or B\u22121/2.\nTheorem 4.2 (running time). Let A,B \u2208 Rd\u00d7d be two symmetric matrices satisfying B 0 and \u2212B A B. Suppose M = B\u22121/2AB\u22121/2 and RanInit(d) is defined in Proposition 3.3 with respect to B. Then, the computation of V \u2190 B\u22121/2LazyEV(A,M, k, \u03b4\u00d7, \u03b5pca, p) can be implemented to run in time\n\u2022 O\u0303 ( knnz(B)+k2d+k\u03a5\u221a\n\u03b4\u00d7\n) where \u03a5 is the time to multiply\nB\u22121A to a vector, or \u2022 O\u0303 ( k \u221a \u03baBnnz(B)+knnz(A)+k\n2d\u221a \u03b4\u00d7\n) if we use Conjugate gra-\ndient to multiply B\u22121A to a vector.\n7Meaning \u03b5pca \u2264 O ( poly(\u03b5, \u03b4\u00d7,\n|\u03bb1| |\u03bbk+1| , 1 d ) ) . The complete\nspecifications of \u03b5pca is included in the full version. Since our final running time only depends on log(1/\u03b5pca), we have not attempted to improve the constants in this polynomial dependency.\nChoosing parameter \u03b4\u00d7 as either gap or \u03b5, our two main theorems above immediately imply the following results for the k-GenEV problem: (proved in full version)\nTheorem 4.3 (gap-dependent GenEV, informal). Let A,B \u2208 Rd\u00d7d be two symmetric matrices satisfying B 0 and \u2212B A B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are {(\u03bbi, ui)}di=1, and it satisfies 1 \u2265 |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|. Then, LazyEV outputs V k \u2208 Rd\u00d7k satisfying\nV > k BV k = I and \u2016V > k BW\u20162 \u2264 \u03b5\nin time O\u0303 (k\u221a\u03baBnnz(B) + knnz(A) + k2d\u221a\ngap ) Here, W = (uk+1, . . . , ud) and gap =\n|\u03bbk|\u2212|\u03bbk+1| |\u03bbk| .\nTheorem 4.4 (gap-free GenEV, informal). In the same setting as Theorem 4.3, our LazyEV outputs V k = (v1, . . . , vk) \u2208 Rd\u00d7k satisfying V > k BV k = I and\n\u2200s \u2208 [k] : \u2223\u2223v>s Avs\u2223\u2223 \u2208 [(1\u2212 \u03b5)|\u03bbs|, |\u03bbs|1\u2212 \u03b5]\nin time O\u0303 (k\u221a\u03baBnnz(B) + knnz(A) + k2d\u221a\n\u03b5\n) ."}, {"heading": "5 Ideas Behind Theorems 4.1 and 4.2", "text": "In Section 5.1 we discuss how to ensure accuracy: that is, why does LazyEV guarantee to approximately find the top eigenvectors ofM . In the full version of this paper, we also discuss how to implement LazyEV without compute B1/2 explicitly, thus proving Theorem 4.2."}, {"heading": "5.1 Ideas Behind Theorem 4.1", "text": "Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M (Allen-Zhu & Li, 2016). That method is called LazySVD and we summarize it as follows.\nAt a high level, LazySVD finds the top k eigenvectors oneby-one and approximately. Starting with M0 = M , in the s-th iteration where s \u2208 [k], LazySVD computes approximately the leading eigenvector of matrix Ms\u22121 and call it vs. Then, LazySVD projects Ms \u2190 (I \u2212 vsv>s )Ms\u22121(I \u2212 vsv > s ) and proceeds to the next iteration.\nWhile the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas. Most notably, if vs is an approximate leading eigenvector of Ms\u22121, then one needs to prove that the small eigenvectors of Ms\u22121 somehow still \u201cembed\u201d into that of Ms after projection. This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of (Allen-Zhu & Li, 2016).\nAlgorithm 2 LazyEV(A,M, k, \u03b4\u00d7, \u03b5pca, p)\nInput: A, an approximate matrix inversion method; M \u2208 Rd\u00d7d, a matrix satisfying \u2212I M I; k \u2208 [d], the desired rank; \u03b4\u00d7 \u2208 (0, 1), a multiplicative error; \u03b5pca \u2208 (0, 1), a numerical accuracy; and p \u2208 (0, 1), a confidence parameter.\n1: M0 \u2190M ; V0 = []; 2: for s = 1 to k do 3: (\u223c, v\u2032s)\u2190 AppxPCA\u00b1(A,Ms\u22121, \u03b4\u00d7/2, \u03b5pca, p/k); v\u2032s is an approximate two-sided leading eigenvector of Ms\u22121 4: vs \u2190 ( (I \u2212 Vs\u22121V >s\u22121)v\u2032s ) / \u2225\u2225(I \u2212 Vs\u22121V >s\u22121)v\u2032s\u2225\u2225; project v\u2032s to V \u22a5s\u22121 5: Vs \u2190 [Vs\u22121, vs]; 6: Ms \u2190 (I \u2212 vsv>s )Ms\u22121(I \u2212 vsv>s ) we also have Ms = (I \u2212 VsV >s )M(I \u2212 VsV >s ) 7: end for 8: return Vk.\nIn this paper, to relax the assumption thatM is PSD, and to find leading eigenvectors whose absolute eigenvalues are large, we have to make several non-trivial changes. On the algorithm side, LazyEV uses our two-sided shift-andinvert method in Section 3 to find the leading eigenvector of Ms\u22121 with largest absolute eigenvalue. On the analysis side, we have to make sure all lemmas properly deal with negative eigenvalues. For instance:\n\u2022 If we perform a projection M \u2032 \u2190 (I \u2212 vv>)M(I \u2212 vv>) where v correlates by at most \u03b5 with all eigenvectors ofM whose absolute eigenvalues are smaller than a threshold \u00b5, then, after the projection, we need to prove that these eigenvectors can be approximately \u201cembedded\u201d into the eigenspace spanned by all eigenvectors of M \u2032 whose absolute eigenvalues are smaller than \u00b5+ \u03c4 . The approximation of this embedding should depend on \u03b5, \u00b5 and \u03c4 .\nThe full proof of Theorem 4.1 is in the arXiv version. It relies on a few matrix algebraic lemmas (including the aforementioned \u201cembedding lemma\u201d)."}, {"heading": "6 Conclusion", "text": "In this paper we propose new iterative methods to solve the generalized eigenvector and the canonical correlation analysis problems. Our methods find the most significant k eigenvectors or correlation vectors, and have running times that linearly scales with k.\nMost importantly, our methods are doubly-accelerated: the running times have square-root dependencies both with respect to the condition number of the matrix (i.e., \u03ba) and with respect to the eigengap (i.e., gap). They are the first doubly-accelerated iterative methods at least for k > 1. They can also be made gap-free, and are the first gap-free iterative methods even for 1-GenEV or 1-CCA.\nAlthough this is a theory paper, we believe that if implemented carefully, our methods can outperform not only previous iterative methods (such as GenELin, AppGrad, CCALin), but also the commercial mathematics libraries for sparse matrices of dimension more than 10, 000. We\nleave it a future work for such careful comparisons."}], "year": 2017, "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "authors": ["Allen-Zhu", "Zeyuan"], "venue": "In STOC,", "year": 2017}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In NIPS,", "year": 2016}, {"title": "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "authors": ["Allen-Zhu", "Zeyuan", "Orecchia", "Lorenzo"], "venue": "In Proceedings of the 8th Innovations in Theoretical Computer Science,", "year": 2017}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "authors": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "In ICML,", "year": 2016}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "authors": ["Allen-Zhu", "Zeyuan", "Richt\u00e1rik", "Peter", "Qu", "Zheng", "Yuan", "Yang"], "venue": "In ICML,", "year": 2016}, {"title": "Expander flows, geometric embeddings and graph partitioning", "authors": ["Arora", "Sanjeev", "Rao", "Satish", "Vazirani", "Umesh V"], "venue": "Journal of the ACM,", "year": 2009}, {"title": "Stability of over-relaxations for the forward-backward algorithm, application to fista", "authors": ["Aujol", "J-F", "Dossal", "Ch"], "venue": "SIAM Journal on Optimization,", "year": 2015}, {"title": "A survey of preconditioned iterative methods for linear systems of algebraic equations", "authors": ["Axelsson", "Owe"], "venue": "BIT Numerical Mathematics,", "year": 1985}, {"title": "Multi-view clustering via canonical correlation analysis", "authors": ["Chaudhuri", "Kamalika", "Kakade", "Sham M", "Livescu", "Karen", "Sridharan", "Karthik"], "venue": "In ICML, pp", "year": 2009}, {"title": "Multi-view learning of word embeddings via cca", "authors": ["Dhillon", "Paramveer", "Foster", "Dean P", "Ungar", "Lyle H"], "venue": "In NIPS, pp", "year": 2011}, {"title": "Principal Component Projection Without Principal Component Analysis", "authors": ["Frostig", "Roy", "Musco", "Cameron", "Christopher", "Sidford", "Aaron"], "venue": "In ICML,", "year": 2016}, {"title": "Fast and simple PCA via convex optimization", "authors": ["Garber", "Dan", "Hazan", "Elad"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "authors": ["Garber", "Dan", "Hazan", "Elad", "Jin", "Chi", "Kakade", "Sham M", "Musco", "Cameron", "Netrapalli", "Praneeth", "Sidford", "Aaron"], "year": 2016}, {"title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "authors": ["Ge", "Rong", "Jin", "Chi", "Kakade", "Sham M", "Netrapalli", "Praneeth", "Sidford", "Aaron"], "year": 2016}, {"title": "Multi-view regression via canonical correlation analysis", "authors": ["Kakade", "Sham M", "Foster", "Dean P"], "venue": "In Learning theory,", "year": 2007}, {"title": "Discriminative features via generalized eigenvectors", "authors": ["Karampatziakis", "Nikos", "Mineiro", "Paul"], "venue": "In ICML,", "year": 2014}, {"title": "Large scale canonical correlation analysis with iterative least squares", "authors": ["Lu", "Yichao", "Foster", "Dean P"], "venue": "In NIPS, pp", "year": 2014}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "authors": ["Ma", "Zhuang", "Lu", "Yichao", "Foster", "Dean"], "venue": "In ICML, pp", "year": 2015}, {"title": "Nonparametric canonical correlation analysis", "authors": ["Michaeli", "Tomer", "Wang", "Weiran", "Livescu", "Karen"], "venue": "arXiv preprint,", "year": 2015}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "authors": ["Musco", "Cameron", "Christopher"], "venue": "In NIPS,", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "authors": ["Nesterov", "Yurii"], "venue": "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),", "year": 1983}, {"title": "SDCA without Duality, Regularization, and Individual Convexity", "authors": ["Shalev-Shwartz", "Shai"], "venue": "In ICML,", "year": 2016}, {"title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization", "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "year": 2014}, {"title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "authors": ["Shamir", "Ohad"], "venue": "In ICML, pp", "year": 2015}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "authors": ["Shewchuk", "Jonathan Richard"], "year": 1994}, {"title": "Large-scale approximate kernel canonical correlation analysis", "authors": ["Wang", "Weiran", "Livescu", "Karen"], "venue": "arXiv preprint,", "year": 2015}, {"title": "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "authors": ["Wang", "Weiran", "Jialei", "Garber", "Dan", "Srebro", "Nathan"], "venue": "In NIPS,", "year": 2016}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "authors": ["Witten", "Daniela M", "Tibshirani", "Robert", "Hastie", "Trevor"], "venue": "Biostatistics, pp. kxp008,", "year": 2009}], "id": "SP:30471acf1e119e3e77beaf69027ddad713902fef", "authors": [{"name": "Zeyuan Allen-Zhu", "affiliations": []}, {"name": "Yuanzhi Li", "affiliations": []}], "abstractText": "We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonicalcorrelation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on k. Furthermore, our algorithms are doubly-accelerated: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both kGenEV or k-CCA. We also provide the first gapfree results, which provide running times that depend on 1/ \u221a \u03b5 rather than the eigengap.", "title": "Doubly Accelerated Methods for Faster CCA  and Generalized Eigendecomposition"}