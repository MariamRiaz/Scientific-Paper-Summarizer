16	89	Our work is most similar to that of Srivastava et al. (2017), who also use natural language explanations to train a classifier, but with two important differences.
19	57	Second, while they use the logical forms of explanations to produce features that are fed directly to a classifier, we use them as functions for labeling a much larger training set.
20	37	In Section 4, we show that using functions yields a 9.5 F1 improvement (26% relative improvement) over features, and that the F1 score scales with the amount of available unlabeled data.
22	52	We find empirically that users are able to train classifiers with comparable F1 scores up to two orders of magnitude faster when they provide natural language explanations instead of individual labels.
23	98	Our code and data can be found at https:// github.com/HazyResearch/babble.
24	172	2 The BabbleLabble Framework The BabbleLabble framework converts natural language explanations and unlabeled data into a noisily-labeled training set (see Figure 2).
26	69	The semantic parser converts natural language explanations into a set of logical forms representing labeling functions (LFs).
29	30	This label matrix is passed into the label aggregator, which combines these potentially conflicting and overlapping labels into one label for each example.
31	121	To create the input explanations, the user views a subset S of an unlabeled dataset D (where |S| |D|) and provides for each input xi ∈ S a label yi and a natural language explanation ei, a sentence explaining why the example should receive that label.
33	71	The semantic parser takes a natural language explanation ei and returns a set of LFs (logical forms or labeling functions) {f1, .
36	28	Formally, the parser uses a set of rules of the form α → β, where α can be replaced by the token(s) in β (see Figure 3 for example rules).
37	27	To identify candidate LFs, we recursively construct a set of valid parses for each span of the explanation, based on the substitutions defined by the grammar rules.
38	25	At the end, the parser returns all valid parses (LFs in our case) corresponding to the entire explanation.
42	31	All predicates included in our grammar (summarized in Table 1) are provided to annotators, with minimal examples of each in use (Appendix A).
48	25	The input to the filter bank is a set of candidate LFs produced by the semantic parser.
56	186	LF 3b is redundant, since even though it has a different syntax tree from LF 3a, it labels the training set identically and therefore provides no new signal.
61	81	Concretely, if m LFs pass the filter bank and are applied to n examples, the label aggregator implements a function f : {−1, 0, 1}m×n → [0, 1]n. A naive solution would be to use a simple majority vote, but this fails to account for the fact that LFs can vary widely in accuracy and coverage.
62	61	Instead, we use data programming (Ratner et al., 2016), which models the relationship between the true labels and the output of the labeling functions as a factor graph.
66	33	Intuitively, we infer accuracies of the LFs based on the way they overlap and conflict with one another.
68	39	The noisily-labeled training set that the label aggregator outputs is used to train an arbitrary discriminative model.
70	25	A discriminative model, on the other hand, can incorporate features that were not identified by the user but are nevertheless informative.2 Consequently, even examples for which all LFs abstained can still be classified correctly.
77	30	In the Spouse task, annotators were shown a sentence with two highlighted names and asked to label whether the sentence suggests that the two people are spouses.
83	60	Because this task requires specialized domain expertise, we obtained explanations by having someone unfamiliar with BabbleLabble translate from Python to natural language labeling functions from an existing publication that explored applying weak supervision to this task (Ratner et al., 2018).
89	48	Hyperparameters for all methods we report were selected via random search over thirty configurations on the same held-out development set.
92	60	In Table 3 we report the average F1 score of a classifier trained with BabbleLabble using 30 explanations or traditional supervision with the indicated number of labels.
93	34	On average, it took the same amount of time to collect 30 explanations as 60 labels.7 We observe that in all three tasks, BabbleLabble achieves a given F1 score with far fewer user inputs than traditional supervision, by as much as 100 times in the case of the Spouse task.
95	106	We also observe, however, that once the number of labeled examples is sufficiently large, traditional supervision once again dominates, since ground truth labels are preferable to noisy ones generated by labeling functions.
96	75	However, in domains where there is much more unlabeled data available than labeled data (which in our experience is most domains), we can gain in supervision efficiency from using BabbleLabble.
99	51	The remainder were due to unrecognized paraphrases (e.g., the explanation said “the order of appearance is X, Y” instead of a supported phrasing like “X comes before Y”).
103	54	Furthermore, among those LFs that pass through the filter bank, we found that the average difference in end-task accuracy between correct and incorrect parses is less than 2.5%.
