8	28	Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses.
9	29	We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces.
25	28	One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases.
28	21	The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements.
29	26	This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies.
30	55	The other common approach has been to thread a trace through the tree structure on the non-terminal symbols.
38	52	In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; FernándezGonzález and Martins 2015; Kong et al. 2015).
46	23	Within the CKY framework, the key to defining our algorithm is a set of rules that specify which items are allowed to combine.
48	23	Example: To build intuition for the algorithm, we will describe the derivation in Figure 1.
50	46	(1) We initialize with spans of width one, going between adjacent words, e.g. between ROOT and We.
51	31	∅ 7→ I0,1 (2) Edges can be introduced in exactly two ways, either by linking the two ends of a span, e.g. like– running, or by linking one end of a span with a word outside the span, e.g. like–.
208	23	Since edges are only created between visible vertices, no edge can cross edge ij.
210	41	First, consider an edge ij added to an item [ij.x] of type B, L, R, or N. This edge is crossed by all x–(ij) edges, and in these items |x–(ij)| ≥ 1 by definition.
215	114	We will show how this set of edges is affected by the rules and what that implies for e. Consider each input item A[kl.y] in each rule, with output item C. Every item A falls into one of four categories: (1) ∀f ∈ E(A), f is crossed by an edge in another of the rule’s input items, (2) E(A) ⊆ E(C), (3) A∧ kl 7→ C and there are no ky or ly edges in A, (4) A contains edge kl and there are no ky or ly edges in A.
217	49	For an example of the first case, consider the rightmost item in rule 4.
220	32	Since i < k < l < j, all k–(lj] edges will cross all l–[ik) edges.
262	34	A spine is the ordered set of non-terminals that the word is the head of, e.g. SVP for like.
269	55	We represent co-indexation with edges, one per reference, going from the null element to the non-terminal.
270	32	There are three special cases of co-indexation: (1) It is possible for trace edges to have the same start and end points as a non-trace edge.
285	28	Different heads often represent more syntactic or semantic aspects of the phrase.
300	24	The remaining cases were all instances of pseudo-attachment, which the treebank uses to show that non-adjacent constituents are related (Bies et al. 1995).
325	29	Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing.
333	24	Shifting to use their head rules, we score 88.9.
334	34	Second-order features could be added to our model through the use of forest reranking, an improvement that would be orthogonal to this paper’s contributions.
340	35	Trace Accuracy: Table 2 shows results using Johnson (2002)’s trace metric.
341	80	Our parser is competitive with previous work that has highly-engineered models: Johnson’s system has complex non-local features on tree fragments, and similarly Kato and Matsubara (K&M 2016) consider complete items in the stack of their transition-based parser.
343	34	Converting to our representation, our parser has higher precision than K&M on trace edges (84.1 vs. 78.1) but lower recall (59.5 vs. 71.3).
344	269	One modeling challenge we observed is class imbalance: of the many places a trace could be added, only a small number are correct, and so our model tends to be conservative (as shown by the P/R tradeoff).
345	63	We propose a representation and algorithm that cover 97.3% of graph structures in the PTB.
347	88	A proof of concept parser shows that our algorithm can be used to parse and recover traces.
