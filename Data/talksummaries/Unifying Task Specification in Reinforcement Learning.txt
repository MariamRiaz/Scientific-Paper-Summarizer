0	80	Reinforcement learning is a formalism for trial-and-error interaction between an agent and an unknown environment.
1	239	This interaction is typically specified by a Markov decision process (MDP), which contains a transition model, reward model, and potentially discount parameters specifying a discount on the sum of future values in the return.
2	120	Domains are typically separated into two cases: episodic problems (finite horizon) and continuing problems (infinite horizon).
3	158	In episodic problems, the agent reaches some terminal state, and is teleported back to a start state.
4	90	In continuing problems, the agent interaction is continual, with a discount to ensure a finite total reward (e.g., constant < 1).
8	47	Generalizations to problem specifications provide exciting learning opportunities, but can also reduce clarity and complicate algorithm development and theory.
13	44	Though natural, this represents a significant change in the way tasks are currently specified in reinforcement learning and has important ramifications for simplifying implementation, algorithm development and theory.
18	46	Second, we prove novel contraction bounds on the Bellman operator for these generalized RL tasks, and show that previous bounds for both episodic and continuing tasks are subsumed by this more general result.
25	12	[0,1) is an interest function that specifies the user defined interest in a state.
45	20	The interest is set to 1 in all states, which is the typical case, meaning performance from each state is equally important.
70	29	This proof is mainly definitional, but we state it as an explicit proposition for clarity.
75	77	The rewards for the RL task correspond to the rewards associated with the MDP.
77	35	Further, the policies associated with RL subtasks can be used as macro- actions, to specify a semi-Markov decision process (Sutton et al., 1999, Theorem 1).
80	53	The work on Horde (Sutton et al., 2011) and nexting (Modayil et al., 2014) provide numerous examples of the utility of the types of questions that can be specified by general value functions, and so by RL tasks, because general value functions can naturally can be specified as an RL task.
83	55	These demons share the underlying dynamics, and even feature representation, but have separate prediction and control tasks; keeping these separate from the MDP is key for avoiding complications (see Appendix B.2).
85	37	Consider the taxi domain, described more fully in Section 3, where the agent’s goal is to pick up and drop off passengers in a grid world with walls.
87	19	This can be encoded by setting (s, a, s) = 0 if a movement action causes the agent to remain in the same state, which occurs when trying to move through a wall.
88	66	In addition to episodic problems and hard termination, transition-based questions also enable soft termination for transitions.
89	40	Hard termination uses (s, a, s0) = 0 and soft termination (s, a, s0) = ✏ for some small positive value ✏.
90	35	Soft terminations can be useful for incorporating some of the value of a policy right after the soft termination.
91	16	If two policies are equivalent up to a transition, but have very different returns after the transition, a soft termination will reflect that difference.
93	68	To better ground this generalized formalism and provide some intuition, we provide a demonstration of RL task specification.
94	20	We explore different transition-based discounts in the taxi domain (Dietterich, 2000; Diuk et al., 2008).
95	79	The goal of the agent is to take a passenger from a source platform to a destination platform, depicted in Figure 2.
96	16	The agent receives a reward of -1 on each step, except for successful pickup and drop-off, giving reward 0.
98	43	This encodes that turning right, left or going backwards are more costly than going forwards, with additional negative rewards of -0.05, -0.1 and -0.2 respectively.
101	24	The location of the passenger can be in one of the pickup/drop-off locations, or in the taxi.
102	57	Optimal policies and value functions are computed iteratively, with an extensive number of iterations.
107	13	We also compare to a constant , which corresponds to an average reward goal, as demonstrated in Equation (8).
110	33	Because most of the rewards are negative per step, small differences in orientation can be more difficult to distinguish amongst for an infinite discounted sum.
