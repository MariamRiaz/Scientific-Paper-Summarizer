0	21	Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.
1	9	There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by the following words.
20	27	Weighted M3Ns We use a sequence labeling model for edit detection.
21	28	Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).
23	26	Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.
31	15	Such a weighted loss function allows us to balance the model’s precision and recall rates.
32	12	For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.
37	11	It has the advantage of incorporating non-local features as well as nonlinear classifiers.
41	33	In the first step, we automatically detect filler words.
43	29	For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency.
44	13	Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.
53	8	Besides that, word n-grams, POS n-grams and logic n-grams extracted from filler word removed text are included.
54	8	Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words.
60	8	Usually, the pronoun following than is accusative case.
62	13	This kind of n-gram features is similar to the language models used in (Zwarts and Johnson, 2011).
63	41	They have the benefit of measuring the fluency of the cleaned text.
64	22	Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.
67	8	The observation is that coordinates are often longer than edited sequences.
69	10	If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).
73	8	We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).
76	8	The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.
77	12	The optimal weight matrix is shown in Table 4.
79	26	We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning.
81	9	For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.
82	9	Regularization penalty is C = 0.1 and iteration number is 30.
84	9	The baseline models use only the ngrams features extracted from the original text.
94	20	In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.
96	28	The third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.
97	18	To minimize the F-loss approximately, we modified the hamming loss in M3Ns.
98	18	Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources.
