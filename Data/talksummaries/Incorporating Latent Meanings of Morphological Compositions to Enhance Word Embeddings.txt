1	40	The derived word embeddings have been used in plenty of tasks such as text classification (Liu et al., 2015), information retrieval (Manning et al., 2008), sentiment analysis (Shin et al., 2016), machine translation (Cho et al., 2014) and so on.
2	23	Recently, some classic word embedding methods have been proposed, like Continuous Bag-ofWord (CBOW), Skip-gram (Mikolov et al., 2013a), Global Vectors (GloVe) (Pennington et al., 2014).
5	29	Some of them compute the word embeddings by directly adding the representations of morphemes/characters to context words or optimizing a joint objective over distributional statistics and morphological properties (Qiu et al., 2014; Botha and Blunsom, 2014; Chen et al., 2015; Luong et al., 2013; Lazaridou et al., 2013), while others introduce some probabilistic graphical models to build relationship between words and their internal compositions.
10	55	1) where internal compositions are directly used to encode morphological regularities into words and the composition embeddings like morpheme embeddings are generated as by-products, we explore a new way to employ the latent meanings of morphological compositions rather than the compositions themselves to train word embeddings.
11	15	1, according to the distributional semantics hypothesis (Sahlgren, 2008), incredible and unbelievable probably have similar word embeddings because they have similar context.
14	35	Fortunately, the latent meanings of the different morphemes are the same (e.g., the latent meanings of roots cred, believ are “believe”) as listed in the lookup table (derived from the resources provided by Michigan State University),1 which evidently implies that incredible and unbelievable share the same meanings.
16	36	Subsequently, the similarities are utilized to calculate the weights of latent meanings of morphemes for each word.
17	59	In this paper, we try different strategies to modify the input layer and update rules of a neural language model, e.g., CBOW, Skipgram, and propose three lightweight and efficient models, which are termed Latent Meaning Models (LMMs), to not only encode morphological properties into words but also enhance the semantic similarities among word embeddings.
19	21	Rather than generating and training extra embeddings for latent meanings, we directly override the embeddings of the corresponding words in the vocabulary.
67	49	LMM-A assumes that all latent meanings of morphemes of a word have equal contributions to the word.
86	42	For LMM-S, the modified embedding of ti can be rewritten as v̂ti = 1 2 [ vti + ∑ w∈Mi ω<ti,w> · vw ] , (4) where vti is the original vector of ti, and ω<ti,w> denotes the weight between ti and the latent meaning w (w ∈Mi).
93	20	(1) can be rewritten as L̂ = 1 n n∑ i=1 log p ( vti | ∑ tj∈context(ti) v̂tj ) , (8) where v̂tj is the modified vector of vtj (tj ∈ context(ti)).
94	24	Since the word map describes top-level relations between words and the latent meanings, these relations don’t change during the training period.
95	18	All parameters introduced by our models can be directly derived using the word map and word vectors, thus no extra parameter needs to be trained.
108	36	Although the lookup table employed in this paper contains latent meanings for only 90 prefixes, 382 roots and 67 suffixes, we focus on validating the feasibility of enhancing word embeddings with the latent meanings of morphemes, and expending the lookup table is left as future work.
109	21	For comparison, we choose three word-level state-of-the-art word embedding models including CBOW, Skip-gram (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), and we also implement an Explicitly Morpheme-related Model (EMM), which is a variant version of the previous work (Qiu et al., 2014).
110	15	The architecture of EMM is based on our LMM-A, where latent meanings are replaced back to morphemes and the embeddings of morphemes are also learned when training word embeddings.
111	19	This enables our evaluation to focus on the critical difference between our models and the explicit model (Bhatia et al., 2016).
124	14	For English word similarity, we employ two gold standard datasets including Wordsim-353 (Finkelstein et al., 2001) and RG-65 (Rubenstein and Goodenough, 1965) as well as some other widely-used datasets including Rare-Word (Luong et al., 2013), SCWS (Huang et al., 2012), Men-3k (Bruni et al., 2014) and WS-353-Related (Agirre et al., 2009).
136	14	To further evaluate the learned word embeddings, we also conduct 4 text classification tasks using the 20 Newsgroups dataset.6 The dataset totally contains around 19000 documents of 20 different newsgroups, and each corresponding to a different topic, such as guns, motorcycles, electronics and so on.
137	15	For each task, we randomly select the documents of 10 topics and split them into training/validation/test subsets at the ratio of 6:2:2, which are emplyed to train, validate and test an L2-regularized 10-categorization logistic regression (LR) classifier.
159	15	The average classification accuracy across the 4 tasks is utilized as the evaluation metric for different models.
163	16	Moreover, it can be found that incorporating morphological knowledge (morphemes or latent meanings of morphemes) into word embeddings can contribute to enhancing the performance of word embeddings in the downstream NLP tasks.
172	14	Secondly, the performance of CBOW is sensitive to the corpus size.
179	15	To visualize the embeddings of our models, we randomly select several words from the results of LMM-A.
180	45	The dimensions of the selected word embeddings are reduced from 200 to 2 using Principal Component Analysis (PCA), and the 2-D word embeddings are illustrated in Fig.
181	64	The words with different colors reflect that they have different morphemes.
182	37	It is apparent that words with similar morphemes have a trend to group together and stay near the latent meanings of their morphemes.
189	15	The experimental results demonstrate that our models outperform the baselines on five word similarity datasets.
191	19	In the future, we intend to evaluate our models for some morpheme-rich languages like Russian, German and so on.
