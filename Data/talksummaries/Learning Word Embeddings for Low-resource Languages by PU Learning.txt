23	1	• Our experimental results show that PULearning improves the word embedding training in the low-resource setting.
52	1	More specifically, each entry of PMI matrix can be defined by PMI(w, c) = log P̂ (w, c) P̂ (w) · P̂ (c) , (1) where P̂ (w), P̂ (c) and P̂ (w, c) are the the frequency of word w, word c, and word pairs (w, c), respectively.
54	1	Extending from the PMI metric, the PPMI metric replaces all the negative entries in PMI matrix by 0: PPMI(w, c) = max(PMI(w, c), 0).
68	1	λi and λj are weights of regularization term.
69	1	They are hyperparameters that need to be tuned.
75	1	Eq (3) is very similar to the one used in previous matrix factorization approaches such as GloVe, but we propose a new way to set the weights Cij .
90	1	To incorporate the bias term in (3), we propose the following training algorithm based on the coordinate descent approach.
97	1	When t = k+2, we do not update the corresponding column of H ′ (they are all 1) and we update the k+ 2-th column of W ′ (corresponding to b1, .
108	1	Post-processing of Word/Context Vectors The PU-Learning framework factorizes the PPMI matrix and generates two vectors for each word i, wi ∈ Rk and hi ∈ Rk.
113	1	Our goal in this paper is to train word embedding models for low-resource languages.
114	1	In this section, we describe the experimental designs to evaluate the proposed PU-learning approach.
116	1	Then, we provide details of parameter tuning.
118	1	In the word similarity task, each question contains a word pairs and an annotated similarity score.
121	1	Following the settings in literature, the experiments are conducted on five data sets, WordSim353 (Finkelstein et al., 2001), WordSim Similarity (Zesch et al., 2008), WordSim Relatedness (Agirre et al., 2009), Mechanical Turk (Radinsky et al., 2011) and MEN (Bruni et al., 2012).
123	1	We evaluate the performances on Google analogy dataset (Mikolov et al., 2013a) which contains 8,860 semantic and 10,675 syntactic questions.
136	1	The proposed PULearning framework is implemented based on Yu et al. (2017a).
148	1	We analyze the sensitivity of the model to these hyper-parameters in the experimental result section.
153	1	The results show that the proposed PULearning framework outperforms the two baseline approaches significantly in most datasets.
157	1	The results in analogy task are obtained by 3CosMul method (Levy and Goldberg, 2014a).
158	1	As the corpus size grows, the performance of all models improves, and the PU-learning model consistently outperforms other methods in all the tasks.
161	1	Impacts of ρ and λ We investigate how sensitive the model is to the hyper-parameters, ρ and λ.
169	1	In this paper, we presented a PU-Learning framework for learning word embeddings of lowresource languages.
172	21	We are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small.
173	124	Besides, we plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting.
174	132	Acknowledge This work was supported in part by National Science Foundation Grant IIS-1760523, IIS-1719097 and an NVIDIA Hardware Grant.
