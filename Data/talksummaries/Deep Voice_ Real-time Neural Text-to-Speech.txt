0	64	Synthesizing artificial human speech from text, commonly known as text-to-speech (TTS), is an essential component in many applications such as speech-enabled devices, navigation systems, and accessibility for the visually-impaired.
1	22	*Listed alphabetically 1Baidu Silicon Valley Artificial Intelligence Lab, 1195 Bordeaux Dr. Sunnyvale, CA 94089 2Baidu Corporation, No.
2	13	10 Xibeiwang East Road, Beijing 100193, China.
6	62	Deep Voice is inspired by traditional text-to-speech pipelines and adopts the same structure, while replacing all components with neural networks and using simpler features: first we convert text to phoneme and then use an audio synthesis model to convert linguistic features into speech (Taylor, 2009).
8	24	), our only features are phonemes with stress annotations, phoneme durations, and fundamental frequency (F0).
9	46	This choice of features makes our system more readily applicable to new datasets, voices, and domains without any manual data annotation or additional feature engineering.
13	12	Prior work has demonstrated that a WaveNet (van den Oord et al., 2016) can generate close to human-level speech.
48	73	Our grapheme-to-phoneme model is based on the encoderdecoder architecture developed by (Yao & Zweig, 2015).
63	38	To illustrate our label encoding, consider the string “Hello!”.
64	261	To convert this to a sequence of phoneme pair labels, convert the utterance to phonemes (using a pronunciation dictionary such as CMUDict or a grapheme-tophoneme model) and pad the phoneme sequence on either end with the silence phoneme to get “sil HH EH L OW sil”.
65	29	Finally, construct consecutive phoneme pairs and get “(sil, HH), (HH, EH), (EH, L), (L, OW), (OW, sil)”.
66	96	Input audio is featurized by computing 20 Mel-frequency cepstral coefficients (MFCCs) with a ten millisecond stride.
67	73	On top of the input layer, there are two convolution layers (2D convolutions in time and frequency), three bidirectional recurrent GRU layers, and finally a softmax output layer.
68	15	The convolution layers use kernels with unit stride, height nine (in frequency bins), and width five (in time) and the recurrent layers use 512 GRU cells (for each direction).
70	46	To compute the phoneme-pair error rate (PPER), we decode using beam search.
72	15	For training, we use the Adam optimization algorithm with 1 = 0.9, 2 = 0.999, " = 10 8, a batch size of 128, a learning rate of 10 4, and an annealing rate of 0.95 applied every 500 iterations (Kingma & Ba, 2014).
74	36	The input to the model is a sequence of phonemes with stresses, with each phoneme and stress being encoded as a one-hot vector.
81	12	Our audio synthesis model is a variant of WaveNet.
85	25	We break the convolution into two matrix multiplies per timestep with Wprev and Wcur.
94	10	In addition, we present audio synthesis results for our models trained on a subset of the Blizzard 2013 data (Prahallad et al., 2013).
111	13	The 20, 30, and 40 layer models all produce high quality recognizable speech, but the 40 layer models have less noise than the 20 layer models, which can be detected with highquality over-ear headphones.
115	17	This suggests the receptive field of the 20 layer models is sufficient, and we conjecture the difference in audio quality is due to some other factor than receptive field size.
121	29	While models with unusually high loss sound distinctly noisy, models that optimize below a certain threshold do not have a loss indicative of their quality.
126	17	We purposefully include ground truth samples in every batch of samples that raters evaluate to highlight the delta from human speech and allow raters to distinguish finer grained differences between models; the downside of this approach is that the resulting MOS scores will be significantly lower than if raters are presented only with synthesized audio samples.
127	17	First of all, we find a significant drop in MOS when simply downsampling the audio stream from 48 kHz to 16 kHz, especially in combination with µ-law companding and quantization, likely because a 48 kHz sample is presented to the raters as a baseline for a 5 score, and a low quality noisy synthesis result is presented as a 1.
131	31	Finally, our best models run slightly slower than real-time (see Table 2), so we demonstrate that synthesis quality can be traded for inference speed by adjusting model size by obtaining scores for models that run 1X and 2X faster than real-time.
132	9	We also tested WaveNet models trained on the full set of features from the original WaveNet publication, but found no perceptual difference between those models and models trained on our reduced feature set.
135	31	We evaluated the model using the procedure described in Section 4.4, which encourages raters to compare synthesized audio directly with the ground truth.
138	10	WaveNet inference poses an incredibly challenging computational problem due to the high-frequency, autoregressive nature of the model, which requires orders of magnitude more timesteps than traditional recurrent neural networks.
148	139	(See Appendix E for a complete performance model.)
