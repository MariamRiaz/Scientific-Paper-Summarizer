28	1	Furthermore, we show that the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metric and human evaluation on Amazon Mechanical Turk.
30	1	While our framework is generic and can be applied to both classification and regression models, the current discussion is restricted to classification models.
32	1	Our method is derived from considering the mutual information between a particular pair of random vectors, so we begin by providing some basic background.
34	1	More precisely, the mutual information is given by the Kullback-Leibler divergence of the product of marginal distributions of X and Y from the joint distribution of X and Y (Cover & Thomas, 2012); it takes the form I(X;Y ) = EX,Y [ log pXY (X,Y ) pX(X)pY (Y ) ] , where pXY and pX , pY are the joint and marginal probability densities if X,Y are continuous, or the joint and marginal probability mass functions if they are discrete.
36	1	One can show the mutual information is nonnegative and symmetric in two random variables.
37	1	The mutual information has been a popular criteria in feature selection, where one selects the subset of features that approximately maximizes the mutual information between the response variable and the selected features (Gao et al., 2016; Peng et al., 2005).
44	1	We have thus defined a new random vector XS ∈ Rk; see Figure 1 for a probabilistic graphical model representing its construction.
51	1	Conversely, any global optimum of Problem (1) degenerates to E∗ almost surely over the marginal distribution PX .
54	1	In the case when Pm(Y |xS) is unknown or computationally expensive to estimate accurately, we can choose to restrict E to suitably controlled families so as to prevent overfitting.
94	1	In the case of classification with c classes, we can write EX,ζ [ c∑ y=1 [Pm(y | X) log gα(V (θ, ζ) X, y) ] .
120	1	The sufficient statistic is formed by an additive model of the first four features.
138	1	The underlying true features are known for each sample, and hence the median ranks of selected features for each sample in a validation data set are reported as a performance metric, the box plots of which have been plotted in Figure 3.
150	1	The Large Movie Review Dataset (IMDB) is a dataset of movie reviews for sentiment classification (Maas et al., 2011).
151	1	It contains 50, 000 labeled movie reviews, with a split of 25, 000 for training and 25, 000 for testing.
159	1	We would like to find out which k words make the most influence on the decision of the model in a specific review.
188	1	This indicates the selected words by L2X can serve as key words for human to understand the model behavior.
221	2	We observe that L2X captures most of the informative patches, in particular those containing patterns that can distinguish 3 and 8.
222	25	We have proposed a framework for instancewise feature selection via mutual information, and a method L2X which seeks a variational approximation of the mutual information, and makes use of a Gumbel-softmax relaxation of discrete subset sampling during training.
223	121	To our best knowledge, L2X is the first method to realize real-time interpretation of a black-box model.
224	120	We have shown the efficiency and the capacity of L2X for instancewise feature selection on both synthetic and real data sets.
