0	39	Online communications are increasingly becoming fast-paced and frequent, and hidden in these abundant user-generated social media posts are insights for understanding users and their preferences.
1	83	However, these social media posts often come in unstructured text or images, making massive-scale opinion mining extremely challeng- ing.
2	67	Named entity disambiguation (NED), the task of linking ambiguous entities from free-form text mention to specific entities in a pre-defined knowledge base (KB), is thus a critical step for extracting structured information which leads to its application for recommendations, advertisement, personalized assistance, etc.
3	23	While many previous approaches on NED been successful for well-formed text in disambiguating polysemous entities via context resolution, several additional challenges remain for disambiguating entities from extremely short and coarse text found in social media posts (e.g. “juuustin ” as opposed to “I love Justin Bieber/Justin Trudeau/etc.”).
6	25	However, as popular social media platforms are increasingly incorporating a mix of text and images (e.g. Snapchat, Instargram, Pinterest, etc.
8	31	For example, the mention of ‘juuustin’ is completely ambiguous in its textual form, but an accompanying snap image of a concert scene may help disambiguate or re-rank among several lexical candidates (e.g. Justin Bieber (a pop singer) versus Justin Trudeau (a politician) in Figure 1).
9	60	To this end, we introduce a new task called Multimodal Named Entity Disambiguation (MNED) that handles unique challenges for social media posts composed of extremely short text and images, aimed at disambiguationg entities by leveraging both textual and visual contexts.
15	26	Note that our method takes different perspectives from the previous work on NED (He et al., 2013; Yamada et al., 2016; Eshel et al., 2017) in the following important ways.
21	55	Our contributions are as follows: for the new MNED task we introduce, we propose a deep zeroshot multimodal network with (1) a CNNLSTM hybrid module that extracts contexts from both image and text, (2) a zeroshot learning layer which via embeddings projection allows for entity linking with 1M knowledge graph entities even for entities unseen from captions in training set, and (3) a lexical language model called Deep Levenshtein to compute lexical similarities between mentions and entities, relaxing the need for fixed candidates generation.
22	24	We show that the proposed approaches successfully disambiguate incomplete mentions as well as polysemous entities, outperforming the state-of-the-art models on our newly crawled SnapCaptionsKB dataset, composed of 12K image-caption pairs with named entities annotated and linked with an external KB.
23	14	Figure 2 illustrates the proposed model, which maps each multimodal social media post data to one of the corresopnding entities in the KB.
25	25	We also obtain lexical character-level representation of a mention to compare with lexical representation of KB entities, using a proposed model called Deep Levenshtein (Section 2.3).
31	28	We represent each output label in two modalities: y = {yKB;yc}, where yKB is a knowledge base label embeddings representation (Sec- tion 2.4), and and yc is a character embeddings representation of KB entities (Section 2.3: Deep Levenshtein).
38	17	Word embeddings are thus represented as distributional semantics of words.
43	54	Towards this goal, we train a separate deep neural network to compute approximate Levenshtein distance which we call Deep Levenshtein (Figure 3), composed of a shared bi-directional character LSTM, shared character embedding matrix, fully connected layers, and a dot product merge operation layer.
52	15	Knowledge graph label embeddings are learned from known relations among entities within a graph (e.g. ‘IS-A’, ‘LOCATED-AT’, etc.
53	42	), the resulting embeddings of which can group similar entities closer in the same space (e.g. ‘pop stars’ are in a small cluster, ‘people’ and ‘organizations’ clusters are far apart, etc.)
57	18	In our experiments, we use the 1M subset of the Freebase knowledge graph (Bast et al., 2014) to obtain label embeddings with the Holographic KB implementation by (Nickel et al., 2016).
58	15	Using the contextual information extracted from surrounding text and an accompanying image (Section 2.2) and lexical embeddings of a mention (Section 2.3), we build a Deep Zeroshot MNED network (DZMNED) which predicts a corresponding KB entity based on its knowledge graph embeddings (Section 2.4) and lexical similarity (Section 2.3) with the following objective: min W LKB(x,yKB;Ww,Wv,Wf )+Lc(xc,yc;Wc) where LKB(·)= 1 N N∑ i=1 ∑ ỹ 6=y(i)KB max[0, ỹ · y(i)KB−f(x (i)) · (y(i)KB− ỹ) >] Lc(·) = 1 N N∑ i=1 ∑ ỹ 6=y(i)c max[0, ỹ · y(i)c −c(x(i)c ) · (y(i)c − ỹ)>] R(W): regularization where LKB(·) is the supervised hinge rank loss for knowledge graph embeddings prediction, Lc(·) is the loss for lexical mapping between mentions and KB entities, x is a weighted average of three modalities x = {xw;xv;xc} via the modality attention module.
59	63	f(·) is a transformation function with stacked layers that projects weighted input to the KB embeddings space, ỹ refers to the embeddings of negative samples randomly sampled from KB entities except the ground truth label of the instance, W = {Wf ,Wc,Ww,Wv} are the learnable parameters for f , c, w, and v respectively, andR(W) is a weight decay regularization term.
63	15	Task: Given a caption and an accompanying image (if available), the goal is to disambiguate and link a target mention in a caption to a corresponding entity from the knowledge base (1M subset of the Freebase knowledge graph (Bast et al., 2014)).
64	16	Our SnapCaptionsKB dataset is composed of 12K user-generated image and textual caption pairs where named entities in captions and their links to KB entities are manually labeled by expert human annotators.
93	56	We see that the proposed approach significantly outperforms the baselines which use fixed candidates generation method.
95	60	k-NN methods which retrieve lexical neighbors of mention (in an attempt to perform soft normalization on mentions) also do not perform well.
97	120	This result indicates that the proposed zeroshot model is capable of predicting for unseen entities as well.
100	42	The modality attention module also adds performance gain by re-weighting the modalities based on their informativeness.
101	18	Error Analysis: Table 3 shows example cases where incorporation of visual contexts affects disambiguation of mentions in textual captions.
102	80	For example, polysemous entities such as ‘Jordan’ in the caption “Taking the new Jordan for a walk” or ‘CID’ as in “LETS GO CID” are hard to disambiguate due to the limited textual contexts provided, while visual information (e.g. visual tags ‘footwear’ for Jordan, ‘DJ’ for CID) provides similarities to each mention’s distributional semantics from other training examples.
121	41	We introduce a new task called Multimodal Named Entity Disambiguation (MNED), which is applied on short user-generated social media posts that are composed of text and accompanying images.
122	37	Our proposed MNED model improves upon the state-of-the-art models by 1) extracting visual contexts complementary to textual contexts, 2) by leveraging lexical embeddings into entity matching which accounts for various surface forms of entities, removing the need for fixed candidates generation process, and 3) by performing entity matching in the distributed knowledge graph embeddings space, allowing for matching of unseen mentions and entities by context resolutions.
