0	29	We consider finite-sum problems of the form min w∈W F (w) = 1 n n∑ i=1 fi(w), (1) where W is a closed convex subset of some Euclidean or Hilbert space, each fi is convex and µ-smooth, and F is λ-strongly convex1.
1	13	Such problems are ubiquitous in machine learning, for example in order to perform empirical risk minimization using convex losses.
3	23	The complexity of the algorithm is measured in terms of the number of oracle calls required to optimize the function to within some prescribed accuracy.
5	15	, n returns fi(w) and ∇fi(w), the number of oracle queries required to find an -optimal solution is at least of order Ω ( n+ √ nµ λ log ( 1 )) , either under algorithmic assumptions or assuming the dimension is sufficiently large2 (Agarwal and Bottou, 2014; Lan, 2015; Woodworth and Srebro, 2016; Arjevani and Shamir, 2016a).
8	13	A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt − αt ( ∇2F (w) )−1∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter.
9	14	Second-order methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear).
13	17	However, for optimization problems as in Eq.
15	16	For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = `i(〈w,xi〉), where xi is a training instance and `i is some loss function.
51	20	As mentioned earlier, the observation that such first-order oracle bounds can be extended to higher-order oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin (1983, Section 7.2.6).
57	14	This is done in such a way, that the Hessian observed by the algorithm does not provide more information than the gradient, and cannot be used to improve the algorithm’s performance.
60	14	To adapt the setting to a finite-sum problem, we assume that the second-order oracle is given both a point w and an index i ∈ {1, .
72	14	Moreover, these manipulations can only depend on (at most) the last bn/2c Hessians returned by the oracle.
75	27	wt belongs to the set Wt ⊆ Rd, defined recursively as follows: W1 = {0}, and Wt+1 is the closure of the set of vectors derived fromWt ∪ {∇fit(wt)} by a finite number of operations of the following form: • w,w′ → αw + α′w′, where α, α′ are arbitrary scalars.
80	14	, dr in order, then H can also be decomposed into r blocks of size d1, .
84	22	Moreover, for quadratic functions, it is easily verified that the assumption also allows prox operations (i.e. returning arg minw fi(w) + ρ 2‖w − w ′‖2 for some ρ, i and previously computed point w′).
87	12	As mentioned earlier, this is necessary, since if we could compute the average of all Hessians, then we could implement the Newton method.
88	13	The assumption that the algorithm only “remembers” the last bn/2c Hessians is also realistic, as existing computationally-efficient methods seek to use much fewer than n individual Hessians at a time.
101	39	, fn : Rd → R (for d = Õ(1 + √ µ/λn), hiding factors logarithmic in n, µ, λ, ), such that the number of calls T to a second-order oracle, so that E [ F (wT )− min w∈Rd F (w) ] ≤ · ( F (0)− min w∈Rd F (w) ) , must be at least Ω ( n+ √ nµ λ · log ( (λ/µ)3/2 √ n )) .
102	21	Comparing this with the (tight) first-order oracle complexity bounds discussed in the introduction, we see that the lower bound is the same up to log-factors, despite the availability of second-order information.
103	45	In particular, the lower bound exhibits none of the favorable properties associated with full second-order methods, which can compute and invert Hessians of F : Whereas the full Newton method can attain O(log log(1/ )) rates, and be independent of µ, λ if F satisfies a self-concordance property (Boyd and Vandenberghe, 2004), here we only get a linear O(log(1/ )) rate, and there is a strong dependence on µ, λ, even though the function is quadratic and hence self-concordant.
105	58	, n} independently and uniformly at random, and define fi(w) = a · w21 + â · d−1∑ l=1 1jl=i(wl − wl+1)2 + ā · w2d − ã · w1 + λ 2 ‖w‖2, where 1A is the indicator function of the event A, and a, â, ā, ã are parameters chosen based on λ, µ, n. The average function F (w) = 1n ∑n i=1 fi(w) equals F (w) = a·w21+ â n · d−1∑ l=1 (wl−wl+1)2+ā·w2d−ã·w1+ λ 2 ‖w‖2.
106	27	By setting the parameters appropriately, it can be shown that F is λ-strongly convex and each fi is µ-smooth.
108	15	, qd) for q = √ κ− 1√ κ+ 1 , where κ = µ λ − 1 n + 1 (4) is the so-called condition number of F .
112	11	The main difference is in how we construct the individual functions fi, and in analyzing the effect of second-order rather than just first-order information.
113	51	To upper bound lT , we let lt (where t = 1, .
114	39	, T ) be the largest non-zero coordinate in wt, and track how lt increases with t. The key insight is that if w1, .
115	17	,wt−1 are zero beyond some coordinate l, then any linear combinations of them, as well as multiplying them by matrices based on second-order information, as specified in Assumption 1, will still result in vectors with zeros beyond coordinate l. The only way to “advance” and increase the set of non-zero coordinates is by happening to query the function fjl .
119	18	It can be shown that the number of coordinates needed to get an -optimal solution is Ω̃( √ µ/nλ·log(1/ )) (hiding some log-factors).
125	13	Indeed, knowing the Hessians of fi, one can devise an index-schedule which gains at least one coordinate at every iteration (by querying the function which holds the desired 2 × 2 block), as opposed to O(1/n) on average in the oblivious case.
157	83	One standard and well-known approach is to use only gradient information to construct such an approximation, leading to the family of quasi-Newton methods (Nocedal and Wright, 2006).
