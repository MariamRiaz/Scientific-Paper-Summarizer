0	25	Improving the relationship between police officers and the communities they serve is a critical societal goal.
1	61	We propose to study this relationship by applying NLP techniques to conversations between officers and community members in traffic stops.
2	136	Traffic stops are one of the most common forms of police contact with community members, with 10% of U.S. adults pulled over every year (Langton and Durose, 2013).
4	84	The rapid adoption of body-worn cameras by police departments in the U.S. (laws in 60% of states in the U.S. encourage the use of body cameras) and across the world has provided unprecedented insight into traffic stops.1 While footage from these cameras is used as evidence in contentious cases, the unstructured nature and immense volume of video data means that most of this footage is untapped.
14	31	Traffic stops are a kind of institutional talk; as are, for example, doctor-patient interactions, counseling conversations, and citizen calls for help from police.
22	48	Recent work has integrated speech act theory (Austin, 1975) and conversational analysis (Schegloff and Sacks, 1973; Sacks et al., 1974; Schegloff, 1979) into models of dialog acts for domains like meetings (Ang et al., 2005), telephone calls (Stolcke et al., 2006), emails (Cohen et al., 2004), chats (Kim et al., 2010), and Twitter (Ritter et al., 2010).
24	87	Actions, their sequences, and interpretations during institutional talk depend not only on the speaker (as speech act theory suggests) or the dialog (as conversational analysts argue), but they are inherently tied to the institutional context.
28	24	Such recurring patterns in language and conversation exist across different institutional contexts such as doctor-patient interactions, psychological counseling, sales calls, court room conversations, as well as traffic stops (Heritage, 2005).
30	34	A police officer failing to explain the reason for the traffic stop can lead to aggravation in the driver (Giles et al., 2007), and an officer’s perceived communication skills (e.g. do they listen, take civilian views into account) predict civilian’s attitudes towards the police (Giles et al., 2006).
54	64	To develop the taxonomy of institutional dialog acts, we begin with a data-oriented exploration: identifying recurring sequences of topic segments using the (unsupervised) mixed membership Markov model (Paul, 2012).3 Figure 1 shows the topic segments assigned by a 10-topic model on the traffic stop of Table 1.
55	23	The model identified different spans of conversation; the officer gives the reason for the stop (orange), asks for documents (blue), collects driver information (purple), then in the end, there are spans of issuing a sanction (beige) and closing (yellow).
74	40	Stops were chosen at random from the entire corpus for each round; however, seven of the previously annotated stops were incorrectly included in the final round of annotations, resulting in a total of 113 annotated stops (7081 sentences, 4245 turns).
88	73	Common co-occurrences were GREETING and REASON, and GREETING and ORDERS, e.g., Hey, turn the car off.
125	20	We first upsample Switchboard speech to the 16 KHz of our data, and then mix them with noise samples randomly picked from our data where speech is not identified, using a random speech-tonoise-ratio between 0 and 10.
134	38	The BLSTM is used to model short segments of speech (with a sliding window of 40 frames), and predict frame-level HMM states at each time frame9.
159	38	We could not build a standard supervised officer-versus-other classifier, because the stops contain large untranscribed regions of officer speech (we did not transcribe segments where the officer was, for example, talking to the dispatcher in the car).
160	20	We therefore instead built a two-output classifier to discriminate between the officer and community member speech, and used a tuned threshold (0.55) on the posterior probability of officer as our voice activity detector, drawing on the intuitions of (Williams and Ellis, 1999; Verma et al., 2015) who found that posterior features on speech tasks also improved speech/nonspeech performance.
176	21	We call this task stop-level act detection, in which each stop is labeled as a positive instance of an act if that particular act occurred in it in the gold labels.
183	22	Despite our relatively small training resources (113 stops with dialog act labels, ASR and segmentation training data from one month), performance at the stop level directly from raw audio is surprisingly high.
193	61	Was the reason given before or after asking for their documentation?
194	50	We first apply our high performance (78% F-score at turn level; 89% at stop level) tagging model on manual transcripts.
197	68	Only 69% of the stops started with a greeting, and an even smaller percentage of stops ended with a positive closing.
199	23	Using the turn-level tags assigned by our system, we calculate the transition probabilities between dialog acts.
202	60	Figure 6, for example, shows different conversational paths that officers take before explaining the reason for the stop.
206	22	First, our work is based on data from a single police department (the Oakland Police Department in the State of California) in the U.S.
211	157	Like any data-oriented approach, our machine learning models may have captured the idiosyncrasies of the particular department represented in our dataset.
217	157	However, drivers’ speech may also need to be taken into account sometimes; e.g., if an officer says yes to a driver’s question did you stop me for running the red light?, the officer has in fact given the reason for the stop even though their words alone don’t convey that fact.
227	217	Our tagger detects institutional acts at the stop level directly from raw bodycamera audio with 81% F-score, with even higher accuracy on important acts like giving the reason for a stop.
234	29	Current work aims to improve the performance of the segmentation and diarization components, with the hope of reducing some of the performance gap with our system run on gold transcripts.
235	30	We also plan to extend the preliminary analyses we describe in Section 9, for instance, studying how the different conversational paths and the presence or absence of certain acts (such as greetings or reason) shapes the rest of the conversation, including how it changes the community member’s language use.
