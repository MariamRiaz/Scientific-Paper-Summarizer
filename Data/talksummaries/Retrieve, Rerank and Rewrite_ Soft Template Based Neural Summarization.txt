0	36	The exponentially growing online information has necessitated the development of effective automatic summarization systems.
1	19	In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning.
3	16	Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016).
7	18	For example, 3% of summaries contain less than 3 words, while there are 4 summaries repeating a word for even 99 times.
13	22	For instance, a concise template to conclude the stock market quotation is: [REGION] shares [open/close] [NUMBER] percent [lower/higher], e.g., “hong kong shares close #.# percent lower”.
17	18	Inspired by retrieve-based conversation systems (Ji et al., 2014), we assume the golden summaries of the similar sentences can provide a reference point to guide the input sentence summarization process.
18	39	We call these existing summaries soft templates since no actual rules are nee- ded to build new summaries from them.
19	65	Due to the strong rewriting ability of the seq2seq framework (Cao et al., 2017a), in this paper, we propose to combine the seq2seq and template based summarization approaches.
20	40	We call our summarization system Re3Sum, which consists of three modules: Retrieve, Rerank and Rewrite.
22	19	Then, we extend the seq2seq model to jointly learn template saliency measurement (Rerank) and final summary generation (Rewrite).
23	16	Specifically, a Recurrent Neural Network (RNN) encoder is applied to convert the input sentence and each candidate template into hidden states.
35	19	Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri} from the training corpus.
45	14	Since the size of our dataset is quite large (over 3M), we leverage the widely-used Information Retrieve (IR) system Lucene1 to index and search efficiently.
48	14	To conduct template-aware seq2seq generation (rewriting), it is a necessary step to encode both the source sentence x and soft template r into hidden states.
56	16	In Retrieve, the template candidates are ranked according to the text similarity between the corresponding indexed sentences and the input sentence.
58	15	Here we use the widely-used summarization evaluation metrics ROUGE (Lin, 2004) to measure the actual saliency s∗(r,y∗) (see Section 3.2).
67	17	Therefore, we leverage the strong rewriting ability of the seq2seq model to generate more faithful and informative summaries.
68	26	Specifically, since the input of our system consists of both the sentence and soft template, we use the concatenation function3 to combine the hidden states of the sentence and template: Hc = [h x 1 ; · · · ;hx−1;hr1; · · · ;hr−1] (5) The combined hidden states are fed into the prevailing attentional RNN decoder (Bahdanau et al., 2014) to generate the decoding hidden state at the position t: st = Att-RNN(st−1, yt−1,Hc), (6) where yt−1 is the previous output summary word.
70	26	There are two types of costs in our system.
74	22	We adopt the common negative log-likelihood (NLL) as the loss function: JG(θ) = − log(p(y∗|x, r)) (9) = − ∑ t log(ot[y ∗ t ]) To make full use of supervisions from both sides, we combine the above two costs as the final loss function: J(θ) = JR(θ) + JG(θ) (10) We use mini-batch Stochastic Gradient Descent (SGD) to tune model parameters.
78	17	This parallel corpus is produced by pairing the first sentence in the news article and its headline as the summary with heuristic rules.
87	20	We also introduce a series of metrics to measure the summary quality from the following aspects: LEN DIF The absolute value of the length diffe- rence between the generated summaries and the actual summaries.
93	15	A seriously large copy ratio indicates that the summarization system pays more attention to compression rather than required abstraction.
94	27	NEW NE The number of the named entities that do not appear in the source sentence or actual summary.
127	17	Even though, our model surpasses Featseq2seq by 22% and ABS+ by 60% on ROUGE-2.
128	25	When soft templates are ignored, our model is equivalent to the standard at- tentional seq2seq model OpenNMTI .
129	15	Therefore, it is safe to conclude that soft templates have great contribute to guide the generation of summaries.
131	17	We introduce five types of different soft templates: Random An existing summary randomly selected from the training corpus.
174	14	In contrast, the 2-best results of OpenNMT is almost the same, and often a shorter summary is only a piece of the other one.
197	15	Experiments show that our model can generate informative, readable and stable summaries.
