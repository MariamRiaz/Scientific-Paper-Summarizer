0	12	Vectorial representations derived from large current events datasets such as Google News have been shown to perform well on word similarity tasks (Mikolov, 2013; Levy & Goldberg, 2014).
1	34	This paper shows vectorial representations derived from substantially smaller explanatory text datasets such as English Wikipedia and Simple English Wikipedia preserve enough lexical semantic information to make these kinds of category judgments with equal or better accuracy.
2	4	Analysis shows these results may be driven by a prevalence of commonsense facts in explanatory text.
3	11	These positive results for relatively small datasets suggest vectors derived from slower but more accurate analyses of these resources may be practical for lexical semantic applications.
5	59	Wikipedia as a corpus has been heavily used to train various NLP models.
6	40	Features of Wikipedia are well exploited in research like semantic web (Lehmann et al, 2014) and topic modeling (Dumais, 1988; Gabrilovich, 2007), but more importantly Wikipedia has been a reliable source for word embedding training because of its sheer size and coverage (Qiu, 2014), as recent word embedding models (Mikolov et al, 2013; Pennington et al, 2014) all use Wikipedia as an important corpus to build and evaluate their algorithms for word embedding creation.
11	3	Simple English Wikipedia is often used in simplification research (Coster, 2011; Napoles, 2010) where sentences from full English Wikipedia are matched to sentences from Simple English Wikipedia to explore techniques to simplify sentences.
14	1	Mikolov proposed two neusral network based models for word representation: Continuous Bag-of-Words (CBOW) and Skip-gram.
19	14	As pointed out by Agirre et al (2009) and Levy & Goldberg (2014), relatedness may actually be measuring topical similarity and be better predicted by a bag-of-words model, and similarity may be measuring functional or syntactic similarity and be better predicted by a contextwindow model.
22	6	The WordSim353 (Agirre, 2009) dataset is used as the test dataset.
24	26	There are 100 similar word pairs, 149 related pairs and 104 pairs of words with very weak or no relation.
26	22	The objective of the task is to rank the similar word pairs higher than related ones.
32	10	The word2vec python implementation provided by gensim (Rehurek et al, 2010) package is used to train all the word2vec models.
37	99	This model is trained on the Google News dataset with 100 billion words, which is 30 times as large as the full English Wikipedia and 240 times as large as Simple English Wikipedia.
38	5	Table 1 shows the accuracy rate at every recall rate point, with the sum of all the accuracy rates as the cumulative score.
39	36	It is shown that GN-SG, although not far behind, is not giving the best performance despite being trained on the largest dataset.
40	88	In fact, it is clear that it never excels at any given recall rate point.
41	78	It outperforms various models at certain recall rate points by a small margin, but there is no obvious advantage gained from training using a much larger corpus even when compared with the models trained on Simple English Wikipedia, despite the greater risk of sparse data problems on this smaller data set.
42	68	For models trained on Simple English Wikipedia and full English Wikipedia, it is also interesting to see that the models almost perform equally well.
43	9	The FW-CBOW trained on full English Wikipedia performs the best among the models overall, but for the first few recall rate points, it performs equally well or slightly worse than either SWCBOW or SW-SG trained on Simple English Wikipedia.
45	23	Comparing FW-SG with SW-SG and SWCBOW, there is almost no sign of performance gain from training using full Wikipedia instead of the much smaller Simple Wikipedia.
46	7	FW-SG performs equally well or often slightly worse than both Simple Wikipedia models.
47	50	The main observation in this paper is that Google News is not out-performing other systems substantially and that full Wikipedia systems are not out-performing Simple Wikipedia substantially (that is, comparing the CBOW models to one another and the Skip-gram models to one another).
48	28	The main result from the table is not that smaller training datasets yield better systems, but that systems trained using significantly smaller training datasets of explanatory text have very close performances in this task compared with systems trained on very large datasets, despite the big training data size difference.
