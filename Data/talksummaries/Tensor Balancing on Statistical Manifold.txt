5	36	However, the algorithm converges linearly (Soules, 1991), which is prohibitively slow for recently emerging large and sparse matrices.
6	12	Although Livne & Golub (2004) and Knight & Ruiz (2013) tried to achieve faster convergence by approximating each step of Newton’s method, the exact Newton’s method with quadratic convergence has not been intensively studied yet.
21	16	Finally, we formulate the matrix and tensor balancing problem in Section 5 and summarize our contributions in Section 6.
22	28	Given a nonnegative square matrix A = (aij) ∈ Rn×n≥0 , the task of matrix balancing is to find r, s ∈ Rn that satisfy (RAS)1 = 1, (RAS)T1 = 1, (1) whereR = diag(r) and S = diag(s).
38	12	All methods were implemented in C++ with the Eigen library and compiled with gcc 4.8.31.
41	51	The first set of experiments used a Hessenberg matrix, which has been a standard benchmark for matrix balancing (Parlett & Landis, 1982; Knight & Ruiz, 2013).
43	16	We varied the size n from 10 to 5, 000, and measured running time (in seconds) and the number of iterations of each method.
45	12	Our balancing algorithm with the Newton’s method (plotted in blue in the figures) is clearly the fastest: It is three to five orders of magnitude faster than the standard Sinkhorn-Knopp algorithm (plotted in red).
46	12	Although the BNEWT algorithm (plotted in green) is competitive if n is small, it suddenly fails to converge whenever n ≥ 200, which is consistent with results in the original paper (Knight & Ruiz, 2013) where there is no result for the setting n ≥ 200 on the same matrix.
48	19	To see the behavior of the rate of convergence in detail, we plot the convergence graph in Figure 4 for n = 20, where we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT algorithm, which contrasts with our quick convergence.
53	24	We show that a balanced matrix forms a submanifold and matrix balancing is projection of a given distribution onto the submanifold, where the Jacobian matrix in Equation (4) is derived from the gradient of the manifold.
55	12	We prepare basic notations and the key mathematical tool for posets, the Möbius inversion formula, followed by formulating the log-linear model.
65	24	We consider a probability vector p on (S,≤) that gives a discrete probability distribution with the outcome space S. A probability vector is treated as a mapping p :S → (0, 1) such that ∑ x∈S p(x) = 1, where every entry p(x) is assumed to be strictly larger than zero.
66	16	Using the zeta and the Möbius functions, let us introduce two mappings θ :S → R and η :S → R as θ(x) = ∑ s∈S µ(s, x) log p(s), (6) η(x) = ∑ s∈S ζ(x, s)p(s) = ∑ s≥x p(s).
67	16	(7) From the Möbius inversion formula, we have log p(x) = ∑ s∈S ζ(s, x)θ(s) = ∑ s≤x θ(s), (8) p(x) = ∑ s∈S µ(x, s)η(s).
68	56	(9) They are generalization of the log-linear model (Agresti, 2012) that gives the probability p(x) of an n-dimensional binary vector x = (x1, .
70	18	, η12...n) represents the expectation of variable combinations such that ηi = E[xi] = Pr(xi = 1), ηij = E[xixj ] = Pr(xi = xj = 1), i < j, .
73	12	We exploit this combinatorial property of the loglinear model using the Möbius inversion formula on posets and extend the log-linear model from the power set 2V to any kind of posets (S,≤).
89	14	Since they are connected with each other by the Legendre transformation, they form a dual coordinate system∇ψ(θ) and ∇φ(η) of S (Amari, 2016, Section 1.5), which coincides with θ and η as follows.
104	16	Projection of a distribution onto a submanifold is essential; several machine learning algorithms are known to be formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the target model (Amari, 2016).
106	79	Let S(β) be a submanifold of S such that S(β) = {P ∈ S | θP (x) = β(x), ∀x ∈ dom(β)} (14) specified by a function β with dom(β) ⊆ S+.
111	20	Example 1 (Boltzmann machine).
113	21	The set of probability distributions that can be modeled by a Boltzmann machine G coincides with the submanifold SB = {P ∈ S | θP (x) = 0 if |x| > 2 or x ̸∈ E}, with S = 2V .
130	34	Nowwe are ready to solve the problem of matrix and tensor balancing as projection on a dually flat manifold.
131	14	Recall that the task of matrix balancing is to find r, s ∈ Rn that satisfy (RAS)1 = 1 and (RAS)T1 = 1 with R = diag(r) and S = diag(s) for a given nonnegative square matrix A = (aij) ∈ Rn×n≥0 .
142	11	Moreover, two balancing vectors r and s are exp ( i∑ k=1 θPβ (ιk,m)− θP (ιk,m) ) = { ri if m = 1, ai if m = 2, for every i ∈ [n] and r = rn/ ∑ ij aij .
149	20	It is straightforward to extend matrix balancing to tensor balancing as e-projection onto a submanifold.
160	27	In this paper, we have solved the open problem of tensor balancing and presented an efficient balancing algorithm using Newton’s method.
165	26	Our information geometric formulation can model several machine learning applications such as statistical analysis on a DAG structure.
166	31	Thus, we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models, which we will study in future work.
