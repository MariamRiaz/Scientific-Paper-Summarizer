1	30	The generic cardinality constrained optimization can be expressed as min w2Rp f(w) (1a) subject to kwgk0  sg 8g 2 G (1b) 1University of Rochester, Rochester, NY, USA 2NEC, Cupertino, CA, USA.
2	21	where w is the optimization variable, g is an index subset of [p] := {1, 2, · · · , p}, wg is the sub-vector of w indexed by g. kwgk0 denotes the cardinality of the sub-vector, i.e., the number of nonzeros in wg , G is the hyper set of all predefined groups, and s 2 R|G| is the upper bound vector - sg 2 R refers to the upper bound of the sparsity over group g. Objective f is the loss function which could be defined with different form according to the specific application.
7	11	In this paper, we consider the scenario where the overlapped cardinality constraints (1b) satisfy a Three-view Cardinality Structure (TVCS): Definition 1.
9	46	This definition basically requires that G can be partitioned into three hyper sets G 0 , G 1 , and G 2 , and overlaps can only happen between element sets in different hyper sets.
12	12	The TVCS model is motivated from some important applications, for example, in recommendation, task-worker assignment, and bioinformatics.
14	29	Among the selected books, we want to maintain some diversities - the recommended books by the same author should not exceed a certain number (G 1 based sparsity constraint) and about the same topic should not exceed a certain number either (G 2 based sparsity constraint).
20	20	For example, the total assignments should be bounded by the total budget (corresponding to G 0 ), the total cost of assignments to a single worker cannot exceed a certain threshold (corresponding to G 1 ), and the total cost of assignments on a single task cannot exceed a certain threshold (corresponding to G 2 ).
24	33	The essential goal of identifying gene regulatory network is to identify a weighted directed graph, which can be represented by a square matrix W with p = N ⇥ N elements in total where N is the number of vertices.
31	10	Finally, the proposed TVCS model is validated by the synthetic experiment and two important and novel applications in identification of gene regulatory networks and task assignment problem of crowdsourcing.
65	15	Initialize w0, t = 0; while stop criterion is not met do g = rf(wt) ; // Gradient computation z t = w t g ; // Gradient descent w t+1 = P ⌦(G,s)(z t ) ; // Projection t = t+ 1; end Algorithm 2: Gradient Matching Pursuit.
70	15	This section introduces how to solve the essential projection step.
75	20	The projection problem (2) is equivalent to the following integer linear programming problem (ILP): max x hv2,xi (3) subject to Ax  s x 2 {0, 1}p where v2 is applying element-wise square operation on vector v. A is a |G|⇥ p matrix which is defined as: A =  1 > C (4) where C 2 {0, 1}|G1[G2|⇥p, whose rows represent the indicator vector of each group g 2 G 1 and G 2 .
77	10	The first row 1> corresponds to the overall sparsity i.e. G 0 .
79	18	One common way to handle such ILP is making a linear programming (LP) relaxation.
80	9	In our case, we can use a box constraint x 2 [0, 1]p to replace the integer constraint x 2 {0, 1}p: max x hv2,xi (5) subject to Ax  s x 2 [0, 1]p However, there is no guarantee that a general ILP can be solved via its LP relaxation, because the solution of the relaxed LP is not always integer.
84	8	Given G satisfying TV CS, all the vertices of the feasible set to (5) are integer points.
89	16	Although Simplex method guarantees to find an optimal solution on the vertex and could be very efficient in practice, it does not have a deterministic complexity bound.
92	13	Simplex might be efficient practically, but its worst case may lead to exponential time complexity (Papadimitriou & Steiglitz, 1982).
97	24	This is a convex optimization problem with a quadratic objective and box constraints.
98	8	We adopt the projected gradient descent to solve this problem, and show it converges linearly.
112	19	Theorem 3, we have the linear convergence rate ↵ < 1, so the number of iterations we need is t > log↵ 1 2kz0 z⇤k Therefore, we claim that we can obtain the solution x⇤ by rounding after log↵ 1 2kz0 z⇤k iterations.
140	36	Besides the selection recall, we also compare the classification error.
142	36	Although the overall sparsity has the lowest selection recall, it still has a similar classification error with the methods that consider row or column groups.
144	19	Take the im- age labeling task as an example.
147	15	The goal is to maximize the expected prediction accuracy based on the assignment.
148	19	Let X 2 {0, 1}n⇥m be the assignment matrix, i.e. Xij = 1 if assign the i-th worker to j-th task, otherwise Xij = 0.
149	27	Q 2 [0, 1]n⇥m is the corresponding quality matrix, which is usually estimated from the golden standard test (Ho et al., 2013).
150	28	The whole formulation is defined to maximize the average of the expected prediction accuracy over m tasks over a TVCS constraint: max X 1 m mX j=1 Eacc(Q·,j , X·,j) (8) subject to nX i=1 Xij  sworker, 8j; mX j=1 Xij  stask, 8i; X i,j Xij  stotal; X 2 {0, 1}n⇥m where Eacc(·, ·) is the expected prediction accuracy, sworker is the “worker sparsity”, i.e. the largest number of assigned workers for each task, and stask is the “task sparsity”, i.e. each worker can be assigned with at most stask tasks, and s total is the total sparsity to control the budget, i.e., the maximal number of assignment.
151	17	In image labeling task, we assume that each image can only have two possible classes and the percentage of images in each class is one half.
152	38	We use the Bayesian rule to infer the predicted labels given the workers’ answer.
