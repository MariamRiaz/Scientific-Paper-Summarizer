{"sections": [{"heading": "1. Introduction", "text": "A central goal of Artificial Intelligence is the creation of machines that learn as effectively from human instruction as they do from data. A recent and important step towards this goal is the invention of neural architectures that learn to perform algorithms akin to traditional computers, using primitives such as memory access and stack manipulation (Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2016; Graves et al., 2016). These architectures can be trained through standard gradient descent methods, and enable machines to learn complex behaviour from input-output pairs or program traces. In this context, the role of the human programmer is often limited to providing training data. However, training data is a scarce resource for many tasks. In these cases, the programmer may have\n1Department of Computer Science, University College London, London, UK 2Department of Computer Science, University of Oxford, Oxford, UK 3Department of Theoretical and Applied Linguistics, University of Cambridge, Cambridge, UK. Correspondence to: Matko Bos\u030cnjak <m.bosnjak@cs.ucl.ac.uk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\npartial procedural background knowledge: one may know the rough structure of the program, or how to implement several subroutines that are likely necessary to solve the task. For example, in programming by demonstration (Lau et al., 2001) or query language programming (Neelakantan et al., 2015a) a user establishes a larger set of conditions on the data, and the model needs to set out the details. In all these scenarios, the question then becomes how to exploit various types of prior knowledge when learning algorithms.\nTo address the above question we present an approach that enables programmers to inject their procedural background knowledge into a neural network. In this approach, the programmer specifies a program sketch (Solar-Lezama et al., 2005) in a traditional programming language. This sketch defines one part of the neural network behaviour. The other part is learned using training data. The core insight that enables this approach is the fact that most programming languages can be formulated in terms of an abstract machine that executes the commands of the language. We implement these machines as neural networks, constraining parts of the networks to follow the sketched behaviour. The resulting neural programs are consistent with our prior knowledge and optimised with respect to the training data.\nIn this paper, we focus on the programming language Forth (Brodie, 1980), a simple yet powerful stack-based language that facilitates factoring and abstraction. Underlying Forth\u2019s semantics is a simple abstract machine. We introduce \u22024, an implementation of this machine that is differentiable with respect to the transition it executes at each time step, as well as distributed input representations. Sketches that users write define underspecified behaviour which can then be trained with backpropagation.\nFor two neural programming tasks introduced in previous work (Reed & de Freitas, 2015) we present Forth sketches that capture different degrees of prior knowledge. For example, we define only the general recursive structure of a sorting problem. We show that given only input-output pairs, \u22024 can learn to fill the sketch and generalise well to problems of unseen size. In addition, we apply \u22024 to the task of solving word algebra problems. We show that when provided with basic algorithmic scaffolding and trained jointly with an upstream LSTM (Hochreiter & Schmidhuber, 1997), \u22024 is able to learn to read natural\nar X\niv :1\n60 5.\n06 64\n0v 3\n[ cs\n.N E\n] 2\n3 Ju\nl 2 01\n7\nlanguage narratives, extract important numerical quantities, and reason with these, ultimately answering corresponding mathematical questions without the need for explicit intermediate representations used in previous work.\nThe contributions of our work are as follows: i) We present a neural implementation of a dual stack machine underlying Forth, ii) we introduce Forth sketches for programming with partial procedural background knowledge, iii) we apply Forth sketches as a procedural prior on learning algorithms from data, iv) we introduce program code optimisations based on symbolic execution that can speed up neural execution, and v) using Forth sketches we obtain state-of-the-art for end-to-end reasoning about quantities expressed in natural language narratives."}, {"heading": "2. The Forth Abstract Machine", "text": "Forth is a simple Turing-complete stack-based programming language (ANSI, 1994; Brodie, 1980). We chose Forth as the host language of our work because i) it is an established, general-purpose high-level language relatively close to machine code, ii) it promotes highly modular programs through use of branching, loops and function calls, thus bringing out a good balance between assembly and higher level languages, and importantly iii) its abstract machine is simple enough for a straightforward creation of its continuous approximation. Forth\u2019s underlying abstract machine is represented by a state S = (D,R,H,c), which contains two stacks: a data evaluation pushdown stack D (data stack) holds values for manipulation, and a return address pushdown stackR (return stack) assists with return pointers and subroutine calls. These are accompanied by a heap or random memory access bufferH , and a program counter c.\nA Forth program P is a sequence1 of Forth words (i.e. commands)P =w1...wn. The role of a word varies, encompassing language keywords, primitives, and user-defined subroutines (e.g. DROP discards the top element of the data stack, or DUP duplicates the top element of the data stack).2 Each wordwi defines a transition function between machine states wi : S \u2192 S. Therefore, a program P itself defines a transition function by simply applying the word at the current program counter to the current state. Although usually considered as a part of the heap H , we consider Forth programs P separately to ease the analysis.\nAn example of a Bubble sort algorithm implemented in Forth is shown in Listing 1 in everything except lines 3b-4c. The execution starts from line 12 where literals are pushed on the data stack and the SORT is called. Line 10 executes the main loop over the sequence. Lines 2-7\n1Forth is a concatenative language. 2In this work, we restrict ourselves to a subset of all Forth\nwords, detailed in Appendix A.\n1 : BUBBLE ( a1 ... an n-1 -- one pass ) 2 DUP IF >R\n3a OVER OVER < IF SWAP THEN 4a R> SWAP >R 1- BUBBLE R> 3b { observe D0 D-1 -> permute D-1 D0 R0} 4b 1- BUBBLE R> 3c { observe D0 D-1 -> choose NOP SWAP } 4c R> SWAP >R 1- BUBBLE R> 5 ELSE 6 DROP 7 THEN 8 ; 9 : SORT ( a1 .. an n -- sorted ) 10 1- DUP 0 DO >R R@ BUBBLE R> LOOP DROP 11 ; 12 2 4 2 7 4 SORT \\ Example call\nListing 1: Three code alternatives (white lines are common to all, coloured/lettered lines are alternative-specific): i) Bubble sort in Forth (a lines \u2013 green), ii) PERMUTE sketch (b lines \u2013 blue), and iii) COMPARE sketch (c lines \u2013 yellow).\ndenote the BUBBLE procedure \u2013 comparison of top two stack numbers (line 3a), and the recursive call to itself (line 4a). A detailed description of how this program is executed by the Forth abstract machine is provided in Appendix B. Notice that while Forth provides common control structures such as looping and branching, these can always be reduced to low-level code that uses jumps and conditional jumps (using the words BRANCH and BRANCH0, respectively). Likewise, we can think of sub-routine definitions as labelled code blocks, and their invocation amounts to jumping to the code block with the respective label."}, {"heading": "3. \u22024: Differentiable Abstract Machine", "text": "When a programmer writes a Forth program, they define a sequence of Forth words, i.e., a sequence of known state transition functions. In other words, the programmer knows exactly how computation should proceed. To accommodate for cases when the developer\u2019s procedural background knowledge is incomplete, we extend Forth to support the definition of a program sketch. As is the case with Forth programs, sketches are sequences of transition functions. However, a sketch may contain transition functions whose behaviour is learned from data.\nTo learn the behaviour of transition functions within a program we would like the machine output to be differentiable with respect to these functions (and possibly representations of inputs to the program). This enables us to choose parameterised transition functions such as neural networks.\nTo this end, we introduce \u22024, a TensorFlow (Abadi et al., 2015) implementation of a differentiable abstract machine with continuous state representations, differentiable words and sketches. Program execution in \u22024 is modelled by a recurrent neural network (RNN), parameterised by the transition functions at each time step."}, {"heading": "3.1. Machine State Encoding", "text": "We map the symbolic machine state S = (D, R, H, c) to a continuous representation S = (D, R, H, c) into two differentiable stacks (with pointers), the data stack D= (D,d) and the return stackR= (R,r), a heap H, and an attention vector c indicating which word of the sketchP\u03b8 is being executed at the current time step. Figure 1 depicts the machine together with its elements. All three memory structures, the data stack, the return stack and the heap, are based on differentiable flat memory buffersM\u2208{D,R,H}, where D,R,H\u2208Rl\u00d7v , for a stack size l and a value size v. Each has a differentiable read operation\nreadM(a)=aTM\nand write operation\nwriteM(x,a) :M\u2190M\u2212(a1T ) M+xaT\nakin to the Neural Turing Machine (NTM) memory (Graves et al., 2014), where is the element-wise multiplication, and a is the address pointer.3 In addition to the memory buffers D and R, the data stack and the return stack contain pointers to the current top-of-the-stack (TOS) element d,r\u2208Rl, respectively. This allows us to implement pushing as writing a value x into M and incrementing the TOS pointer as:\npushM(x) :writeM(x,p) (side-effect: p\u2190 inc(p))\nwhere p\u2208{d,r}, inc(p)=pTR1+, dec(p)=pTR\u2212, and R1+ and R1\u2212 are increment and decrement matrices (left and right circular shift matrices).\n3The equal widths ofH andD allow us to directly move vector representations of values between the heap and the stack.\nPopping is realized by multiplying the TOS pointer and the memory buffer, and decreasing the TOS pointer:\npopM( )= readM(p) (side-effect: p\u2190dec(p))\nFinally, the program counter c \u2208 Rp is a vector that, when one-hot, points to a single word in a program of length p, and is equivalent to the c vector of the symbolic state machine.4 We use S to denote the space of all continuous representations S.\nNeural Forth Words It is straightforward to convert Forth words, defined as functions on discrete machine states, to functions operating on the continuous space S. For example, consider the word DUP, which duplicates the top of the data stack. A differentiable version of DUP first calculates the value e on the TOS address ofD, as e=dTD. It then shifts the stack pointer via d\u2190 inc(d), and writes e to D using writeD(e,d). The complete description of implemented Forth Words and their differentiable counterparts can be found in Appendix A."}, {"heading": "3.2. Forth Sketches", "text": "We define a Forth sketch P\u03b8 as a sequence of continuous transition functions P = w1 ...wn. Here, wi \u2208 S \u2192 S either corresponds to a neural Forth word or a trainable transition function (neural networks in our case). We will call these trainable functions slots, as they correspond to underspecified \u201cslots\u201d in the program code that need to be filled by learned behaviour.\nWe allow users to define a slot w by specifying a pair of a state encoder wenc and a decoder wdec. The encoder\n4During training c can become distributed and is considered as attention over the program code.\nproduces a latent representation h of the current machine state using a multi-layer perceptron, and the decoder consumes this representation to produce the next machine state. We hence have w=wdec \u25e6wenc. To use slots within Forth program code, we introduce a notation that reflects this decomposition. In particular, slots are defined by the syntax { encoder -> decoder } where encoder and decoder are specifications of the corresponding slot parts as described below.\nEncoders We provide the following options for encoders:\nstatic produces a static representation, independent of the actual machine state. observe e1 ...em: concatenates the elements e1 ...em of the machine state. An element can be a stack item Di at relative index i, a return stack item Ri, etc. linear N, sigmoid, tanh represent chained transformations, which enable the multilayer perceptron architecture. Linear N projects to N dimensions, and sigmoid and tanh apply same-named functions elementwise.\nDecoders Users can specify the following decoders:\nchoose w1...wm: chooses from the Forth wordsw1...wm. Takes an input vector h of length m to produce a weighted combination of machine states \u2211m i hiwi(S). manipulate e1 ...em: directly manipulates the machine state elements e1 ... em by writing the appropriately reshaped and softmaxed output of the encoder over the machine state elements with writeM. permute e1 ...em: permutes the machine state elements e1...em via a linear combination ofm! state vectors."}, {"heading": "3.3. The Execution RNN", "text": "We model execution using an RNN which produces a state Sn+1 conditioned on a previous state Sn. It does so by first passing the current state to each function wi in the program, and then weighing each of the produced next states by the component of the program counter vector ci that corresponds to program index i, effectively using c as an attention vector over code. Formally we have:\nSn+1=RNN(Sn,P\u03b8)= |P |\u2211 i=1 ciwi(Sn)\nClearly, this recursion, and its final state, are differentiable with respect to the program codeP\u03b8, and its inputs. Furthermore, for differentiable Forth programs the final state of this RNN will correspond to the final state of a symbolic execution (when no slots are present, and one-hot values are used)."}, {"heading": "3.4. Program Code Optimisations", "text": "The \u22024 RNN requires one-time step per transition. After each time step, the program counter is either incremented, decremented, explicitly set or popped from the stack. In turn, a new machine state is calculated by executing all words in the program and then weighting the result states by the program counter. As this is expensive, it is advisable to avoid full RNN steps wherever possible. We use two strategies to avoid full RNN steps and significantly speed-up \u22024: symbolic execution and interpolation of if-branches.\nSymbolic Execution Whenever we have a sequence of Forth words that contains no branch entry or exit points, we can collapse this sequence into a single transition instead of naively interpreting words one-by-one. We symbolically execute (King, 1976) a sequence of Forth words to calculate a new machine state. We then use the difference between the new and the initial state to derive the transition function of the sequence. For example, the sequenceR> SWAP >R that swaps top elements of the data and the return stack yields the symbolic state D= r1d2 ...dl. and R= d1r2 ...rl. Comparing it to the initial state, we derive a single neural transition that only needs to swap the top elements of D and R.\nInterpolation of If-Branches We cannot apply symbolic execution to code with branching points as the branching behaviour depends on the current machine state, and we cannot resolve it symbolically. However, we can still collapse if-branches that involve no function calls or loops by executing both branches in parallel and weighing their output states by the value of the condition. If the if-branch does contain function calls or loops, we simply fall back to execution of all words weighted by the program counter."}, {"heading": "3.5. Training", "text": "Our training procedure assumes input-output pairs of machine start and end states (xi,yi) only. The output yi defines a target memory YDi and a target pointer y d i on the data stack D. Additionally, we have a mask Ki that indicates which components of the stack should be included in the loss (e.g. we do not care about values above the stack depth). We use DT (\u03b8,xi) and dT (\u03b8,xi) to denote the final state of D and d after T steps of execution RNN and using an initial state xi. We define the loss function as\nL(\u03b8)=H(Ki DT (\u03b8,xi),Ki YDi ) +H(Ki dT (\u03b8,xi),Ki ydi )\nwhere H(x, y) = \u2212x log y is the cross-entropy loss, and \u03b8 are parameters of slots in the program P . We can use backpropagation and any variant of gradient descent to optimise this loss function. Note that at this point it would be possible to include supervision of the intermediate states (trace-level), as done by the Neural Program Interpreter (Reed & de Freitas, 2015)."}, {"heading": "4. Experiments", "text": "We evaluate \u22024 on three tasks. Two of these are simple transduction tasks, sorting and addition as presented in (Reed & de Freitas, 2015), with varying levels of program structure. For each problem, we introduce two sketches.\nWe also test \u22024 on the more difficult task of answering word algebra problems. We show that not only can \u22024 act as a standalone solver for such problems, bypassing the intermediary task of producing formula templates which must then be executed, but it can also outperform previous work when trained on the same data."}, {"heading": "4.1. Experimental Setup", "text": "Specific to the transduction tasks, we discretise memory elements during testing. This effectively allows the trained model to generalise to any sequence length if the correct sketch behaviour has been learned. We also compare against a Seq2Seq (Sutskever et al., 2014) baseline. Full details of the experimental setup can be found in Appendix E."}, {"heading": "4.2. Sorting", "text": "Sorting sequences of digits is a hard task for RNNs, as they fail to generalise to sequences even marginally longer than the ones they have been trained on (Reed & de Freitas, 2015). We investigate several strong priors based on Bubble sort for this transduction task and present two \u22024 sketches in Listing 1 that enable us to learn sorting from only a few hundred training examples (see Appendix C.1 for more detail):\nIn both sketches, the outer loop can be specified in \u22024 (Listing 1, line 10), which repeatedly calls a functionBUBBLE. In doing so, it defines sufficient structure so that the behaviour of the network is invariant to the input sequence length.\nResults on Bubble sort A quantitative comparison of our models on the Bubble sort task is provided in Table 1. For a given test sequence length, we vary the training set lengths to illustrate the model\u2019s ability to generalise to sequences longer than those it observed during training. We find that \u22024 quickly learns the correct sketch behaviour, and it is able to generalise perfectly to sort sequences of 64 elements after observing only sequences of length two and three during training. In comparison, the Seq2Seq baseline falters when attempting similar generalisations, and performs close to chance when tested on longer sequences. Both \u22024 sketches perform flawlessly when trained on short sequence lengths, but under-perform when trained on sequences of length 4 due to arising computational difficulties (COMPARE sketch performs better due to more structure it imposes). We discuss this issue further in Section 5."}, {"heading": "4.3. Addition", "text": "Next, we applied \u22024 to the problem of learning to add two n-digit numbers. We rely on the standard elementary school addition algorithm, where the goal is to iterate over pairs of aligned digits, calculating the sum of each to yield the resulting sum. The key complication arises when two digits sum to a two-digit number, requiring that the correct extra digit (a carry) be carried over to the subsequent column.\n1 : ADD-DIGITS ( a1 b1...an bn carry n -- r1 r2...r_{n+1} ) 2 DUP 0 = IF 3 DROP 4 ELSE 5 >R \\ put n on R\n6a { observe D0 D-1 D-2 -> tanh -> linear 70 -> manipulate D-1 D-2 } 7a DROP 6b { observe D0 D-1 D-2 -> tanh -> linear 10 -> choose 0 1 } 7b { observe D-1 D-2 D-3 -> tanh -> linear 50\n-> choose 0 1 2 3 4 5 6 7 8 9 } >R SWAP DROP SWAP DROP SWAP DROP R>\n8 R> 1- SWAP >R \\ new_carry n-1 9 ADD-DIGITS \\ call add-digits on n-1 subseq.\n10 R> \\ put remembered results back on the stack 11 THEN 12 ;\nListing 2: Manipulate sketch (a lines \u2013 green) and the choose sketch (b lines \u2013 blue) for Elementary Addition. Input data is used to fill data stack externally\nWe assume aligned pairs of digits as input, with a carry for the least significant digit (potentially 0), and the length of the respective numbers. The sketches define the high-level operations through recursion, leaving the core addition to be learned from data.\nThe specified high-level behaviour includes the recursive call template and the halting condition of the recursion (no remaining digits, line 2-3). The underspecified addition operation must take three digits from the previous call, the two digits to sum and a previous carry, and produce a single digit (the sum) and the resultant carry (lines 6a, 6b and 7a, 7b). We introduce two sketches for inducing this behaviour:\nMANIPULATE. This sketch provides little prior procedural knowledge as it directly manipulates the \u22024 machine state, filling in a carry and the result digits, based on the top three elements on the data stack (two digits and the carry). Depicted in Listing 2 in green.\nCHOOSE. Incorporating additional prior information, CHOOSE exactly specifies the results of the computation, namely the output of the first slot (line 6b) is the carry, and the output of the second one (line 7b) is the result digit, both conditioned on the two digits and the carry on the data stack. Depicted in Listing 2 in blue.\nThe rest of the sketch code reduces the problem size by one and returns the solution by popping it from the return stack.\nQuantitative Evaluation on Addition In a set of experiments analogous to those in our evaluation on Bubble sort, we demonstrate the performance of \u22024 on the addition task by examining test set sequence lengths of 8 and 64 while varying the lengths of the training set instances (Table 2). The Seq2Seq model again fails to generalise\nto longer sequences than those observed during training. In comparison, both the CHOOSE sketch and the less structured MANIPULATE sketch learn the correct sketch behaviour and generalise to all test sequence lengths (with an exception of MANIPULATE which required more data to train perfectly). In additional experiments, we were able to successfully train both the CHOOSE and the MANIPULATE sketches from sequences of input length 24, and we tested them up to the sequence length of 128, confirming their perfect training and generalisation capabilities."}, {"heading": "4.4. Word Algebra Problems", "text": "Word algebra problems (WAPs) are often used to assess the numerical reasoning abilities of schoolchildren. Questions are short narratives which focus on numerical quantities, culminating with a question. For example:"}, {"heading": "A florist had 50 roses. If she sold 15 of them and then later picked 21 more, how many roses would she have?", "text": "Answering such questions requires both the understanding of language and of algebra \u2014 one must know which numeric operations correspond to which phrase and how to execute these operations.\nPrevious work cast WAPs as a transduction task by mapping a question to a template of a mathematical formula, thus requiring manuall labelled formulas. For instance, one formula that can be used to correctly answer the question in the example above is (50 - 15) + 21 = 56. In previous work, local classifiers (Roy & Roth, 2015; Roy et al., 2015), hand-crafted grammars (Koncel-Kedziorski et al., 2015), and recurrent neural models (Bouchard et al., 2016) have been used to perform this task. Predicted formula templates may be marginalised during training (Kushman et al., 2014), or evaluated directly to produce an answer.\nIn contrast to these approaches, \u22024 is able to learn both, a soft mapping from text to algebraic operations and their execution, without the need for manually labelled equations and no explicit symbolic representation of a formula.\nModel description Our model is a fully end-to-end differentiable structure, consisting of a \u22024 interpreter, a\n\\ first copy data from H: vectors to R and numbers to D 1 { observe R0 R-1 R-2 R-3 -> permute D0 D-1 D-2 } 2 { observe R0 R-1 R-2 R-3 -> choose + - * / } 3 { observe R0 R-1 R-2 R-3 -> choose SWAP NOP } 4 { observe R0 R-1 R-2 R-3 -> choose + - * / } \\ lastly, empty out the return stack\nListing 3: Core of the Word Algebra Problem sketch. The full sketch can be found in the Appendix.\nsketch, and a Bidirectional LSTM (BiLSTM) reader.\nThe BiLSTM reader reads the text of the problem and produces a vector representation (word vectors) for each word, concatenated from the forward and the backward pass of the BiLSTM network. We use the resulting word vectors corresponding only to numbers in the text, numerical values of those numbers (encoded as one-hot vectors), and a vector representation of the whole problem (concatenation of the last and the first vector of the opposite passes) to initialise the \u22024 heap H. This is done in an end-to-end fashion, enabling gradient propagation through the BiLSTM to the vector representations. The process is depicted in Figure 1.\nThe sketch, depicted in Listing 3 dictates the differentiable computation.5 First, it copies values from the heap H \u2013 word vectors to the return stack R, and numbers (as one-hot vectors) on the data stack D. Second, it contains four slots that define the space of all possible operations of four operators on three operands, all conditioned on the vector representations on the return stack. These slots are i) permutation of the elements on the data stack, ii) choosing the first operator, iii) possibly swapping the intermediate result and the last operand, and iv) the choice of the second operator. The final set of commands simply empties out the return stack R. These slots define the space of possible operations, however, the model needs to learn how to utilise these operations in order to calculate the correct result.\nResults We evaluate the model on the Common Core (CC) dataset, introduced by Roy & Roth (2015). CC is notable for having the most diverse set of equation patterns, consisting of four operators (+, -,\u00d7,\u00f7), with up to three operands.\nWe compare against three baseline systems: (1) a local classifier with hand-crafted features (Roy & Roth, 2015), (2) a Seq2Seq baseline, and (3) the same model with a data generation component (GeNeRe) Bouchard et al. (2016). All baselines are trained to predict the best equation, which is executed outside of the model to obtain the answer. In contrast, \u22024 is trained end-to-end from input-output pairs and predicts the answer directly without the need for an intermediate symbolic representation of a formula.\nResults are shown in Table 3. All RNN-based methods 5Due to space constraints, we present the core of the sketch here. For the full sketch, please refer to Listing 4 in the Appendix.\n(bottom three) outperform the classifier-based approach. Our method slightly outperforms a Seq2Seq baseline, achieving the highest reported result on this dataset without data augmentation."}, {"heading": "5. Discussion", "text": "\u22024 bridges the gap between a traditional programming language and a modern machine learning architecture. However, as we have seen in our evaluation experiments, faithfully simulating the underlying abstract machine architecture introduces its own unique set of challenges.\nOne such challenge is the additional complexity of performing even simple tasks when they are viewed in terms of operations on the underlying machine state. As illustrated in Table 1, \u22024 sketches can be effectively trained from small training sets (see Appendix C.1), and generalise perfectly to sequences of any length. However, difficulty arises when training from sequences of modest lengths. Even when dealing with relatively short training length sequences, and with the program code optimisations employed, the underlying machine can unroll into a problematically large number states. For problems whose machine execution is quadratic, like the sorting task (which at input sequences of length 4 has 120 machine states), we observe significant instabilities during training from backpropagating through such long RNN sequences, and consequent failures to train. In comparison, the addition problem was easier to train due to a comparatively shorter underlying execution RNNs.\nThe higher degree of prior knowledge provided played an important role in successful learning. For example, the COMPARE sketch, which provides more structure, achieves higher accuracies when trained on longer sequences. Similarly, employing softmax on the directly manipulated memory elements enabled perfect training for the MANIPULATE sketch for addition. Furthermore, it is encouraging to see that \u22024 can be trained jointly with an upstream LSTM to provide strong procedural prior knowledge for solving a real-world NLP task."}, {"heading": "6. Related Work", "text": "Program Synthesis The idea of program synthesis is as old as Artificial Intelligence, and has a long history in computer science (Manna & Waldinger, 1971). Whereas a large body of work has focused on using genetic programming (Koza, 1992) to induce programs from the given inputoutput specification (Nordin, 1997), there are also various Inductive Programming approaches (Kitzelmann, 2009) aimed at inducing programs from incomplete specifications of the code to be implemented (Albarghouthi et al., 2013; Solar-Lezama et al., 2006). We tackle the same problem of sketching, but in our case, we fill the sketches with neural networks able to learn the slot behaviour.\nProbabilistic and Bayesian Programming Our work is closely related to probabilistic programming languages such as Church (Goodman et al., 2008). They allow users to inject random choice primitives into programs as a way to define generative distributions over possible execution traces. In a sense, the random choice primitives in such languages correspond to the slots in our sketches. A core difference lies in the way we train the behaviour of slots: instead of calculating their posteriors using probabilistic inference, we estimate their parameters using backpropagation and gradient descent. This is similar in-kind to TerpreT\u2019s FMGD algorithm (Gaunt et al., 2016), which is employed for code induction via backpropagation. In comparison, our model which optimises slots of neural networks surrounded by continuous approximations of code, enables the combination of procedural behaviour and neural networks. In addition, the underlying programming and probabilistic paradigm in these programming languages is often functional and declarative, whereas our approach focuses on a procedural and discriminative view. By using an end-to-end differentiable architecture, it is easy to seamlessly connect our sketches to further neural input and output modules, such as an LSTM that feeds into the machine heap.\nNeural approaches Recently, there has been a surge of research in program synthesis, and execution in deep learning, with increasingly elaborate deep models. Many of these models were based on differentiable versions of abstract data structures (Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2016), and a few abstract machines, such as the NTM (Graves et al., 2014), Differentiable Neural Computers (Graves et al., 2016), and Neural GPUs (Kaiser & Sutskever, 2015). All these models are able to induce algorithmic behaviour from training data. Our work differs in that our differentiable abstract machine allows us to seemingly integrate code and neural networks, and train the neural networks specified by slots via backpropagation. Related to our efforts is also the Autograd (Maclaurin et al., 2015), which enables automatic gradient computation in\npure Python code, but does not define nor use differentiable access to its underlying abstract machine.\nThe work in neural approximations to abstract structures and machines naturally leads to more elaborate machinery able to induce and call code or code-like behaviour. Neelakantan et al. (2015a) learned simple SQL-like behaviour\u2013\u2014querying tables from the natural language with simple arithmetic operations. Although sharing similarities on a high level, the primary goal of our model is not induction of (fully expressive) code but its injection. (Andreas et al., 2016) learn to compose neural modules to produce the desired behaviour for a visual QA task. Neural Programmer-Interpreters (Reed & de Freitas, 2015) learn to represent and execute programs, operating on different modes of an environment, and are able to incorporate decisions better captured in a neural network than in many lines of code (e.g. using an image as an input). Users inject prior procedural knowledge by training on program traces and hence require full procedural knowledge. In contrast, we enable users to use their partial knowledge in sketches.\nNeural approaches to language compilation have also been researched, from compiling a language into neural networks (Siegelmann, 1994), over building neural compilers (Gruau et al., 1995) to adaptive compilation (Bunel et al., 2016). However, that line of research did not perceive neural interpreters and compilers as a means of injecting procedural knowledge as we did. To the best of our knowledge, \u22024 is the first working neural implementation of an abstract machine for an actual programming language, and this enables us to inject such priors in a straightforward manner."}, {"heading": "7. Conclusion and Future Work", "text": "We have presented \u22024, a differentiable abstract machine for the Forth programming language, and showed how it can be used to complement programmers\u2019 prior knowledge through the learning of unspecified behaviour in Forth sketches. The \u22024 RNN successfully learns to sort and add, and solve word algebra problems, using only program sketches and program input-output pairs. We believe \u22024, and the larger paradigm it helps establish, will be useful for addressing complex problems where low-level representations of the input are necessary, but higher-level reasoning is difficult to learn and potentially easier to specify.\nIn future work, we plan to apply \u22024 to other problems in the NLP domain, like machine reading and knowledge base inference. In the long-term, we see the integration of non-differentiable transitions (such as those arising when interacting with a real environment), as an exciting future direction which sits at the intersection of reinforcement learning and probabilistic programming."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Guillaume Bouchard, Danny Tarlow, Dirk Weissenborn, Johannes Welbl and the anonymous reviewers for fruitful discussions and helpful comments on previous drafts of this paper. This work was supported by a Microsoft Research PhD Scholarship, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award."}, {"heading": "A. Forth Words and their implementation", "text": "We implemented a small subset of available Forth words in \u22024. The table of these words, together with their descriptions is given in Table 4, and their implementation is given in Table 5. The commands are roughly divided into 7 groups. These groups, line-separated in the table, are:\nData stack operations {num}, 1+, 1-, DUP, SWAP, OVER, DROP, +, -, *, / Heap operations @, ! Comparators >, <, = Return stack operations >R, R>, @R Control statements IF..ELSE..THEN, BEGIN..WHILE..REPEAT, DO..LOOP Subroutine control :, {sub}, ;, MACRO Variable creation VARIABLE, CREATE..ALLOT"}, {"heading": "B. Bubble sort algorithm description", "text": "An example of a Forth program that implements the Bubble sort algorithm is shown in Listing 1. We provide a description of how the first iteration of this algorithm is executed by the Forth abstract machine:\nThe program begins at line 11, putting the sequence [2 4 2 7] on the data stackD, followed by the sequence length 4.6 It then calls the SORTword.\nD R c comment 1 [] [] 11 execution start 2 [2 4 2 7 4] [] 8 pushing sequence to D, calling\nSORT subroutine puts ASORT toR\nFor a sequence of length 4, SORT performs a do-loop in line 9 that calls the BUBBLE sub-routine 3 times. It does so by decrementing the top ofD with the 1-word to 3. Subsequently, 3 is duplicated onD by using DUP, and 0 is pushed ontoD.\n3 [2 4 2 7 3] [ASORT] 9 1- 4 [2 4 2 7 3 3] [ASORT] 9 DUP 6 [2 4 2 7 3 3 0] [ASORT] 9 0\nDO consumes the top two stack elements 3 and 0 as the limit and starting point of the loop, leaving the stack D to be [2,4,2,7,3]. We use the return stackR as a temporary variable buffer and push 3 onto it using the word >R. This drops 3 from D, which we copy fromRwith R@\n7 [2 4 2 7 3] [AddrSORT] 9 DO 8 [2 4 2 7] [AddrSORT 3] 9 >R 9 [2 4 2 7 3] [AddrSORT 3] 9 @R\nNext, we call BUBBLE to perform one iteration of the bubble pass, (calling BUBBLE 3 times internally), and consuming 3. Notice that this call puts the current program counter ontoR, to be used for the program counter cwhen exiting BUBBLE.\nInside the BUBBLE subroutine, DUP duplicates 3 onR. IF consumes the duplicated 3 and interprets is as TRUE. >R puts 3 onR.\n10 [2 4 2 7 3] [ASORT 3 ABUBBLE] 0 calling BUBBLE subroutine puts ABUBBLE toR 11 [2 4 2 7 3 3] [ASORT 3 ABUBBLE] 1 DUP 12 [2 4 2 7 3] [ASORT 3 ABUBBLE] 1 IF 13 [2 4 2 7] [ASORT 3 ABUBBLE 3] 1 >R\nCalling OVER twice duplicates the top two elements of the stack, to test them with <, which tests whether 2< 7. IF tests if the result is TRUE (0), which it is, so it executes SWAP.\n14 [2 4 2 7 2 7] [ASORT 3 ABUBBLE 3] 2 OVER OVER 15 [2 4 2 7 1] [ASORT 3 ABUBBLE 3] 2 < 16 [2 4 2 7] [ASORT 3 ABUBBLE 3] 2 IF 17 [2 4 7 2] [ASORT 3 ABUBBLE 3] 2 SWAP\nTo prepare for the next call to BUBBLEwe move 3 back from the return stackR to the data stackD via R>, SWAP it with the next element, put it back toRwith >R, decrease the TOS with 1- and invoke BUBBLE again. Notice thatRwill accumulate the analysed part of the sequence, which will be recursively taken back.\n18 [2 4 7 2 3] [ASORT 3 ABUBBLE] 3 R> 19 [2 4 7 3 2] [ASORT 3 ABUBBLE] 3 SWAP 20 [2 4 7 3] [ASORT 3 ABUBBLE 2] 3 >R 21 [2 4 7 2] [ASORT 3 ABUBBLE 2] 3 1- 22 [2 4 7 2] [ASORT 3 ABUBBLE 2] 0 ...BUBBLE\nWhen we reach the loop limit we drop the length of the sequence and exit SORT using the ; word, which takes the return address fromR. At the final point, the stack should contain the ordered sequence [7 4 2 2].\n6Note that Forth uses Reverse Polish Notation and that the top of the data stack is 4 in this example."}, {"heading": "C. Learning and Run Time Efficiency", "text": "C.1. Accuracy per training examples\nSorter When measuring the performance of the model as the number of training instances varies, we can observe the benefit of additional prior knowledge to the optimisation process. We find that when stronger prior knowledge is provided (COMPARE), the model quickly maximises the training accuracy. Providing less structure (PERMUTE) results in lower testing accuracy initially, however, both sketches learn the correct behaviour and generalise equally well after seeing 256 training instances. Additionally, it is worth noting that the PERMUTE sketch was not always able to converge into a result of the correct length, and both sketches are not trivial to train.\nIn comparison, Seq2Seq baseline is able to generalise only to the sequence it was trained on (Seq2Seq trained and tested on sequence length 3). When training it on sequence length 3, and testing it on a much longer sequence length of 8, Seq2Seq baseline is not able to achieve more than 45% accuracy.\nAdder We tested the models to train on datasets of increasing size on the addition task. The results, depicted in Table 4 show that both the choose and the manipulate sketch are able to perfectly generalise from 256 examples, trained on sequence lengths of 8, tested on 16. In comparison, the Seq2Seq baseline achieves 98% when trained on 16384 examples, but only when tested on the input of the same length, 8. If we test Seq2Seq as we tested the sketches, it is unable to achieve more 19.7%.\nC.2. Program Code Optimisations\nWe measure the runtime of Bubble sort on sequences of varying length with and without the optimisations described in Section 3.4. The results of ten repeated runs are shown in Figure 5 and demonstrate large relative improvements for symbolic execution and interpolation of if-branches compared to non-optimised \u22024 code."}, {"heading": "D. \u22024 execution of a Bubble sort sketch", "text": "Listing 1 (lines 3b and 4b \u2013 in blue) defines the BUBBLE word as a sketch capturing several types of prior knowledge. In this section, we describe the PERMUTE sketch. In it, we assume BUBBLE involves a recursive call, that terminates at length 1, and that the next BUBBLE call takes as input some function of the current length and the top two stack elements.\nThe input to this sketch are the sequence to be sorted and its length decremented by one, n\u22121 (line 1). These inputs\nare expected on the data stack. After the length (n \u2212 1) is duplicated for further use with DUP, the machine tests whether it is non-zero (using IF, which consumes the TOS during the check). If n\u22121>0, it is stored on theR stack for future use (line 2).\nAt this point (line 3b) the programmer only knows that a decision must be made based on the top two data stack elements D0 and D-1 (comparison elements), and the top return stack,R0 (length decremented by 1). Here the precise nature of this decision is unknown but is limited to variants of permutation of these elements, the output of which produce the input state to the decrement -1 and the recursive BUBBLE call (line 4b). At the culmination of the call, R0, the output of the learned slot behaviour, is moved onto the data stack usingR>, and execution proceeds to the next step.\nFigure 2 illustrates how portions of this sketch are executed on the \u22024 RNN. The program counter initially resides at >R (line 3 in P), as indicated by the vector c, next to program P. Both data and return stacks are partially filled (R has 1 element, D has 4), and we show the content both through horizontal one-hot vectors and their corresponding integer values (colour coded). The vectors d and r point to the top of both stacks, and are in a one-hot state as well. In this execution trace, the slot at line 4 is already showing optimal behaviour: it remembers the element on the return stack (4) is larger and executes BUBBLE on the remaining sequence with the counter n subtracted by one, to 1."}, {"heading": "E. Experimental details", "text": "The parameters of each sketch are trained using Adam (Kingma & Ba, 2015), with gradient clipping (set to 1.0) and gradient noise (Neelakantan et al., 2015b). We tuned the learning rate, batch size, and the parameters of the gradient noise in a random search on a development variant of each task.\nE.1. Seq2Seq baseline\nThe Seq2Seq baseline models are single-layer networks with LSTM cells of 50 dimensions.\nThe training procedure for these models consists of 500 epochs of Adam optimisation, with a batch size of 128, a learning rate of 0.01, and gradient clipping when the L2 norm of the model parameters exceeded 5.0. We vary the size of training and test data (Fig. 3), but observe no indication of the models failing to reach convergence under these training conditions.\nE.2. Sorting\nThe Permute and Compare sketches in Table 1 were trained on a randomly generated train, development and test set\ncontaining 256, 32 and 32 instances, respectively. Note that the low number of dev and test instances was due to the computational complexity of the sketch.\nThe batch size was set to a value between 64 and 16, depending on the problem size, and we used an initial learning rate of 1.0.\nE.3. Addition\nWe trained the addition Choose and Manipulate sketches presented in Table 2 on a randomly generated train, development and test sets of sizes 512, 256, and 1024 respectively. The batch size was set to 16, and we used an initial learning rate of 0.05\nE.4. Word Algebra Problem\nThe Common Core (CC) dataset (Roy & Roth, 2015) is partitioned into a train, dev, and test set containing 300, 100, and 200 questions, respectively. The batch size was set to 50, and we used an initial learning rate of0.02. The BiLSTM word vectors were initialised randomly to vectors of length 75. The stack width was set to 150 and the stack size to 5."}, {"heading": "F. Qualitative Analysis on BubbleSort of PC traces", "text": "In Figure 6 we visualise the program counter traces. The trace follows a single example from start, to middle, and the end of the training process. In the beginning of training, the program counter starts to deviate from the one-hot representation in the first 20 steps (not observed in the figure due to unobservable changes), and after two iterations ofSORT, \u22024 fails to correctly determine the next word. After a few training epochs \u22024 learns better permutations which enable the algorithm to take crisp decisions and halt in the correct state."}, {"heading": "G. The complete Word Algebra Problem sketch", "text": "The Word Algebra Problem (WAP) sketch described in Listing 3 is the core of the model that we use for WAP problems. However, there were additional words before and after the core which took care of copying the data from the heap to data and return stacks, and finally emptying out the return stack.\nThe full WAP sketch is given in Listing 4. We define a QUESTION variable which will denote the address of the question vector on the heap. Lines 4 and 5 create REPR BUFFER and NUM BUFFER variables and denote that they will occupy four sequential memory slots on the heap, where we will store the representation vectors and numbers, respectively. Lines 7 and 8 create variables REPR and NUM which will denote addresses to current representations and numbers on the heap. Lines 10 and 11 store REPR BUFFER to REPR and NUM BUFFER to NUM, essentially setting the values of variables REPR and NUM to starting addresses allotted in lines 4 and 5. Lines 14-16 and 19-20 create macro functions STEP NUM and STEP REPR which increment the NUM and REPR values on call. These macro functions will be used to iterate through the heap space. Lines 24-25 define macro functions CURRENT NUM for fetching the current number, and CURRENT REPR for fetching representation values. Lines 28-32 essentially copy values of numbers from the heap to the data stack by using the CURRENT NUM and STEP NUM macros. After that line 35 pushes the question vector, and lines 36-40 push the word representations of numbers on the return stack.\nFollowing that, we define the core operations of the sketch. Line 43 permutes the elements on the data stack (numbers) as a function of the elements on the return stack (vector representations of the question and numbers). Line 45 chooses an operator to execute over the TOS and NOS elements of the data stack (again, conditioned on elements on the return stack). Line 47 executes a possible swap of the two elements on the data stack (the intermediate result and the last operand) conditioned on the return stack. Finally, line 49 chooses the last operator to execute on the data stack, conditioned on the return stack.\nThe sketch ends with lines 52-55 which empty out the return stack.\n1 \\ address of the question on H 2 VARIABLE QUESTION 3 \\ allotting H for representations and numbers 4 CREATE REPR_BUFFER 4 ALLOT 5 CREATE NUM_BUFFER 4 ALLOT 6 \\ addresses of the first representation and number 7 VARIABLE REPR 8 VARIABLE NUM\n10 REPR_BUFFER REPR ! 11 NUM_BUFFER NUM !\n13 \\ macro function for incrementing the pointer to numbers in H 14 MACRO: STEP_NUM 15 NUM @ 1+ NUM ! 16 ;\n18 \\ macro function for incrementing the pointer to representations in H 19 MACRO: STEP_REPR 20 REPR @ 1+ REPR ! 21 ;\n23 \\ macro functions for fetching current numbers and representations 24 MACRO: CURRENT_NUM NUM @ @ ; 25 MACRO: CURRENT_REPR REPR @ @ ;\n27 \\ copy numbers to D 28 CURRENT_NUM 29 STEP_NUM 30 CURRENT_NUM 31 STEP_NUM 32 CURRENT_NUM\n34 \\ copy question vector, and representations of numbers to R 35 QUESTION @ >R 36 CURRENT_REPR >R 37 STEP_REPR 38 CURRENT_REPR >R 39 STEP_REPR 40 CURRENT_REPR >R\n42 \\ permute stack elements, based on the question and number representations 43 { observe R0 R-1 R-2 R-3 -> permute D0 D-1 D-2 } 44 \\ choose the first operation 45 { observe R0 R-1 R-2 R-3 -> choose + - * / } 46 \\ choose whether to swap intermediate result and the bottom number 47 { observe R0 R-1 R-2 R-3 -> choose SWAP NOP } 48 \\ choose the second operation 49 { observe R0 R-1 R-2 R-3 -> choose + - * / }\n51 \\ empty out R 52 R> DROP 53 R> DROP 54 R> DROP 55 R> DROP\nListing 4: The complete Word Algebra Problem sketch"}], "year": 2017, "references": [{"title": "TensorFlow: Large-scale machine learning", "authors": ["Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "on heterogeneous systems,", "year": 2015}, {"title": "Recursive program synthesis", "authors": ["Albarghouthi", "Aws", "Gulwani", "Sumit", "Kincaid", "Zachary"], "venue": "In Computer Aided Verification,", "year": 2013}, {"title": "Neural module networks", "authors": ["Andreas", "Jacob", "Rohrbach", "Marcus", "Darrell", "Trevor", "Klein", "Dan"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition", "year": 2016}, {"title": "Learning to generate textual data", "authors": ["Bouchard", "Guillaume", "Stenetorp", "Pontus", "Riedel", "Sebastian"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "year": 2016}, {"title": "Adaptive neural compilation", "authors": ["Bunel", "Rudy", "Desmaison", "Alban", "Kohli", "Pushmeet", "Torr", "Philip HS", "Kumar", "M Pawan"], "venue": "In Proceedings of the Conference on Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "TerpreT: A Probabilistic Programming Language for Program Induction", "authors": ["Gaunt", "Alexander L", "Brockschmidt", "Marc", "Singh", "Rishabh", "Kushman", "Nate", "Kohli", "Pushmeet", "Taylor", "Jonathan", "Tarlow", "Daniel"], "venue": "arXiv preprint arXiv:1608.04428,", "year": 2016}, {"title": "Church: a language for generative models", "authors": ["Goodman", "Noah", "Mansinghka", "Vikash", "Roy", "Daniel M", "Bonawitz", "Keith", "Tenenbaum", "Joshua B"], "venue": "In Proceedings of the Conference in Uncertainty in Artificial Intelligence (UAI),", "year": 2008}, {"title": "Learning to Transduce with Unbounded Memory", "authors": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Proceedings of the Conference on Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "A Neural compiler", "authors": ["Gruau", "Fr\u00e9d\u00e9ric", "Ratajszczak", "Jean-Yves", "Wiber", "Gilles"], "venue": "Theoretical Computer Science,", "year": 1995}, {"title": "Long shortterm memory", "authors": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "year": 1997}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "authors": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "In Proceedings of the Conferences on Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Neural GPUs learn algorithms", "authors": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "year": 2015}, {"title": "Symbolic Execution and Program Testing", "authors": ["King", "James C"], "venue": "Commun. ACM,", "year": 1976}, {"title": "Adam: A Method for Stochastic Optimization", "authors": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In Proceedings of the International Conference for Learning Representations (ICLR),", "year": 2015}, {"title": "Inductive Programming: A Survey of Program Synthesis Techniques", "authors": ["Kitzelmann", "Emanuel"], "venue": "In International Workshop on Approaches and Applications of Inductive Programming,", "year": 2009}, {"title": "Parsing Algebraic Word Problems into Equations", "authors": ["Koncel-Kedziorski", "Rik", "Hajishirzi", "Hannaneh", "Sabharwal", "Ashish", "Etzioni", "Oren", "Ang", "Siena"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "year": 2015}, {"title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection, volume 1", "authors": ["Koza", "John R"], "venue": "MIT press,", "year": 1992}, {"title": "Neural Random-Access Machines", "authors": ["Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "year": 2016}, {"title": "Learning to Automatically Solve Algebra Word Problems", "authors": ["Kushman", "Nate", "Artzi", "Yoav", "Zettlemoyer", "Luke", "Barzilay", "Regina"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "year": 2014}, {"title": "Learning repetitive text-editing procedures with smartedit", "authors": ["Lau", "Tessa", "Wolfman", "Steven A", "Domingos", "Pedro", "Weld", "Daniel S"], "venue": "In Your Wish is My Command,", "year": 2001}, {"title": "Gradient-based Hyperparameter Optimization through Reversible Learning", "authors": ["Maclaurin", "Dougal", "Duvenaud", "David", "Adams", "Ryan P"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Toward automatic program synthesis", "authors": ["Manna", "Zohar", "Waldinger", "Richard J"], "venue": "Communications of the ACM,", "year": 1971}, {"title": "Neural Programmer: Inducing latent programs with gradient descent", "authors": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "year": 2015}, {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "authors": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Kurach", "Karol", "Martens", "James"], "venue": "arXiv preprint arXiv:1511.06807,", "year": 2015}, {"title": "Evolutionary Program Induction of Binary Machine Code and its Applications", "authors": ["Nordin", "Peter"], "venue": "PhD thesis, der Universitat Dortmund am Fachereich Informatik,", "year": 1997}, {"title": "Neural programmerinterpreters", "authors": ["Reed", "Scott", "de Freitas", "Nando"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "year": 2015}, {"title": "Solving General Arithmetic Word Problems", "authors": ["Roy", "Subhro", "Roth", "Dan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "year": 2015}, {"title": "Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics (TACL)", "authors": ["Roy", "Subhro", "Vieira", "Tim", "Roth", "Dan"], "year": 2015}, {"title": "Neural Programming Language", "authors": ["Siegelmann", "Hava T"], "venue": "In Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence,", "year": 1994}, {"title": "Programming by Sketching for Bit-streaming Programs", "authors": ["Solar-Lezama", "Armando", "Rabbah", "Rodric", "Bod\u0131\u0301k", "Rastislav", "Ebcio\u011flu", "Kemal"], "venue": "In Proceedings of Programming Language Design and Implementation (PLDI),", "year": 2005}, {"title": "Combinatorial Sketching for Finite Programs", "authors": ["Solar-Lezama", "Armando", "Tancau", "Liviu", "Bodik", "Rastislav", "Seshia", "Sanjit", "Saraswat", "Vijay"], "venue": "In ACM Sigplan Notices,", "year": 2006}, {"title": "Sequence to Sequence Learning with Neural Networks", "authors": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Proceedings of the Conference on Neural Information Processing Systems (NIPS),", "year": 2014}], "id": "SP:9bb713318d8cc95fc4375077b410e30e79382c8d", "authors": [{"name": "Matko Bo\u0161njak", "affiliations": []}, {"name": "Tim Rockt\u00e4schel", "affiliations": []}, {"name": "Jason Naradowsky", "affiliations": []}, {"name": "Sebastian Riedel", "affiliations": []}], "abstractText": "Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.", "title": "Programming with a Differentiable Forth Interpreter"}