3	10	Methods such as Retrofitting (Faruqui et al., 2015), Counterfitting (Mrkšić et al., 2016) or PARAGRAM (Wieting et al., 2015) fall within the same framework.
5	42	For instance, Levy and Goldberg (2014) bring word embeddings towards similarity rather than relatedness by using dependency-based distributional contexts rather than linear bag-of-word contexts.
6	14	Finally, some methods aim at improving word embeddings but without a clearly defined orientation, such as the All-but-the-Top method (Mu, 2018), which focuses on dimensionality reduction, or (Vulić et al., 2017), which exploits morphological relations.
7	10	In this article, we propose Pseudofit, a method that improves word embeddings without external knowledge and focuses on semantic similarity and synonym extraction.
9	15	We show the interest of Pseudofit and its variants through both intrinsic and extrinsic evaluations.
11	15	Without even taking into account the plurality of meanings of a word, this variability also exists inside any corpus C, even if it is quite homogeneous: the distributional representations of a word built from each half of C, C1 and C2, are not identical.
12	14	However, from the more general viewpoint of its meaning, they should be identical, or at least very close, and their differences be considered as incidental.
14	12	The method we propose, Pseudofit, formalizes this approach through the notion of pseudo-sense.
15	16	This notion is related to the notion of pseudo-word introduced in the field of word sense disambiguation by Gale et al. (1992) and Schütze (1992).
18	12	For each wordw, more precisely nouns in our case, it splits arbitrarily its occurrences into two sets: the occurrences of one set are labeled as pseudo-sense w1 while the occurrences of the other set are labeled as pseudo-sense w2.
28	91	TARGET CONTEXTS policeman {a, be, arrest (2), by (2), another} policeman1 {a, be, arrest, by} policeman2 {another, by, arrest} This sentence, which is voluntarily artificial, shows how three different contexts are built for a word in a corpus: one context (first line) is built from all the occurrences of the target word; a second one (second line) is built from half of the occurrences of the target word, representing its first pseudo-sense, while the third context (last line) is built from the other half of the occurrences of the target word, representing its second pseudo-sense.
30	72	More precisely, we adopt the variant of the Skip-gram model (Mikolov et al., 2013) proposed by Levy and Goldberg (2014), which can take as input arbitrary contexts.
37	21	The specificity of PARAGRAM, compared to methods such as Retrofitting, lies in its adaptation term.
40	11	For each word (vector xi) of a relation, a word (vector tj) outside the relation is selected among the words of the mini-batch of the current relation in such a way that tj is the closest word to xi according to the Cosine measure, which represents the most discriminative option.
47	9	We use this corpus under its lemmatized form.
48	50	The building of the embeddings are performed with word2vecf, the adaptation of word2vec from (Levy and Goldberg, 2014), with the best parameter values from (Baroni et al., 2014): minimal count=5, vector size=300, window size=5, 10 negative examples and 10−5 for the subsampling probability of the most frequent words.
50	18	Retrofitting and Counter-fitting are used with the parameter values specified respectively in (Faruqui et al., 2015) and (Mrkšić et al., 2016).
58	11	), MAP (Mean Average Precision) and precisions at various ranks (P@r).
59	13	Our reference is made up of the synonyms of WordNet (Miller, 1990) while both our target words and candidate synonyms are made up of the nouns with more than ten occurrences in each half of our corpus, which represents 20,813 nouns.
60	44	Table 2 gives the result of this second evaluation for 11,481 nouns with synonyms in WordNet among our 20,813 targets.
86	14	The overall principle of this task is similar to the word similarity task of our first evaluation but at the level of sentences: the similarity of a set of sentence pairs is computed by the system to evaluate and compared with a correlation measure, the Pearson correlation coefficient, against a gold standard produced by human annotators.
87	10	This framework is interesting for the evaluation of Pseudofit because the computation of the similarity of a pair of sentences can be achieved by unsupervised approaches based on word embeddings in a very competitive way, as demonstrated by (Hill et al., 2016).
88	27	More precisely, the approach we adopt is a classical baseline that composes the embeddings of the plain words of each sentence to compare by elementwise addition and computes the Cosine measure between the two resulting vectors.
91	12	Table 5 shows the result of this evaluation for the 1,379 sentence pairs of the test part of the STS Benchmark dataset.
94	22	Although our goal is not to compete with the best systems, it is interesting to note that our results are in line with the state of the art since they significantly outperform the two baselines and the lowest unsupervised system as well as other unsupervised systems mentioned in (Cer et al., 2017).
95	11	In this article, we presented Pseudofit, a method that specializes word embeddings towards semantic similarity without external knowledge by exploiting the variability of distributional contexts.
96	110	This method can be described as hybrid since it operates both before and after the building of word embeddings.
97	22	A set of intrinsic and extrinsic evaluations demonstrates the interest of the word embeddings produced by Pseudofit and its variants, with a particular emphasis on the extraction of synonyms.
98	31	In the presented work, the principles underlying Pseudofit, in particular the generation and convergence of different representations of a word, were tested only within the same corpus.
99	79	In conjunction with the work about word meta-embeddings (Yin and Schütze, 2016), it would be interesting to apply these principles to representations built from several corpora, like (Mrkšić et al., 2017) for different languages.
