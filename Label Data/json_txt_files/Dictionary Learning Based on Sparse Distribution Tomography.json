{"sections": [{"heading": "1. Introduction", "text": "The problem of finding the mixing matrix A from a set of observation vectors y in the model\ny = Ax (1)\nis only solvable if one can benefit from strong hypotheses on the signal vector x. For instance, one may assume that\n*Equal contribution 1Biomedical Imaging Group, EPFL, Lausanne, Switzerland 2Computer Communications and Applications Laboratory 3, EPFL, Lausanne, Switzerland. Correspondence to: Pedram Pad <pedram.pad@epfl.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe entries of x are statistically independent, which results in a class of methods refered to as independent component analysis (ICA) (Hyv\u00e4rinen et al., 2004). A more recent trend is to assume that the vector x is sparse, so that the recovery can be recast as a deterministic dictionary learning problem, the prototypical example being sparse component analysis (SCA) (Gribonval & Lesage, 2006; Aharon et al., 2006; Spielman et al., 2012). Extensive research has been conducted on these problems in the past three decades.\nPrior work: In the literature, ICA precedes SCA and can be traced back to (Herault & Jutten, 1986). In fact, ICA constitutes the non-Gaussian generalization of the much older principal component analysis (PCA), which is widely used in classical signal processing. ICA is usually formalized as an optimization problem involving a cost function that measures the independence of the estimated xi (i.e., the entries of the vector x). A common measure of independence, which is inspired by information theory, is the mutual information of the entries of x. However, due to its computational complexity, other measures such as the kurtosis, which measures the non-Gaussianity of the components, are often used (Hyv\u00e4rinen & Oja, 2000; Naik & Kumar, 2011) (except in special cases such as the analysis of stable AR(1) processes by (Pad & Unser, 2015)). The main drawback of ICA is that the system (1) needs to be determined; i.e., A should be square\u2014otherwise the complexity is so high that the methods can only be implemented for small problems (Lathauwer et al., 2007; Lathauwer & Castaing, 2008).\nSCA, on the other hand, is usually achieved by putting constraints on the sparsity of the representation or by optimizing a sparsity-promoting cost function. Thanks to the emergence of very efficient algorithms, SCA has found wide use in different applications (see (Mairal et al., 2010; Marvasti et al., 2012)). The underlying framework for SCA is deterministic\u2014this is the primary difference with ICA, which aims to decouple signals that are realizations of stochastic processes.\n\u03b1-stable distributions: In this paper, we aim to achieve the best of both worlds: the use of a statistical formulation\u2014in the spirit of ICA\u2014with a restriction to a parametric class of stochastic models that is well adapted to the notion of sparsity. Specifically, we assume that the entries of the vector x are random variables that are i.i.d. symmetric-\u03b1-\nstable. The family of \u03b1-stable distributions is a generalization of the Gaussian probability density function (PDF). Since \u03b1-stability is preserved through linear transformation, this class of models has a central position in the study of stochastic processes (Samoradnitsky & Taqqu, 1994; Nikias & Shao, 1995; Shao & Nikias, 1993). The family is parametrized by \u03b1 \u2208 (0, 2], which controls the rate of decay of the distribution. The extreme case of \u03b1 = 2 corresponds to the Gaussian distribution\u2014the only non-sparse member of the family. By contrast, the other members of the S\u03b1S family for \u03b1 < 2 are heavy-tailed with unbounded variance. This property implies that an i.i.d. sequence of such random variables generates a sparse signal (Amini et al., 2011; Gribonval et al., 2012). By decreasing \u03b1, the distribution becomes more heavy-tailed and thus the signal becomes more sparse (the effect of \u03b1 is illustrated in Figure 1).\nThis class of random variables has also been widely used in practice. Typical applications include: modeling of ultrasound RF signals, (Achim et al., 2015), signal detection theory (Kuruoglu et al., 1998), communications (Middleton, 1999), image processing (Achim & Kuruoglu, 2005), audio processing (Georgiou et al., 1999), sea surface (Gallagher, 2001), network traffic (Resnick, 1997), and finance (Nolan, 2003; Ling, 2005).\nMain contributions: Our main contribution in this paper is a new dictionary learning algorithm based on the signal modeling mentioned above. The proposed method has the following advantages:\n1. all parameters can be estimated from the data (it is hyperparameter-free), 2. it learns the dictionary without the need to recover the signal x, and 3. it is fast and remarkably robust.\nOnce the matrix A is estimated, it is then possible to efficiently recover x by using standard procedures (Bickson & Guestrin, 2010).\nWe also show that the proposed algorithm provides an efficient estimator of the spectral measure of a stable random vector. An enabling component of our method is a new theorem that generalizes a classical result about isometries of `p-norms.\nOrganization: In the next section, we briefly review S\u03b1S random variables and present our mathematical model. In Section 3, we establish our main result which then yields an algorithm for finding the matrix A as well as the sparsity index \u03b1. In Section 4, we present the simulation results and compare their performance with existing algorithms. In Section 5, we summarize the paper and give some suggestions for future work."}, {"heading": "2. Preliminaries and problem formulation", "text": "We begin by recalling some basic properties of symmetric\u03b1-stable random variables. We then proceed with the formulation of the estimation problem that we solve in Section 3. The notation that we use throughout the paper is as follows: we use italic symbols for random variables, capital boldface symbols for matrices and lowercase boldface symbols for vectors. Thus, X is a deterministic matrix,X is a random matrix and x is a random variable. Likewise, x and x denote a random and a deterministic vector respectively."}, {"heading": "2.1. Symmetric-\u03b1-stable random variables", "text": "For any \u03b1 \u2208 (0, 2] and \u03b3 > 0, a random variable X with characteristic function\np\u0302X(\u03c9) = exp(\u2212\u03b3|\u03c9|\u03b1) (2)\nis called a symmetric-\u03b1-stable (S\u03b1S) random variable with dispersion \u03b3 and stability parameter \u03b1 (Nikias & Shao, 1995). This class of random variables is a generalization of the Gaussian model: For \u03b1 = 2, X is a Gaussian random variable with zero mean and variance 2\u03b3. As their name suggests, \u03b1-stable variables share the property of stability under linear combination (Nikias & Shao, 1995); i.e., if X1, . . . , Xn are n i.i.d. copies ofX and a1, . . . , an \u2208 R are n real numbers, then the random variable\nY = a1X1 + \u00b7 \u00b7 \u00b7+ anXn (3)\nhas the same distribution as( |a1|\u03b1 + \u00b7 \u00b7 \u00b7+ |an|\u03b1 ) 1 \u03b1X. (4)\nIn other words, the random variable Y is an S\u03b1S random variable with dispersion \u03b3 \u2016a\u2016\u03b1\u03b1 where \u2016a\u2016\u03b1 = ( |a1|\u03b1 +\n\u00b7 \u00b7 \u00b7 + |an|\u03b1 ) 1 \u03b1 is the \u03b1-(pseudo)norm of the vector a = (a1, . . . , an). This property makes S\u03b1S random variables convenient for the study of linear systems.\nThe other property of S\u03b1S random variables with \u03b1 < 2 is their heavy-tailed PDF. When \u03b1 < 2, we have\nlim |x|\u2192\u221e\n|x|1+\u03b1pX(x) = C(\u03b1, \u03b3), (5)\nwhere pX is the PDF of X and C(\u03b1, \u03b3) is a positive constant that depends on \u03b1 and \u03b3 (Nikias & Shao, 1995). This implies that the variance of S\u03b1S random variables is unbounded for \u03b1 < 2. Also, note that a smaller \u03b1 results in heavier tails.\nInfinite-variance random variables are considered to be appropriate candidates for sparse signals (Amini et al., 2011; Gribonval et al., 2012). Because an i.i.d. sequence of heavytailed random variables has most of its energy concentrated on a small fraction of samples, they are good candidates to model signals that exhibit sparse behavior.\nYet, the truly fundamental aspect of \u03b1-stable random variables is their role in the generalized central limit theorem. As we know, the limit distribution of normalized sums of i.i.d. finite-variance random variables are Gaussian. Likewise, any properly normalized sum of heavy-tailed i.i.d. random variables converges to an \u03b1-stable random variable whee the \u03b1 depends on the weight of their tail (Meerschaert & Scheffler, 2001). This implies that a linear combination of a large number of samples of a sparse signal is well represented by \u03b1-stable random variables."}, {"heading": "2.2. Problem formulation", "text": "Our underlying signal model is\ny = Ax (6)\nwhere x is an unknown n\u00d7 1 random vector with S\u03b1S i.i.d. entries and \u03b1 < 2, y is an m\u00d7 1 observation vector and A is an m\u00d7 n dictionary matrix,. We are given K realizations of y; namely, y1, . . . ,yK , and our goal is to estimate A."}, {"heading": "3. Dictionary learning for S\u03b1S signals", "text": "In the problem of dictionary learning, the maximum information that we can asymptotically try to retrieve from y1, . . . ,yK is the exact distribution of y. However, even if we knew y, identifying A is still not tractable in general\u2014 for instance, in the case of Gaussian vectors, A is only identifiable up to right-multiplication by a unitary matrix. Moreover, obtaining an acceptable estimate of the distribution of y requires, in general, a vast amount of data and processing power (since it is a m-dimensional function with m possibly large). In this section, we leverage the property of stability under linear combination of S\u03b1S random variables explained in Section 2.1 to propose a new algorithm to estimate A for the dictionary learning problem stated in Section 2.2."}, {"heading": "3.1. New cost function for sparse S\u03b1S signals", "text": "Recall that, the random vector y (see Equations (2) and (6)) is an m-dimensional \u03b1-stable vector with characteristic function\np\u0302y(\u03c9) = exp ( \u2212\u03b3\u2016A>\u03c9\u2016\u03b1\u03b1 ) (7)\nfor \u03c9 \u2208 Rm. Thus, knowing \u2016A>u\u2016\u03b1 for all u \u2208 Sm\u22121, where Sm\u22121 is the (m\u2212 1)-dimensional unit sphere, i.e.,\nSm\u22121 = {u \u2208 Rm | \u2016u\u20162 = 1} , (8) is equivalent to knowing the distribution of y. Note that u>y = u>Ax (see Equations (3) and (4)) is an S\u03b1S random variable with dispersion\n\u03b3(u) = \u03b3\u2016A>u\u2016\u03b1\u03b1. (9) Thus, knowing the dispersion of the marginal distributions of y for all u \u2208 Sm\u22121 is equivalent to knowing the distribution of y. In other words, in the case of S\u03b1S random vectors, knowing their marginal dispersions is equivalent to knowing the Radon transform of their PDFs or, equivalently, their joint characteristic function (Helgason, 2010). Due to the relationship between the Radon transform and the field of tomography, we call our algorithm sparse distribution tomography (SparsDT).\nAnother interesting fact is that, in the non-Gaussian case (\u03b1 < 2), knowing the marginal dispersions of y, i.e., \u03b3(u), identifies the matrix A uniquely, up to negation and permutation of the columns. Formally, we have the following theorem, which is proved in Appendix A:\nTheorem 1 Let A be an m\u00d7 n matrix where columns are pairwise linearly independent. If \u03b1 \u2208 (0, 2) and B is an m\u00d7 n matrix for which we have\n\u2016A>u\u2016\u03b1\u03b1 = \u2016B>u\u2016\u03b1\u03b1 (10) for all u \u2208 Rm, then B is equal to A up to negation and permutation of its columns.\nRemark 1 This theorem can be seen as a generalization of the result in (Rolewicz, 1985) that states that the isometries of `p-norms are generalized permutation matrices (permutation matrices with some of their rows negated). To the best of our knowledge, this result is novel and could be of independent interest.\nThis theorem suggests that in order to find A all we need is to find \u03b3(u) for u \u2208 Rm. Intuitively, we can say that as A has a finite number of parameters (entries), A is identifiable based on the knowledge of \u03b3(u) for an appropriate finite set of vectors u = u1, . . . ,uL (for some L \u2265 mn). We can then solve the set of non-linear equations\n\u03b3\u2016B>u1\u2016\u03b1\u03b1 = \u03b3(u1), ... (11) \u03b3\u2016B>uL\u2016\u03b1\u03b1 = \u03b3(uL),\nfor B to obtain A.\nNow, the problem is to find \u03b3(u) for a given u \u2208 Rm. Recall that \u03b3(u) is the dispersion of the S\u03b1S random variable uTy. As y1, . . . ,yK are realizations of y, uTy1, . . . ,uTyK are realizations of uTy. There is a rich literature on the estimation of the parameters of \u03b1-stable random variables through their realizations, see, e.g, (Nikias & Shao, 1995). We use the estimation from (Achim et al., 2015) in the following equation\nlog \u03b3\u0302(u) = \u03b1\nK K\u2211 k=1 log |uTyk| \u2212 (\u03b1\u2212 1)\u03c8(1) (12)\nwhere \u03c8 is the digamma function (\u03c8(1) is the negative of the Euler-Mascheroni constant and is approximately 0.5572), and \u03b3\u0302(u) denotes the estimation of \u03b3(u). Note that \u03b3\u0302(u) tends to \u03b3(u) when the number of observations, K, tends to infinity. This means that we can obtain the exact value of \u03b3(u) asymptotically.\nHowever, non-exact values for \u03b3(u`), for ` = 1, . . . , L (which is the case when K is finite), can lead to the nonexistence of a solution for the system of equations (11). To overcome this problem, instead of solving this system of equations exactly, we minimize the following objective function\nE(B) = 1 L L\u2211 `=1 d ( \u03b3\u2016B>u`\u2016\u03b1\u03b1, \u03b3\u0302(u`) ) (13)\n= 1\n\u03b1L L\u2211 `=1 \u2223\u2223log(\u03b3\u2016B>u`\u2016\u03b1\u03b1)\u2212 log(\u03b3\u0302(u`))\u2223\u2223 where log \u03b3\u0302(u1), . . . , log \u03b3\u0302(uL) are L numbers calculated from (12). The cost function d(a, b) = 1\u03b1 | log a\u2212 log b| is a continuous positive function1 from R2 to R, whose global minimum is 0 and is reached over the line a = b. When \u03b3\u0302(u) = \u03b3(u), B = A minimizes E(B). Thus, if \u03b3\u0302(u) is close enough to \u03b3(u), due to the continuity of d, we expect that the minimizer of E will be close to A. Therefore, our approach to dictionary learning is to solve\nA\u0302 = argmin B E(B) (14)\n= argmin B\n1\n\u03b1L L\u2211 `=1 \u2223\u2223log (\u03b3\u2016B>u`\u2016\u03b1\u03b1)\u2212 log \u03b3\u0302(u`)\u2223\u2223 . The only parameter that needs to be set now is the stability parameter \u03b1. Note that the dispersion parameter \u03b3 in Equation (14) does not need to be set as it will be automatically\n1In our simulations we also implemented other natural candidates for d(a, b) and all of them gave approximately the same performance. Due to the limited space, we do not present results for other cost functions.\nmerged into the learned dictionary. Recall that there are well-known methods for estimating \u03b1 from data; among which we use\n\u03b1\u0302(u) =\n( 6\n\u03c02K K\u2211 k=1 ( log |u>yk| \u2212 log \u03ba\u0302(u) )2 \u2212 1 2 )\u2212 12 (15)\nfrom (Achim et al., 2015), where\nlog \u03ba\u0302(u) = 1\nK K\u2211 k=1 log |u>yk|. (16)\nThis gives us an estimate for \u03b1 for any given u \u2208 Rm. Hence, the estimated value of \u03b1 is the average over all \u03b1\u0302(u`) for ` = 1, . . . , L, i.e.,\n\u03b1\u0302 = 1\nL L\u2211 `=1 \u03b1\u0302(u`). (17)\nNow, using this estimate, Equation (12) becomes\nlog \u03b3\u0302(u) = \u03b1\u0302 log \u03ba\u0302(u)\u2212 (\u03b1\u0302\u2212 1)\u03c8(1). (18)\nWe also replace \u03b1 with \u03b1\u0302 in Equation (13) which results in a parameter-free cost function. This is in contrast with most existing cost functions that have parameters one must set."}, {"heading": "3.2. Learning algorithm", "text": "To solve the minimization problem in Equation (14), we propose a variation on a gradient-descent algorithm with an adaptive step size that has a changing cost function. To do so, we first derive the gradient of E at B. Using matrix calculus (see Appendix B), we find that\n\u2207E(B) = (19)\n1\n\u03b1L L\u2211 `=1 sgn ( log(\u03b3\u2016B>u`\u2016\u03b1\u03b1)\u2212 log \u03b3\u0302(u`) ) \u00b7 \u2207\u2016B >u`\u2016\u03b1\u03b1 \u2016B>u`\u2016\u03b1\u03b1\nwhere sgn(\u00b7) is the sign function (i.e., sgn(e) = 1 if e > 0 and sgn(e) = 0 otherwise) and\n\u2207\u2016B>u\u2016\u03b1\u03b1 = \u03b1  sgn ( b>1 u ) \u2223\u2223b>1 u\u2223\u2223\u03b1\u22121 u> ...\nsgn ( b>nu ) \u2223\u2223b>nu\u2223\u2223\u03b1\u22121 u>  >\n(20)\nwhere bi is the ith column of B.\nThe cost function in Equation (13) is non-convex in B. In order to avoid getting trapped in local minima, we iteratively change the cost function inside the gradient descent algorithm. The idea is that instead of keeping u1, . . . ,uL fixed throughout the optimization process, we regenerate them randomly with a uniform distribution on Rm after some\niterations of steepest descent. We repeat this process until convergence. Note that (11) holds for any u1, . . . ,uL and thus changing this set does not change the end result of (11).\nRemark 2 Using this idea always results in convergence to the global minimum in our computer simulations. A plausible explanation of this phenomenon is that each set of u1, . . . ,uL yields a non-convex cost function with different local minima. Yet they all have the same global minimum. Therefore, switching between them during the optimization process prevents getting trapped in any of the local minima, which ultimately results in finding the global minimum of the cost function.\nThe pseudocode of our dictionary learning method is given in Algorithm 1. There, \u03b7 is the step size of the gradient descent that increases or decreases by factors of \u03ba+ or \u03ba\u2212 upon taking a good or poor step. The adaptive step size is especially helpful for \u03b1 \u2264 1, where the cost function is not smooth. The algorithm does not depend on the exact choice of convergence criteria.\nRemark 3 Algorithm 1 can also be seen as an efficient method for estimating the spectral measure of stable random vectors. In fact, the problem of estimating A from a set of realizations of y can also be seen as parameter estimation for a stable random vector y with a symmetric distribution around the origin. Such random vectors are parametrized by a measure \u0393 on Sm\u22121 that is called the spectral measure. In our problem, we have \u0393(\u00b7) = \u2211n i=1 \u2016ai\u2016\u03b1 \u03b4ai(\u00b7) where the \u03b4ais are unit point masses at ai \u2016ai\u20162 and ai is the i th column of A. Some methods have been proposed to solve this problem, e.g., (Nolan et al., 2001). However, they tend to be computationally intractable for dimensions greater than 3.\nRemark 4 According to the generalized central limit theorem, under some mild conditions, the distribution of the sum of symmetric heavy-tailed random variables tends to a S\u03b1S distribution as the number of summands tends to infinity. This means that we can represent u>y = u>Ax with an S\u03b1S random variable for large enough n irrespective of the distribution of the xis provided that the latter are heavy tailed. Therefore, we expect Algorithm 1 to find applications for other classes of sparse signals, provided that n is sufficiently large."}, {"heading": "4. Empirical results", "text": "In this section, we analyze the performance of the proposed algorithm SparsDT and compare it with existing methods. Recall that the actual dictionary is A and the learned dictionary is A\u0302. We run two types of the experiments: We first test the algorithm on synthetic S\u03b1S data and then we test it on real images.\nAlgorithm 1 SparseDT 1: initialize: \u03b7 > 0 2: initialize: \u03ba+ \u2265 1 and \u03ba\u2212 \u2264 1 3: initialize: generate b1, . . . , bn \u223c N (0, Im\u00d7m) and\nB\u2190 [ b1| . . . |bn ] 4: repeat 5: initialize: generate u1, . . . ,uL \u223c N (0, Im\u00d7m) 6: estimate \u03b1\u0302 from (15) 7: E \u2190 E(B) 8: repeat 9: Bold \u2190 B 10: Eold \u2190 E 11: B\u2190 B\u2212 \u03b7 \u2207E(B) 12: E \u2190 E(B) 13: if E \u2264 Eold then 14: \u03b7 \u2190 \u03ba+ \u00b7 \u03b7 15: else 16: B\u2190 Bold 17: E \u2190 Eold 18: \u03b7 \u2190 \u03ba\u2212 \u00b7 \u03b7 19: end if 20: until B converges (for this choice of u1, . . . ,uL) 21: until B converges\nreturn B"}, {"heading": "4.1. Benchmarks", "text": "We compare our algorithm with three commonly used algorithms that are available in the Python package SPAMS2. These constrained optimization problems3 are as follows:\n1. `2/`1: Maximizing the data fidelity while controling the sparsity of representation with parameter \u03bb1:\nA\u0302`2/`1 = argmin B\n1\n2K K\u2211 k=1 \u2016yk \u2212Bxk\u201622\ns.t. \u2016xi\u20161 \u2264 \u03bb1.\n2. `1/`2: Maximizing the sparsity of representation while controling the data fidelity with parameter \u03bb2:\nA\u0302`1/`2 = argmin B\n1\n2K K\u2211 k=1 \u2016xk\u20161\ns.t. \u2016yk \u2212Bxk\u20162 \u2264 \u03bb2.\n3. `1 + `2: Combining sparsity and data fidelity in the cost function using Lagrange multipliers:\nA\u0302`1+`2 = argmin B\n1\n2K K\u2211 k=1 \u2016yk \u2212Bxk\u201622\n+ \u03bb3\u2016xk\u20161 + \u03bb4\u2016xk\u201622. 2http://spams-devel.gforge.inria.fr/ 3Other cost functions are also available in the package SPAMS,\nbut those retained here yield the best results in our experiments.\nOne of the challenges in utilizing these benchmarks is determining the regularization parameters \u03bb1, . . . , \u03bb4. In our experiments, the regularization parameters are optimized (by grid search) in order to maximize the performance of each of the benchmarks above. This is in contrast to our algorithm, which has no regularization parameter to tune."}, {"heading": "4.2. Experimental results for synthetic data", "text": "We first test the algorithms on synthetic data. In order to quantify the performance of the algorithms, we use several metrics. One is the average correlation of the dictionaries. Specifically, we calculate the correlation between all columns of A\u0302 and A, and then match each column of A\u0302 with one of the columns of A (a one-to-one map) such that the average correlation between the corresponding columns is maximized. Additionally, we say that the dictionary is found if the average correlation of the columns is larger than 0.97.\nEffect of K: We demonstrate the effect of the number of samples K on the performance of our proposed algorithm SparsDT. Intuitively, the precision of the estimation increases with the number of samples K, and, as K goes to infinity, the estimation error goes to zero, which ultimately gives the exact A. We demonstrate this effect with the following experiment: We take m = 16, n = 24 and \u03b1 = 1 and 1.5. Then, for each K, we run the experiment for 50 random matrices A, and, for each case, we run Algorithm 1 with both exact and estimated \u03b1 (from (17)). The results are depicted in Figure 2, where the vertical axis is the average correlation of the estimated dictionary with the exact one, and the horizontal axis is the number of samples K. Interestingly, the performance of the algorithm is almost the same when using the exact or estimated value of \u03b1, which suggests that the estimator of \u03b1 is robust and accurate. Moreover, we see that the average correlation is an increasing function of K, as expected. Also note that the convergence is faster for \u03b1 = 1, which corresponds to the setting with more sparsity.\nComparison metrics\nAlgorithm % found Avg. time (s)\nComparison against benchmarks: We compare SparsDT against the `2/`1, `1/`2 and `1 + `2 methods described above. We compare the algorithms with regard to their success rate (i.e., the percentage of the dictionaries found by the algorithm), and the time that they take to find the dictionary (in the cases of success only). We again take m = 16, n = 24 and generate 100 random matrices A. In Table 1, the results for \u03b1 = 1.2 and K = 500 are given. Finally, in Figure 3 we compare the algorithms success rate for different \u03b1, we take m = 16, n = 24, and K = 1000. These results indicate that SparsDT outperforms the other methods in the rate of success. Also, its average learning time is typically much less than the others, except for `2/`1 which does not find the correct dictionary at best in 10% of the time. The range of \u03b1 that was observed in our experiments is \u03b1 \u2208 [1, 1.6], which is also the range where our algorithm works well and which is interesting for many applications including image processing. We do not recommend using the method for \u03b1 > 1.7 because the convergence properties degrade as we get closer to 2 (a larger value of K is then needed to reach high success rates)."}, {"heading": "4.3. Experimental results for real images", "text": "Since images often have sparse representations, we apply our algorithm to problems in image processing applications. Our experiments are missing pixel recovery (in-painting) and denoising, based on dictionary learning. We use a\nAlgorithm PSNR (dB)\ndatabase of face images provided by AT&T4 and crop them to have size 112 \u00d7 91 so we can chop each image to 208 patches of size 7\u00d7 7, which correspond to yi in our model.\nIn this situation, the data is not exactly S\u03b1S, so we must adapt our choice of u in Step 5 of Algorithm 1. Specifically, in Equation (17) we eliminate projection vectors u that result in \u03b1 greater than 2 (as \u03b1 is required to be less than 2). In addition, we only select u that results in an \u03b1 close to our estimated \u03b1\u0302 in (17). The number of iterations are chosen such that all algorithms converge.\nMissing pixel recovery: In this experiment, we reconstruct an image from 50% of its pixels. We take out the image shown in Figure 4, remove 50% of its pixels uniformly at random, and learn the dictionary using the patches of the other images in the collection. We assume 248 atoms in the dictionary. Then, using the learned dictionary, we reconstruct the image using orthogonal matching pursuit (for a detailed analysis see (Sahoo & Makur, 2015)). The results for different dictionary learning methods are depicted in Figure 4; SparsDT outperforms existing methods both visually and in term of PSNR.\nImage denoising: In this experiment, we use the dictionaries learned in the previous experiment to denoise the image in Figure 4. More precisely, we add Gaussian noise with standard deviation 10 to the original image and use orthogonal matching pursuit to denoise it. The performance of each method in PSNR can be seen in Table 2. As we see, SparsDT outperforms the other methods by at least 0.6 dB."}, {"heading": "5. Summary and future work", "text": "In this paper, we consider a stochastic generation model of sparse signals that involves an S\u03b1S innovation. Then, by designing an estimator of the spectral measure of so-defined stable random vectors, we propose a new dictionary learning algorithm. The proposed algorithm (SparsDT) turns out to be quite robust; it works well on sparse real-world signals, even when these do not rigorously follow the S\u03b1S model. This surprising fact can be explained by invoking the generalized central limit theorem. We validate SparsDT on several image-processing tasks and found it to outperform popular dictionary learning methods often significantly.\n4www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\nMoreover, SparsDT has no parameter to tune, contrary to other algorithms.\nExtending this work to non-symmetric \u03b1-stable random variables is a possible direction of future research. Given the excellent numerical behavior of the algorithm, it is of interest to get a good handle on the accuracy of the estimation in terms of the number of samples and the dimensionality of signals."}, {"heading": "A. Proof of Theorem 1", "text": "Denote the jth column of A and B by aj and bj , respectively. Also, denote the set of indices j for which bj 6= 0 by B \u2286 {1, . . . , n}. Note that due to the assumption of the pairwise linear independence of columns of A, aj 6= 0 for all j \u2208 {1, . . . , n}. Since\n\u2225\u2225A>u\u2225\u2225\u03b1 \u03b1 = n\u2211 j=1 \u2223\u2223u>aj\u2223\u2223\u03b1 = \u2211 j\u2208B \u2223\u2223u>bj\u2223\u2223\u03b1 = \u2225\u2225B>u\u2225\u2225\u03b1\u03b1 . for all u \u2208 Rm, the partial derivatives of any order of the two side of the equation are also equal. In particular, we have\n\u2202d\n\u2202ui d\n\u2225\u2225A>u\u2225\u2225\u03b1 \u03b1 = \u2202d\n\u2202ui d\n\u2225\u2225B>u\u2225\u2225\u03b1 \u03b1\n(21)\nfor all i = 1, . . . ,m and d \u2208 N, where ui is the ith entry of u.\nFirst we prove the theorem for 0 < \u03b1 \u2264 1. In (21), we set d = 1 and obtain\nn\u2211 j=1 \u2223\u2223u>aj\u2223\u2223\u03b1\u22121 sgn (u>aj) aij = \u2211 j\u2208B\n\u2223\u2223u>bj\u2223\u2223\u03b1\u22121 sgn (u>bj) bij . (22) Exploiting this equation, we prove the following lemma:\nLemma 1 Under the assumptions of Theorem 1, for any j\u2032 \u2208 {1, . . . , n}, there exists j \u2208 B and tj\u2032 6= 0, such that tj\u2032aj\u2032 = bj .\nProof: Take i\u2032 such that ai\u2032j\u2032 6= 0. Also, for all r = 1, . . . , n, define\nKar = { u \u2208 Rm|u>ar = 0 } (23)\nwhich is an (m \u2212 1)-dimensional subspace of Rm. Since for any j 6= j\u2032, aj\u2032 and aj are linearly independent, the subspace Kaj\u2032 \u2229 Kaj is (m \u2212 2)-dimensional. This implies that their (m\u2212 1)-dimensional Lebesgue measure is zero; and the same holds for the union \u22c3 j 6=j\u2032 ( Kaj\u2032 \u2229 Kaj ) . Since\nKaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj = Kaj\u2032 \\ \u22c3 j 6=j\u2032 ( Kaj\u2032 \u2229 Kaj ) , (24)\nwe conclude that the (m\u2212 1)-dimensional Lebesgue measure of Kaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj is infinity.\nNote that any u \u2208 Kaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj is only orthogonal to aj\u2032 and not to any other column of A. This yields that if we set i = i\u2032 in the left-hand side of (22), for any u \u2208 Kaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj , the only discontinuous term at u is the j\u2032th one (because the function |x|\u03b1\u22121sgn(x) has a single point of discontinuity at x = 0). As a result, the sum itself is discontinuous over Kaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj . Hence, the same should hold for the right-hand side of the equation.\nSimilar to (23), define Kbr = { u \u2208 Rm|u>br = 0 } . (25)\nThe set of discontinuity points of the right-hand side of (22) is a subset of \u22c3 j\u2208B Kbj . Therefore, we have\nKaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj \u2286 \u22c3 j\u2208B Kbj (26)\nwhich can also be written as Kaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj \u2286 \u22c3 j\u2208B ( Kaj\u2032 \u2229 Kbj ) (27)\nNow, if none of the columns of B is linearly dependent to aj\u2032 , all Kaj\u2032 \u2229 Kbj will be (m\u2212 2)-dimensional spaces, and their (m\u2212 1)-dimesnional Lebesgue measure is zero. This implies that the (m\u2212 1)-dimensional Lebesgue measure of the right-hand side of (27) is also zero, which contradicts the result after (24). Therefore, there exists a j \u2208 B such that bj is linearly dependent to aj\u2032 , which completes the proof of the lemma.\nThe first consequence of Lemma 1 is that none of the columns of B is the zero vector and thus B = {1, . . . , n}.\nAlso, since all pairs of columns of A are linearly independent, the correspondence between a column of A and a column of B that are linearly dependent is one-to-one. Thus, we can simplify (22) to be\nn\u2211 j=1 (1\u2212 |tj |) \u2223\u2223u>aj\u2223\u2223\u03b1\u22121 sgn (u>aj) aij = 0, (28)\nwhich holds for all u. This implies that the left hand-side of the above equation is a continuous function. However, as we saw in the proof of the lemma, every u \u2208 Kaj\u2032 \\ \u22c3 j 6=j\u2032 Kaj is a discontinuity point of the left-hand unless 1\u2212 |t\u2032j | = 0 which completes the proof for the case of 0 < \u03b1 \u2264 1.\nFor the case of 1 < \u03b1 < 2, we set d = 2 in (21) and obtain n\u2211 j=1 \u2223\u2223u>aj\u2223\u2223\u03b1\u22122 a2ij = n\u2211 j=1 \u2223\u2223u>bj\u2223\u2223\u03b1\u22122 b2ij . (29) Replacing (22) by (29), the same reasoning as for 0 < \u03b1 \u2264 1 works to prove the theorem for 1 < \u03b1 < 2.\nB. Derivation of the gradient of E(B) To calculate the gradient of E(B), we first calculate the gradient of \u2016B>u\u2016\u03b1\u03b1 using the definition of the gradient, i.e.\n\u3008\u2207\u2016B>u\u2016\u03b1\u03b1,C\u3009 = \u2202\n\u2202\n\u2225\u2225(B> \u2212 C>)u\u2225\u2225\u03b1 \u03b1 \u2223\u2223\u2223 =0\n= \u03b1 n\u2211 j=1 c>j u sgn ( b>j u ) \u2223\u2223b>j u\u2223\u2223\u03b1\u22121 . Here, \u3008D,C\u3009 = tr(D>C) is the standard inner product on the space of matrices. Writing the last equation in the matrix form, we obtain (20). Now, using the fact ddx |log x| = sgn(log x) 1x and the chain rule for differentiation yields (19)."}, {"heading": "Acknowledgements", "text": "The research was partially supported by the Hasler Foundation under Grant 16009, by the European Research Council under Grant 692726 (H2020-ERC Project GlobalBioIm) and by the SNF Project Grant (205121 163385)."}], "year": 2017, "references": [{"title": "Image denoising using bivariate \u03b1-stable distributions in the complex wavelet domain", "authors": ["A. Achim", "E. Kuruoglu"], "venue": "IEEE Signal Processing Letters,", "year": 2005}, {"title": "Reconstruction of ultrasound RF echoes modeled as stable random variables", "authors": ["A. Achim", "A. Basarab", "G. Tzagkarakis", "P. Tsakalides", "D. Kouam\u00e9"], "venue": "IEEE Transactions on Computational Imaging,", "year": 2015}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "authors": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on signal processing,", "year": 2006}, {"title": "Compressibility of deterministic and random infinite sequences", "authors": ["A. Amini", "M. Unser", "F. Marvasti"], "venue": "IEEE Transactions on Signal Processing,", "year": 2011}, {"title": "Inference with multivariate heavy-tails in linear models", "authors": ["D. Bickson", "C. Guestrin"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "A method for fitting stable autoregressive models using the autocovariation function", "authors": ["C. Gallagher"], "venue": "Statistics & probability letters,", "year": 2001}, {"title": "Alphastable modeling of noise and robust time-delay estimation in the presence of impulsive noise", "authors": ["P. Georgiou", "P. Tsakalides", "C. Kyriakakis"], "venue": "IEEE transactions on multimedia,", "year": 1999}, {"title": "A survey of sparse component analysis for blind source separation: principles, perspectives, and new challenges", "authors": ["R. Gribonval", "S. Lesage"], "venue": "In ESANN\u201906 proceedings14th European Symposium on Artificial Neural Networks,", "year": 2006}, {"title": "Compressible distributions for high-dimensional statistics", "authors": ["R. Gribonval", "V. Cevher", "M.E. Davies"], "venue": "IEEE Transactions on Information Theory,", "year": 2012}, {"title": "Integral Geometry and Radon Transforms", "authors": ["S. Helgason"], "year": 2010}, {"title": "Space or time adaptive signal processing by neural network models", "authors": ["J. Herault", "C. Jutten"], "venue": "In Neural networks for computing,", "year": 1986}, {"title": "Independent component analysis: algorithms and applications", "authors": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks,", "year": 2000}, {"title": "Independent component analysis, volume 46", "authors": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "year": 2004}, {"title": "Near optimal detection of signals in impulsive noise modeled with a symmetric/spl alpha/-stable distribution", "authors": ["E.E. Kuruoglu", "W.J. Fitzgerald", "P.J. Rayner"], "venue": "IEEE Communications Letters,", "year": 1998}, {"title": "Blind identification of underdetermined mixtures by simultaneous matrix diagonalization", "authors": ["Lathauwer", "L. De", "J. Castaing"], "venue": "IEEE Transactions on Signal Processing,", "year": 2008}, {"title": "Fourth-order cumulant-based blind identification of underdetermined mixtures", "authors": ["Lathauwer", "L. De", "J. Castaing", "J. Cardoso"], "venue": "IEEE Transactions on Signal Processing,", "year": 2007}, {"title": "Self-weighted least absolute deviation estimation for infinite variance autoregressive models", "authors": ["S. Ling"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2005}, {"title": "Online learning for matrix factorization and sparse coding", "authors": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "A unified approach to sparse signal processing", "authors": ["F. Marvasti", "A. Amini", "F. Haddadi", "M. Soltanolkotabi", "B.H. Khalaj", "A. Aldroubi", "S. Sanei", "J. Chambers"], "venue": "EURASIP journal on advances in signal processing,", "year": 2012}, {"title": "Limit distributions for sums of independent random vectors: Heavy tails in theory and practice, volume 321", "authors": ["M. Meerschaert", "H. Scheffler"], "year": 2001}, {"title": "Non-gaussian noise models in signal processing for telecommunications: new methods an results for class a and class b noise models", "authors": ["D. Middleton"], "venue": "IEEE Transactions on Information Theory,", "year": 1999}, {"title": "An overview of independent component analysis and its applications", "authors": ["G. Naik", "D. Kumar"], "year": 2011}, {"title": "Signal Processing with AlphaStable Distributions and Applications", "authors": ["C.L. Nikias", "M. Shao"], "year": 1995}, {"title": "Modeling financial data with stable distributions", "authors": ["Nolan", "JP"], "venue": "Handbook of Heavy Tailed Distributions in Finance, Handbooks in Finance: Book,", "year": 2003}, {"title": "Estimation of stable spectral measures", "authors": ["Nolan", "JP", "Panorska", "AK", "McCulloch", "JH"], "venue": "Mathematical and Computer Modelling,", "year": 2001}, {"title": "Optimality of operator-like wavelets for representing sparse AR(1) processes", "authors": ["P. Pad", "M. Unser"], "venue": "IEEE Transactions on Signal Processing,", "year": 2015}, {"title": "Heavy tail modeling and teletraffic data: special invited paper", "authors": ["S. Resnick"], "venue": "The Annals of Statistics,", "year": 1997}, {"title": "Metric Linear Spaces. Mathematics and its applications (D", "authors": ["S. Rolewicz"], "venue": "Reidel Publishing Company).: East European series. D. Reidel,", "year": 1985}, {"title": "Signal recovery from random measurements via extended orthogonal matching pursuit", "authors": ["S. Sahoo", "A. Makur"], "venue": "IEEE Trans. Signal Processing,", "year": 2015}, {"title": "Stable non-Gaussian random processes: stochastic models with infinite variance, volume 1", "authors": ["G. Samoradnitsky", "M. Taqqu"], "venue": "CRC press,", "year": 1994}, {"title": "Signal processing with fractional lower order moments: stable processes and their applications", "authors": ["M. Shao", "C.L. Nikias"], "venue": "Proceedings of the IEEE,", "year": 1993}, {"title": "Exact recovery of sparsely-used dictionaries", "authors": ["D. Spielman", "H. Wang", "J. Wright"], "venue": "In COLT, pp", "year": 2012}], "id": "SP:13cabc82f93f87c029c8db95db1c12a744e16b67", "authors": [{"name": "Pedram Pad", "affiliations": []}, {"name": "Farnood Salehi", "affiliations": []}, {"name": "Elisa Celis", "affiliations": []}, {"name": "Patrick Thiran", "affiliations": []}, {"name": "Michael Unser", "affiliations": []}], "abstractText": "We propose a new statistical dictionary learning algorithm for sparse signals that is based on an \u03b1-stable innovation model. The parameters of the underlying model\u2014that is, the atoms of the dictionary, the sparsity index \u03b1 and the dispersion of the transform-domain coefficients\u2014are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily \u03b1-stable. We evaluate our algorithm by performing two types of experiments: image inpainting and image denoising. In both cases, we find that our approach is competitive with stateof-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of `p-norms that constitutes the foundation of our approach.", "title": "Dictionary Learning Based on Sparse Distribution Tomography"}