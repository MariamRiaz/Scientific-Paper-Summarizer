0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 78–88 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Word Sense Disambiguation (WSD) is a key task in computational lexical semantics, inasmuch as it addresses the lexical ambiguity of text by making explicit the meaning of words occurring in a given context (Navigli, 2009).",1 Introduction,[0],[0]
"Anyone who has struggled with frustratingly unintelligible translations from an automatic system, or with the meaning bias of search engines, can understand the importance for an intelligent system to go beyond the surface appearance of text.
",1 Introduction,[0.9536245578447922],"['The current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch.']"
There are two mainstream lines of research in WSD: supervised and knowledge-based WSD.,1 Introduction,[0],[0]
"Supervised WSD frames the problem as a classical machine learning task in which, first a training phase occurs aimed at learning a classification model from sentences annotated with word senses and, second the model is applied to previouslyunseen sentences focused on a target word.",1 Introduction,[0.9539403220194921],"['Regardless of whether the speaker is a DCP and the listener a model human or vice-versa, translation based on the belief-matching criterion in Section 5 achieves the best performance; indeed, when translating neuralese color names to natural language, the listener is able to achieve a slightly higher score than it is natively.']"
"A key
difference from many other problems, however, is that the classes to choose from (i.e., the senses of a target word) vary for each word, therefore requiring a separate training process to be performed on a word by word basis.",1 Introduction,[0],[0]
"As a result, hundreds of training instances are needed for each ambiguous word in the vocabulary.",1 Introduction,[0],[0]
"This would necessitate a million-item training set to be manually created for each language of interest, an endeavour that is currently beyond reach even in resource-rich languages like English.
",1 Introduction,[0],[0]
"The second paradigm, i.e., knowledge-based WSD, takes a radically different approach: the idea is to exploit a general-purpose knowledge resource like WordNet (Fellbaum, 1998) to develop an algorithm which can take advantage of the structural and lexical-semantic information in the resource to choose among the possible senses of a target word occurring in context.",1 Introduction,[0.9540232025840943],"['We would like to use the representation of πh, the behavior of which is transparent to human users, in order to understand the behavior of πr (which is in general an uninterpretable learned model); we will do this by inducing bilingual dictionaries that map message vectors zr of πr to natural language strings zh of πh and vice-versa.']"
"For example, a PageRank-based algorithm can be developed to determine the probability of a given sense being reached starting from the senses of its context words.",1 Introduction,[0],[0]
"Recent approaches of this kind have been shown to obtain competitive results (Agirre et al., 2014; Moro et al., 2014).",1 Introduction,[0.9513314713029967],"['DCPs have been shown to solve a variety of coordination problems, including reference games (Lazaridou et al., 2016b), logic puzzles (Foerster et al., 2016), and simple control (Sukhbaatar et al., 2016).']"
"However, due to its inherent nature, knowledge-based WSD tends to adopt bag-of-word approaches which do not exploit the local lexical context of a target word, including function and collocation words, which limits this approach in some cases.
",1 Introduction,[0.9599136446364818],"['More broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (Wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors.']"
"In this paper we get the best of both worlds and present Train-O-Matic, a novel method for generating huge high-quality training sets for all the words in a language’s vocabulary.",1 Introduction,[0],[0]
"The approach is language-independent, thanks to its use of a multilingual knowledge resource, BabelNet (Navigli and Ponzetto, 2012), and it can be applied to any kind of corpus.",1 Introduction,[0],[0]
"The training sets produced with Train-O-Matic are shown to provide competitive performance with those of manually and semi-
78
automatically tagged corpora.",1 Introduction,[0],[0]
"Moreover, state-ofthe-art performance is also reported for low resourced languages (i.e., Italian and Spanish) and domains, where manual training data is not available.",1 Introduction,[0],[0]
"In this Section we present Train-O-Matic, a language-independent approach to the automatic construction of a sense-tagged training set.",2 Building a Training Set from Scratch,[0],[0]
"TrainO-Matic takes as input a corpus C (e.g., Wikipedia) and a semantic network G = (V,E).",2 Building a Training Set from Scratch,[0],[0]
"We assume a WordNet-like structure of G, i.e., V is the set of concepts (i.e., synsets) such that, for each word w in the vocabulary, Senses(w) is the set of vertices in V that are expressed by w, e.g., the WordNet synsets that include w as one of their senses.
",2 Building a Training Set from Scratch,[0],[0]
"Train-O-Matic consists of three steps:
• Lexical profiling: for each vertex in the semantic network, we compute its Personalized PageRank vector, which provides its lexicalsemantic profile (Section 2.1).
",2 Building a Training Set from Scratch,[0],[0]
"• Sentence scoring: For each sentence containing a word w, we compute a probability distribution over all the senses of w based on its context (Section 2.2).
",2 Building a Training Set from Scratch,[0],[0]
"• Sentence ranking and selection: for each sense s of a word w in the vocabulary, we select those sentences that are most likely to use w in the sense of s (Section 2.3).",2 Building a Training Set from Scratch,[0],[0]
In terms of semantic networks the probability of reaching a node v′ starting from v can be interpreted as a measure of relatedness between the synsets v,2.1 Lexical profiling,[0],[0]
and v′.,2.1 Lexical profiling,[0],[0]
"Thus we define the lexical profile of a vertex v in a graph G = (V,E) as the probability distribution over all the vertices v′ in the graph.",2.1 Lexical profiling,[0],[0]
"Such distribution is computed by applying the Personalized PagaRank algorithm, a variant of the traditional PageRank (Brin and Page, 1998).",2.1 Lexical profiling,[0],[0]
"While the latter is equivalent to performing random walks with uniform restart probability on every vertex at each step, PPR, on the other hand, makes the restart probability non-uniform, thereby concentrating more probability mass in the surroundings of those vertices having higher restart
probability.",2.1 Lexical profiling,[0],[0]
"Formally, (P)PR is computed as follows:
v(t+1) =",2.1 Lexical profiling,[0],[0]
"(1− α)v(0) + αMv(t) (1)
where M is the row-normalized adjacency matrix of the semantic network, the restart probability distribution is encoded by vector v(0), and α is the well-known damping factor usually set to 0.85 (Brin and Page, 1998).",2.1 Lexical profiling,[0],[0]
"If we set v(0) to a unit probability vector (0, . . .",2.1 Lexical profiling,[0],[0]
", 0, 1, 0, . . .",2.1 Lexical profiling,[0],[0]
", 0), i.e., restart is always on a given vertex, PPR outputs the probability of reaching every vertex starting from the restart vertex after a certain number of steps.",2.1 Lexical profiling,[0],[0]
"This approach has been used in the literature to create semantic signatures (i.e., profiles) of individual concepts, i.e., vertices of the semantic network (Pilehvar et al., 2013), and then to determine the semantic similarity of concepts.",2.1 Lexical profiling,[0],[0]
"As also done by Pilehvar and Collier (2016), we instead use the PPR vector as an estimate of the conditional probability of a word w′",2.1 Lexical profiling,[0],[0]
"given the target sense1 s ∈ V of word w:
P (w′|s, w) = maxs′∈Senses(w′) vs(s ′)
Z (2)
where Z = ∑
w” P (w”|s, w) is a normalization constant, vs is the vector resulting from an adequate number of random walks used to calculate PPR, and vs(s′) is the vector component corresponding to sense s′.",2.1 Lexical profiling,[0],[0]
"To fix the number of iterations needed to have a sufficiently accurate vector, we follow Lofgren et al. (2014) and set the error δ = 0.00001 and the number of iterations to 1 δ = 100, 000.
",2.1 Lexical profiling,[0],[0]
As a result of this lexical profiling step we have a probability distribution over vocabulary words for each given word sense of interest.,2.1 Lexical profiling,[0],[0]
The objective of the second step is to score the importance of word senses for each of the corpus sentences which contain the word of interest.,2.2 Sentence scoring,[0.9508708561849307],"['While structurally quite similar to the task of machine translation between pairs of human languages, interpretation of neuralese poses a number of novel challenges.']"
Given a sentence σ =,2.2 Sentence scoring,[0],[0]
"w1, w2, . .",2.2 Sentence scoring,[0],[0]
.,2.2 Sentence scoring,[0],[0]
", wn, for a given target wordw in the sentence (w ∈ σ), and for each of its senses s ∈ Senses(w), we compute the probability P (s|σ,w).",2.2 Sentence scoring,[0],[0]
"Thanks to Bayes’ theorem we can determine the probability of sense s of w given the
1Note that we use senses and concepts (synsets) interchangeably, because – given a word – a word sense unambiguously determines a concept (i.e., the synset it is contained in) and vice versa.
sentence as follows:
P (s|σ,w) =",2.2 Sentence scoring,[0],[0]
"P (σ|s, w)P (s|w) P (σ|w) (3)
= P (w1, . . .",2.2 Sentence scoring,[0],[0]
", wn|s, w)P (s|w) P (w1, . . .",2.2 Sentence scoring,[0],[0]
", wn|w) ∝",2.2 Sentence scoring,[0],[0]
"P (w1, . .",2.2 Sentence scoring,[0],[0]
.,2.2 Sentence scoring,[0],[0]
", wn|s, w)P (s|w) (4) ≈ P (w1|s, w) . . .",2.2 Sentence scoring,[0],[0]
"P (wn|s, w)P (s|w)
(5)
where Formula 4 is proportional to the original probability (due to removing the constant in the denominator) and is approximated with Formula 5 due to the assumption of independence of the words in the sentence.",2.2 Sentence scoring,[0],[0]
"P (wi|s, w) is calculated as in Formula 2 and P (s|w) is set to 1/|Senses(w)| (recall that s is a sense of w).",2.2 Sentence scoring,[0],[0]
"For example, given the sentence σ =",2.2 Sentence scoring,[0],[0]
"“A match is a tool for starting a fire”, the target word w = match and its set of senses Smatch = {s1match, s2match}, where s1match is the sense of lighter and s2match is the sense of game match, we want to calculate the probability of each simatch ∈ Smatch of being the correct sense of match in the sentence σ.",2.2 Sentence scoring,[0],[0]
"Following Formula 5 we have:
P (s1match|σ,match)",2.2 Sentence scoring,[0],[0]
"≈ P (tool|s1match,match) · P (start|s1match,match) · P (fire|s1match,match) · P (s1match|match) = 2.1 · 10−4 · 2 · 10−3 · 10−2 · 5 · 10−1 = 2.1 · 10−9
P (s2match|σ,match)",2.2 Sentence scoring,[0],[0]
"≈ P (tool|s2match,match) ·",2.2 Sentence scoring,[0],[0]
"P (start|s2match,match) · P (fire|s2match,match) · P (s2match|match) = 10−5 · 2.9 · 10−4 · 10−6 · 5 · 10−1 = 1.45 · 10−15
As can be seen, the first sense of match has a much higher probability due to its stronger relatedness to the other words in the context (i.e. start, fire and tool).",2.2 Sentence scoring,[0],[0]
Note also that all the probabilities for the second sense are at least one magnitude less than the probability of the first sense.,2.2 Sentence scoring,[0],[0]
"Finally, for a given word w and a given sense s1 ∈ Senses(w), we score each sentence σ in which w appears and s1 is its most likely sense according to a formula that takes into account the difference between the first (i.e., s1) and the second most likely sense of w in σ:
∆s1(σ)",2.3 Sense-based sentence ranking and selection,[0],[0]
"= P (s1|σ,w)− P (s2|σ,w) (6) where s1 = arg maxs∈Senses(w) P (s|σ,w), and s2 = arg maxs∈Senses(w)\{s1} P (s|σ,w).",2.3 Sense-based sentence ranking and selection,[0],[0]
We then sort all sentences based on ∆s1(·) and return a ranked list of sentences where word w is most likely to be sense-annotated with s1.,2.3 Sense-based sentence ranking and selection,[0],[0]
"Although we recognize that other scoring strategies could have been used, this was experimentally the most effective one when compared to alternative strategies, i.e., the sense probability, the number of words related to the target word w, the sentence length or a combination thereof.",2.3 Sense-based sentence ranking and selection,[0],[0]
"In the previous Section we assumed that WordNet was our semantic network, with synsets as vertices and edges represented by its semantic relations.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"However, while its lexical coverage is high, with a rich set of fine-grained synsets, at the relation level WordNet provides mainly paradigmatic information, i.e., relations like hypernymy (is-a) and meronymy (part-of).",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"It lacks, on the other hand, syntagmatic relations, such as those that connect verb synsets to their arguments (e.g., the appropriate senses of eatv and foodn), or pairs of noun synsets (e.g., the appropriate senses of busn and drivern).
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Intuitively, Train-O-Matic would suffer from such a lack of syntagmatic relations, as the relevance of a sense for a given word in a sentence depends directly on the possibility of visiting senses of the other words in the same sentence (cf.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
Formula 5) via random walks as calculated with Formula 1.,3 Creating a Denser and Multilingual Semantic Network,[0],[0]
Such reachability depends on the connections available between synsets.,3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Because syntagmatic relations are sparse in WordNet, if it was used on its own, we would end up with a poor ranking of sentences for any given word sense.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Moreover, even though the methodology presented in Section 2 is languageindependent, Train-O-Matic would lack informa-
tion (e.g. senses for a word in an arbitrary vocabulary) for languages other than English.
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"To cope with these issues, we exploit BabelNet,2 a huge multilingual semantic network obtained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Navigli and Ponzetto, 2012), and create the BabelNet subgraph induced by the WordNet vertices.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to WordNet (such as hyperlinks in a Wikipedia article connecting it to other articles).,3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"The greatest contribution of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related articles (e.g., the English Wikipedia Bus article3 is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Because not all Wikipedia (and other resources’) pages are connected with the same degree of relatedness (e.g., countries are often linked, but they are not necessarily closely related to the source article in which the link occurs), we apply the following weighting strategy to each edge (s, s′) ∈ E of our WordNet-induced subgraph of BabelNet G = (V,E):
w(s, s′) =",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"{ 1 (s, s′) ∈ E(WordNet)",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"WO(s, s′)",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"otherwise
(7) where E(WordNet) is the edge set of the original WordNet graph andWO(s, s′) is the weighted
2http://babelnet.org 3Retrieved on February 3rd, 2017.
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"overlap measure which calculates the similarity between two synsets:
WO(s, s′) = ∑|S|",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"i=1(r 1 i + r
2 i ) −1∑|S|
i=1(2i)−1
where r1i and r 2",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"i are the rankings of the i-th synsets in the set S of the components in common between the vectors associated with s and s′, respectively.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Because at this stage we still have to calculate our synset vector representation, we use the precomputed NASARI vectors (Camacho-Collados et al., 2015) to calculate WO.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"This choice is due to WO’s higher performance over cosine similarity for vectors with explicit dimensions (Pilehvar et al., 2013).
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"As a result, each row of the original adjacency matrix M of G will be replaced with the weights calculated in Formula 7 and then normalized in order to be ready for PPR calculation (see Formula 1).",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"An idea of why a denser semantic network has more useful connections and thus leads to better results is provided by the example in
iment for the animal, and operating system and Windows for the device sense, among others).",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Corpora for sense annotation We used two different corpora to extract sentences: Wikipedia and the United Nations Parallel Corpus (Ziemski et al., 2016).",4 Experimental Setup,[0],[0]
"The first is the largest and most up-to-date encyclopedic resource, containing definitional information, the second, on the other hand, is a public collection of parliamentary documents of the United Nations.",4 Experimental Setup,[0],[0]
"The application of TrainO-Matic to the two corpora produced two senseannotated datasets, which we named T-O-MWiki and T-O-MUN , respectively.
",4 Experimental Setup,[0],[0]
"Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3).
",4 Experimental Setup,[0],[0]
"Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015)",4 Experimental Setup,[0],[0]
WSD datasets.,4 Experimental Setup,[0],[0]
"We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns.
",4 Experimental Setup,[0],[0]
"Comparison sense-annotated corpora To show the impact of our T-O-M corpora in WSD, we compared its performance on the above gold standard datasets, against training with:
• SemCor (Miller et al., 1993), a corpus containing about 226,000 words annotated manually with WordNet senses.
",4 Experimental Setup,[0],[0]
"• One Million Sense-Tagged Instances (Taghipour and Ng, 2015, OMSTI), a sense-annotated dataset obtained via a semi-automatic approach based on the disambiguation of a parallel corpus, i.e., the United Nations Parallel Corpus, performed by exploiting manually translated word senses.",4 Experimental Setup,[0],[0]
"Because OMSTI integrates SemCor
to increase coverage, to keep a level playing field we excluded the latter from the corpus.
",4 Experimental Setup,[0],[0]
"We note that T-O-M, instead, is fully automatic and does not require any WSD-specific human intervention nor any aligned corpus.
Reference system In all our experiments, we used It Makes Sense (Zhong and Ng, 2010, IMS), a state-of-the-art WSD system based on linear Support Vector Machines, as our reference system for comparing its performance when trained on TO-M, against the same WSD system trained on other sense-annotated corpora (i.e., SemCor and OMSTI).",4 Experimental Setup,[0],[0]
"Following the WSD literature, unless stated otherwise, we report performance in terms of F1, i.e., the harmonic mean of precision and recall.
",4 Experimental Setup,[0],[0]
"We note that it is not the purpose of this paper to show that T-O-M, when integrated into IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art.
",4 Experimental Setup,[0],[0]
Baseline,4 Experimental Setup,[0],[0]
"As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet.",4 Experimental Setup,[0],[0]
"The MFS is a very competitive baseline, due to the sense skewness phenomenon in language (Navigli, 2009).
",4 Experimental Setup,[0],[0]
"Number of training sentences per sense Given a target word w, we sorted its senses Senses(w) following the WordNet ordering and selected the top ki training sentences for the i-th sense according to Formula 6, where:
ki = 1 iz ∗K",4 Experimental Setup,[0],[0]
"(8)
with K = 500 and z = 2 which were tuned on a separate small in-house development dataset5.",4 Experimental Setup,[0],[0]
The first result we report regards the impact of vanilla WordNet vs. our WordNet-induced subgraph of BabelNet (WordNetBN ) when calculating PPR vectors.,5.1 Impact of syntagmatic relations,[0],[0]
"As can be seen from Table 2 – which shows the performance of the T-O-MWiki corpora generated with the two semantic networks – using WordNet for PPR computation decreases
550 word-sense pairs annotated manually.
",5.1 Impact of syntagmatic relations,[0],[0]
"the overall performance of IMS from 0.5 to around 4 points across the five datasets, with an overall loss of 1.6 F1 points.",5.1 Impact of syntagmatic relations,[0],[0]
Similar performance losses were observed when using T-O-MUN (see Table 3).,5.1 Impact of syntagmatic relations,[0],[0]
This corroborates our hunch discussed in Section 3 that a resource like BabelNet can contribute important syntagmatic relations that are beneficial for identifying (and ranking high) sentences which are semantically relevant for the target word sense.,5.1 Impact of syntagmatic relations,[0.9507967842373927],"['While this translation criterion directly encodes the semantic notion of meaning described in Section 4, it is doubly intractable: the KL divergence and outer expectation involve a sum over all observations xa and xb respectively; these sums are not in general possible to compute efficiently.']"
"In the following experiments, we report only results using WordNetBN .",5.1 Impact of syntagmatic relations,[0],[0]
"We now move to comparing the performance of T-O-M, which is fully automatic, against corpora which are annotated manually (SemCor) and semi-automatically (OMSTI).",5.2 Comparison against sense-annotated corpora,[0],[0]
"In Table 3 we show the F1-score of IMS on each gold standard dataset in the evaluation framework and on all datasets merged together (last row), when it is trained with the various corpora described above.
",5.2 Comparison against sense-annotated corpora,[0],[0]
"As can be seen, T-O-MWiki and T-O-MUN obtain higher performance than OMSTI (up to 5.5 points above) on 3 out of 5 datasets, and, overall, T-O-MWiki scores 1 point above OMSTI.",5.2 Comparison against sense-annotated corpora,[0],[0]
"The MFS is in the same ballpark as T-O-MWiki, performing better on some datasets and worse on others.",5.2 Comparison against sense-annotated corpora,[0],[0]
We note that IMS trained on T-O-MWiki succeeds in surpassing or obtaining the same results as IMS trained on SemCor on SemEval15 and SemEval-13.,5.2 Comparison against sense-annotated corpora,[0],[0]
"We view this as a significant achievement given the total absence of manual effort involved in T-O-M. Because overall T-O-MWiki outperforms T-O-MUN , in what follows we report all the results with T-O-MWiki, except for the domain-oriented evaluation (see Section 5.4).",5.2 Comparison against sense-annotated corpora,[0],[0]
"IMS uses the MFS as a backoff strategy when no sense can be output for a target word in context (Zhong and Ng, 2010).",5.3 Performance without backoff strategy,[0],[0]
"Consequently, the performance of the MFS is mixed up with that of the SVM classifier.",5.3 Performance without backoff strategy,[0],[0]
"As shown in Table 4, OMSTI is able to provide annotated sentences for roughly half of the tokens in the datasets.",5.3 Performance without backoff strategy,[0],[0]
"Train-O-Matic, on the other hand, is able to cover almost all words in each dataset with at least one training sentence.",5.3 Performance without backoff strategy,[0],[0]
"This means that in around 50% of cases OMSTI gives an answer based on the IMS backoff strategy.
",5.3 Performance without backoff strategy,[0],[0]
"To determine the real impact of the different training data, we therefore decided to perform an additional analysis of the IMS performance when the MFS backoff strategy is disabled.",5.3 Performance without backoff strategy,[0],[0]
"Because we suspected the system would not always return a sense for each target word, in this experiment we measured precision, recall and their harmonic mean, i.e., F1.",5.3 Performance without backoff strategy,[0],[0]
"The results in Table 5 confirm our hunch, showing that OMSTI’s recall drops heavily, thereby affecting F1 considerably.",5.3 Performance without backoff strategy,[0],[0]
"T-O-M performances, instead, remain high in terms of precision, recall and F1.",5.3 Performance without backoff strategy,[0],[0]
"This confirms that OMSTI relies heavily on data (those obtained for the MFS and from SemCor) that are produced manually, rather than semi-automatically.",5.3 Performance without backoff strategy,[0],[0]
"To further inspect the ability of T-O-M to enable disambiguation in different domains, we decided to evaluate on specific documents from the various gold standard datasets which could be clearly assigned a domain label.",5.4 Domain-oriented WSD,[0],[0]
"Specifically, we tested on 13 SemEval-13 documents from various domains6 and 2 SemEval-15 documents (namely, maths & computers, and biomedicine) and carried out two separate tests and evaluations of T-O-M on each domain: once using the MFS backoff strategy, and once not using it.",5.4 Domain-oriented WSD,[0],[0]
"In Tables 6 and 7 we report the results of both T-O-MWiki and T-O-MUN to determine the impact of the corpus type.
",5.4 Domain-oriented WSD,[0],[0]
"As can be seen in the tables, T-O-MWiki systematically attains higher scores than OMSTI (except for the biology domain), and, in most cases, attains higher scores than MFS when the backoff is used, with a drastic, systematic increase over OMSTI with both Train-O-Matic configurations
6Namely biology, climate, finance, health care, politics, social issues and sport.
in recall and F1 when the backoff strategy is disabled.",5.4 Domain-oriented WSD,[0],[0]
"This demonstrates the usefulness of the corpora annotated by Train-O-Matic not only on open text, but also on specific domains.",5.4 Domain-oriented WSD,[0],[0]
"We note that T-O-MUN obtains the best results in the politics domain, which is the closest domain to the UN corpus from which its training sentences are obtained.",5.4 Domain-oriented WSD,[0],[0]
"Experimental Setup In this section we investigate the ability of Train-O-Matic to scale to lowresourced languages, such as Italian and Spanish, for which training data for WSD is not available.
",6 Scaling up to Multiple Languages,[0],[0]
"Thanks to BabelNet, in fact, Train-O-Matic can
be used to generate sense-annotated data for any language supported by the knowledge base.",6 Scaling up to Multiple Languages,[0],[0]
"Thus, in order to build new training datasets for the two languages, we ran Train-O-Matic on their corresponding versions of Wikipedia, then we tuned the two parameters K and z on an in-house development dataset7.",6 Scaling up to Multiple Languages,[0],[0]
"In contrast to the English setting, in order to calculate Formula 8 we sorted the senses of each word by vertex degree.",6 Scaling up to Multiple Languages,[0],[0]
"Finally we used the output data to train IMS.
Results To perform our evaluation we chose the most recent multilingual task (SemEval 2015 task 13) which includes gold data for Italian and Spanish.",6 Scaling up to Multiple Languages,[0],[0]
"As can be seen from Table 8 TrainO-Matic enabled IMS to perform better than the best participating system (Manion and Sainudiin, 2014, SUDOKU) in all three settings (All domains, Maths & Computer and Biomedicine).",6 Scaling up to Multiple Languages,[0],[0]
"Its performance was in fact, 1 to 3 points higher, with a 6-point peak on Maths & Computer in Spanish and on Biomedicine in Italian.",6 Scaling up to Multiple Languages,[0],[0]
This demonstrates the ability of Train-O-Matic to enable supervised WSD systems to surpass state-of-theart knowledge-based WSD approaches in lowresourced languages without relying on manually curated data for training.,6 Scaling up to Multiple Languages,[0],[0]
There are two mainstream approaches to Word Sense Disambiguation: supervised and knowledge-based approaches.,7 Related Work,[0],[0]
"Both suffer in different ways from the so-called knowledge acquisition bottleneck, that is, the difficulty in obtaining an adequate amount of lexical-semantic data: for training in the case of supervised systems, and for enriching semantic networks in the case of knowledge-based ones (Pilehvar and
7We set K = 100 and z",7 Related Work,[0],[0]
"= 2.3 for Spanish and K = 100 and z = 2.5 for Italian.
Navigli, 2014; Navigli, 2009).
",7 Related Work,[0],[0]
"State-of-the-art supervised systems include Support Vector Machines such as IMS (Zhong and Ng, 2010) and, more recently, LSTM neural networks with attention and multitask learning (Raganato et al., 2017b) as well as LSTMs paired with nearest neighbours classification (Melamud et al., 2016; Yuan et al., 2016).",7 Related Work,[0],[0]
The latter also integrates a label propagation algorithm in order to enrich the sense annotated dataset.,7 Related Work,[0],[0]
"The main difference from our approach is its need for a manually annotated dataset to start the label propagation algorithm, whereas Train-O-Matic is fully automatic.",7 Related Work,[0],[0]
"An evaluation against this system would have been interesting, but neither the proprietary training data nor the code are available at the time of writing.
",7 Related Work,[0],[0]
"In order to generalize effectively, these supervised systems require large numbers of training in-
stances annotated with senses for each target word occurrence.",7 Related Work,[0],[0]
"Overall, this amounts to millions of training instances for each language of interest, a number that is not within reach for any language.",7 Related Work,[0],[0]
"In fact, no supervised system has been submitted in major multilingual WSD competitions for languages other than English (Navigli et al., 2013; Moro and Navigli, 2015).",7 Related Work,[0],[0]
"To overcome this problem, new methodologies have recently been developed which aim to create sense-tagged corpora automatically.",7 Related Work,[0],[0]
Raganato et al. (2016) developed 7 heuristics to grow the number of hyperlinks in Wikipedia pages.,7 Related Work,[0],[0]
"Otegi et al. (2016) applied a different disambiguation pipeline for each language to parallel text in Europarl (Koehn, 2005) and QTLeap (Agirre et al., 2015) in order to enrich them with semantic annotations.",7 Related Work,[0],[0]
"Taghipour and Ng (2015), the work closest to ours, exploits the alignment from English to Chinese sentences of
the United Nation Parallel Corpus (Ziemski et al., 2016) to reduce the ambiguity of English words and sense-tag English sentences.",7 Related Work,[0],[0]
The assumption is that the second language is less ambiguous than the first one and that hand-made translations of senses are available for each WordNet synset.,7 Related Work,[0],[0]
"This approach is, therefore, semi-automatic and relies on certain assumptions, in contrast to TrainO-Matic which is, instead, fully automatic and can be applied to any kind of corpus (and language) depending on the specific need.",7 Related Work,[0],[0]
Earlier attempts at the automatic extraction of training samples were made by Agirre and De Lacalle (2004) and Fernández et al. (2004).,7 Related Work,[0],[0]
"Both exploited the monosemous relatives method (Leacock et al., 1998) in order to retrieve sentences from the Web which contained a given monosemous noun or a relative monosemous word (e.g., a synonym, a hypernym, etc.).",7 Related Work,[0],[0]
"As can be seen in (Fernández et al., 2004)",7 Related Work,[0],[0]
"this approach can lead to the retrieval of very accurate examples, but its main drawback lies in the number of senses covered.",7 Related Work,[0],[0]
"In fact, for all those synsets that do not have any monosemous relative, the system is unable to retrieve examples, thus heavily affecting the performance in terms of recall and F1.",7 Related Work,[0],[0]
"Knowledge-based WSD, instead, bypasses the heavy requirement of sense-annotated corpora by applying algorithms that exploit a general-purpose semantic network, such as WordNet, which encodes the relational information that interconnects synsets via different kinds of relation.",7 Related Work,[0],[0]
"Approaches include variants of Personalized PageRank (Agirre et al., 2014) and densest subgraph approximation algorithms (Moro et al., 2014) which, thanks to the availability of multilingual resources such as BabelNet, can easily be extended to perform WSD in arbitrary languages.",7 Related Work,[0],[0]
Other approaches to knowledge-based WSD exploit the definitional knowledge contained in a dictionary.,7 Related Work,[0],[0]
"The Lesk algorithm (Lesk, 1986) and its variants (Banerjee and Pedersen, 2002; Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004) aim to determine the correct sense of a word by comparing each wordsense definition with the context in which the target word appears.",7 Related Work,[0],[0]
"The limit of knowledge-based WSD, however, lies in the absence of mechanisms that can take into account the very local context of a target word occurrence, including non-content words such as prepositions and articles.",7 Related Work,[0],[0]
"Furthermore, recent studies seem to suggest that such
approaches are barely able to surpass supervised WSD systems when they enrich their networks starting from a comparable amount of annotated data (Pilehvar and Navigli, 2014).",7 Related Work,[0],[0]
"With T-O-M, rather than further enriching an existing semantic network, we exploit the information available in the network to annotate raw sentences with sense information and train a state-of-the-art supervised WSD system without task-specific human annotations.",7 Related Work,[0],[0]
"In this paper we presented Train-O-Matic, a novel approach to the automatic construction of large training sets for supervised WSD in an arbitrary language.",8 Conclusion,[0],[0]
"Train-O-Matic removes the burden of manual intervention by leveraging the structural semantic information available in the WordNet graph enriched with additional relational information from BabelNet, and achieves performance competitive to that of semi-automatic approaches and, in some cases, of manually-curated training data.",8 Conclusion,[0],[0]
"T-O-M was shown to provide training data for virtually all the target ambiguous nouns, in marked contrast to alternatives like OMSTI, which covers in many cases around half of the tokens, resorting to the MFS otherwise.",8 Conclusion,[0],[0]
"Moreover Train-O-Matic has proven to scale well to lowresourced languages, for which no manually annotated dataset exists, surpassing the current state of the art of knowledge-based systems.
",8 Conclusion,[0],[0]
"We believe that the ability of T-O-M to overcome the current paucity of annotated data for WSD, coupled with video games with a purpose for validation purposes (Jurgens and Navigli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD.",8 Conclusion,[0],[0]
"All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at http://trainomatic.org.
",8 Conclusion,[0],[0]
"As future work we plan to extend our approach to verbs, adjectives and adverbs.",8 Conclusion,[0],[0]
Following Bennett et al. (2016) we will also experiment on more realistic estimates of P (s|w) in Formula 5 as well as other assumptions made in our work.,8 Conclusion,[0],[0]
The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE,Acknowledgments,[0],[0]
No.,Acknowledgments,[0],[0]
726487.,Acknowledgments,[0],[0]
Annotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation.,abstractText,[0],[0]
"We present Train-O-Matic, a languageindependent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language’s vocabulary.",abstractText,[0],[0]
The approach is fully automatic: no human intervention is required and the only type of human knowledge used is a WordNet-like resource.,abstractText,[0],[0]
"Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation.",abstractText,[0],[0]
All the training data is available for research purposes at http://trainomatic.org.,abstractText,[0],[0]
Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1968–1978 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Neural machine translation has recently become a method of choice in machine translation research.,1 Introduction,[0],[0]
"Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation.",1 Introduction,[0],[0]
"For instance, neural machine translation has successfully proven itself to be capable of
handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015).",1 Introduction,[0],[0]
"Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016).
",1 Introduction,[0],[0]
A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood.,1 Introduction,[0],[0]
"That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus.",1 Introduction,[0.9510003906714999],['Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies: Proposition 2.']
"Once the model is trained in this way, it defines the conditional distribution over all possible translations given a source sentence, and the task of translation becomes equivalent to finding a translation to which the model assigns the highest conditional probability.",1 Introduction,[0],[0]
"Since it is computationally intractable to do so exactly, it is a usual practice to resort to approximate search/decoding algorithms such as greedy decoding or beam search.",1 Introduction,[0],[0]
"In this scenario, we have identified two points where improvements could be made.",1 Introduction,[0],[0]
"They are (1) training (including the selection of a model architecture) and (2) decoding.
",1 Introduction,[0],[0]
"Much of the research on neural machine translation has focused solely on the former, that is, on improving the model architecture.",1 Introduction,[0],[0]
"Neural machine translation started with with a simple encoderdecoder architecture in which a source sentence is encoded into a single, fixed-size vector (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).",1 Introduction,[0],[0]
"It soon evolved with the attention mechanism (Bahdanau et al., 2014).",1 Introduction,[0],[0]
"A few variants of the attention mechanism, or its regularization, have been proposed recently to improve both the translation quality as well as the computational efficiency (Luong et al., 2015b; Cohn et al., 2016; Tu et al., 2016b).",1 Introduction,[0],[0]
"More recently, convolutional net-
1968
works have been adopted either as a replacement of or a complement to a recurrent network in order to efficiently utilize parallel computing (Kalchbrenner et al., 2016; Lee et al., 2016; Gehring et al., 2016).
",1 Introduction,[0],[0]
"On the aspect of decoding, only a few research groups have tackled this problem by incorporating a target decoding algorithm into training.",1 Introduction,[0],[0]
Wiseman and Rush (2016) and Shen et al. (2015) proposed a learning algorithm tailored for beam search.,1 Introduction,[0],[0]
"Ranzato et al. (2015) and (Bahdanau et al., 2016) suggested to use a reinforcement learning algorithm by viewing a neural machine translation model as a policy function.",1 Introduction,[0],[0]
"Investigation on decoding alone has, however, been limited.",1 Introduction,[0],[0]
Cho (2016) showed the limitation of greedy decoding by simply injecting unstructured noise into the hidden state of the neural machine translation system.,1 Introduction,[0],[0]
"Tu et al. (2016a) similarly showed that the exactness of beam search does not correlate well with actual translation quality, and proposed to augment the learning cost function with reconstruction to alleviate this problem.",1 Introduction,[0],[0]
"Li et al. (2016) proposed a modification to the existing beam search algorithm to improve its exploration of the translation space.
",1 Introduction,[0],[0]
"In this paper, we tackle the problem of decoding in neural machine translation by introducing a concept of trainable greedy decoding.",1 Introduction,[0],[0]
"Instead of manually designing a new decoding algorithm suitable for neural machine translation, we propose to learn a decoding algorithm with an arbitrary decoding objective.",1 Introduction,[0],[0]
"More specifically, we introduce a neural-network-based decoding algorithm that works on an already-trained neural machine translation system by observing and manipulating its hidden state.",1 Introduction,[0],[0]
"We treat such a neural network as an agent with a deterministic, continuous action and train it with a variant of the deterministic policy gradient algorithm (Silver et al., 2014).
",1 Introduction,[0],[0]
"We extensively evaluate the proposed trainable greedy decoding on four language pairs (En-Cs, En-De, En-Ru and En-Fi; in both directions) with two different decoding objectives; sentence-level BLEU and negative perplexity.",1 Introduction,[0],[0]
"By training such trainable greedy decoding using deterministic policy gradient with the proposed critic-aware actor learning, we observe that we can improve decoding performance with minimal computational overhead.",1 Introduction,[0],[0]
"Furthermore, the trained actors are found to improve beam search as well, suggesting a future research direction in extending the proposed idea of trainable decoding for more sophisticated
underlying decoding algorithms.",1 Introduction,[0.9549604528868939],"['After introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conventional machine translation approach at recovering the content of message vectors and facilitating collaboration between humans and learned agents.']"
"Neural machine translation is a special case of conditional recurrent language modeling, where the source and target are natural language sentences.",2.1 Neural Machine Translation,[0],[0]
"Let us use X = {x1, . . .",2.1 Neural Machine Translation,[0],[0]
", xTs} and Y = {y1, . . .",2.1 Neural Machine Translation,[0],[0]
", yT } to denote source and target sentences, respectively.",2.1 Neural Machine Translation,[0],[0]
"Neural machine translation then models the target sentence given the source sentence as: p(Y |X) = ∏Tt=1 p(yt|y<t, X).",2.1 Neural Machine Translation,[0],[0]
Each term on the r.h.s.,2.1 Neural Machine Translation,[0],[0]
"of the equation above is modelled as a composite of two parametric functions:
p(yt|y<t, X) ∝",2.1 Neural Machine Translation,[0],[0]
"exp (g (yt, zt; θg)) ,
where zt = f(zt−1, yt−1, et(X; θe); θf ).",2.1 Neural Machine Translation,[0],[0]
"g is a read-out function that transforms the hidden state zt into the distribution over all possible symbols, and f is a recurrent function that compresses all the previous target words y<t and the time-dependent representation et(X; θe) of the source sentence X .",2.1 Neural Machine Translation,[0],[0]
"This time-dependent representation et is often implemented as a recurrent network encoder of the source sentence coupled with an attention mechanism (Bahdanau et al., 2014).
",2.1 Neural Machine Translation,[0],[0]
"Maximum Likelihood Learning We train a neural machine translation model, or equivalently estimate the parameters θg, θf and θe, by maximizing the log-probability of a reference translation Ŷ = {ŷ1, ..., ŷT } given a source sentence.",2.1 Neural Machine Translation,[0],[0]
"That is, we maximize the log-likelihood function:
JML(θg, θf , θe) = 1 N N∑ n=1",2.1 Neural Machine Translation,[0],[0]
"Tn∑ t=1 log pθ(ŷnt |ŷn<t, Xn),
given a training set consisting of N source-target sentence pairs.",2.1 Neural Machine Translation,[0],[0]
It is important to note that this maximum likelihood learning does not take into account how a trained model would be used.,2.1 Neural Machine Translation,[0],[0]
"Rather, it is only concerned with learning a distribution over all possible translations.",2.1 Neural Machine Translation,[0],[0]
"Once the model is trained, either by maximum likelihood learning or by any other recently proposed algorithms (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2015), we can let the model translate a given sentence by
finding a translation that maximizes
Ŷ = arg max Y
log pθ(Y |X),
where θ = (θg, θf , θe).",2.2 Decoding,[0],[0]
"This is, however, computationally intractable, and it is a usual practice to resort to approximate decoding algorithms.
",2.2 Decoding,[0],[0]
Greedy Decoding One such approximate decoding algorithm is greedy decoding.,2.2 Decoding,[0],[0]
"In greedy decoding, we follow the conditional dependency path and pick the symbol with the highest conditional probability so far at each node.",2.2 Decoding,[0],[0]
This is equivalent to picking the best symbol one at a time from left to right in conditional language modelling.,2.2 Decoding,[0],[0]
"A decoded translation of greedy decoding is Ŷ = (ŷ1, . . .",2.2 Decoding,[0],[0]
", ŷT ), where
ŷt = arg max y∈V
log pθ(y|ŷ<t, X).",2.2 Decoding,[0],[0]
"(1)
Despite its preferable computational complexity O(|V | × T ), greedy decoding has been over time found to be undesirably sub-optimal.
",2.2 Decoding,[0],[0]
"Beam Search Beam search keeps K > 1 hypotheses, unlike greedy decoding which keeps only a single hypothesis during decoding.",2.2 Decoding,[0],[0]
"At each time step t, beam search picks K hypotheses with the highest scores ( ∏t t′=1 p(yt|y<t, X)).",2.2 Decoding,[0],[0]
"When all the hypotheses terminate (outputting the end-of-thesentence symbol), it returns the hypothesis with the highest log-probability.",2.2 Decoding,[0],[0]
"Despite its superior performance compared to greedy decoding, the computational complexity grows linearly w.r.t.",2.2 Decoding,[0],[0]
"the size of beam K, which makes it less preferable especially in the production environment.",2.2 Decoding,[0],[0]
"Although we have described decoding in neural machine translation as a maximum-a-posteriori estimation in log p(Y |X), this is not necessarily the only nor the desirable decoding objective.
",3.1 Many Decoding Objectives,[0],[0]
"First, each potential scenario in which neural machine translation is used calls for a unique decoding objective.",3.1 Many Decoding Objectives,[0],[0]
"In simultaneous translation/interpretation, which has recently been studied in the context of neural machine translation (Gu et al., 2016), the decoding objective is formulated as a trade-off between the translation quality and delay.",3.1 Many Decoding Objectives,[0],[0]
"On the other hand, when a machine translation system is used as a part of a larger information
extraction system, it is more important to correctly translate named entities and events than to translate syntactic function words.",3.1 Many Decoding Objectives,[0],[0]
"The decoding objective in this case must account for how the translation is used in subsequent modules in a larger system.
",3.1 Many Decoding Objectives,[0],[0]
"Second, the conditional probability assigned by a trained neural machine translation model does not necessarily reflect our perception of translation quality.",3.1 Many Decoding Objectives,[0],[0]
"Although Cho (2016) provided empirical evidence of high correlation between the logprobability and BLEU, a de facto standard metric in machine translation, there have also been reports on large mismatch between the log-probability and BLEU.",3.1 Many Decoding Objectives,[0],[0]
"For instance, Tu et al. (2016a) showed that beam search with a very large beam, which is supposed to find translations with better logprobabilities, suffers from pathological translations of very short length, resulting in low translation quality.",3.1 Many Decoding Objectives,[0],[0]
"This calls for a way to design or learn a decoding algorithm with an objective that is more directly correlated to translation quality.
",3.1 Many Decoding Objectives,[0],[0]
"In short, there is a significant need for designing multiple decoding algorithms for neural machine translation, regardless of how it was trained.",3.1 Many Decoding Objectives,[0],[0]
It is however non-trivial to manually design a new decoding algorithm with an arbitrary objective.,3.1 Many Decoding Objectives,[0],[0]
"This is especially true with neural machine translation, as the underlying structure of the decoding/search process – the high-dimensional hidden state of a recurrent network – is accessible but not interpretable.",3.1 Many Decoding Objectives,[0],[0]
"Instead, in the remainder of this section, we propose our approach of trainable greedy decoding.",3.1 Many Decoding Objectives,[0],[0]
"We start from the noisy, parallel approximate decoding (NPAD) algorithm proposed in (Cho, 2016).",3.2 Trainable Greedy Decoding,[0],[0]
The main idea behind NPAD algorithm is that a better translation with a higher log-probability may be found by injecting unstructured noise in the transition function of a recurrent network.,3.2 Trainable Greedy Decoding,[0],[0]
"That is,
zt = f(zt−1 + t, yt−1, et(X; θe); θf ),
where t ∼ N (0, (σ0/t)2).",3.2 Trainable Greedy Decoding,[0],[0]
NPAD avoids potential degradation of translation quality by running such a noisy greedy decoding process multiple times in parallel.,3.2 Trainable Greedy Decoding,[0],[0]
"An important lesson of NPAD algorithm is that there exists a decoding strategy with the asymptotically same computational complexity that results in a better translation quality, and that such a better translation can be found by manipulating the hidden state of the recurrent network.
",3.2 Trainable Greedy Decoding,[0.9586938873207865],"['Based on this intuition, we introduce a translation criterion that matches neuralese messages with natural language strings by minimizing statistical distance in a common representation space of distributions over speaker states.']"
"In this work, we propose to significantly extend NPAD by replacing the unstructured noise t with a parametric function approximator, or an agent, πφ.",3.2 Trainable Greedy Decoding,[0],[0]
"This agent takes as input the previous hidden state zt−1, previously decoded word ŷt−1 and the time-dependent context vector et(X; θe) and outputs a real-valued vectorial action at ∈ Rdim(zt).",3.2 Trainable Greedy Decoding,[0],[0]
"Such an agent is trained such that greedy decoding with the agent finds a translation that maximizes any predefined, arbitrary decoding objective, while the underlying neural machine translation model is pretrained and fixed.",3.2 Trainable Greedy Decoding,[0],[0]
"Once the agent is trained, we generate a translation given a source sentence by greedy decoding however augmented with this agent.",3.2 Trainable Greedy Decoding,[0],[0]
"We call this decoding strategy trainable greedy decoding.
",3.2 Trainable Greedy Decoding,[0],[0]
Related Work:,3.2 Trainable Greedy Decoding,[0],[0]
"Soothsayer prediction function Independently from and concurrently with our work here, Li et al. (2017) proposed, just two weeks earlier, to train a neural network that predicts an arbitrary decoding objective given a source sentence and a partial hypothesis, or a prefix of translation, and to use it as an auxiliary score in beam search.",3.2 Trainable Greedy Decoding,[0],[0]
"For training such a network, referred to as a Q network in their paper, they generate each training example by either running beam search or using a ground-truth translation (when appropriate) for each source sentence.",3.2 Trainable Greedy Decoding,[0],[0]
"This approach allows one to use an arbitrary decoding objective, but it still re-
lies heavily on the log-probability of the underlying neural translation system in actual decoding.",3.2 Trainable Greedy Decoding,[0],[0]
We expect a combination of these and our approaches may further improve decoding for neural machine translation in the future.,3.2 Trainable Greedy Decoding,[0],[0]
"While all the parameters—θg, θf and θe— of the underlying neural translation model are fixed, we only update the parameters φ of the agent π.",3.3 Learning and Challenges,[0],[0]
"This ensures the generality of the pretrained translation model, and allows us to train multiple trainable greedy decoding agents with different decoding objectives, maximizing the utility of a single trained translation model.
",3.3 Learning and Challenges,[0.9512792819892479],"['The evaluation selects a full game trace from a human player, and replays both the human’s actions and messages exactly (disregarding any incoming messages); the evaluation measures the quality of the natural-language-toneuralese translator, and the extent to which the learned agent model can accommodate a (real) human given translations of the human’s messages.']"
Let us denote by R our arbitrary decoding objective as a function that scores a translation generated from trainable greedy decoding.,3.3 Learning and Challenges,[0],[0]
"Then, our learning objective for trainable greedy decoding is
JA(φ) = EŶ=Gπ(X)X∼D",3.3 Learning and Challenges,[0],[0]
"[ R(Ŷ ) ] ,
where we used Gπ(X) as a shorthand for trainable greedy decoding with an agent π.
",3.3 Learning and Challenges,[0],[0]
There are two major challenges in learning an agent with such an objective.,3.3 Learning and Challenges,[0],[0]
"First, the decoding objective R may not be differentiable with respect to the agent.",3.3 Learning and Challenges,[0],[0]
"Especially because our goal is to accommodate an arbitrary decoding objective, this becomes a problem.",3.3 Learning and Challenges,[0],[0]
"For instance, BLEU, a standard
quality metric in machine translation, is a piecewise linear function with zero derivatives almost everywhere.",3.3 Learning and Challenges,[0],[0]
"Second, the agent here is a real-valued, deterministic policy with a very high-dimensional action space (1000s of dimensions), which is well known to be difficult.",3.3 Learning and Challenges,[0],[0]
"In order to alleviate these difficulties, we propose to use a variant of the deterministic policy gradient algorithm (Silver et al., 2014; Lillicrap et al., 2015).",3.3 Learning and Challenges,[0],[0]
"It is highly unlikely for us to have access to the gradient of an arbitrary decoding objective R with respect to the agent π, or its parameters φ.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Furthermore, we cannot estimate it stochastically because our policy π is defined to be deterministic without a predefined nor learned distribution over the action.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Instead, following (Silver et al., 2014; Lillicrap et al., 2015), we use a parametric, differentiable approximator, called a critic Rc, for the non-differentiable objective R. We train the critic by minimizing
JC(ψ)",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
= EŶ=Gπ(X)X∼D [ Rcψ(z1:T )−R(Ŷ ),4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"]2 .
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"The critic observes the state-action sequence of the agent π via the modified hidden states (z1, . . .",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
", zT )",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"of the recurrent network, and predicts the associated decoding objective.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"By minimizing the mean squared error above, we effectively encourage the critic to approximate the non-differentiable objective as closely as possible in the vicinity of the state-action sequence visited by the agent.
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"We implement the critic Rc as a recurrent network, similarly to the underlying neural machine translation system.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"This implies that we can compute the derivative of the predicted decoding objective with respect to the input, that is, the state-action sequence z1:T , which allows us to update the actor π, or equivalently its parameters φ, to maximize the predicted decoding objective.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Effectively we avoid the issue of non-differentiability of the original decoding objective by working with its proxy.
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"With the critic, the learning objective of the actor is now to maximize not the original decoding objective R but its proxy RC such that
ĴA(φ)",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
= EŶ=Gπ(X)X∼D,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"[ RC(Ŷ ) ] .
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Algorithm 1 Trainable Greedy Decoding Require: NMT θ, actor φ, critic ψ, Nc, Na, Sc, Sa, τ
1: Train θ using MLE on training set D; 2: Initialize φ and ψ; 3:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
Shuffle D twice into Dφ and Dψ 4: while stopping criterion is not met do 5: for t = 1 :,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Nc do 6: Draw a translation pair: (X,Y ) ∼ Dψ; 7: r, rc = DECODE(Sc, X, Y, 1) 8:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
Update ψ using∇ψ ∑ k (r c,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
k,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"− rk)2/(Sc + 1)
9: for t = 1 : Na do 10: Draw a translation pair: (X,Y ) ∼ Dφ; 11: r, rc = DECODE(Sa, X, Y, 0) 12: Compute wk = exp
(− (rck − rk)2 /τ) 13: Compute w̃k = wk/ ∑ k wk
14: Update φ using −∑k",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
(w̃k · ∇φrck),4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Function: DECODE(S,X, Y, c)
1: Ys = {}, Zs = {}, r = {}, rc = {}; 2: for k = 1 : S do 3: Sample noise ∼ N (0, σ2) for each action; 4: Greedy decoding Ŷ k = Gθ,φ(X) with ; 5: Collect hidden states zk1:T given X , Ŷ , θ, φ 6: Ys ← Ys ∪ {Y k} 7:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Zs ← Zs ∪ {zk1:T } 8: if c = 1 then 9: Collect hidden states z1:T given X , Y , θ
10: Ys ← Ys ∪ {Y } 11:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Zs ← Zs ∪ {z1:T } 12: for Ŷ , Z ∈ Ys, Zs do 13: Compute the critic output rc ← Rcψ(Z, Ŷ ) 14: Compute true reward r ← R(Y, Ŷ ) 15: return r, rc
Unlike the original objective, this objective function is fully differentiable with respect to the agent π.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"We thus use a usual stochastic gradient descent algorithm to train the agent, while simultaneously training the critic.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
We do so by alternating between training the actor and critic.,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Note that we maximize the return of a full episode rather than the Q value, unlike usual approaches in reinforcement learning.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
Challenges The most apparent challenge for training such a deterministic actor with a large action space is that most of action configurations will lead to zero return.,4.2 Critic-Aware Actor Learning,[0],[0]
It is also not trivial to devise an efficient exploration strategy with a deterministic actor with real-valued actions.,4.2 Critic-Aware Actor Learning,[0],[0]
"This issue has however turned out to be less of a problem than in a usual reinforcement learning setting, as the state and action spaces are well structured thanks to pretraining by maximum likelihood learning.",4.2 Critic-Aware Actor Learning,[0],[0]
"As observed by Cho (2016), any reasonable perturbation to the hidden state of the recurrent network generates a reasonable translation which would re-
ceive again a reasonable return.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Although this property of dense reward makes the problem of trainable greedy decoding more manageable, we have observed other issues during our preliminary experiment with the vanilla deterministic policy gradient.",4.2 Critic-Aware Actor Learning,[0.9535532855547516],"['While our evaluation has focused on understanding the behavior of deep communicating policies, the framework proposed in this paper could be much more generally applied.']"
"In order to avoid these issues that caused instability, we propose the following modifications to the vanilla algorithm.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Critic-Aware Actor Learning A major goal of the critic is not to estimate the return of a given episode, but to estimate the gradient of the return evaluated given an episode.",4.2 Critic-Aware Actor Learning,[0],[0]
"In order to do so, the critic must be trained, or presented, with stateaction sequences z1:T ′ similar though not identical to the state-action sequence generated by the current actor π.",4.2 Critic-Aware Actor Learning,[0],[0]
"This is achieved, in our case, by injecting unstructured noise to the action at each
time step, similar to (Heess et al., 2015):
ãt = φ(zt, at−1) + σ · , (2)
where is a zero-mean, unit-variance normal variable.",4.2 Critic-Aware Actor Learning,[0],[0]
"This noise injection procedure is mainly used when training the critic.
",4.2 Critic-Aware Actor Learning,[0],[0]
We have however observed that the quality of the reward and its gradient estimate of the critic is very noisy even when the critic was trained with this kind of noisy actor.,4.2 Critic-Aware Actor Learning,[0],[0]
This imperfection of the critic often led to the instability in training the actor in our preliminary experiments.,4.2 Critic-Aware Actor Learning,[0],[0]
"In order to avoid this, we describe here a technique which we refer to as critic-aware actor gradient estimation.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Instead of using the point estimate ∂R c
∂φ of the gradient of the predicted objective with respect to the actor’s parameters φ, we propose to use the expected gradient of the predicted objective with
respect to the critic-aware distribution Q.",4.2 Critic-Aware Actor Learning,[0],[0]
"That is,
EQ [ ∂Rcψ ∂φ ] , (3)
where we define the critic-aware distribution Q as
Q( ) ∝",4.2 Critic-Aware Actor Learning,[0],[0]
exp(−(Rcψ −R)2/τ︸,4.2 Critic-Aware Actor Learning,[0],[0]
︷︷ ︸,4.2 Critic-Aware Actor Learning,[0],[0]
"Critic-awareness
) exp(− 2
2σ2︸ ︷︷ ︸ Locality ).",4.2 Critic-Aware Actor Learning,[0],[0]
"(4)
",4.2 Critic-Aware Actor Learning,[0],[0]
"This expectation allows us to incorporate the noisy, non-uniform nature of the critic’s approximation of the objective by up-weighting the gradient computed at a point with a higher critic quality and down-weighting the gradient computed at a point with a lower critic quality.",4.2 Critic-Aware Actor Learning,[0],[0]
"The first term in Q reflects this, while the second term ensures that our estimation is based on a small region around the state-action sequence generated by the current, noise-free actor π.
",4.2 Critic-Aware Actor Learning,[0],[0]
Since it is intractable to compute Eq.,4.2 Critic-Aware Actor Learning,[0],[0]
"(3) exactly, we resort to importance sampling with the proposed distribution equal to the second term in Eq.",4.2 Critic-Aware Actor Learning,[0],[0]
(4).,4.2 Critic-Aware Actor Learning,[0],[0]
"Then, our gradient estimate for the actor becomes the sum of the gradients from multiple realizations of the noisy actor in Eq.",4.2 Critic-Aware Actor Learning,[0],[0]
"(2), where each gradient is weighted by the quality of the critic exp(−(Rcφ − R)2/τ).",4.2 Critic-Aware Actor Learning,[0],[0]
τ is a hyperparameter that controls the smoothness of the weights.,4.2 Critic-Aware Actor Learning,[0],[0]
"We observed in our preliminary experiment that the use of this criticaware actor learning significantly stabilizes general learning of both the actor and critic.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Reference Translations for Training the Critic In our setting of neural machine translation, we have access to a reference translation for each source sentence X , unlike in a usual setting of reinforcement learning.",4.2 Critic-Aware Actor Learning,[0],[0]
"By force-feeding the reference translation into the underlying neural machine translation system (rather than feeding the decoded symbols), we can generate the reference state-action sequence.",4.2 Critic-Aware Actor Learning,[0],[0]
"This sequence is much less correlated with those sequences generated by the actor, and facilitates computing a better estimate of the gradient w.r.t.",4.2 Critic-Aware Actor Learning,[0],[0]
"the critic.
",4.2 Critic-Aware Actor Learning,[0],[0]
"In Alg. 1, we present the complete algorithm.",4.2 Critic-Aware Actor Learning,[0],[0]
"To make the description less cluttered, we only show the version of minibatch size = 1 which can be naturally extended.",4.2 Critic-Aware Actor Learning,[0],[0]
We also illustrate the proposed trainable greedy decoding and the proposed learning strategy in Fig. 1.,4.2 Critic-Aware Actor Learning,[0],[0]
"We empirically evaluate the proposed trainable greedy decoding on four language pairs – EnDe, En-Ru, En-Cs and En-Fi – using a standard attention-based neural machine translation system (Bahdanau et al., 2014).",5 Experimental Settings,[0],[0]
We train underlying neural translation systems using the parallel corpora made available from WMT’15.1 The same set of corpora are used for trainable greedy decoding as well.,5 Experimental Settings,[0],[0]
"All the corpora are tokenized and segmented into subword symbols using byte-pair encoding (BPE) (Sennrich et al., 2015).",5 Experimental Settings,[0],[0]
We use sentences of length up to 50 subword symbols for MLE training and 200 symbols for trainable decoding.,5 Experimental Settings,[0],[0]
"For validation and testing, we use newstest-2013 and newstest-2015, respectively.",5 Experimental Settings,[0],[0]
"Underlying NMT Model For each language pair, we implement an attention-based neural machine translation model whose encoder and decoder recurrent networks have 1,028 gated recurrent units (GRU, Cho et al., 2014) each.",5.1 Model Architectures and Learning,[0],[0]
Source and target symbols are projected into 512-dimensional embedding vectors.,5.1 Model Architectures and Learning,[0],[0]
"We trained each model for approximately 1.5 weeks using Adadelta (Zeiler, 2012).
",5.1 Model Architectures and Learning,[0],[0]
Actor π,5.1 Model Architectures and Learning,[0],[0]
We use a feedforward network with a single hidden layer as the actor.,5.1 Model Architectures and Learning,[0],[0]
"The input is a 2,056-dimensional vector which is the concatenation of the decoder hidden state and the timedependent context vector from the attention mech-
1http://www.statmt.org/wmt15/
anism, and it outputs a 1,028-dimensional action vector for the decoder.",5.1 Model Architectures and Learning,[0],[0]
"We use 32 units for the hidden layer with tanh activations.
",5.1 Model Architectures and Learning,[0],[0]
Critic Rc The critic is implemented as a variant of an attention-based neural machine translation model that takes a reference translation as a source sentence and a state-action sequence from the actor as a target sentence.,5.1 Model Architectures and Learning,[0],[0]
Both the size of GRU units and embedding vectors are the same with the underlying model.,5.1 Model Architectures and Learning,[0],[0]
"Unlike a usual neural machine translation system, the critic does not language-model the target sentence but simply outputs a scalar value to predict the true return.",5.1 Model Architectures and Learning,[0],[0]
"When we predict a bounded return, such as sentence BLEU, we use a sigmoid activation at the output.",5.1 Model Architectures and Learning,[0],[0]
"For other unbounded return like perplexity, we use a linear activation.
",5.1 Model Architectures and Learning,[0],[0]
Learning We train the actor and critic simultaneously by alternating between updating the actor and critic.,5.1 Model Architectures and Learning,[0],[0]
"As the quality of the critic’s approximation of the decoding objective has direct influence on the actor’s learning, we make ten updates to the critic before each time we update the actor once.",5.1 Model Architectures and Learning,[0],[0]
"We use RMSProp (Tieleman and Hinton, 2012) with the initial learning rates of 2× 10−6 and 2× 10−4, respectively, for the actor and critic.
",5.1 Model Architectures and Learning,[0],[0]
We monitor the progress of learning by measuring the decoding objective on the validation set.,5.1 Model Architectures and Learning,[0],[0]
"After training, we pick the actor that results in the best decoding objective on the validation set, and test it on the test set.
",5.1 Model Architectures and Learning,[0],[0]
"Decoding Objectives For each neural machine translation model, pretrained using maximum likelihood criterion, we train two trainable greedy decoding actors.",5.1 Model Architectures and Learning,[0],[0]
"One actor is trained to maximize BLEU (or its smoothed version for sentence-level
scoring (Lin and Och, 2004))",5.1 Model Architectures and Learning,[0],[0]
"as its decoding objective, and the other to minimize perplexity (or equivalently the negative log-probability normalized by the length.)
",5.1 Model Architectures and Learning,[0],[0]
We have chosen the first two decoding objectives for two purposes.,5.1 Model Architectures and Learning,[0],[0]
"First, we demonstrate that it is possible to build multiple trainable decoders with a single underlying model trained using maximum likelihood learning.",5.1 Model Architectures and Learning,[0],[0]
"Second, the comparison between these two objectives provides a glimpse into the relationship between BLEU (the most widely used automatic metric for evaluating translation systems) and log-likelihood (the most widely used learning criterion for neural machine translation).
",5.1 Model Architectures and Learning,[0],[0]
Evaluation We test the trainable greedy decoder with both greedy decoding and beam search.,5.1 Model Architectures and Learning,[0],[0]
"Although our decoder is always trained with greedy decoding, beam search in practice can be used together with the actor of the trainable greedy decoder.",5.1 Model Architectures and Learning,[0],[0]
Beam search is expected to work better especially when our training of the trainable greedy decoder is unlikely to be optimal.,5.1 Model Architectures and Learning,[0],[0]
"In both cases, we report both the perplexity and BLEU.",5.1 Model Architectures and Learning,[0],[0]
We present the improvements of BLEU and perplexity (or its negation) in Fig. 2 for all the language pair-directions.,5.2 Results and Analysis,[0],[0]
It is clear from these plots that the best result is achieved when the trainable greedy decoder was trained to maximize the target decoding objective.,5.2 Results and Analysis,[0],[0]
"When the decoder was trained to maximize sentence-level BLEU, we see the improvement in BLEU but often the degradation in the perplexity (see the left plots in Fig. 2.)",5.2 Results and Analysis,[0],[0]
"On the other hand, when the actor was trained to minimize the perplexity, we only see the improvement in per-
plexity (see the right plots in Fig. 2.)",5.2 Results and Analysis,[0],[0]
"This confirms our earlier claim that it is necessary and desirable to tune for the target decoding objective regardless of what the underlying translation system was trained for, and strongly supports the proposed idea of trainable decoding.
",5.2 Results and Analysis,[0],[0]
"The improvement from using the proposed trainable greedy decoding is smaller when used together with beam search, as seen in Fig. 2 (b).",5.2 Results and Analysis,[0],[0]
"However, we still observe statistically significant improvement in terms of BLEU (marked with red stars.)",5.2 Results and Analysis,[0],[0]
"This suggests a future direction in which we extend the proposed trainable greedy decoding to directly incorporate beam search into its training procedure to further improve the translation quality.
",5.2 Results and Analysis,[0],[0]
It is worthwhile to note that we achieved all of these improvements with negligible computational overhead.,5.2 Results and Analysis,[0],[0]
"This is due to the fact that our actor is a very small, shallow neural network, and that the more complicated critic is thrown away after training.",5.2 Results and Analysis,[0],[0]
We suspect the effectiveness of such a small actor is due to the well-structured hidden state space of the underlying neural machine translation model which was trained with a large amount of parallel corpus.,5.2 Results and Analysis,[0],[0]
"We believe this favourable computational complexity makes the proposed method suitable for production-grade neural machine translation (Wu et al., 2016; Crego et al., 2016).
",5.2 Results and Analysis,[0],[0]
"Importance of Critic-Aware Actor Learning In Fig. 3, we show sample learning curves with and without the proposed critic-aware actor learning.",5.2 Results and Analysis,[0],[0]
Both curves were from the models trained under the same condition.,5.2 Results and Analysis,[0],[0]
"Despite a slower start in the early stage of learning, we see that the critic-aware actor learning has greatly stabilized the learning progress.",5.2 Results and Analysis,[0],[0]
"We emphasize that we would not have been able to train all these 16 actors without the proposed critic-aware actor learning.
",5.2 Results and Analysis,[0],[0]
"Examples In Fig. 4, we present three examples from Ru-En.",5.2 Results and Analysis,[0],[0]
"We defined the influence as the KL divergence between the conditional distributions without the trainable greedy decoding and with the trainable greedy decoding, assuming the fixed previous hidden state and target symbol.",5.2 Results and Analysis,[0],[0]
"We colored a target word with magenta, when the influence of the trainable greedy decoding is large (> 0.001).",5.2 Results and Analysis,[0],[0]
Manual inspection of these examples as well as others has revealed that the trainable greedy decoder focuses on fixing prepositions and removing any unnecessary symbol generation.,5.2 Results and Analysis,[0],[0]
"More in-depth
analysis is however left as future work.",5.2 Results and Analysis,[0],[0]
We proposed trainable greedy decoding as a way to learn a decoding algorithm for neural machine translation with an arbitrary decoding objective.,6 Conclusion,[0],[0]
"The proposed trainable greedy decoder observes and manipulates the hidden state of a trained neural translation system, and is trained by a novel variant of deterministic policy gradient, called critic-aware actor learning.",6 Conclusion,[0],[0]
Our extensive experiments on eight language pair-directions and two objectives confirmed its validity and usefulness.,6 Conclusion,[0],[0]
"The proposed trainable greedy decoding is a generic idea that can be applied to any recurrent language modeling, and we anticipate future research both on the fundamentals of the trainable decoding as well as on the applications to more diverse tasks such as image caption generating and dialogue modeling.",6 Conclusion,[0],[0]
"KC thanks the support by TenCent, eBay, Facebook, Google (Google Faculty Award 2016) and NVidia.",Acknowledgement,[0],[0]
This work was partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI).,Acknowledgement,[0],[0]
"We sincerely thank Martin Arjovsky, Zihang Dai, Graham Neubig, Pengcheng Yin and Chunting Zhou for helpful discussions and insightful feedbacks.",Acknowledgement,[0],[0]
Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-toend learning algorithms.,abstractText,[0],[0]
"The problem of decoding, however, has received relatively little attention from the research community.",abstractText,[0],[0]
"In this paper, we solely focus on the problem of decoding given a trained neural machine translation model.",abstractText,[0],[0]
"Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective.",abstractText,[0],[0]
"More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient.",abstractText,[0],[0]
"We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.",abstractText,[0],[0]
Trainable Greedy Decoding for Neural Machine Translation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1884–1895 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1884",text,[0],[0]
"The standard protocol for obtaining a labeled dataset is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification).",1 Introduction,[0],[0]
"However, this only provides one bit of information per example.",1 Introduction,[0],[0]
"This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example?
",1 Introduction,[0],[0]
"Previous works have relied on identifying relevant parts of the input such as labeling features (Druck et al., 2009; Raghavan et al., 2005; Liang et al., 2009), highlighting rationale phrases in
text (Zaidan and Eisner, 2008; Arora and Nyberg, 2009), or marking relevant regions in images (Ahn et al., 2006).",1 Introduction,[0],[0]
"But there are certain types of information which cannot be easily reduced to annotating a portion of the input, such as the absence of a certain word, or the presence of at least two words.",1 Introduction,[0],[0]
"In this work, we tap into the power of natural language and allow annotators to provide supervision to a classifier via natural language explanations.
",1 Introduction,[0],[0]
"Specifically, we propose a framework in which annotators provide a natural language explanation for each label they assign to an example (see Figure 1).",1 Introduction,[0],[0]
"These explanations are parsed into logical forms representing labeling functions (LFs), functions that heuristically map examples to labels (Ratner et al., 2016).",1 Introduction,[0],[0]
"The labeling functions are
then executed on many unlabeled examples, resulting in a large, weakly-supervised training set that is then used to train a classifier.
",1 Introduction,[0],[0]
"Semantic parsing of natural language into logical forms is recognized as a challenging problem and has been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Liang, 2016).",1 Introduction,[0],[0]
"One of our major findings is that in our setting, even a simple rule-based semantic parser suffices for three reasons: First, we find that the majority of incorrect LFs can be automatically filtered out either semantically (e.g., is it consistent with the associated example?) or pragmatically (e.g., does it avoid assigning the same label to the entire training set?).",1 Introduction,[0],[0]
"Second, LFs near the gold LF in the space of logical forms are often just as accurate (and sometimes even more accurate).",1 Introduction,[0],[0]
"Third, techniques for combining weak supervision sources are built to tolerate some noise (Alfonseca et al., 2012; Takamatsu et al., 2012; Ratner et al., 2018).",1 Introduction,[0],[0]
The significance of this is that we can deploy the same semantic parser across tasks without task-specific training.,1 Introduction,[0],[0]
"We show how we can tackle a real-world biomedical application with the same semantic parser used to extract instances of spouses.
",1 Introduction,[0],[0]
"Our work is most similar to that of Srivastava et al. (2017), who also use natural language explanations to train a classifier, but with two important differences.",1 Introduction,[0],[0]
"First, they jointly train a task-specific semantic parser and classifier, whereas we use a
simple rule-based parser.",1 Introduction,[0],[0]
"In Section 4, we find that in our weak supervision framework, the rule-based semantic parser and the perfect parser yield nearly identical downstream performance.",1 Introduction,[0.9508263830588626],"['In this paper, we focus our experiments specifically on the problem of interpreting communication in deep policies, and apply our approach to the driving game in Figure 1 and two reference games of the kind shown in Figure 2.']"
"Second, while they use the logical forms of explanations to produce features that are fed directly to a classifier, we use them as functions for labeling a much larger training set.",1 Introduction,[0],[0]
"In Section 4, we show that using functions yields a 9.5 F1 improvement (26% relative improvement) over features, and that the F1 score scales with the amount of available unlabeled data.
",1 Introduction,[0],[0]
We validate our approach on two existing datasets from the literature (extracting spouses from news articles and disease-causing chemicals from biomedical abstracts) and one real-world use case with our biomedical collaborators at OccamzRazor to extract protein-kinase interactions related to Parkinson’s disease from text.,1 Introduction,[0],[0]
We find empirically that users are able to train classifiers with comparable F1 scores up to two orders of magnitude faster when they provide natural language explanations instead of individual labels.,1 Introduction,[0],[0]
"Our code and data can be found at https:// github.com/HazyResearch/babble.
2",1 Introduction,[0],[0]
"The BabbleLabble Framework
The BabbleLabble framework converts natural language explanations and unlabeled data into a noisily-labeled training set (see Figure 2).",1 Introduction,[0],[0]
"There are three key components: a semantic parser, a filter bank, and a label aggregator.",1 Introduction,[0],[0]
"The semantic
parser converts natural language explanations into a set of logical forms representing labeling functions (LFs).",1 Introduction,[0],[0]
The filter bank removes as many incorrect LFs as possible without requiring ground truth labels.,1 Introduction,[0],[0]
The remaining LFs are applied to unlabeled examples to produce a matrix of labels.,1 Introduction,[0],[0]
"This label matrix is passed into the label aggregator, which combines these potentially conflicting and overlapping labels into one label for each example.",1 Introduction,[0],[0]
The resulting labeled examples are then used to train an arbitrary discriminative model.,1 Introduction,[0],[0]
"To create the input explanations, the user views a subset S of an unlabeled dataset D (where |S| |D|) and provides for each input xi ∈ S a label yi and a natural language explanation ei, a sentence explaining why the example should receive that label.",2.1 Explanations,[0],[0]
"The explanation ei generally refers to specific aspects of the example (e.g., in Figure 2, the location of a specific string “his wife”).",2.1 Explanations,[0],[0]
"The semantic parser takes a natural language explanation ei and returns a set of LFs (logical forms or labeling functions) {f1, . . .",2.2 Semantic Parser,[0],[0]
", fk} of the form fi : X → {−1, 0, 1} in a binary classification setting, with 0 representing abstention.",2.2 Semantic Parser,[0],[0]
"We emphasize that the goal of this semantic parser is not to generate the single correct parse, but rather to have coverage over many potentially useful LFs.1
1Indeed, we find empirically that an incorrect LF nearby the correct one in the space of logical forms actually has higher end-task accuracy 57% of the time (see Section 4.2).
",2.2 Semantic Parser,[0],[0]
We choose a simple rule-based semantic parser that can be used without any training.,2.2 Semantic Parser,[0],[0]
"Formally, the parser uses a set of rules of the form α → β, where α can be replaced by the token(s) in β (see Figure 3 for example rules).",2.2 Semantic Parser,[0],[0]
"To identify candidate LFs, we recursively construct a set of valid parses for each span of the explanation, based on the substitutions defined by the grammar rules.",2.2 Semantic Parser,[0],[0]
"At the end, the parser returns all valid parses (LFs in our case) corresponding to the entire explanation.
",2.2 Semantic Parser,[0],[0]
We also allow an arbitrary number of tokens in a given span to be ignored when looking for a matching rule.,2.2 Semantic Parser,[0],[0]
"This improves the ability of the parser to handle unexpected input, such as unknown words or typos, since the portions of the input that are parseable can still result in a valid parse.",2.2 Semantic Parser,[0],[0]
"For example, in Figure 3, the word “person” is ignored.
",2.2 Semantic Parser,[0],[0]
"All predicates included in our grammar (summarized in Table 1) are provided to annotators, with minimal examples of each in use (Appendix A).",2.2 Semantic Parser,[0],[0]
"Importantly, all rules are domain independent (e.g., all three relation extraction tasks that we tested used the same grammar), making the semantic parser easily transferrable to new domains.",2.2 Semantic Parser,[0],[0]
"Additionally, while this paper focuses on the task of relation extraction, in principle the BabbleLabble framework can be applied to other tasks or settings by extending the grammar with the necessary primitives (e.g., adding primitives for rows and columns to enable explanations about the alignments of words in tables).",2.2 Semantic Parser,[0],[0]
"To guide the construction of the grammar, we collected 500 explanations for the Spouse domain from workers
on Amazon Mechanical Turk and added support for the most commonly used predicates.",2.2 Semantic Parser,[0],[0]
These were added before the experiments described in Section 4.,2.2 Semantic Parser,[0],[0]
Altogether the grammar contains 200 rule templates.,2.2 Semantic Parser,[0],[0]
The input to the filter bank is a set of candidate LFs produced by the semantic parser.,2.3 Filter Bank,[0],[0]
The purpose of the filter bank is to discard as many incorrect LFs as possible without requiring additional labels.,2.3 Filter Bank,[0],[0]
"It consists of two classes of filters: semantic and pragmatic.
",2.3 Filter Bank,[0],[0]
"Recall that each explanation ei is collected in the context of a specific labeled example (xi, yi).",2.3 Filter Bank,[0],[0]
"The semantic filter checks for LFs that are inconsistent with their corresponding example; formally, any LF f for which f(xi) 6=",2.3 Filter Bank,[0],[0]
yi is discarded.,2.3 Filter Bank,[0],[0]
"For example, in the first explanation in Figure 2, the word “right” can be interpreted as either “immediately” (as in “right before”) or simply “to the
right.”",2.3 Filter Bank,[0],[0]
"The latter interpretation results in a function that is inconsistent with the associated example (since “his wife” is actually to the left of person 2), so it can be safely removed.
",2.3 Filter Bank,[0],[0]
"The pragmatic filters removes LFs that are constant, redundant, or correlated.",2.3 Filter Bank,[0],[0]
"For example, in Figure 2, LF 2a is constant, as it labels every example positively (since all examples contain two people from the same sentence).",2.3 Filter Bank,[0],[0]
"LF 3b is redundant, since even though it has a different syntax tree from LF 3a, it labels the training set identically and therefore provides no new signal.
",2.3 Filter Bank,[0],[0]
"Finally, out of all LFs from the same explanation that pass all the other filters, we keep only the most specific (lowest coverage) LF.",2.3 Filter Bank,[0],[0]
"This prevents multiple correlated LFs from a single example from dominating.
",2.3 Filter Bank,[0],[0]
"As we show in Section 4, over three tasks, the filter bank removes 86% of incorrect parses, and the incorrect ones that remain have average endtask accuracy within 2.5% of the corresponding correct parses.",2.3 Filter Bank,[0],[0]
The label aggregator combines multiple (potentially conflicting) suggested labels from the LFs and combines them into a single probabilistic label per example.,2.4 Label Aggregator,[0],[0]
"Concretely, if m LFs pass the filter bank and are applied to n examples, the label aggregator implements a function f : {−1, 0, 1}m×n",2.4 Label Aggregator,[0],[0]
"→ [0, 1]n.
",2.4 Label Aggregator,[0],[0]
"A naive solution would be to use a simple majority vote, but this fails to account for the fact that LFs can vary widely in accuracy and coverage.",2.4 Label Aggregator,[0],[0]
"Instead, we use data programming (Ratner et al., 2016), which models the relationship between the true labels and the output of the labeling functions as a factor graph.",2.4 Label Aggregator,[0],[0]
"More specifically, given the true labels Y ∈ {−1, 1}n (latent) and label matrix Λ ∈ {−1, 0, 1}m×n (observed) where Λi,j = LFi(xj), we define two types of factors representing labeling propensity and accuracy:
φLabi,j (Λ, Y ) = 1{Λi,j 6= 0} (1) φAcci,j (Λ, Y ) = 1{Λi,j = yj}.",2.4 Label Aggregator,[0],[0]
"(2)
Denoting the vector of factors pertaining to a given data point xj as φj(Λ, Y ) ∈ Rm, define the model:
pw(Λ, Y )",2.4 Label Aggregator,[0],[0]
= Z −1 w exp,2.4 Label Aggregator,[0],[0]
"( n∑ j=1 w · φj(Λ, Y ) )",2.4 Label Aggregator,[0],[0]
", (3)
where w ∈ R2m is the weight vector and Zw is the normalization constant.",2.4 Label Aggregator,[0],[0]
"To learn this model without knowing the true labels Y , we minimize the negative log marginal likelihood given the observed labels Λ:
ŵ = arg min w − log ∑ Y pw(Λ, Y ) (4)
using SGD and Gibbs sampling for inference, and then use the marginals pŵ(Y | Λ) as probabilistic training labels.
",2.4 Label Aggregator,[0],[0]
"Intuitively, we infer accuracies of the LFs based on the way they overlap and conflict with one another.",2.4 Label Aggregator,[0],[0]
"Since noisier LFs are more likely to have high conflict rates with others, their corresponding accuracy weights in w will be smaller, reducing their influence on the aggregated labels.",2.4 Label Aggregator,[0],[0]
The noisily-labeled training set that the label aggregator outputs is used to train an arbitrary discriminative model.,2.5 Discriminative Model,[0],[0]
One advantage of training a discriminative model on the task instead of using the label aggregator as a classifier directly is that the label aggregator only takes into account those signals included in the LFs.,2.5 Discriminative Model,[0],[0]
"A discriminative model, on the other hand, can incorporate features that were not identified by the user but are nevertheless informative.2 Consequently, even examples for which all LFs abstained can still be classified correctly.",2.5 Discriminative Model,[0],[0]
"On the three tasks we evaluate, using the discriminative model averages 4.3 F1 points higher than using the label aggregator directly.
",2.5 Discriminative Model,[0],[0]
"For the results reported in this paper, our discriminative model is a simple logistic regression classifier with generic features defined over dependency paths.3 These features include unigrams,
2We give an example of two such features in Section 4.3.",2.5 Discriminative Model,[0],[0]
"3https://github.com/HazyResearch/treedlib
bigrams, and trigrams of lemmas, dependency labels, and part of speech tags found in the siblings, parents, and nodes between the entities in the dependency parse of the sentence.",2.5 Discriminative Model,[0],[0]
"We found this to perform better on average than a biLSTM, particularly for the traditional supervision baselines with small training set sizes; it also provided easily interpretable features for analysis.",2.5 Discriminative Model,[0],[0]
"We evaluate the accuracy of BabbleLabble on three relation extraction tasks, which we refer to as Spouse, Disease, and Protein.",3 Experimental Setup,[0],[0]
"The goal of each task is to train a classifier for predicting whether the two entities in an example are participating in the relationship of interest, as described below.",3 Experimental Setup,[0],[0]
"Statistics for each dataset are reported in Table 2, with one example and one explanation for each given in Figure 4 and additional explanations shown in Appendix B.
In the Spouse task, annotators were shown a sentence with two highlighted names and asked to label whether the sentence suggests that the two people are spouses.",3.1 Datasets,[0],[0]
"Sentences were pulled from the Signal Media dataset of news articles (Corney
et al., 2016).",3.1 Datasets,[0],[0]
"Ground truth data was collected from Amazon Mechanical Turk workers, accepting the majority label over three annotations.",3.1 Datasets,[0],[0]
"The 30 explanations we report on were sampled randomly from a pool of 200 that were generated by 10 graduate students unfamiliar with BabbleLabble.
",3.1 Datasets,[0],[0]
"In the Disease task, annotators were shown a sentence with highlighted names of a chemical and a disease and asked to label whether the sentence suggests that the chemical causes the disease.",3.1 Datasets,[0],[0]
"Sentences and ground truth labels came from a portion of the 2015 BioCreative chemical-disease relation dataset (Wei et al., 2015), which contains abstracts from PubMed.",3.1 Datasets,[0],[0]
"Because this task requires specialized domain expertise, we obtained explanations by having someone unfamiliar with BabbleLabble translate from Python to natural language labeling functions from an existing publication that explored applying weak supervision to this task (Ratner et al., 2018).
",3.1 Datasets,[0],[0]
"The Protein task was completed in conjunction with OccamzRazor, a neuroscience company targeting biological pathways of Parkinson’s disease.",3.1 Datasets,[0],[0]
"For this task, annotators were shown a sentence from the relevant biomedical literature with highlighted names of a protein and a kinase and asked to label whether or not the kinase influences the protein in terms of a physical interaction or phosphorylation.",3.1 Datasets,[0],[0]
"The annotators had domain expertise but minimal programming experience, making BabbleLabble a natural fit for their use case.",3.1 Datasets,[0],[0]
Text documents are tokenized with spaCy.4,3.2 Experimental Settings,[0],[0]
"The semantic parser is built on top of the Python-based
4https://github.com/explosion/spaCy
implementation SippyCup.5 On a single core, parsing 360 explanations takes approximately two seconds.",3.2 Experimental Settings,[0],[0]
"We use existing implementations of the label aggregator, feature library, and discriminative classifier described in Sections 2.4–2.5 provided by the open-source project Snorkel (Ratner et al., 2018).
",3.2 Experimental Settings,[0],[0]
Hyperparameters for all methods we report were selected via random search over thirty configurations on the same held-out development set.,3.2 Experimental Settings,[0],[0]
"We searched over learning rate, batch size, L2 regularization, and the subsampling rate (for improving balance between classes).6 All reported F1 scores are the average value of 40 runs with random seeds and otherwise identical settings.",3.2 Experimental Settings,[0],[0]
"We evaluate the performance of BabbleLabble with respect to its rate of improvement by number of user inputs, its dependence on correctly parsed logical forms, and the mechanism by which it utilizes logical forms.",4 Experimental Results,[0],[0]
In Table 3 we report the average F1 score of a classifier trained with BabbleLabble using 30 explanations or traditional supervision with the indicated number of labels.,4.1 High Bandwidth Supervision,[0],[0]
"On average, it took the same amount of time to collect 30 explanations as 60 labels.7 We observe that in all three tasks, BabbleLabble achieves a given F1 score with far fewer user inputs than traditional supervision, by
5https://github.com/wcmac/sippycup 6Hyperparameter ranges: learning rate (1e-2 to 1e-4), batch size (32 to 128), L2 regularization (0 to 100), subsampling rate (0 to 0.5)
7Zaidan and Eisner (2008) also found that collecting annotator rationales in the form of highlighted substrings from the sentence only doubled annotation time.
as much as 100 times in the case of the Spouse task.",4.1 High Bandwidth Supervision,[0],[0]
"Because explanations are applied to many unlabeled examples, each individual input from the user can implicitly contribute many (noisy) labels to the learning algorithm.
",4.1 High Bandwidth Supervision,[0],[0]
"We also observe, however, that once the number of labeled examples is sufficiently large, traditional supervision once again dominates, since ground truth labels are preferable to noisy ones generated by labeling functions.",4.1 High Bandwidth Supervision,[0],[0]
"However, in domains where there is much more unlabeled data available than labeled data (which in our experience is most domains), we can gain in supervision efficiency from using BabbleLabble.
",4.1 High Bandwidth Supervision,[0],[0]
"Of those explanations that did not produce a correct LF, 4% were caused by the explanation referring to unsupported concepts (e.g., one explanation referred to “the subject of the sentence,” which our simple parser doesn’t support).",4.1 High Bandwidth Supervision,[0],[0]
Another 2% were caused by human errors (the correct LF for the explanation was inconsistent with the example).,4.1 High Bandwidth Supervision,[0],[0]
"The remainder were due to unrecognized paraphrases (e.g., the explanation said “the order of appearance is X, Y” instead of a supported phrasing like “X comes before Y”).",4.1 High Bandwidth Supervision,[0],[0]
"In Table 4, we report LF summary statistics before and after filtering.",4.2 Utility of Incorrect Parses,[0],[0]
LF correctness is based on exact match with a manually generated parse for each explanation.,4.2 Utility of Incorrect Parses,[0],[0]
"Surprisingly, the simple heuristic-based filter bank successfully removes over 95% of incorrect LFs in all three tasks, resulting in final LF sets that are 86% correct on average.",4.2 Utility of Incorrect Parses,[0],[0]
"Furthermore, among those LFs that pass through the filter bank, we found that the average difference in end-task accuracy between correct and incorrect parses is less than 2.5%.",4.2 Utility of Incorrect Parses,[0],[0]
"Intuitively, the filters are effective because it is quite difficult for an LF to be parsed from the explana-
tion, label its own example correctly (passing the semantic filter), and not label all examples in the training set with the same label or identically to another LF (passing the pragmatic filter).
",4.2 Utility of Incorrect Parses,[0],[0]
"We went one step further: using the LFs that would be produced by a perfect semantic parser as starting points, we searched for “nearby” LFs (LFs differing by only one predicate) with higher endtask accuracy on the test set and succeeded 57% of the time (see Figure 5 for an example).",4.2 Utility of Incorrect Parses,[0],[0]
"In other words, when users provide explanations, the signals they describe provide good starting points, but they are actually unlikely to be optimal.",4.2 Utility of Incorrect Parses,[0],[0]
"This observation is further supported by Table 5, which shows that the filter bank is necessary to remove clearly irrelevant LFs, but with that in place, the simple rule-based semantic parser and a perfect parser have nearly identical average F1 scores.",4.2 Utility of Incorrect Parses,[0],[0]
"Once we have relevant logical forms from userprovided explanations, we have multiple options for how to use them.",4.3 Using LFs as Functions or Features,[0],[0]
Srivastava et al. (2017) propose using these logical forms as features in a linear classifier.,4.3 Using LFs as Functions or Features,[0],[0]
"We choose instead to use them as functions for weakly supervising the creation of a larger training set via data programming (Ratner et al., 2016).",4.3 Using LFs as Functions or Features,[0],[0]
"In Table 6, we compare the two approaches directly, finding that the the data programming approach outperforms a feature-based one by 9.5 F1 points with the rule-based parser, and by 4.5 points with a perfect parser.
",4.3 Using LFs as Functions or Features,[0],[0]
We attribute this difference primarily to the ability of data programming to utilize unlabeled data.,4.3 Using LFs as Functions or Features,[0],[0]
"In Figure 6, we show how the data programming approach improves with the number of unlabeled examples, even as the number of LFs remains constant.",4.3 Using LFs as Functions or Features,[0],[0]
We also observe qualitatively that data programming exposes the classifier to additional patterns that are correlated with our explanations but not mentioned directly.,4.3 Using LFs as Functions or Features,[0.9503263327201787],['We find that this approach outperforms a more conventional machine translation criterion both when attempting to interoperate with neuralese speakers and when predicting their state.']
"For example, in the Disease task, two of the features weighted most
highly by the discriminative model were the presence of the trigrams “could produce a” or “support diagnosis of” between the chemical and disease, despite none of these words occurring in the explanations for that task.",4.3 Using LFs as Functions or Features,[0],[0]
In Table 6 we see a 4.3 F1 point improvement (10%) when we use the discriminative model that can take advantage of these features rather than applying the LFs directly to the test set and making predictions based on the output of the label aggregator.,4.3 Using LFs as Functions or Features,[0],[0]
Our work has two themes: modeling natural language explanations/instructions and learning from weak supervision.,5 Related Work and Discussion,[0],[0]
The closest body of work is on “learning from natural language.”,5 Related Work and Discussion,[0],[0]
"As mentioned earlier, Srivastava et al. (2017) convert natural language explanations into classifier features (whereas we convert them into labeling functions).",5 Related Work and Discussion,[0],[0]
"Goldwasser and Roth (2011) convert natural lan-
guage into concepts (e.g., the rules of a card game).",5 Related Work and Discussion,[0],[0]
Ling and Fidler (2017) use natural language explanations to assist in supervising an image captioning model.,5 Related Work and Discussion,[0],[0]
Weston (2016); Li et al. (2016) learn from natural language feedback in a dialogue.,5 Related Work and Discussion,[0],[0]
"Wang et al. (2017) convert natural language definitions to rules in a semantic parser to build up progressively higher-level concepts.
",5 Related Work and Discussion,[0],[0]
"We lean on the formalism of semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang, 2016).",5 Related Work and Discussion,[0],[0]
"One notable trend is to learn semantic parsers from weak supervision (Clarke et al., 2010; Liang et al., 2011), whereas our goal is to obtain weak supervision signal from semantic parsers.
",5 Related Work and Discussion,[0],[0]
The broader topic of weak supervision has received much attention; we mention some works most related to relation extraction.,5 Related Work and Discussion,[0],[0]
"In distant supervision (Craven et al., 1999; Mintz et al., 2009) and multi-instance learning (Riedel et al., 2010; Hoffmann et al., 2011), an existing knowledge base is used to (probabilistically) impute a training set.",5 Related Work and Discussion,[0],[0]
"Various extensions have focused on aggregating a variety of supervision sources by learning generative models from noisy labels (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth and Klakow, 2013; Ratner et al., 2016; Varma et al., 2017).
",5 Related Work and Discussion,[0],[0]
"Finally, while we have used natural language explanations as input to train models, they can also be output to interpret models (Krening et al., 2017; Lei et al., 2016).",5 Related Work and Discussion,[0],[0]
"More generally, from a machine learning perspective, labels are the primary asset, but they are a low bandwidth signal between annotators and the learning algorithm.",5 Related Work and Discussion,[0],[0]
Natural language opens up a much higher-bandwidth communication channel.,5 Related Work and Discussion,[0],[0]
"We have shown promising results in relation extraction (where one explanation can be “worth” 100 labels), and it would be interesting to extend our framework to other tasks and more interactive settings.",5 Related Work and Discussion,[0],[0]
"The code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x900e7e41deaa4ec5b2fe41dc50594548/.",Reproducibility,[0],[0]
We gratefully acknowledge the support of the following organizations: DARPA under No.,Acknowledgments,[0],[0]
"N66001-15-C-4043 (SIMPLEX), No. FA8750-17-2-0095 (D3M), No. FA8750-122-0335 (XDATA), and No. FA8750-13-2-0039 (DEFT), DOE under No. 108845, NIH under No. U54EB020405 (Mobilize), ONR under No. N000141712266 and No. N000141310129, AFOSR under No. 580K753, the Intel/NSF CPS Security grant No. 1505728, the Michael J. Fox Foundation for Parkinsons Research under Grant No. 14672, the Secure Internet of Things Project, Qualcomm, Ericsson, Analog Devices, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the National Science Foundation Graduate Research Fellowship under Grant No.",Acknowledgments,[0],[0]
"DGE-114747, the Stanford Finch Family Fellowship, the Joseph W. and Hon Mai Goodman Stanford Graduate Fellowship, an NSF CAREER Award IIS-1552635, and the members of the Stanford DAWN project: Facebook, Google, Intel, Microsoft, NEC, Teradata, and VMware.
",Acknowledgments,[0],[0]
"We thank Alex Ratner and the developers of Snorkel for their assistance with data programming, as well as the many members of the Hazy Research group and Stanford NLP group who provided feedback and tested early prototyptes.",Acknowledgments,[0],[0]
"Thanks as well to the OccamzRazor team: Tarik Koc, Benjamin Angulo, Katharina S. Volz, and Charlotte Brzozowski.
",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, AFOSR, NSF, or the U.S. Government.",Acknowledgments,[0],[0]
"Below are the predicates in the rule-based semantic parser grammar, each of which may have many supported paraphrases, only one of which is listed here in a minimal example.",A Predicate Examples,[0],[0]
and: X is true and Y is true or: X is true or Y is true not: X is not true any: Any of X or Y or Z is true all: All of X and Y and Z are true none:,Logic,[0],[0]
None of X or Y or Z is true,Logic,[0],[0]
=: X is equal to Y 6=: X is not Y <: X is smaller than Y ≤: X is no more than Y >: X is larger than Y ≥: X is at least Y,Comparison,[0],[0]
"lower: X is lowercase upper: X is upper case capital: X is capitalized all caps: X is in all caps starts with: X starts with ""cardio"" ends with: X ends with ""itis"" substring: X contains ""-induced""",Syntax,[0],[0]
person:,Named-entity Tags,[0],[0]
A person is between X and Y location: A place is within two words of X date: A date is between X and Y number: There are three numbers in the sentence organization: An organization is right after X,Named-entity Tags,[0],[0]
"list: (X, Y) is in Z set: X, Y, and Z are true count:",Lists,[0],[0]
There is one word between X and Y contains: X is in Y intersection: At least two of X are in Y map: X is at the start of a word in Y filter: There are three capitalized words to the left of X alias: A spouse word is in the sentence (“spouse” is a predefined list from the user),Lists,[0],[0]
word distance: X is two words before Y char distance: X is twenty characters after Y left: X is before Y right: X is after Y between: X is between Y and Z within: X is within five words of Y,Position,[0],[0]
The following are a sample of the explanations provided by users for each task.,B Sample Explanations,[0],[0]
"Users referred to the first person in the sentence as “X” and the second as “Y”.
",Spouse,[0],[0]
"Label true because ""and"" occurs between X and Y and ""marriage"" occurs one word after person1.
",Spouse,[0],[0]
"Label true because person Y is preceded by ‘beau’.
",Spouse,[0],[0]
"Label false because the words ""married"", ""spouse"", ""husband"", and ""wife"" do not occur in the sentence.
",Spouse,[0],[0]
"Label false because there are more than 2 people in the sentence and ""actor"" or ""actress"" is left of person1 or person2.",Spouse,[0],[0]
"Label true because the disease is immediately after the chemical and ’induc’ or ’assoc’ is in the chemical name.
",Disease,[0],[0]
"Label true because a word containing ’develop’ appears somewhere before the chemical, and the word ’following’ is between the disease and the chemical.
",Disease,[0],[0]
"Label true because ""induced by"", ""caused by"", or ""due to"" appears between the chemical and the disease.",Disease,[0],[0]
"""
Label false because ""none"", ""not"", or ""no"" is within 30 characters to the left of the disease.",Disease,[0],[0]
"Label true because ""Ser"" or ""Tyr"" are within 10 characters of the protein.
",Protein,[0],[0]
"Label true because the words ""by"" or ""with"" are between the protein and kinase and the words ""no"", ""not"" or ""none"" are not in between the protein and kinase and the total number of words between them is smaller than 10.
",Protein,[0],[0]
"Label false because the sentence contains ""mRNA"", ""DNA"", or ""RNA"".
",Protein,[0],[0]
"Label false because there are two "","" between the protein and the kinase with less than 30 characters between them.",Protein,[0],[0]
"Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification).",abstractText,[0],[0]
"In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision.",abstractText,[0],[0]
"A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier.",abstractText,[0],[0]
"On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5–100 faster by providing explanations instead of just labels.",abstractText,[0],[0]
"Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.",abstractText,[0],[0]
Training Classifiers with Natural Language Explanations,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2775–2779 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2775",text,[0],[0]
"End-to-end dialogue systems, based on neural architectures like bidirectional LSTMs or Memory Networks (Sukhbaatar et al., 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",1 Introduction,[0],[0]
One of their main advantages is that they can rely on large data sources of existing dialogues to learn to cover various domains without requiring any expert knowledge.,1 Introduction,[0],[0]
"However, the flip side is that they also exhibit limited engagement, especially in chit-chat settings: they lack consistency and do not leverage proactive engagement strategies as (even partially) scripted chatbots do.
Zhang et al. (2018) introduced the PERSONACHAT dataset as a solution to cope with this issue.",1 Introduction,[0],[0]
"This dataset consists of dialogues between pairs of agents with text profiles, or personas, attached to
each of them.",1 Introduction,[0],[0]
"As shown in their paper, conditioning an end-to-end system on a given persona improves the engagement of a dialogue agent.",1 Introduction,[0],[0]
"This paves the way to potentially end-to-end personalized chatbots because the personas of the bots, by being short texts, could be easily edited by most users.",1 Introduction,[0],[0]
"However, the PERSONA-CHAT dataset was created using an artificial data collection mechanism based on Mechanical Turk.",1 Introduction,[0],[0]
"As a result, neither dialogs nor personas can be fully representative of real user-bot interactions and the dataset coverage remains limited, containing a bit more than 1k different personas.
",1 Introduction,[0],[0]
"In this paper, we build a very large-scale persona-based dialogue dataset using conversations previously extracted from REDDIT1.",1 Introduction,[0],[0]
"With simple heuristics, we create a corpus of over 5 million personas spanning more than 700 million conversations.",1 Introduction,[0],[0]
We train persona-based end-to-end dialogue models on this dataset.,1 Introduction,[0],[0]
"These models outperform their counterparts that do not have access to personas, confirming results of Zhang et al. (2018).",1 Introduction,[0],[0]
"In addition, the coverage of our dataset seems very good since pre-training on it also leads to state-of-the-art results on the PERSONA-CHAT dataset.",1 Introduction,[0],[0]
"With the rise of end-to-end dialogue systems, personalized trained systems have started to appear.",2 Related work,[0],[0]
Li et al. (2016) proposed to learn latent variables representing each speaker’s bias/personality in a dialogue model.,2 Related work,[0],[0]
"Other classic strategies include extracting explicit variables from structured knowledge bases or other symbolic sources as in (Ghazvininejad et al., 2017; Joshi et al., 2017; Young et al., 2017).",2 Related work,[0],[0]
"Still, in the context of per-
1https://www.reddit.com/r/datasets/ comments/3bxlg7/
sonal chatbots, it might be more desirable to condition on data that can be generated and interpreted by the user itself such as text rather than relying on some knowledge base facts that might not exist for everyone or a great variety of situations.",2 Related work,[0],[0]
"PERSONA-CHAT (Zhang et al., 2018) recently introduced a dataset of conversations revolving around human habits and preferences.",2 Related work,[0],[0]
"In their experiments, they showed that conditioning on a text description of each speaker’s habits, their persona, improved dialogue modeling.
",2 Related work,[0],[0]
"In this paper, we use a pre-existing REDDIT data dump as data source.",2 Related work,[0],[0]
REDDIT is a massive online message board.,2 Related work,[0],[0]
Dodge et al. (2015) used it to assess chit-chat qualities of generic dialogue models.,2 Related work,[0],[0]
Yang et al. (2018) used response prediction on REDDIT as an auxiliary task in order to improve prediction performance on natural language inference problems.,2 Related work,[0],[0]
Our goal is to learn to predict responses based on a persona for a large variety of personas.,3 Building a dataset of millions of persona-based dialogues,[0],[0]
"To that end, we build a dataset of examples of the following form using data from REDDIT:
• Persona:",3 Building a dataset of millions of persona-based dialogues,[0],[0]
"[“I like sport”, “I work a lot”] • Context: “I love running.”",3 Building a dataset of millions of persona-based dialogues,[0],[0]
• Response: “Me too!,3 Building a dataset of millions of persona-based dialogues,[0],[0]
"But only on weekends.”
",3 Building a dataset of millions of persona-based dialogues,[0],[0]
"The persona is a set of sentences representing the personality of the responding agent, the context is the utterance that it responds to, and the response is the answer to be predicted.",3 Building a dataset of millions of persona-based dialogues,[0],[0]
"As in (Dodge et al., 2015), we use a preexisting dump of REDDIT that consists of 1.7 billion comments.",3.1 Preprocessing,[0],[0]
We tokenize sentences by padding all special characters with a space and splitting on whitespace characters.,3.1 Preprocessing,[0],[0]
We create a dictionary containing the 250k most frequent tokens.,3.1 Preprocessing,[0],[0]
We truncate comments that are longer than 100 tokens.,3.1 Preprocessing,[0],[0]
"We construct the persona of a user by gathering all the comments they wrote, splitting them into sentences, and selecting the sentences that satisfy the following rules: (i) each sentence must contain between 4 and 20 words or punctuation marks, (ii) it contains either the word I or my, (iii) at least
one verb, and (iv) at least one noun, pronoun or adjective.
",3.2 Persona extraction,[0],[0]
"To handle the quantity of data involved, we limit the size of a persona to N sentences for each user.",3.2 Persona extraction,[0],[0]
We compare four different setups for persona creation.,3.2 Persona extraction,[0],[0]
"In the rules setup, we select up to N random sentences that satisfy the rules above.",3.2 Persona extraction,[0],[0]
"In the rules + classifier setup, we filter with the rules then score the resulting sentences using a bag-of-words classifier that is trained to discriminate PERSONACHAT persona sentences from random comments.",3.2 Persona extraction,[0],[0]
We manually tune a threshold on the score in order to select sentences.,3.2 Persona extraction,[0],[0]
"If there are more than N eligible persona sentences for a given user, we keep the highest-scored ones.",3.2 Persona extraction,[0],[0]
"In the random from user setup, we randomly select sentences uttered by the user while keeping the sentence length requirement above (we ignore the other rules).",3.2 Persona extraction,[0],[0]
The random from dataset baseline refers to random sentences from the dataset.,3.2 Persona extraction,[0],[0]
They do not necessarily come from the same user.,3.2 Persona extraction,[0],[0]
"This last setup serves as a control mechanism to verify that the gains in prediction accuracy are due to the user-specific information contained in personas.
",3.2 Persona extraction,[0],[0]
"In the example at the beginning of this section, the response is clearly consistent with the persona.",3.2 Persona extraction,[0],[0]
"There may not always be such an obvious relationship between the two: the discussion topic may not be covered by the persona, a single user may write contradictory statements, and due to errors in the extraction process, some persona sentences may not represent a general trait of the user (e.g. I am feeling happy today).",3.2 Persona extraction,[0],[0]
We take each pair of successive comments in a thread to form the context and response of an example.,3.3 Dataset creation,[0],[0]
The persona corresponding to the response is extracted using one of the methods of Section 3.2.,3.3 Dataset creation,[0.952205260410369],"['The existing literature suggests two broad approaches: Semantic representation The meaning of a message za is given by its denotations: that is, by the set of world states of which za may be felicitously predicated, given the existing context available to a listener.']"
"We split the dataset randomly between training, validation and test.",3.3 Dataset creation,[0],[0]
Validation and test sets contain 50k examples each.,3.3 Dataset creation,[0],[0]
"We extract personas using training data only: test set responses cannot be contained explicitly in the persona.
",3.3 Dataset creation,[0],[0]
"In total, we select personas covering 4.6m users in the rule-based setups and 7.2m users in the random setups.",3.3 Dataset creation,[0],[0]
"This is a sizable fraction of the total 13.2m users of the dataset; depending on the persona selection setup, between 97 and 99.4 % of the training set examples are linked to a persona.",3.3 Dataset creation,[0],[0]
"We model dialogue by next utterance retrieval (Lowe et al., 2016), where a response is picked among a set of candidates and not generated.",4 End-to-end dialogue models,[0],[0]
The overall architecture is depicted in Fig. 1.,4.1 Architecture,[0],[0]
We encode the persona and the context using separate modules.,4.1 Architecture,[0],[0]
"As in Zhang et al. (2018), we combine the encoded context and persona using a 1-hop memory network with a residual connection, using the context as query and the set of persona sentences as memory.",4.1 Architecture,[0],[0]
We also encode all candidate responses and compute the dot-product between all those candidate representations and the joint representation of the context and the persona.,4.1 Architecture,[0],[0]
"The predicted response is the candidate that maximizes the dot product.
",4.1 Architecture,[0],[0]
We train by passing all the dot products through a softmax and maximizing the log-likelihood of the correct responses.,4.1 Architecture,[0],[0]
"We use mini-batches of training examples and, for each example therein, all the responses of the other examples of the same batch are used as negative responses.",4.1 Architecture,[0],[0]
Both context and response encoders share the same architecture and word embeddings but have different weights in the subsequent layers.,4.2 Context and response encoders,[0],[0]
"We train three different encoder architectures.
",4.2 Context and response encoders,[0],[0]
Bag-of-words applies two linear projections separated by a tanh non-linearity to the word embeddings.,4.2 Context and response encoders,[0],[0]
"We then sum the resulting sentence representation across all positions in the sentence and divide the result by √ n where n is the length of the sequence.
",4.2 Context and response encoders,[0],[0]
LSTM applies a 2-layer bidirectional LSTM.,4.2 Context and response encoders,[0],[0]
"We use the last hidden state as encoded sentence.
",4.2 Context and response encoders,[0],[0]
"Transformer is a variation of an End-to-end Memory Network (Sukhbaatar et al., 2015) introduced by Vaswani et al. (2017).",4.2 Context and response encoders,[0],[0]
"Based solely on attention mechanisms, it exhibited state-of-the-art performance on next utterance retrieval tasks in dialogues (Yang et al., 2018).",4.2 Context and response encoders,[0],[0]
Here we use only its encoding module.,4.2 Context and response encoders,[0],[0]
"We subsequently average the resulting representation across all positions in the sentence, yielding a fixed-size representation.",4.2 Context and response encoders,[0],[0]
The persona encoder encodes each persona sentence separately.,4.3 Persona encoder,[0],[0]
It relies on the same word embeddings as the context encoder and applies a linear layer on top of them.,4.3 Persona encoder,[0],[0]
"We then sum the representations across the sentence.
",4.3 Persona encoder,[0],[0]
We deliberately choose a simpler architecture than the other encoders for performance reasons as the number of personas encoded for each batch is an order of magnitude greater than the number of training examples.,4.3 Persona encoder,[0],[0]
Most personas are short sentences; we therefore expect a bag-of-words representation to encode them well.,4.3 Persona encoder,[0],[0]
We train models on the persona-based dialogue dataset described in Section 3.3 and we evaluate its accuracy both on the original task and when transferring onto PERSONA-CHAT.,5 Experiments,[0],[0]
We optimize network parameters using Adamax with a learning rate of 8e−4 on mini-batches of size 512.,5.1 Experimental details,[0],[0]
"We initialize embeddings with FastText word vectors and optimize them during learning.
",5.1 Experimental details,[0],[0]
"REDDIT LSTMs use a hidden size of 150; we concatenate the last hidden states for both directions and layers, resulting in a final representation of size 600.",5.1 Experimental details,[0],[0]
"Transformer architectures on reddit use 4 layers with a hidden size of 300 and 6 attention heads, resulting in a final representation of size 300.",5.1 Experimental details,[0],[0]
We use Spacy for part-of-speech tagging in order to verify the persona extraction rules.,5.1 Experimental details,[0],[0]
"We distribute the training by splitting each batch across 8 GPUs; we stop training after 1 full epoch, which takes about 3 days.
",5.1 Experimental details,[0],[0]
"PERSONA-CHAT We used the revised version of the dataset where the personas have been rephrased, making it a harder task.",5.1 Experimental details,[0],[0]
"The dataset being only a few thousands samples, we had to reduce the architecture to avoid overfitting for the models trained purely on PERSONA-CHAT.",5.1 Experimental details,[0],[0]
"2 layers, 2 attention heads, a dropout of 0.2 and keeping the size of the word embeddings to 300 units yield the highest accuracy on the validation set.
",5.1 Experimental details,[0],[0]
IR Baseline,5.1 Experimental details,[0],[0]
"As basic baseline, we use an information retrieval (IR) system that ranks candidate responses according to a TF-IDF weighted exactmatch similarity with the context alone.",5.1 Experimental details,[0],[0]
Impact of personas We report the accuracy of the different architectures on the reddit task in Table 1.,5.2 Results,[0],[0]
Conditioning on personas improves the prediction performance regardless of the encoder architecture.,5.2 Results,[0],[0]
"Table 2 gives some examples of how the persona affects the predicted answer.
",5.2 Results,[0],[0]
"Influence of the persona extraction In Table 3, we report precision results for several persona extraction setups.",5.2 Results,[0],[0]
"The rules setup improves the results somewhat, however adding the persona classifier actually degrades the results.",5.2 Results,[0],[0]
"A possible interpretation is that the persona classifier is trained only on the PERSONA-CHAT revised personas, and that this selection might be too narrow and lack di-
versity.",5.2 Results,[0],[0]
"Increasing the maximum persona size also improves the prediction performance.
",5.2 Results,[0],[0]
Transfer learning,5.2 Results,[0],[0]
We compare the performance of transformer models trained on REDDIT and on PERSONA-CHAT on both datasets.,5.2 Results,[0],[0]
We report results in Table 4.,5.2 Results,[0],[0]
"This architecture provides a strong improvement over the results of (Zhang et al., 2018), jumping from 35.4% hits@1 to 42.1%.",5.2 Results,[0],[0]
"Pretraining the model on REDDIT and then fine-tuning on PERSONA-CHAT pushes this score to 60.7%, largely improving the state of the art.",5.2 Results,[0],[0]
"As expected, fine-tuning on PERSONA-CHAT reduces the performance on REDDIT.",5.2 Results,[0],[0]
"However, directly testing on PERSONA-CHAT the model trained on REDDIT without fine-tuning yields a very low result.",5.2 Results,[0],[0]
"This could be a consequence of a discrepancy
between the style of personas of the two datasets.",5.2 Results,[0],[0]
This paper shows how to create a very large dataset for persona-based dialogue.,6 Conclusion,[0],[0]
We show that training models to align answers both with the persona of their author and the context improves the predicting performance.,6 Conclusion,[0],[0]
The trained models show promising coverage as exhibited by the stateof-the-art transfer results on the PERSONA-CHAT dataset.,6 Conclusion,[0],[0]
"As pretraining leads to a considerable improvement in performance, future work could be done fine-tuning this model for various dialog systems.",6 Conclusion,[0],[0]
Future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the prediction performance.,6 Conclusion,[0],[0]
"Current dialogue systems are not very engaging for users, especially when trained end-toend without relying on proactive reengaging scripted strategies.",abstractText,[0],[0]
Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model.,abstractText,[0],[0]
"However, the dataset used in (Zhang et al., 2018) is synthetic and of limited size as it contains around 1k different personas.",abstractText,[0],[0]
In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues.,abstractText,[0],[0]
"Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems.",abstractText,[0],[0]
"In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from (Zhang et al., 2018) and achieving state-of-the-art results.",abstractText,[0],[0]
Training Millions of Personalized Dialogue Agents,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 130–135 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Structured prediction, or the task of predicting multiple inter-dependent variables, is important in many domains, including computer vision, computational biology and natural language processing.",1 Introduction,[0],[0]
"For example, in sequence labelling, image segmentation, and parsing we are given input variables x, and must predict output variables y, where the number of possible y values are typically exponential in the number of variables that comprise it.",1 Introduction,[0],[0]
"Not only does this sometimes give rise to computational difficulties, it also leads to statistical parameter estimation issues, where learning precise models requires large amounts of labeled training data.
",1 Introduction,[0],[0]
"In some cases, unsupervised learning from plentiful unlabeled data may provide helpful outputs (Daumé III, 2009; Ammar et al., 2014).",1 Introduction,[0],[0]
But usually some form of more direct supervision is required to create a model truly useful to the task at hand.,1 Introduction,[0],[0]
In the absence of abundant labeled data we may consider alternative forms of supervision.,1 Introduction,[0],[0]
"For example, rather than providing labeled data instances, humans may more easily inject their
domain knowledge by providing “labels on features,” or “expectations” about correct outputs, as in generalized expectation criteria (Mann and McCallum, 2010), or by providing constraints, as in posterior regularization (Ganchev et al., 2010) or constraint driven learning (Chang et al., 2007).",1 Introduction,[0],[0]
"A major weakness of these methods, however, is that at training time inference must be done in the factor graph encompassing the union of the model’s factor graph and the expectation dependencies— often leading to prohibitively expensive inference.",1 Introduction,[0],[0]
"Moreover, these methods cannot learn from nondecomposable domain knowledge, where the domain knowledge is not in a form of a set of labeled features or constraints.
",1 Introduction,[0],[0]
"An easy way for humans to express domain knowledge is by writing a simple scalar scoring function that indicates preferences among choices for y given x. These human-coded functions may, for example, be based on arbitrary rule systems (or even Turing-complete programs) of the sort written by humans to solve problems before machine learning became so wide-spread.
",1 Introduction,[0.9521801987340672],['Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network.']
"In general, the human written domain knowledge functions are not expected to be perfect— most likely only examining a subset of features and not covering all cases.",1 Introduction,[0],[0]
"Thus we are now faced with two challenges: (1) the domain knowledge functions have limited generalization; (2) the domain knowledge functions provide a ranking, but do not provide an inference (search) procedure.
",1 Introduction,[0],[0]
"This paper presents a new training method for structured prediction energy networks (SPENs) (Belanger and McCallum, 2016; Belanger et al., 2017) that aims to address both these challenges, yielding efficient inference for structured prediction, trained from human-coded domain knowledge plus unlabeled data, but not requiring any labeled data instances.",1 Introduction,[0],[0]
"In SPENs, the factor graph that typically represents
130
output variable dependencies is replaced with a deep neural network that takes y and x as input and outputs a scalar energy score, but is able to learn much richer correlations than are typically captured in factor graphs.",1 Introduction,[0],[0]
"Inference in SPENs is performed by gradient descent in the energy, back-propagated to cause steps in a relaxed y space.",1 Introduction,[0],[0]
"Whereas previous training procedures for SPENs used labeled data, here we train SPENs from only unlabeled data plus human-coded domain knowledge in the form of a scoring function.",1 Introduction,[0],[0]
"We do so by building on SampleRank (Rohanimanesh et al., 2011; Singh et al., 2010), which enforces that the rank of two sampled ys according to the trained factor graph is consistent with their rank according to distance to the labeled, true y.",1 Introduction,[0],[0]
"In our training method, pairs of y’s are obtained from successive steps of training-time gradient-descent inference on y; when their rank is not consistent with that of the domain knowledge function, we accordingly update the energy network parameters.
",1 Introduction,[0],[0]
"We demonstrate our method on a citation field extraction task, for which we learn a neural network (1) that generalizes beyond the original domain knowledge function, and (2) that provides efficient test-time inference by gradient descent.",1 Introduction,[0],[0]
"In general, SPEN parameterizes an energy function Ew(y,x) using deep neural networks over output variables y as well as input variables x, where w denotes the neural network’s parameters.",2 Structured Prediction Energy Networks,[0],[0]
Belanger and McCallum (2016) separate the energy function into global and local terms.,2 Structured Prediction Energy Networks,[0],[0]
"The role of the local terms is to capture the dependency among input x and each individual output variable yi, while the global term aims to capture long-range dependencies among output variables.
",2 Structured Prediction Energy Networks,[0],[0]
Prediction in SPENs requires finding ŷ,2 Structured Prediction Energy Networks,[0],[0]
=,2 Structured Prediction Energy Networks,[0],[0]
argminy∈Y,2 Structured Prediction Energy Networks,[0],[0]
"Ew(y,x)",2 Structured Prediction Energy Networks,[0],[0]
for the given input x.,2 Structured Prediction Energy Networks,[0],[0]
This inference problem is solved using gradient descent.,2 Structured Prediction Energy Networks,[0],[0]
"However, the energy surface is non-convex, which prevents gradient descent inference from finding the exact structure ymin that globally minimizes the energy function.",2 Structured Prediction Energy Networks,[0],[0]
One approach to address this problem is to parameterize the energy function such that the SPEN is convex in the output variables y,2 Structured Prediction Energy Networks,[0],[0]
"(Amos et al., 2017), but this limits the representational power of SPENs.",2 Structured Prediction Energy Networks,[0],[0]
"Al-
though gradient descent inference does not guarantee an exact solution, it has successfully been used in several domains such as multi-label classification (Belanger and McCallum, 2016), imagesegmentation (Gygli et al., 2017), and semantic role labeling (Belanger et al., 2017).",2 Structured Prediction Energy Networks,[0],[0]
"Different methods have been introduced for training SPENs: margin-based training (Belanger and McCallum, 2016), end-to-end learning (Belanger et al., 2017), and value matching (Gygli et al., 2017).",3 Rank-Based Training of SPENs,[0],[0]
"Margin-based training enforces the energy of the ground truth structure to be lower than the energy of every incorrect structure by a margin, which is calculated as the Hamming loss between the two structures.",3 Rank-Based Training of SPENs,[0],[0]
End-to-end learning unrolls the energy minimization into a differentiable computation graph to output the predicted structure.,3 Rank-Based Training of SPENs,[0],[0]
It then trains the model by directly minimizing the loss between the predicted and ground-truth structures.,3 Rank-Based Training of SPENs,[0],[0]
"Finally, the value matching approach trains SPENs such that the energy value matches the value of a given target function, such as the L2 distance between the ground-truth and predicted structures.
",3 Rank-Based Training of SPENs,[0],[0]
All of these methods strongly depend on the existence of the ground truth values either as labeled data or as the value of a function applied to it.,3 Rank-Based Training of SPENs,[0],[0]
"While dependence of the margin-based and endto-end learning approaches on the labeled data is explicit, this dependency in the case of valuematching may not be obvious.",3 Rank-Based Training of SPENs,[0],[0]
"In the absence of labeled data, we have to use the model’s predictions instead, for training.",3 Rank-Based Training of SPENs,[0],[0]
"These predictions are often incorrect, especially at early stages of training.",3 Rank-Based Training of SPENs,[0],[0]
"As a result, value-matching training is constrained to match the score of these predictions with the value of the energy function defined by SPEN.",3 Rank-Based Training of SPENs,[0],[0]
"This requires matching several incorrect structures for a given input, which hinders gradient descent inference from finding the exact solution by introducing many local optima.",3 Rank-Based Training of SPENs,[0],[0]
"To address this problem, we use a ranking objective similar to SampleRank",3 Rank-Based Training of SPENs,[0],[0]
"(Rohanimanesh et al., 2011) such that it preserves the optimum points of the score function.
",3 Rank-Based Training of SPENs,[0],[0]
"In general, if SPEN ranks every pair of output structures identical to the score function, the optimum points of the score function match those of SPEN.",3 Rank-Based Training of SPENs,[0],[0]
"However, forcing the ranking constraint for every pair of output structures is not tractable, so
we need to approximate it by sampling some candidate pairs.",3 Rank-Based Training of SPENs,[0],[0]
"Given a score function V (y,x), we are able to rank every two consecutive candidate structures based on their score values.",3 Rank-Based Training of SPENs,[0],[0]
Consider two candidate output structures y1 and y2 for the given input x.,3 Rank-Based Training of SPENs,[0],[0]
"We define yh and yl based on the score function as the following:
yh = argmax y∈{y1,y2} V (y,x),
yl = argmin y∈{y1,y2} V (y,x).",3 Rank-Based Training of SPENs,[0],[0]
"(1)
We expect that these two structures have the same ranking with respect to Ew(.,x), which can be described as: α(V (yh,x)",3 Rank-Based Training of SPENs,[0],[0]
"− V (yl,x))",3 Rank-Based Training of SPENs,[0],[0]
"< Ew(yh,x)−Ew(yl,x), where α is a tunable positive scalar.",3 Rank-Based Training of SPENs,[0],[0]
"Therefore, the rank-based objective minimizes the constraint violations:
min w
∑ x∈D [α(V (yh,x)− V (yl,x))−
Ew(yh,x) +",3 Rank-Based Training of SPENs,[0],[0]
"Ew(yl,x)]+ (2)
",3 Rank-Based Training of SPENs,[0],[0]
"[.]+ is max(., 0).",3 Rank-Based Training of SPENs,[0],[0]
Figure 1 shows a ranking violation for two structures y1 and y2 for a given x. The arrows indicate the direction of update over the energy surface.,3 Rank-Based Training of SPENs,[0],[0]
"Note that we ignore the dependence of y on w, which introduces approximation in the gradient of Eq. 2.",3 Rank-Based Training of SPENs,[0],[0]
"For the supervised setting, Belanger et al. (2017) address this problem by unrolling the inference steps as an inference network and back-propagating through the inference network.",3 Rank-Based Training of SPENs,[0],[0]
We leave exploring similar approaches for rank-based training for future work.,3 Rank-Based Training of SPENs,[0],[0]
"To compute Eq. 2, we need to find configurations yi and yj such that both are candidate solutions for argminy∈Y Ew(y,x).",3 Rank-Based Training of SPENs,[0],[0]
"If not, the number of required samples would be exponential in |Y|.",3 Rank-Based Training of SPENs,[0],[0]
"Since at test time we use gradient descent inference, a similar method is used for generating candidate structures: the trajectory of points in the inference mechanism is used as the set of possible candidates.",3 Rank-Based Training of SPENs,[0],[0]
The idea of deterministic sampling from SPEN energy surface was first introduced by David Belanger (2017).,3 Rank-Based Training of SPENs,[0],[0]
"We define the inference trajectory, T (x), as a sequence of output structures generated using projected gradient descent inference in order to find the minimum solution of Ew(.,x).
",3 Rank-Based Training of SPENs,[0.952458624301998],"['We draw a collection of samples (xa, xb) from the prior over world states, and then generate for each sample a sequence of distractors (x′a, xb) from p(x ′ a|xb) (we assume access to both of these distributions from the problem representation).']"
"Given a random initial structure y0, we define the inference trajectory as: T (x) =
{y1, · · · ,ym}, where yt+1 = Py∈∆L(yt − η ∂∂yEw(yt,x)).",3 Rank-Based Training of SPENs,[0],[0]
Py∈∆L projects the values of y onto the probability simplex ∆L overL values that each variable y can take.,3 Rank-Based Training of SPENs,[0],[0]
"For each input x in the training data, we find the first consecutive structures yi, yi+1 ∈ T (x) that violate the ranking constraint, then use Eq. 2 to reduce the number of violations.",3 Rank-Based Training of SPENs,[0],[0]
"Algorithm 1 describes the complete training algorithm.
",3 Rank-Based Training of SPENs,[0],[0]
"Algorithm 1 Rank-based training of SPEN D ← unlabeled mini-batch of training data V (., .)← scoring function Ew(., .)← input SPEN for each x in D do T (x)← samples using GD inference in Ew(.,x).",3 Rank-Based Training of SPENs,[0],[0]
"ξ ← ∅. for each pair (yi,yi+1) in T (x) do
Construct yh and yl using Eq.1 if α(V (yh,x)",3 Rank-Based Training of SPENs,[0],[0]
"− V (yl,x))",3 Rank-Based Training of SPENs,[0],[0]
"> Ew(yh,x)",3 Rank-Based Training of SPENs,[0],[0]
"−
Ew(yl,x) then ξ ← ξ ∪ (x,yh,yl).
end if end for Optimize Eq.2 using ξ.
end for",3 Rank-Based Training of SPENs,[0],[0]
"To show the success of rank-based learning with indirect supervision, we conduct experiments on citation field extraction as an instance of structured prediction problems.",4 Citation Field Extraction,[0],[0]
"The goal of citation field extraction is to segment citation text into its constituent parts such as Author, Title, Journal, Page, and Date.",4 Citation Field Extraction,[0],[0]
"We used the Cora citation dataset (Seymore et al., 1999), which includes 100 labeled examples as the test set and another 100 labeled examples for the validation set.",4 Citation Field Extraction,[0],[0]
"Our training data consists of 300 training examples from the Cora citation data set for which we dismiss the labels,
as well as another 700 unlabeled citations acquired across the web, which adds up to 1000 unlabeled data points.",4 Citation Field Extraction,[0],[0]
Each token can be labeled with one of 13 possible tags.,4 Citation Field Extraction,[0],[0]
"We use fixed-length input data by padding all citation text to the maximum citation length in the dataset, which is 118 tokens.",4 Citation Field Extraction,[0],[0]
"We report token-level accuracy measured on non-pad tokens.
",4 Citation Field Extraction,[0],[0]
We provide the learning algorithm with a human written score function that takes the citation text and predicted tags as input.,4 Citation Field Extraction,[0],[0]
The score function then checks for violations of its rules and penalizes the predicted tags accordingly.,4 Citation Field Extraction,[0],[0]
Figure 2 shows examples of rules in the score function.,4 Citation Field Extraction,[0],[0]
"Our complete score function consists of around 50 rules.
",4 Citation Field Extraction,[0],[0]
We used two 2-layer neural networks with 1000 and 500 hidden nodes to parameterize the local and global energy functions of SPEN.,4 Citation Field Extraction,[0],[0]
"We examine different α (Eq. 2) values of 0.1, 1.0, 2.0, 5.0, and 10.0, and setting α value to 2.0 has the best performance on the validation set.",4 Citation Field Extraction,[0],[0]
"We use gradient descent inference with ten gradient descent steps and η = 0.1 for both training and test.
",4 Citation Field Extraction,[0],[0]
We include the results of generalized expectation (GE) from Mann and McCallum (2010) that use the same dataset and setting.,4 Citation Field Extraction,[0],[0]
"Our results show that R-SPEN achieves significantly better tokenlevel accuracy as compared to GE.
",4 Citation Field Extraction,[0],[0]
We also compare R-SPEN with different inference algorithms that search using the score function to find the best configuration with maximum score.,4 Citation Field Extraction,[0],[0]
The results of these are listed in Table 1.,4 Citation Field Extraction,[0],[0]
Greedy search first randomly initializes the output tags and then iteratively replaces each assigned tag with a tag that results in the maximum score until the end of the citation is reached.,4 Citation Field Extraction,[0],[0]
"This process is repeated until convergence, measured by no tag changing in an iteration.",4 Citation Field Extraction,[0],[0]
"To avoid the effects of random initialization, this is repeated with varied number of random restarts, as reported in Table 1, where the best configuration is used in the scores reported.",4 Citation Field Extraction,[0],[0]
"For the baseline that implements beam search, each citation is labeled by employing a beam search on the space of all tags for each token and their subsequent configurations, while keeping track of the best k configurations from one token to the next.",4 Citation Field Extraction,[0],[0]
"This search is further augmented by restarting the search from the best k found after one complete search, for a total of 10 times and 10 random restarts.
",4 Citation Field Extraction,[0],[0]
"Consulting Table 1, we can confirm that both greedy search and beam search find much better output structures in term of score values as compared to R-SPEN; however, they achieve poor accuracy because the domain knowledge function does not comprehensively provide rules regarding all possible output structures.",4 Citation Field Extraction,[0],[0]
We report the average score values of the R-SPEN predictions on test data as a function of training iterations in Figure 3.,4 Citation Field Extraction,[0],[0]
"Within 1000 iterations, R-SPEN is able to achieve a test set accuracy of 38%, outperforming all baselines, while the average score is -18.0.",4 Citation Field Extraction,[0],[0]
"R-SPEN generalizes beyond the domain knowledge function because it successfully captures the correlation among output variables through rank-based training on unlabeled data, so its predictions may have lower score values but are more accurate.
",4 Citation Field Extraction,[0],[0]
The test time inference of R-SPEN is much faster than search algorithms because SPEN provides efficient approximate inference.,4 Citation Field Extraction,[0],[0]
"Generalized Expectation (GE) (Mann and McCallum, 2010), Posterior Regularization (Ganchev et al., 2010) and Constraint Driven Learning (Chang et al., 2007) are among well-known approaches to learn from domain knowledge decomposed over a set of constraints or labeled features.",5 Related Work,[0],[0]
"However, these methods cannot learn from black box domain knowledge based score functions.",5 Related Work,[0],[0]
"Score functions of this type are abundant in
various fields, for example, when the score is the result of evaluating a non-differentiable function over output structures.
",5 Related Work,[0],[0]
Stewart and Ermon (2017) train a neural network using a score function that guides the training based on physics of moving objects.,5 Related Work,[0],[0]
They have defined a differentiable score function which provides the learning algorithm with the gradient of the score function.,5 Related Work,[0],[0]
"However, in our approach the score function could be any complex non-differentiable function.
",5 Related Work,[0],[0]
Peng et al. (2017) and Iyyer et al. (2017) use energy-based max-margin training for learning from an implicit source of supervision.,5 Related Work,[0],[0]
This can be viewed as a score function evaluating the predicted output structure based on some real-world domain.,5 Related Work,[0],[0]
"For example, if the output structure is the SQL query associated with a natural language question, the score function can be specified as the Jaccard similarity of the extracted cells from the table using the generated SQL query and the set of
gold answers for the natural language query as in Iyyer et al (2017).",5 Related Work,[0],[0]
We have introduced a method to train structured prediction energy networks with indirect supervision that is derived from domain knowledge.,6 Conclusion and Future Work,[0],[0]
"This domain knowledge is a scalar function that is represented in the form of certain set of rules, easily provided by humans.",6 Conclusion and Future Work,[0],[0]
"By using a rank-based training we are able to effectively generalize beyond the domain knowledge function in problem instances where we do not have access to labeled data, thus establishing a viable option for solving structured prediction problems in those regimes.
",6 Conclusion and Future Work,[0],[0]
R-SPEN only uses unlabeled data and domain knowledge for training.,6 Conclusion and Future Work,[0],[0]
We should also effectively benefit from annotated data if any is available for the task.,6 Conclusion and Future Work,[0],[0]
"This can be accomplished by augmenting the domain knowledge with rules that take into account the distance between predicted and ground truth labels.
",6 Conclusion and Future Work,[0],[0]
"In the future, we wish to explore the effectiveness of R-SPEN on various tasks using domain knowledge functions with varying degrees of complexity.",6 Conclusion and Future Work,[0],[0]
This research was funded by DARPA grant FA8750-17-C-0106.,Acknowledgments,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government.",Acknowledgments,[0],[0]
This paper introduces rank-based training of structured prediction energy networks (SPENs).,abstractText,[0],[0]
Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined using domain knowledge.,abstractText,[0],[0]
"We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function.",abstractText,[0],[0]
Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.,abstractText,[0],[0]
Training Structured Prediction Energy Networks with Indirect Supervision,title,[0],[0]
"Inspired by human beings’ capabilities to transfer knowledge across tasks, transfer learning aims to leverage knowledge from a source domain to improve the learning performance or minimize the number of labeled examples required in a target domain.",1. Introduction,[0],[0]
It is of particular significance when tackling tasks with limited labeled examples.,1. Introduction,[0],[0]
"Transfer learning has proved its wide applicability in, for example,
1Hong Kong University of Science and Technology, Hong Kong 2Tencent AI Lab, Shenzhen, China.",1. Introduction,[0],[0]
"Correspondence to: Ying Wei <judyweiying@gmail.com>, Qiang Yang <qyang@cse.ust.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
image classification (Long et al., 2015), sentiment classification (Blitzer et al., 2006), dialog systems (Mo et al., 2016), and urban computing (Wei et al., 2016).
",1. Introduction,[0],[0]
"Three key research issues in transfer learning, pointed by Pan & Yang, are when to transfer, how to transfer, and what to transfer.",1. Introduction,[0],[0]
"Once transfer learning from a source domain is considered to benefit a target domain (when to transfer), an algorithm (how to transfer) discovers the transferable knowledge across domains (what to transfer).",1. Introduction,[0],[0]
"Different algorithms are likely to discover different transferable knowledge, and thereby lead to uneven transfer learning effectiveness which is evaluated by the performance improvement over non-transfer baselines in a target domain.",1. Introduction,[0],[0]
"To achieve the optimal performance improvement for a target domain given a source domain, researchers may try tens to hundreds of transfer learning algorithms covering instance (Dai et al., 2007), parameter (Tommasi et al., 2014), and feature (Pan et al., 2011) based algorithms.",1. Introduction,[0],[0]
Such bruteforce exploration is computationally expensive and practically impossible.,1. Introduction,[0],[0]
"As a tradeoff, a sub-optimal improvement is usually obtained from a heuristically selected algorithm, which unfortunately requires considerable expertise in an ad-hoc and unsystematic manner.
",1. Introduction,[0],[0]
Exploring different algorithms is not the only way to optimize what to transfer.,1. Introduction,[0],[0]
"Previous transfer learning experiences do also help, which has been widely accepted in educational psychology (Luria, 1976; Belmont et al., 1982).",1. Introduction,[0],[0]
Human beings sharpen transfer learning skills of deciding what to transfer by conducting meta-cognitive reflection on diverse transfer learning experiences.,1. Introduction,[0],[0]
"For example, children who are good at playing chess may transfer mathematical skills, visuospatial skills, and decision making skills learned from chess to solve arithmetic problems, to solve pattern matching puzzles, and to play basketball, respectively.",1. Introduction,[0],[0]
"At a later age, it will be easier for them to decide to transfer mathematical and decision making skills learned from chess, rather than visuospatial skills, to market investment.",1. Introduction,[0],[0]
"Unfortunately, all existing transfer learning algorithms transfer from scratch and ignore previous transfer learning experiences.
",1. Introduction,[0],[0]
"Motivated by this, we propose a novel transfer learning framework called Learning to Transfer (L2T).",1. Introduction,[0],[0]
"The key idea of the L2T is to enhance the transfer learning effectiveness from a source to a target domain by leveraging previous
transfer learning experiences to optimize what and how to transfer between them.",1. Introduction,[0],[0]
"To achieve the goal, we establish the L2T in two stages.",1. Introduction,[0],[0]
"During the first stage, we encode each transfer learning experience into three components: a pair of source and target domains, the transferred knowledge between them parameterized as latent feature factors, and performance improvement.",1. Introduction,[0],[0]
We learn from all experiences a reflection function which maps a pair of domains and the transferred knowledge between them to the performance improvement.,1. Introduction,[0],[0]
"The reflection function, therefore, is believed to encrypt transfer learning skills of deciding what and how to transfer.",1. Introduction,[0],[0]
"In the second stage, what to transfer between a newly arrived pair of domains is optimized so that the value of the learned reflection function, matching to the performance improvement, is maximized.
",1. Introduction,[0],[0]
The contribution of this paper lies in that we propose a novel transfer learning framework which opens a new door to improve transfer learning effectiveness by taking advantage of previous transfer learning experiences.,1. Introduction,[0],[0]
The L2T can discover more transferable knowledge in a systematic and automatic fashion without requiring considerable expertise.,1. Introduction,[0],[0]
"We have also provided theoretic analyses to its algorithmic stability and generalization bound, and conducted comprehensive empirical studies showing the L2T’s superiority over state-of-the-art transfer learning algorithms.",1. Introduction,[0],[0]
"Transfer Learning Pan & Yang identified three key research issues in transfer learning as what, how, and when to transfer.",2. Related Work,[0],[0]
"Parameters (Yang et al., 2007a; Tommasi et al., 2014), instances (Dai et al., 2007), or latent feature factors (Pan et al., 2011) can be transferred between domains.",2. Related Work,[0],[0]
"A few works (Yang et al., 2007a; Tommasi et al., 2014) transfer parameters from source domains to regularize parameters of SVM-based models in a target domain.",2. Related Work,[0],[0]
"In (Dai et al., 2007), a basic learner in a target domain is boosted by borrowing the most useful source instances.",2. Related Work,[0],[0]
Various techniques capable of learning transferable latent feature factors between domains have been investigated extensively.,2. Related Work,[0],[0]
"These techniques include manually selected pivot features (Blitzer et al., 2006), dimension reduction (Pan et al., 2011; Baktashmotlagh et al., 2013; 2014), collective matrix factorization (Long et al., 2014), dictionary learning and sparse coding (Raina et al., 2007; Zhang et al., 2016), manifold learning (Gopalan et al., 2011; Gong et al., 2012), and deep learning (Yosinski et al., 2014; Long et al., 2015; Tzeng et al., 2015).",2. Related Work,[0],[0]
"Unlike L2T, all existing transfer learning studies transfer from scratch, i.e., only considering the pair of domains of interest but ignoring previous transfer learning experiences.",2. Related Work,[0],[0]
"Better yet, L2T can even collect all algorithms’ wisdom together, considering that any algorithm mentioned above can be applied in a transfer learning experience.
",2. Related Work,[0],[0]
"Multi-task Learning Multi-task learning (Caruana, 1997; Argyriou et al., 2007) trains multiple related tasks simultaneously and learns shared knowledge among tasks, so that all tasks reinforce each other in generalization abilities.",2. Related Work,[0],[0]
"However, multi-task learning assumes that training and testing examples follow the same distribution, as Figure 1 shows, which is different from transfer learning we focus on.
",2. Related Work,[0],[0]
Lifelong Learning,2. Related Work,[0],[0]
"Assuming a new learning task to lie in the same environment as training tasks, learning to learn (Thrun & Pratt, 1998) or meta-learning (Maurer, 2005; Finn et al., 2017; Al-Shedivat et al., 2018) transfers the knowledge shared among training tasks to the new task.",2. Related Work,[0],[0]
"(Ruvolo & Eaton, 2013; Pentina & Lampert, 2015) consider lifelong learning as online meta-learning.",2. Related Work,[0],[0]
"Though L2T and lifelong (meta) learning both aim to improve a learning system by leveraging histories, L2T differs from them in that each historical experience we consider is a transfer learning task rather than a traditional learning task as Figure 1 illustrates.",2. Related Work,[0],[0]
Thus we learn transfer learning skills instead of task-sharing knowledge.,2. Related Work,[0],[0]
We begin by first briefing the proposed L2T framework.,3. Learning to Transfer,[0],[0]
"Then we detail the two stages in L2T, i.e., learning transfer learning skills from previous transfer learning experiences and applying those skills to infer what and how to transfer for a future pair of source and target domains.",3. Learning to Transfer,[0],[0]
"A L2T agent previously conducted transfer learning several times, and kept a record of Ne transfer learning experiences.",3.1. The L2T Framework,[0],[0]
"We define each transfer learning experience as Ee = (〈Se, Te〉, ae, le) in which Se = {Xse,yse} and Te = {Xte,yte} denote a source domain and a target domain, respectively.",3.1. The L2T Framework,[0],[0]
X∗e ∈,3.1. The L2T Framework,[0],[0]
"Rn ∗ e×m represents the feature matrix if either domain has n∗e examples in a m-dimensional feature space X ∗e , where the superscript ∗ can be either s or t to denote a source or a target domain.",3.1. The L2T Framework,[0],[0]
y∗e ∈ Y∗e denotes the vector of labels with the length being n∗le.,3.1. The L2T Framework,[0],[0]
"The number of target labeled examples is much smaller than that of source labeled examples, i.e., ntle nsle.",3.1. The L2T Framework,[0],[0]
"We focus on the
learned reflection function f ( 1 ), we optimize the transferred knowledge between them, i.e., W∗Ne+1, by maximizing the value of f ( 2 ).
setting X se = X te and Yse = Yte for each pair of domains.",3.1. The L2T Framework,[0],[0]
ae ∈,3.1. The L2T Framework,[0],[0]
"A = {a1, · · · , aNa} denotes a transfer learning algorithm having been applied between Se and Te.",3.1. The L2T Framework,[0],[0]
Suppose that the transferred knowledge by the algorithm ae can be parameterized as We.,3.1. The L2T Framework,[0],[0]
"Finally, each transfer learning experience is labeled by the performance improvement ratio le = pste /p",3.1. The L2T Framework,[0],[0]
"t e, where pte is the learning performance (e.g., classification accuracy) on a test dataset in Te without transfer and pste is that on the same test dataset after transferring We from Se.",3.1. The L2T Framework,[0],[0]
"With Ne transfer learning experiences {E1, · · · , ENe} as the input, the L2T agent learns a function f such that f(Se, Te,We) approximates le as shown in the training stage of Figure 2.",3.1. The L2T Framework,[0],[0]
We call f a reflection function which encrypts meta-cognitive transfer learning skills - what and how to transfer can maximize the improvement ratio given a pair of domains.,3.1. The L2T Framework,[0],[0]
"Whenever a new pair of domains 〈SNe+1, TNe+1〉 arrives, the L2T agent can optimize the knowledge to be transferred, i.e., W∗Ne+1, by maximizing the value of f (see step 2 of the testing stage in Figure 2).",3.1. The L2T Framework,[0],[0]
Transfer learning algorithms applied can vary from experience to experience.,3.2. Parameterizing What to Transfer,[0],[0]
Uniformly parameterizing “what to transfer” for any algorithm out of the base algorithm set A is a prerequisite for learning the reflection function.,3.2. Parameterizing What to Transfer,[0],[0]
"In this work, we consider A to contain algorithms transferring single-level latent feature factors, because existing parameter-based and instance-based algorithms cannot address the transfer learning setting we focus on (i.e., X es = X et and Yes = Yet ).",3.2. Parameterizing What to Transfer,[0],[0]
"Though limited parameter-based algorithms (Yang et al., 2007a; Tommasi et al., 2014) can transfer across domains in heterogeneous label spaces, they can only handle binary classification problems.",3.2. Parameterizing What to Transfer,[0],[0]
"Deep neural network based algorithms (Yosinski et al., 2014; Long et al., 2015; Tzeng et al., 2015) transferring latent feature factors in multiple levels are left for our future research.",3.2. Parameterizing What to Transfer,[0],[0]
"As a result, we parameterize what to transfer with a latent feature factor matrix W which is elaborated in the following.
",3.2. Parameterizing What to Transfer,[0],[0]
Latent feature factor based algorithms aim to learn domaininvariant feature factors across domains.,3.2. Parameterizing What to Transfer,[0],[0]
Consider classifying dog pictures as a source domain and cat pictures as a target domain.,3.2. Parameterizing What to Transfer,[0],[0]
"The domain-invariant feature factors may include eyes, mouth, tails, etc.",3.2. Parameterizing What to Transfer,[0],[0]
"What to transfer, in this case, is the shared feature factors across domains.",3.2. Parameterizing What to Transfer,[0],[0]
"The way of defining domain-invariant feature factors dictates two groups of latent feature factor based algorithms, i.e., common latent space based and manifold ensemble based algorithms.
",3.2. Parameterizing What to Transfer,[0],[0]
"Common Latent Space Based This line of algorithms, including but not limited to TCA (Pan et al., 2011),",3.2. Parameterizing What to Transfer,[0],[0]
"LSDT (Zhang et al., 2016), and DIP (Baktashmotlagh et al., 2013), assumes that domain-invariant feature factors lie in a single shared latent space.",3.2. Parameterizing What to Transfer,[0],[0]
We denote by ϕ,3.2. Parameterizing What to Transfer,[0],[0]
the function mapping original feature representation into the latent space.,3.2. Parameterizing What to Transfer,[0],[0]
"If ϕ is linear, it can be represented as an embedding matrix W ∈ Rm×u where u is the dimensionality of the latent space.",3.2. Parameterizing What to Transfer,[0],[0]
"Therefore, we can parameterize what to transfer we focus on with W which describes u latent feature factors.",3.2. Parameterizing What to Transfer,[0],[0]
"Otherwise, if ϕ is nonlinear, what to transfer can still be parameterized with W. Though a nonlinear ϕ is not explicitly specified in most cases such as LSDT using sparse coding, target examples represented in the latent space Zte=ϕ(X t e)∈Rn t e×u are always available.",3.2. Parameterizing What to Transfer,[0],[0]
"Consequently, we obtain the similarity metric matrix (Cao et al., 2013) in the latent space, i.e., G=(Xte) †Zte(Z t e) T",3.2. Parameterizing What to Transfer,[0],[0]
"[(Xte) T ]†∈Rm×m according to XteG(X t e) T =Zte(Z t e) T , where (Xte) † is the pseudo-inverse of Xte.",3.2. Parameterizing What to Transfer,[0],[0]
"LDL decomposition on G = LDL T brings the latent feature factor matrix W = LD1/2.
",3.2. Parameterizing What to Transfer,[0],[0]
"Manifold Ensemble Based Initiated by Gopalan et al., manifold ensemble based algorithms consider that a source and a target domain share multiple subspaces (of the same dimension) as points on the Grassmann manifold between them.",3.2. Parameterizing What to Transfer,[0],[0]
"The representation of target examples on u domain-invariant latent factors turns to Zt(nu)e =[ϕ1(Xte), · · ·, ϕnu(Xte)] ∈",3.2. Parameterizing What to Transfer,[0],[0]
"Rn t e×nuu, if nu subspaces on the manifold are sampled.",3.2. Parameterizing What to Transfer,[0],[0]
"When all continuous subspaces on the manifold are sampled, i.e., nu →∞, Gong et al. proved
that Zt(∞)e (Z t(∞) e )T =XteG(X t e)
T where G is the similarity metric matrix.",3.2. Parameterizing What to Transfer,[0],[0]
"For computational details of G, please refer to (Gong et al., 2012).",3.2. Parameterizing What to Transfer,[0],[0]
"W=LD1/2 with L and D obtained from performing LDL decomposition on G=LDLT , therefore, is also qualified to represent latent feature factors distributed in a series of subspaces on a manifold.",3.2. Parameterizing What to Transfer,[0],[0]
"The goal here is to learn a reflection function f such that f(Se, Te,We) can approximate le for all experiences {E1, · · · , ENe}.",3.3. Learning from Experiences,[0],[0]
"The improvement ratio le is closely related to two aspects: 1) the difference between a source and a target domain in the shared latent space, and 2) the discriminative ability of a target domain in the latent space.",3.3. Learning from Experiences,[0],[0]
"The smaller difference guarantees more overlap between domains in the latent space, which signifies more transferable latent feature factors and higher improvement ratios as a result.",3.3. Learning from Experiences,[0],[0]
The discriminative ability of a target domain in the latent space is also vital to improve performances.,3.3. Learning from Experiences,[0],[0]
"Therefore, we build f to take both aspects into consideration.
",3.3. Learning from Experiences,[0],[0]
"The Difference between a Source and a Target Domain We follow (Pan et al., 2011) and adopt the maximum mean discrepancy (MMD) (Gretton et al., 2012b) to measure the difference between domains.",3.3. Learning from Experiences,[0],[0]
"By mapping two domains into the reproducing kernel Hilbert space (RKHS), MMD empirically evaluates the distance between the mean of source examples and that of target examples:
d̂2e(X s eWe,X t eWe)
= ∥∥∥∥",3.3. Learning from Experiences,[0],[0]
1nse nse∑ i=1 φ(xseiWe)−,3.3. Learning from Experiences,[0],[0]
1 nte nte∑ j=1 φ(xtejWe) ∥∥∥∥,3.3. Learning from Experiences,[0],[0]
2,3.3. Learning from Experiences,[0],[0]
"H
= 1
(nse)2 nse∑ i,i′=1 K(xseiWe,xsei′We)
",3.3. Learning from Experiences,[0],[0]
"+ 1
(nte)2 nte∑ j,j′=1 K(xtejWe,xtej′We)
",3.3. Learning from Experiences,[0],[0]
"− 2 nsente
nse,n t e∑
i,j=1
K(xseiWe,xtejWe), (1)
where xtej is the j-th example in X t e, and φ maps from the u-dimensional latent space to the RKHS H. K(·, ·) = 〈φ(·), φ(·)〉 is the kernel function.",3.3. Learning from Experiences,[0],[0]
Different kernels K lead to different MMD distances and thereby different values of f .,3.3. Learning from Experiences,[0],[0]
Thus learning the reflection function f is equivalent to optimizing K so that the MMD distance can well characterize the improvement ratio le for all pairs of domains.,3.3. Learning from Experiences,[0],[0]
"Inspired by multi-kernel MMD (Gretton et al., 2012b), we parameterize K as a linear combination of Nk PSD kernels, i.e., K=∑Nkk=1 βkKk (βk≥0, ∀k), and learn the coefficients β=[β1,· · ·, βNk ] instead.",3.3. Learning from Experiences,[0],[0]
"Using β, the MMD can be rewritten as d̂2e(XseWe,XteWe)= ∑Nk k=1 βkd̂ 2 e(k)(X s eWe,X t eWe)=
βT",3.3. Learning from Experiences,[0],[0]
"d̂e, where d̂e=[d̂2e(1),· · ·, d̂2e(Nk)] with d̂ 2 e(k) computed by the k-th kernel Kk.",3.3. Learning from Experiences,[0],[0]
"In this paper, we consider RBF kernels Kk(a,b)=exp(−‖a−b‖2/δk) by varying the bandwidth δk.
",3.3. Learning from Experiences,[0],[0]
"Unfortunately, the MMD alone is insufficient to measure the difference between domains.",3.3. Learning from Experiences,[0],[0]
The distance variance among all pairs of instances across domains is also required to fully characterize the difference.,3.3. Learning from Experiences,[0],[0]
A pair of domains with small MMD but extremely high variance still have little overlap.,3.3. Learning from Experiences,[0],[0]
"Equation (1) is actually the empirical estimation of d2e(XseWe,XteWe) =",3.3. Learning from Experiences,[0],[0]
"Exsexs′e xtext′e h(x s e,x s′ e ,x t e,x t′ e )",3.3. Learning from Experiences,[0],[0]
"(Gretton et al., 2012b) where h(xse,x s′ e ,x t e,x t′ e ) = K(xseWe,xs′e We)+K(xteWe,xt′e",3.3. Learning from Experiences,[0],[0]
"We)− K(xseWe, xt′e",3.3. Learning from Experiences,[0],[0]
We),3.3. Learning from Experiences,[0],[0]
"− K(xs′e We,xteWe).",3.3. Learning from Experiences,[0],[0]
"Consequently, the distance variance, σ2e , equals σ2e(X s eWe,X t eWe) =Exsexs′e xtext′e",3.3. Learning from Experiences,[0],[0]
"[(h(x s e,x s′ e ,x t e,x t′ e )
−Exsexs′e xtext′e h(x s e,x s′ e ,x t e,x t′ e )) 2].
",3.3. Learning from Experiences,[0],[0]
"To be consistent with the MMD characterized with Nk PSD kernels, we rewrite σ2e = βTQeβ where Qe = cov(h) =[
σe(1,1) ··· σe(1,Nk)··· ··· ··· σe(Nk,1) ···σe(Nk,Nk)
] .",3.3. Learning from Experiences,[0],[0]
"Each element σe(k1,k2) = cov(hk1 ,
hk2) =",3.3. Learning from Experiences,[0],[0]
E,3.3. Learning from Experiences,[0],[0]
[(hk1−Ehk1)(hk2−Ehk2)].,3.3. Learning from Experiences,[0],[0]
Note that Ehk1 is shorthand for Exsexs′e xtext′e,3.3. Learning from Experiences,[0],[0]
"hk1(x s e,x s′ e ,x t e,x t′ e ) where hk1 is calculated using the k1-th kernel.",3.3. Learning from Experiences,[0],[0]
"We detail the empirical estimate Q̂e of Qe in the supplementary due to page limit.
",3.3. Learning from Experiences,[0],[0]
"The Discriminative Ability of a Target Domain In view of limited labeled examples in a target domain, we resort to unlabeled examples to evaluate the discriminative ability.",3.3. Learning from Experiences,[0],[0]
The principles of the unlabeled discriminant criterion are two-fold: 1) similar examples should still be neighbours after being embedded into the latent space; and 2) dissimilar examples should be far away.,3.3. Learning from Experiences,[0],[0]
"We adopt the unlabeled discriminant criterion proposed in (Yang et al., 2007b),
τe = tr(WTe S N e We)/tr(W T e S L e We),
where SLe = ∑nte
j,j′=1 Hjj′ (nte) 2",3.3. Learning from Experiences,[0],[0]
(x t ej,3.3. Learning from Experiences,[0],[0]
− xtej′)(xtej,3.3. Learning from Experiences,[0],[0]
"− xtej′)T
is the local scatter covariance matrix with the neighbour information Hjj′ defined as Hjj′ ={ K(xtej ,xtej′), if xtej ∈ Nr(xtej′) and xtej′ ∈ Nr(xtej) 0, otherwise .
",3.3. Learning from Experiences,[0],[0]
"If xtej and x t ej′ are mutual r-nearest neighbours to each other, Hjj′ equals the kernel value K(xtej ,xtej′).",3.3. Learning from Experiences,[0],[0]
"By maximizing the unlabeled discriminant criterion τe, the local scatter covariance matrix guarantees the first principle, while
SNe = ∑nte",3.3. Learning from Experiences,[0],[0]
"j,j′=1 K(xtej ,xtej′ )−Hjj′
(nte) 2 (x
t ej",3.3. Learning from Experiences,[0],[0]
− xtej′)(xtej,3.3. Learning from Experiences,[0],[0]
"− xtej′)T ,
the non-local scatter covariance matrix, enforces the second principle.",3.3. Learning from Experiences,[0],[0]
τe also depends on kernels which in this case indicate different neighbour information and different degrees of similarity between neighboured examples.,3.3. Learning from Experiences,[0],[0]
"With τe(k) obtained from the k-th kernel Kk, the unlabeled discriminant criterion τe can be written as τe = ∑Nk k=1 βkτe(k) = β
T τ e where τ e =",3.3. Learning from Experiences,[0],[0]
"[τe(1), · · · , τe(Nk)].
",3.3. Learning from Experiences,[0],[0]
"The Optimization Problem Combining the two aspects abovementioned to model the reflection function f , we finally formulate the optimization problem as follows,
β∗, λ∗, μ∗, b∗ =
arg min β,λ,μ,",3.3. Learning from Experiences,[0],[0]
b Ne∑ e=1,3.3. Learning from Experiences,[0],[0]
Lh ( βT d̂e,3.3. Learning from Experiences,[0],[0]
+ λβ T Q̂eβ,3.3. Learning from Experiences,[0],[0]
+ μ βT,3.3. Learning from Experiences,[0],[0]
τ,3.3. Learning from Experiences,[0],[0]
"e + b, 1 le )
+ γ1R(β, λ, μ, b),
s.t.",3.3. Learning from Experiences,[0],[0]
"βk ≥ 0, ∀k ∈ {1, · · · , Nk}, λ ≥ 0, μ ≥ 0, (2) where 1/f = βT",3.3. Learning from Experiences,[0],[0]
d̂e +,3.3. Learning from Experiences,[0],[0]
"λβT Q̂eβ + μβT τe + b and Lh(·) is the Huber regression loss (Huber et al., 1964) constraining the value of 1/f to be as close to 1/le as possible.",3.3. Learning from Experiences,[0],[0]
γ1 controls the complexity of the parameters by l2-regularization.,3.3. Learning from Experiences,[0],[0]
"Minimizing the difference between domains, including the MMD distance βT",3.3. Learning from Experiences,[0],[0]
d̂e and the distance variance βT,3.3. Learning from Experiences,[0],[0]
"Q̂eβ, and meanwhile maximizing the discriminant criterion βT τ",3.3. Learning from Experiences,[0],[0]
"e in the target domain will contribute a large performance improvement ratio le (i.e., a small 1/le).",3.3. Learning from Experiences,[0],[0]
"λ and μ balance the importance of the three terms in f , and b is the bias term.",3.3. Learning from Experiences,[0],[0]
"Once the L2T agent has learned the reflection function f(S, T ,W;β∗, λ∗, μ∗, b∗), it takes advantage of the function to optimize what to transfer, i.e., the latent feature factor matrix W, for a newly arrived source domain SNe+1 and a target domain TNe+1.",3.4. Inferring What to Transfer,[0],[0]
The optimal latent feature factor matrix W∗Ne+1 should maximize the value of f .,3.4. Inferring What to Transfer,[0],[0]
"To this end, we optimize the following objective with regard to W, W
∗ Ne+1",3.4. Inferring What to Transfer,[0],[0]
"=argmax W f(SNe+1, TNe+1,W;β ∗ , λ ∗ , μ ∗ , b ∗ )",3.4. Inferring What to Transfer,[0],[0]
"− γ2‖W‖2F
=argmin W
(β ∗ )",3.4. Inferring What to Transfer,[0],[0]
T d̂W + λ ∗ (β ∗ ),3.4. Inferring What to Transfer,[0],[0]
"T Q̂Wβ ∗ + μ ∗ 1 (β∗)T τW + γ2‖W‖2F , (3)
where ‖ · ‖F denotes the matrix Frobenius norm and γ2 controls the complexity of W. The first and second terms in problem (3) can be calculated as
(β ∗ )",3.4. Inferring What to Transfer,[0],[0]
T d̂W = Nk∑ k=1 β,3.4. Inferring What to Transfer,[0],[0]
∗,3.4. Inferring What to Transfer,[0],[0]
"k
[ 1
a2 a∑ i,i′=1 Kk(viW,vi′W)+
1 b2 b∑ j,j′=1 Kk(wjW,wj′W)",3.4. Inferring What to Transfer,[0],[0]
"− 2 ab a,b∑ i,j=1 Kk(viW,wjW) ] ,
(β ∗ )",3.4. Inferring What to Transfer,[0],[0]
"T Q̂Wβ ∗ =
1
n2 − 1 n∑ i,i′=1 Nk∑ k=1 { β ∗",3.4. Inferring What to Transfer,[0],[0]
k,3.4. Inferring What to Transfer,[0],[0]
"[ Kk(viW,vi′W)+
Kk(wiW,wi′W)",3.4. Inferring What to Transfer,[0],[0]
"− 2Kk(viW,wi′W) − 1
n2 n∑ i,i′=1 ( Kk(viW,vi′W)
+ Kk(wiW,wi′W)",3.4. Inferring What to Transfer,[0],[0]
"− 2Kk(viW,wi′W) )",3.4. Inferring What to Transfer,[0],[0]
"]}2 ,
where the shorthand vi = xs(Ne+1)i, vi′ = x s (Ne+1)i′ , wj = xt(Ne+1)j , wj′ =x t (Ne+1)j′ , a=n s Ne+1, and b=n t Ne+1 are used due to space limit.",3.4. Inferring What to Transfer,[0],[0]
"Note that n=min(nsNe+1, n t Ne+1
).",3.4. Inferring What to Transfer,[0],[0]
"The third term in problem (3) can be computed as (β∗)T τW =∑Nk
k=1 β ∗ k tr(WTSNk W) tr(WTSLkW) .",3.4. Inferring What to Transfer,[0],[0]
"We optimize the non-convex prob-
lem (3) w.r.t W by employing a conjugate gradient method in which the gradient is listed in the supplementary material.",3.4. Inferring What to Transfer,[0],[0]
"In this section, we would theoretically investigate how previous transfer learning experiences influence a transfer learning task of interest.",4. Stability and Generalization Bounds,[0],[0]
"We also provide and prove the algorithmic stability and generalization bound for latent feature factor based transfer learning algorithms without experiences considered in the supplementary.
",4. Stability and Generalization Bounds,[0],[0]
"Consider S = {〈S1, T1〉,· · ·, 〈SNe , TNe〉} to be Ne transfer learning experiences or the so-called meta-samples (Maurer, 2005).",4. Stability and Generalization Bounds,[0],[0]
"Let L(S) be our algorithm that learns meta-cognitive knowledge from Ne transfer learning experiences in S and applies the knowledge to the (Ne+1)-th transfer learning task 〈SNe+1, TNe+1〉.",4. Stability and Generalization Bounds,[0],[0]
"To analyse the stability and give the generalization bound, we make an assumption on the distribution from which all Ne transfer learning experiences as meta-samples are sampled.",4. Stability and Generalization Bounds,[0],[0]
"For every environment E we have, all Ne pairs of source and target domains in S are drawn according to an algebraic β-mixing stationary distribution (DE)Ne , which is not i.i.d.. Intuitively, the algebraical β-mixing stationary distribution (see Definition 2 in (Mohri & Rostamizadeh, 2010)) with the β-mixing coefficient β(m)≤β0/mr models the dependence between future samples and past samples by a distance of at least m. The independent block technique (Bernstein, 1927) has been widely adopted to deal with non-i.i.d.",4. Stability and Generalization Bounds,[0],[0]
learning problems.,4. Stability and Generalization Bounds,[0],[0]
"Under this assumption, L(S) is uniformly stable.
",4. Stability and Generalization Bounds,[0],[0]
Theorem 1.,4. Stability and Generalization Bounds,[0],[0]
Suppose that for any xte and for any yte we have ‖xte‖2≤rx and |yte|≤B.,4. Stability and Generalization Bounds,[0],[0]
"Meanwhile, for any e-th transfer learning experience, we assume that the latent feature factor matrix ‖We‖≤ rW .",4. Stability and Generalization Bounds,[0],[0]
"To meet the assumption above, we reasonably simplify L(S) so that the latent feature factor matrix for the (Ne+1)-th transfer learning task is a linear combination of all Ne historical latent factor feature matrices plus a noisy latent feature matrix W satisfying ‖W ‖≤r , i.e., WNe+1= ∑Ne e=1 ceWe+W with each coefficient 0≤ce≤1.",4. Stability and Generalization Bounds,[0],[0]
Our algorithm L(S) is uniformly stable.,4. Stability and Generalization Bounds,[0],[0]
"For any 〈S, T 〉 as the coming transfer learning task, the following inequality holds:∣∣lemp(L(S), (S, T ))",4. Stability and Generalization Bounds,[0],[0]
"− lemp(L(Se0), (S, T ))∣∣
≤",4. Stability and Generalization Bounds,[0],[0]
4(4Ne − 3 + r /rW ),4. Stability and Generalization Bounds,[0],[0]
"B 2rx
λN2e ∼",4. Stability and Generalization Bounds,[0],[0]
O,4. Stability and Generalization Bounds,[0],[0]
"( B2rx λNe ) , (4)
where S = {〈S1, T1〉, · · · , 〈Se0−1, Te0−1〉, 〈Se0 , Te0〉, 〈Se0+1, Te0+1〉, · · · , 〈SNe , TNe〉} denotes the full set of meta-samples, and Se0 = {〈S1, T1〉, · · · , 〈Se0−1, Te0−1〉, 〈Se′0 , Te′0〉, 〈Se0+1, Te0+1〉, · · · , 〈SNe , TNe〉} represents the meta-samples with the e0-th meta-example replaced as 〈Se′0 , Te′0〉.",4. Stability and Generalization Bounds,[0],[0]
"By generalizing S to be meta-samples S and hS to be L2T L(S), we apply Corollary 21 in (Mohri & Rostamizadeh,
2010) to give the generalization bound of our algorithm L(S) in Theorem 2.
Theorem 2.",4. Stability and Generalization Bounds,[0],[0]
Let δ′ = δ−(Ne) 1 2(r+1),4. Stability and Generalization Bounds,[0],[0]
− 14 (r > 1 is required).,4. Stability and Generalization Bounds,[0],[0]
"Then for any sample S of size Ne drawn according to an algebraic β-mixing stationary distribution, and δ ≥ 0 such that δ′ ≥ 0, the following generalization bound holds with probability at least 1− δ: ∣∣R(L(S))−RNe(L(S))∣∣ < O ( (Ne) 1 2(r+1)",4. Stability and Generalization Bounds,[0],[0]
"− 1 4 √ log( 1
δ′ )
) ,
where R(L(S)) and RNe(L(S)) denote the expected risk and the empirical risk of L2T over meta-samples, respectively.",4. Stability and Generalization Bounds,[0],[0]
"A larger mixing parameter r, indicating more independence, would lead to a tighter bound.
",4. Stability and Generalization Bounds,[0],[0]
"Theorem 2 tells that as the number of transfer learning experiences, i.e., Ne, increases, L2T tends to produce a tighter generalization bound.",4. Stability and Generalization Bounds,[0],[0]
This fact lays the foundation for further conducting L2T in an online manner which can gradually assimilate transfer learning experiences and continuously improve.,4. Stability and Generalization Bounds,[0],[0]
The detailed proofs for Theorem 1 and 2 can be found in the supplementary.,4. Stability and Generalization Bounds,[0],[0]
"Datasets We evaluate the L2T framework on two image datasets, Caltech-256 (Griffin et al., 2007) and Sketches (Eitz et al., 2012).",5. Experiments,[0],[0]
"Caltech-256, collected from Google Images, contains a total of 30,607 images in 256 categories.",5. Experiments,[0],[0]
"The Sketches dataset, however, consists of 20,000 unique sketches by human beings that are evenly distributed over 250 different categories.",5. Experiments,[0],[0]
"We construct each pair of source and target domains by randomly sampling three categories from Caltech-256 as the source domain and randomly sampling three categories from Sketches as the target domain, which we give an example in the supplementary material.",5. Experiments,[0],[0]
"Consequently, there are 20, 000/250× 3 = 720 examples in a target domain of each pair.",5. Experiments,[0],[0]
"In total, we generate 1,000 training pairs for preparing transfer learning experiences, 500 validation pairs to determine hyperparameters of the reflection function, and 500 testing pairs to evaluate the reflection function.",5. Experiments,[0],[0]
"We characterize each image from both datasets with 4,096-dimensional features extracted by a convolutional neural network pre-trained by ImageNet.
",5. Experiments,[0],[0]
"In this paper we generate transfer learning experiences by ourselves, because we are the first to consider transfer learning experiences and there exists no off-the-shelf datasets.",5. Experiments,[0],[0]
"In real-world applications, either the number of labeled examples in a target domain or the transfer learning algorithm could vary from experience to experience.",5. Experiments,[0],[0]
"In order to mimic the real environment, we prepare each transfer learning experience by randomly selecting a transfer learning algorithm from a base set A and randomly setting the number of labeled target examples in the range of [3, 120].",5. Experiments,[0],[0]
"The randomly
generated training experiences, lying in the same environment (generated by one dataset), are non i.i.d., which fit the algebraical β-mixing assumption theoretically in Section 4.
",5. Experiments,[0],[0]
"Baselines and Evaluation Metrics We compare L2T with the following nine baseline algorithms in three classes:
• Non-transfer: Original builds a model using labeled data in a target domain only.",5. Experiments,[0],[0]
"• Common latent space based transfer learning algorithms: TCA (Pan et al., 2011), ITL (Shi & Sha, 2012), CMF (Long et al., 2014), LSDT (Zhang et al., 2016), STL (Raina et al., 2007), DIP (Baktashmotlagh et al., 2013) and SIE (Baktashmotlagh et al., 2014).",5. Experiments,[0],[0]
"• Manifold ensemble based algorithms: GFK (Gong et al., 2012).
",5. Experiments,[0],[0]
The eight feature-based transfer learning algorithms also constitute the base set A.,5. Experiments,[0],[0]
"Based on feature representations obtained by different algorithms, we use the nearestneighbor classifier to perform three-class classification for the target domain.
",5. Experiments,[0],[0]
One evaluation metric is classification accuracy on testing examples of a target domain.,5. Experiments,[0],[0]
"However, accuracies are incomparable for different target domains at different levels of difficulty.",5. Experiments,[0],[0]
"The other evaluation metric we adopt is the performance improvement ratio defined in Section 3.1, so as to compare the L2T over different pairs of domains.
",5. Experiments,[0],[0]
"Performance Comparison In this experiment, we learn a reflection function from 1,000 transfer learning experiences, and evaluate the reflection function on 500 testing pairs of source and target domains by comparing the average performance improvement ratio to the baselines.",5. Experiments,[0],[0]
"In building the reflection function, we use 33 RBF kernels with the bandwidth δk in the range of [2−8η : 20.5η : 28η] where η = 1
nsen t eNe
∑Ne e=1 ∑nse,nte i,j=1 ‖xseiW",5. Experiments,[0],[0]
"− xtejW‖22 follows
the median trick (Gretton et al., 2012a).",5. Experiments,[0],[0]
"As Figure 4 shows, on average the proposed L2T framework outperforms the baselines up to 10% when varying the number of labeled samples in the target domain.",5. Experiments,[0],[0]
"As the number of labeled target examples increases from 3 to 120, the performance improvement ratio becomes smaller because the accuracy of Original without transfer tends to increase.",5. Experiments,[0],[0]
"The baseline
algorithms behave differently.",5. Experiments,[0],[0]
"The transferable knowledge learned by LSDT helps a target domain a lot when training examples are scarce, while GFK performs poorly until training examples become more.",5. Experiments,[0],[0]
STL is almost the worst baseline because it learns a dictionary from the source domain only but ignores the target domain.,5. Experiments,[0],[0]
It runs at a high risk of failure especially when two domains are distant.,5. Experiments,[0],[0]
"DIP and SIE, which minimize the MMD and Hellinger distance between domains subject to manifold constraints, are competent.",5. Experiments,[0],[0]
"Note that we have run the paired t-test between L2T and each baseline with all the p-values in the order of 10−12, concluding that the L2T is significantly superior.
",5. Experiments,[0],[0]
We also randomly select six of the 500 testing pairs and compare classification accuracies by different algorithms for each pair in Figure 3.,5. Experiments,[0],[0]
The performance of all baselines varies from pair to pair.,5. Experiments,[0],[0]
"Among all the baseline methods, TCA performs the best when transferring between domains in Figure 3a and LSDT is the most superior in Figure 3c.",5. Experiments,[0],[0]
"However, L2T consistently outperforms the baselines on all the settings.",5. Experiments,[0],[0]
"For some pairs, e.g., Figures 3a, 3c and 3f, the three classes in a target domain are comparably easy to tell apart, hence Original without transfer can achieve even better results than some transfer learning algorithms.",5. Experiments,[0],[0]
"In this case, L2T still improves by discovering the best transferable knowledge from the source domain, especially when the number of labeled examples is small (see Figure 3c and 3f).",5. Experiments,[0],[0]
"If two domains are very related, e.g., the source with “galaxy” and “saturn” and the target with “sun” in Figure 3a, L2T even finds out more transferable knowledge and contributes more significant improvement.
",5. Experiments,[0],[0]
"Varying the Experiences We further investigate how transfer learning experiences used to learn the reflection function influence the performance of L2T. In this experiment, we evaluate on 50 randomly sampled pairs out of the 500 testing pairs in order to efficiently investigate a wide range of cases in the following.",5. Experiments,[0],[0]
"The sampled set is unbiased and sufficient to characterize such influence, evidenced by the asymptotic consistency between the average performance improvement ratio on the 500 pairs in Figure 4 and that on the 50 pairs in the last line of Table 1.",5. Experiments,[0],[0]
"First, we fix the number of transfer learning experiences to be 1,000 and vary the set of base transfer learning algorithms.",5. Experiments,[0],[0]
The results are shown in Table 1.,5. Experiments,[0],[0]
"Even with experiences generated by single base algorithm, e.g., ITL or DIP, the L2T can still learn a reflection function that significantly better (p-value < 0.05) decides what to transfer than using ITL or DIP directly.",5. Experiments,[0],[0]
"With more base algorithms involved, the transfer learning experiences are more diverse to cover more situations of source-target pairs and the knowledge transferred between them.",5. Experiments,[0],[0]
"As a result, the L2T learns a better reflection function and thereby achieves higher performance improvement ratios, which coincides with Theorem 2 where a larger r indicating more independence between experiences gives a tighter bound.",5. Experiments,[0],[0]
"Second, we fix the set of base algorithms to include all the eight baselines and vary the number of transfer learning experiences used for training.",5. Experiments,[0],[0]
"As shown in Figure 5, the average performance improvement ratio achieved by L2T tends to increase as the number of labeled examples in the target domain decreases, given that Original without transfer performs extremely poor with scarce labeled examples.
",5. Experiments,[0],[0]
Figure 6.,5. Experiments,[0],[0]
"Varying the components constituted in the f .
Figure 7.",5. Experiments,[0],[0]
"Varying the number of kernels considered in the f .
",5. Experiments,[0],[0]
"More importantly, it increases as the number of experiences increases, which coincides with Theorem 2.
",5. Experiments,[0],[0]
"Varying the Reflection Function We also study the influence of different configurations of the reflection function on the performance of L2T. First, we vary the components to be considered in building the reflection function f as shown in Figure 6.",5. Experiments,[0],[0]
"Considering single type, either MMD, variance, or the discriminant criterion, brings inferior performance and even negative transfer.",5. Experiments,[0],[0]
"L2T taking all the three factors into consideration outperforms the others, demonstrating that the three components are all necessary and mutually reinforcing.",5. Experiments,[0],[0]
"With all the three components included, we plot values of the learned β∗ in the supplementary material.",5. Experiments,[0],[0]
"Second, we change the kernels used.",5. Experiments,[0],[0]
"In Figure 7, we present results by either narrowing down or extending the range [2−8η : 20.5η : 28η].",5. Experiments,[0],[0]
"Obviously, more kernels (e.g., [2−12η : 20.5η : 212η]), capable of encrypting better trans-
fer learning skills in the reflection function, achieve larger performance improvement ratios.",5. Experiments,[0],[0]
"In this paper, we propose a novel L2T framework for transfer learning which automatically optimizes what and how to transfer between a source and a target domain by leveraging previous transfer learning experiences.",6. Conclusion,[0],[0]
"In particular, L2T learns a reflection function mapping a pair of domains and the knowledge transferred between them to the performance improvement ratio.",6. Conclusion,[0],[0]
"When a new pair of domains arrives, L2T optimizes what and how to transfer by maximizing the value of the learned reflection function.",6. Conclusion,[0],[0]
We believe that L2T opens a new door to improve transfer learning by leveraging transfer learning experiences.,6. Conclusion,[0],[0]
"Many research issues, e.g., incorporating hierarchical latent feature factors as what to transfer and designing online L2T, can be further examined.",6. Conclusion,[0],[0]
We thank the reviewers for their valuable comments to improve this paper.,Acknowledgements,[0],[0]
"The research has been supported by National Grant Fundamental Research (973 Program) of China under Project 2014CB340304, Hong Kong CERG projects 16211214/16209715/16244616, Hong Kong ITF ITS/391/15FX and NSFC 61673202.",Acknowledgements,[0],[0]
"In transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a target domain result in different knowledge transferred and thereby the performance improvement in the target domain.",abstractText,[0],[0]
Determining the optimal one that maximizes the performance improvement requires either exhaustive exploration or considerable expertise.,abstractText,[0],[0]
"Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices.",abstractText,[0],[0]
"Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences.",abstractText,[0],[0]
We establish the L2T framework in two stages: 1) we learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the reflection function.,abstractText,[0],[0]
"We also theoretically analyse the algorithmic stability and generalization bound of L2T, and empirically demonstrate its superiority over several state-ofthe-art transfer learning algorithms.",abstractText,[0],[0]
Transfer Learning via Learning to Transfer,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 946–956 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
946",text,[0],[0]
"Target-oriented (also mentioned as “target-level” or “aspect-level” in some works) sentiment classification aims to determine sentiment polarities over “opinion targets” that explicitly appear in the sentences (Liu, 2012).",1 Introduction,[0],[0]
"For example, in the sentence “I am pleased with the fast log on, and the long battery life”, the user mentions two targets
∗The work was done when Xin Li was an intern at Tencent AI Lab.",1 Introduction,[0],[0]
"This project is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).
",1 Introduction,[0],[0]
"1Our code is open-source and available at https:// github.com/lixin4ever/TNet
“log on” and “better life”, and expresses positive sentiments over them.",1 Introduction,[0],[0]
"The task is usually formulated as predicting a sentiment category for a (target, sentence) pair.
",1 Introduction,[0],[0]
"Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task.",1 Introduction,[0],[0]
"For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction.",1 Introduction,[0],[0]
"In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy.",1 Introduction,[0],[0]
"For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”.",1 Introduction,[0],[0]
"To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015).
",1 Introduction,[0],[0]
Another observation is that the sentiment of a target is usually determined by key phrases such as “is my favorite”.,1 Introduction,[0],[0]
"By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem.",1 Introduction,[0],[0]
"However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service was dreadful!”.",1 Introduction,[0],[0]
"One reason is that CNN cannot fully explore the target information as done by RNN-based meth-
ods (Tang et al.,",1 Introduction,[0],[0]
"2016a).2 Moreover, it is hard for vanilla CNN to differentiate opinion words of multiple targets.",1 Introduction,[0],[0]
"Precisely, multiple active local features holding different sentiments (e.g., “great food” and “service was dreadful”) may be captured for a single target, thus it will hinder the prediction.
",1 Introduction,[0],[0]
"We propose a new architecture, named TargetSpecific Transformation Networks (TNet), to solve the above issues in the task of target sentiment classification.",1 Introduction,[0],[0]
TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs.,1 Introduction,[0],[0]
"To integrate the target information into the word representations, TNet introduces a novel Target-Specific Transformation (TST) component for generating the target-specific word representations.",1 Introduction,[0],[0]
"Contrary to the previous attention-based approaches which apply the same target representation to determine the attention scores of individual context words, TST firstly generates different representations of the target conditioned on individual context words, then it consolidates each context word with its tailor-made target representation to obtain the transformed word representation.",1 Introduction,[0],[0]
"Considering the context word “long” and the target “battery life” in the above example, TST firstly measures the associations between “long” and individual target words.",1 Introduction,[0],[0]
Then it uses the association scores to generate the target representation conditioned on “long”.,1 Introduction,[0],[0]
"After that, TST transforms the representation of “long” into its target-specific version with the new target representation.",1 Introduction,[0],[0]
"Note that “long” could also indicate a negative sentiment (say for “startup time”), and the above TST is able to differentiate them.
",1 Introduction,[0],[0]
"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST, we design a contextpreserving mechanism to contextualize the generated target-specific word representations.",1 Introduction,[0],[0]
Such mechanism also allows deep transformation structure to learn abstract features3.,1 Introduction,[0],[0]
"To help the CNN feature extractor locate sentiment indicators more accurately, we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target.
",1 Introduction,[0],[0]
"2One method could be concatenating the target representation with each word representation, but the effect as shown in (Wang et al., 2016) is limited.
3Abstract features usually refer to the features ultimately useful for the task (Bengio et al., 2013; LeCun et al., 2015).
",1 Introduction,[0],[0]
"In summary, our contributions are as follows: • TNet adapts CNN to handle target-level sentiment classification, and its performance dominates the state-of-the-art models on benchmark datasets.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
A novel Target-Specific Transformation component is proposed to better integrate target information into the word representations.,1 Introduction,[0],[0]
"• A context-preserving mechanism is designed to forward the context information into a deep transformation architecture, thus, the model can learn more abstract contextualized word features from deeper networks.",1 Introduction,[0],[0]
"Given a target-sentence pair (wτ ,w), where wτ = {wτ1 , wτ2 , ..., wτm} is a sub-sequence of w = {w1, w2, ..., wn}, and the corresponding word embeddings xτ = {xτ1 , xτ2 , ..., xτm} and x = {x1, x2, ..., xn}, the aim of target sentiment classification is to predict the sentiment polarity y ∈ {P,N,O} of the sentence w over the target wτ , where P , N and O denote “positive”, “negative” and “neutral” sentiments respectively.
",2 Model Description,[0],[0]
The architecture of the proposed TargetSpecific Transformation Networks (TNet) is shown in Fig. 1.,2 Model Description,[0],[0]
"The bottom layer is a BiLSTM which transforms the input x = {x1, x2, ..., xn} ∈ Rn×dimw into the contextualized word representations h(0) = {h(0)1 , h (0) 2 , ..., h (0) n } ∈ Rn×2dimh (i.e. hidden states of BiLSTM), where dimw and dimh denote the dimensions of the word embeddings and the hidden representations respectively.",2 Model Description,[0],[0]
"The middle part, the core part of our TNet, consists of L Context-Preserving Transformation (CPT) layers.",2 Model Description,[0],[0]
The CPT layer incorporates the target information into the word representations via a novel Target-Specific Transformation (TST) component.,2 Model Description,[0],[0]
"CPT also contains a contextpreserving mechanism, resembling identity mapping (He et al., 2016a,b) and highway connection (Srivastava et al., 2015a,b), allows preserving the context information and learning more abstract word-level features using a deep network.",2 Model Description,[0],[0]
"The top most part is a position-aware convolutional layer which first encodes positional relevance between a word and a target, and then extracts informative features for classification.",2 Model Description,[0],[0]
"As observed in Lai et al. (2015), combining contextual information with word embeddings is an
effective way to represent a word in convolutionbased architectures.",2.1 Bi-directional LSTM Layer,[0],[0]
"TNet also employs a BiLSTM to accumulate the context information for each word of the input sentence, i.e., the bottom part in Fig. 1.",2.1 Bi-directional LSTM Layer,[0],[0]
"For simplicity and space issue, we denote the operation of an LSTM unit on xi as LSTM(xi).",2.1 Bi-directional LSTM Layer,[0],[0]
"Thus, the contextualized word representation h(0)i ∈ R2dimh is obtained as follows:
h (0)",2.1 Bi-directional LSTM Layer,[0],[0]
i =,2.1 Bi-directional LSTM Layer,[0],[0]
"[ −−−−→ LSTM(xi); ←−−−− LSTM(xi)], i ∈",2.1 Bi-directional LSTM Layer,[0],[0]
"[1, n].",2.1 Bi-directional LSTM Layer,[0],[0]
(1),2.1 Bi-directional LSTM Layer,[0],[0]
The above word-level representation has not considered the target information yet.,2.2 Context-Preserving Transformation,[0],[0]
Traditional attention-based approaches keep the word-level features static and aggregate them with weights as the final sentence representation.,2.2 Context-Preserving Transformation,[0],[0]
"In contrast, as shown in the middle part in Fig. 1, we introduce multiple CPT layers and the detail of a single CPT is shown in Fig. 2.",2.2 Context-Preserving Transformation,[0],[0]
"In each CPT layer, a tailor-made TST component that aims at better consolidating word representation and target representation is proposed.",2.2 Context-Preserving Transformation,[0],[0]
"Moreover, we design a context-preserving mechanism enabling the learning of target-specific word representations in a deep neural architecture.",2.2 Context-Preserving Transformation,[0],[0]
TST component is depicted with the TST block in Fig. 2.,2.2.1 Target-Specific Transformation,[0],[0]
The first task of TST is to generate the representation of the target.,2.2.1 Target-Specific Transformation,[0],[0]
"Previous methods (Chen
et al., 2017; Liu and Zhang, 2017) average the embeddings of the target words as the target representation.",2.2.1 Target-Specific Transformation,[0],[0]
This strategy may be inappropriate in some cases because different target words usually do not contribute equally.,2.2.1 Target-Specific Transformation,[0],[0]
"For example, in the target “amd turin processor”, the word “processor” is more important than “amd” and “turin”, because the sentiment is usually conveyed over the phrase head, i.e.,“processor”, but seldom over modifiers (such as brand name “amd”).",2.2.1 Target-Specific Transformation,[0],[0]
Ma et al. (2017) attempted to overcome this issue by measuring the importance score between each target word representation and the averaged sentence vector.,2.2.1 Target-Specific Transformation,[0],[0]
"However, it may be ineffective for sentences expressing multiple sentiments (e.g., “Air has higher resolution but the fonts are small.”), because taking the average tends to neutralize different sentiments.
",2.2.1 Target-Specific Transformation,[0],[0]
We propose to dynamically compute the importance of target words based on each sentence word rather than the whole sentence.,2.2.1 Target-Specific Transformation,[0],[0]
"We first employ another BiLSTM to obtain the target word representations hτ ∈ Rm×2dimh :
hτj =",2.2.1 Target-Specific Transformation,[0],[0]
[ −−−−→ LSTM(xτj ); ←−−−− LSTM(xτj ),2.2.1 Target-Specific Transformation,[0],[0]
"], j ∈",2.2.1 Target-Specific Transformation,[0],[0]
"[1,m].",2.2.1 Target-Specific Transformation,[0],[0]
"(2)
Then, we dynamically associate them with each word wi in the sentence to tailor-make target representation rτi at the time step",2.2.1 Target-Specific Transformation,[0],[0]
"i:
rτi = m∑ j=1 hτj ∗ F(h (l) i , h τ j ), (3)
where the function F measures the relatedness between the j-th target word representation hτj and
the i-th word-level representation h(l)i :
F(h(l)i , h τ j ) =
exp (h (l)>",2.2.1 Target-Specific Transformation,[0],[0]
i h τ j ),2.2.1 Target-Specific Transformation,[0],[0]
"∑m
k=1 exp (h (l)>",2.2.1 Target-Specific Transformation,[0],[0]
i h τ k) .,2.2.1 Target-Specific Transformation,[0],[0]
"(4)
Finally, the concatenation of rτi and h (l) i is fed into a fully-connected layer to obtain the i-th targetspecific word representation h̃i (l) :
h̃ (l) i = g(W τ",2.2.1 Target-Specific Transformation,[0],[0]
[h (l) i :,2.2.1 Target-Specific Transformation,[0],[0]
r τ,2.2.1 Target-Specific Transformation,[0],[0]
i ],2.2.1 Target-Specific Transformation,[0],[0]
"+ b τ ), (5)
where g(∗) is a non-linear activation function and “:” denotes vector concatenation.",2.2.1 Target-Specific Transformation,[0],[0]
W τ,2.2.1 Target-Specific Transformation,[0],[0]
and bτ are the weights of the layer.,2.2.1 Target-Specific Transformation,[0],[0]
"After the non-linear TST (see Eq. 5), the context information captured with contextualized representations from the BiLSTM layer will be lost since the mean and the variance of the features within the feature vector will be changed.",2.2.2 Context-Preserving Mechanism,[0],[0]
"To take advantage of the context information, which has been proved to be useful in (Lai et al., 2015), we investigate two strategies: Lossless Forwarding (LF) and Adaptive Scaling (AS), to pass the context information to each following layer, as depicted by the block “LF/AS” in Fig. 2.",2.2.2 Context-Preserving Mechanism,[0],[0]
"Accordingly, the model variants are named TNet-LF and TNet-AS.
Lossless Forwarding.",2.2.2 Context-Preserving Mechanism,[0],[0]
This strategy preserves context information by directly feeding the features before the transformation to the next layer.,2.2.2 Context-Preserving Mechanism,[0],[0]
"Specifically, the input h(l+1)i of the (l+1)-th CPT layer is formulated as:
h (l+1)",2.2.2 Context-Preserving Mechanism,[0],[0]
i = h,2.2.2 Context-Preserving Mechanism,[0],[0]
(l) i + h̃ (l),2.2.2 Context-Preserving Mechanism,[0],[0]
"i , i ∈",2.2.2 Context-Preserving Mechanism,[0],[0]
"[1, n], l ∈",2.2.2 Context-Preserving Mechanism,[0],[0]
"[0, L], (6)
where h(l)i is the input of the l-th layer and h̃ (l) i is the output of TST in this layer.",2.2.2 Context-Preserving Mechanism,[0],[0]
"We unfold the recursive form of Eq. 6 as follows:
h (l+1)",2.2.2 Context-Preserving Mechanism,[0],[0]
i = h (0),2.2.2 Context-Preserving Mechanism,[0],[0]
i +TST(h (0) i )+· · ·+TST(h (l) i ).,2.2.2 Context-Preserving Mechanism,[0],[0]
"(7)
Here, we denote h̃(l)i as TST(h (l) i ).",2.2.2 Context-Preserving Mechanism,[0],[0]
"From Eq. 7, we can see that the output of each layer will contain the contextualized word representations (i.e., h (0) i ), thus, the context information is encoded into the transformed features.",2.2.2 Context-Preserving Mechanism,[0],[0]
"We call this strategy “Lossless Forwarding” because the contextualized representations and the transformed representations (i.e., TST(h(l)i )) are kept unchanged during the feature combination.
",2.2.2 Context-Preserving Mechanism,[0],[0]
Adaptive Scaling.,2.2.2 Context-Preserving Mechanism,[0],[0]
"Lossless Forwarding introduces the context information by directly adding back the contextualized features to the transformed features, which raises a question: Can the weights of the input and the transformed features be adjusted dynamically?",2.2.2 Context-Preserving Mechanism,[0],[0]
"With this motivation, we propose another strategy, named “Adaptive Scaling”.",2.2.2 Context-Preserving Mechanism,[0],[0]
"Similar to the gate mechanism in RNN variants (Jozefowicz et al., 2015), Adaptive Scaling introduces a gating function to control the passed proportions of the transformed features and the input features.",2.2.2 Context-Preserving Mechanism,[0],[0]
"The gate t(l) as follows:
t (l)",2.2.2 Context-Preserving Mechanism,[0],[0]
"i = σ(Wtransh (l) i + btrans), (8)
where t(l)i is the gate for the i-th input of the l-th CPT layer, and σ is the sigmoid activation function.",2.2.2 Context-Preserving Mechanism,[0],[0]
"Then we perform convex combination of h(l)i and h̃(l)i based on the gate:
h (l+1)",2.2.2 Context-Preserving Mechanism,[0],[0]
i = t,2.2.2 Context-Preserving Mechanism,[0],[0]
(l) i h̃ (l) i + (1− t (l) i ),2.2.2 Context-Preserving Mechanism,[0],[0]
h,2.2.2 Context-Preserving Mechanism,[0],[0]
(l) i .,2.2.2 Context-Preserving Mechanism,[0],[0]
"(9)
Here, denotes element-wise multiplication.",2.2.2 Context-Preserving Mechanism,[0],[0]
"The non-recursive form of this equation is as follows (for clarity, we ignore the subscripts):
h(l+1) =",2.2.2 Context-Preserving Mechanism,[0],[0]
"[ l∏ k=0 (1− t(k))] h(0)
+[t(0) l∏
k=1
(1− t(k))]",2.2.2 Context-Preserving Mechanism,[0],[0]
TST(h(0)),2.2.2 Context-Preserving Mechanism,[0],[0]
"+ · · ·
+t(l−1)(1− t(l))",2.2.2 Context-Preserving Mechanism,[0],[0]
TST(h(l−1)),2.2.2 Context-Preserving Mechanism,[0],[0]
"+ t(l) TST(h(l)).
",2.2.2 Context-Preserving Mechanism,[0],[0]
"Thus, the context information is integrated in each upper layer and the proportions of the contextualized representations and the transformed representations are controlled by the computed gates in different transformation layers.",2.2.2 Context-Preserving Mechanism,[0],[0]
Recall that the second issue that blocks CNN to perform well is that vanilla CNN may associate a target with unrelated general opinion words which are frequently used as modifiers for different targets across domains.,2.3 Convolutional Feature Extractor,[0],[0]
"For example, “service” in “Great food but the service is dreadful” may be associated with both “great” and “dreadful”.",2.3 Convolutional Feature Extractor,[0],[0]
"To solve it, we adopt a proximity strategy, which is observed effective in (Chen et al., 2017; Li and Lam, 2017).",2.3 Convolutional Feature Extractor,[0],[0]
"The idea is a closer opinion word is more likely to be the actual modifier of the target.
",2.3 Convolutional Feature Extractor,[0],[0]
"Specifically, we first calculate the position relevance vi between the i-th word and the target4:
vi =  1− (k+m−i)C i < k",2.3 Convolutional Feature Extractor,[0],[0]
+m 1− i−kC k +m ≤,2.3 Convolutional Feature Extractor,[0],[0]
i ≤ n 0,2.3 Convolutional Feature Extractor,[0],[0]
"i > n
(10)
where k is the index of the first target word, C is a pre-specified constant, and m is the length of the target wτ .",2.3 Convolutional Feature Extractor,[0],[0]
"Then, we use v to help CNN locate the correct opinion w.r.t.",2.3 Convolutional Feature Extractor,[0],[0]
"the given target:
ĥ (l) i = h",2.3 Convolutional Feature Extractor,[0],[0]
"(l) i ∗ vi, i ∈",2.3 Convolutional Feature Extractor,[0],[0]
"[1, n], l ∈",2.3 Convolutional Feature Extractor,[0],[0]
"[1, L].",2.3 Convolutional Feature Extractor,[0],[0]
"(11)
",2.3 Convolutional Feature Extractor,[0],[0]
"Based on Eq. 10 and Eq. 11, the words close to the target will be highlighted and those far away will be downgraded.",2.3 Convolutional Feature Extractor,[0],[0]
v is also applied on the intermediate output to introduce the position information into each CPT layer.,2.3 Convolutional Feature Extractor,[0],[0]
"Then we feed the weighted h(L) to the convolutional layer, i.e., the top-most layer in Fig. 1, to generate the feature map c ∈",2.3 Convolutional Feature Extractor,[0],[0]
"Rn−s+1 as follows:
ci = ReLU(w > convh (L) i:i+s−1 + bconv), (12)
where h(L)i:i+s−1 ∈ Rs·dimh is the concatenated vector of ĥ(L)i , · · · , ĥ (L) i+s−1, and s is the kernel size.",2.3 Convolutional Feature Extractor,[0],[0]
wconv ∈ Rs·dimh and bconv ∈ R are learnable weights of the convolutional kernel.,2.3 Convolutional Feature Extractor,[0],[0]
"To capture the most informative features, we apply max pooling (Kim, 2014) and obtain the sentence representation z ∈",2.3 Convolutional Feature Extractor,[0],[0]
"Rnk by employing nk kernels:
z =",2.3 Convolutional Feature Extractor,[0],[0]
"[max(c1), · · · ,max(cnk)]",2.3 Convolutional Feature Extractor,[0],[0]
>.,2.3 Convolutional Feature Extractor,[0],[0]
"(13)
Finally, we pass z to a fully connected layer for sentiment prediction:
p(y|wτ ,w) = Softmax(Wfz + bf ).",2.3 Convolutional Feature Extractor,[0],[0]
"(14)
where Wf and bf are learnable parameters.",2.3 Convolutional Feature Extractor,[0],[0]
"4As we perform sentence padding, it is possible that the index i is larger than the actual length n of the sentence.",2.3 Convolutional Feature Extractor,[0],[0]
"As shown in Table 1, we evaluate the proposed TNet on three benchmark datasets: LAPTOP and REST are from SemEval ABSA challenge (Pontiki et al., 2014), containing user reviews in laptop domain and restaurant domain respectively.",3.1 Experimental Setup,[0],[0]
"We also remove a few examples having the “conflict label” as done in (Chen et al., 2017); TWITTER is built by Dong et al. (2014), containing twitter posts.",3.1 Experimental Setup,[0],[0]
"All tokens are lowercased without removal of stop words, symbols or digits, and sentences are zero-padded to the length of the longest sentence in the dataset.",3.1 Experimental Setup,[0],[0]
Evaluation metrics are Accuracy and Macro-Averaged F1 where the latter is more appropriate for datasets with unbalanced classes.,3.1 Experimental Setup,[0],[0]
"We also conduct pairwise t-test on both Accuracy and Macro-Averaged F1 to verify if the improvements over the compared models are reliable.
",3.1 Experimental Setup,[0],[0]
"TNet is compared with the following methods.
",3.1 Experimental Setup,[0],[0]
"• SVM (Kiritchenko et al., 2014):",3.1 Experimental Setup,[0],[0]
"It is a traditional support vector machine based model with extensive feature engineering;
• AdaRNN (Dong et al., 2014):",3.1 Experimental Setup,[0],[0]
"It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree;
• AE-LSTM, and ATAE-LSTM (Wang et al., 2016): AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention;
• IAN (Ma et al., 2017): IAN employs two LSTMs to learn the representations of the context and the target phrase interactively;
• CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding;
• TD-LSTM (Tang et al., 2016a):",3.1 Experimental Setup,[0],[0]
"It employs two LSTMs to model the left and right contexts of the target separately, then performs predictions based on concatenated context representations;
• MemNet (Tang et al., 2016b):",3.1 Experimental Setup,[0],[0]
"It applies attention mechanism over the word embeddings multiple times and predicts sentiments
based on the top-most sentence representations;
• BILSTM-ATT-G (Liu and Zhang, 2017):",3.1 Experimental Setup,[0],[0]
"It models left and right contexts using two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction;
• RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation.
",3.1 Experimental Setup,[0],[0]
"We run the released codes of TD-LSTM and BILSTM-ATT-G to generate results, since their papers only reported results on TWITTER.",3.1 Experimental Setup,[0],[0]
"We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.5
We use pre-trained GloVe vectors (Pennington et al., 2014) to initialize the word embeddings and the dimension is 300 (i.e., dimw = 300).",3.1 Experimental Setup,[0],[0]
"For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014).",3.1 Experimental Setup,[0],[0]
"We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (Zhang and Wallace, 2017).",3.1 Experimental Setup,[0],[0]
"To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and",3.1 Experimental Setup,[0],[0]
the ultimate sentence representation z.,3.1 Experimental Setup,[0],[0]
"All weight matrices are initialized with the uniform distribution U(−0.01, 0.01) and the biases are initialized
5The codes of TD-LSTM/MemNet and BILSTM-ATTG are available at: http://ir.hit.edu.cn/˜dytang and http://leoncrashcode.github.io.",3.1 Experimental Setup,[0],[0]
"Note that MemNet was only evaluated with accuracy.
as zeros.",3.1 Experimental Setup,[0],[0]
"The training objective is cross-entropy, and Adam (Kingma and Ba, 2015) is adopted as the optimizer by following the learning rate and the decay rates in the original paper.
",3.1 Experimental Setup,[0],[0]
The hyper-parameters of TNet-LF and TNetAS are listed in Table 2.,3.1 Experimental Setup,[0],[0]
"Specifically, all hyperparameters are tuned on 20% randomly held-out training data and the hyper-parameter collection producing the highest accuracy score is used for testing.",3.1 Experimental Setup,[0],[0]
Our model has comparable number of parameters compared to traditional LSTM-based models as we reuse parameters in the transformation layers and BiLSTM.6,3.1 Experimental Setup,[0],[0]
"As shown in Table 3, both TNet-LF and TNet-AS consistently achieve the best performance on all datasets, which verifies the efficacy of our whole TNet model.",3.2 Main Results,[0],[0]
"Moreover, TNet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in LAPTOP and REST, and tweets with more ungrammatical sentences in TWITTER.",3.2 Main Results,[0],[0]
The reason is the CNN-based feature extractor arms TNet with more power to extract accurate features from ungrammatical sentences.,3.2 Main Results,[0],[0]
"Indeed, we can also observe that another CNN-based baseline, i.e., CNNASP implemented by us, also obtains good results on TWITTER.
",3.2 Main Results,[0],[0]
"On the other hand, the performance of those comparison methods is mostly unstable.",3.2 Main Results,[0],[0]
"For the tweet in TWITTER, the competitive BILSTMATT-G and RAM cannot perform as effective as they do for the reviews in LAPTOP and REST, due to the fact that they are heavily rooted in LSTMs and the ungrammatical sentences hinder their ca-
6All experiments are conducted on a single NVIDIA GTX 1080.",3.2 Main Results,[0],[0]
"The prediction cost of a sentence is about 2 ms.
pability in capturing the context features.",3.2 Main Results,[0],[0]
"Another difficulty caused by the ungrammatical sentences is that the dependency parsing might be errorprone, which will affect those methods such as AdaRNN using dependency information.
",3.2 Main Results,[0],[0]
"From the above observations and analysis, some takeaway message for the task of target sentiment classification could be:
• LSTM-based models relying on sequential information can perform well for formal sentences by capturing more useful context features;
",3.2 Main Results,[0],[0]
"• For ungrammatical text, CNN-based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns.",3.2 Main Results,[0],[0]
"To investigate the impact of each component such as deep transformation, context-preserving mechanism, and positional relevance, we perform comparison between the full TNet models and its ablations (the third group in Table 3).",3.3 Performance of Ablated TNet,[0],[0]
"After removing the deep transformation (i.e., the techniques introduced in Section 2.2), both TNet-LF and TNetAS are reduced to TNet w/o transformation (where
position relevance is kept), and their results in both accuracy and F1 measure are incomparable with those of TNet.",3.3 Performance of Ablated TNet,[0],[0]
"It shows that the integration of target information into the word-level representations is crucial for good performance.
",3.3 Performance of Ablated TNet,[0],[0]
"Comparing the results of TNet and TNet w/o context (where TST and position relevance are kept), we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST7, while on TWITTER, TNet w/o context performs very competitive (p-values with TNetLF and TNet-AS are 0.066 and 0.053 respectively for Accuracy).",3.3 Performance of Ablated TNet,[0],[0]
"Again, we could attribute this phenomenon to the ungrammatical user generated content of twitter, because the contextpreserving component becomes less important for such data.",3.3 Performance of Ablated TNet,[0],[0]
TNet,3.3 Performance of Ablated TNet,[0],[0]
"w/o context performs consistently better than TNet w/o transformation, which verifies the efficacy of the target specific transformation (TST), before applying context-preserving.
",3.3 Performance of Ablated TNet,[0],[0]
"As for the position information, we conduct statistical t-test between TNet-LF/AS and TNetLF/AS w/o position together with performance comparison.",3.3 Performance of Ablated TNet,[0],[0]
"All of the produced p-values are less than 0.05, suggesting that the improvements brought in by position information are significant.
",3.3 Performance of Ablated TNet,[0],[0]
"7Without specification, the significance level is set to 0.05.",3.3 Performance of Ablated TNet,[0],[0]
"The next interesting question is what if we replace the transformation module (i.e., the CPT layers in Fig.1) of TNet with other commonly-used components?",3.4 CPT versus Alternatives,[0],[0]
"We investigate two alternatives: attention mechanism and fully-connected (FC) layer, resulting in three pipelines as shown in the second group of Table 3 (position relevance is kept for them).
",3.4 CPT versus Alternatives,[0],[0]
"LSTM-ATT-CNN applies attention as the alternative8, and it does not need the contextpreserving mechanism.",3.4 CPT versus Alternatives,[0],[0]
It performs unexceptionally worse than the TNet variants.,3.4 CPT versus Alternatives,[0],[0]
We are surprised that LSTM-ATT-CNN is even worse than TNet w/o transformation (a pipeline simply removing the transformation module) on TWITTER.,3.4 CPT versus Alternatives,[0],[0]
"More concretely, applying attention results in negative effect on TWITTER, which is consistent with the observation that all those attention-based state-of-the-art methods (i.e., TD-LSTM, MemNet, BILSTM-ATT-G, and RAM) cannot perform well on TWITTER.
",3.4 CPT versus Alternatives,[0],[0]
"LSTM-FC-CNN-LF and LSTM-FC-CNN-AS are built by applying FC layer to replace TST and keeping the context-preserving mechanism (i.e., LF and AS).",3.4 CPT versus Alternatives,[0],[0]
"Specifically, the concatenation of word representation and the averaged target vector is fed to the FC layer to obtain targetspecific features.",3.4 CPT versus Alternatives,[0],[0]
Note that LSTM-FC-CNNLF/AS are equivalent to TNet-LF/AS when processing single-word targets (see Eq. 3).,3.4 CPT versus Alternatives,[0],[0]
They obtain competitive results on all datasets: comparable with or better than the state-of-the-art methods.,3.4 CPT versus Alternatives,[0],[0]
"The TNet variants can still outperform LSTMFC-CNN-LF/AS with significant gaps, e.g., on LAPTOP and REST, the accuracy gaps between TNet-LF and LSTM-FC-CNN-LF are 0.42% (p < 0.03) and 0.38% (p < 0.04) respectively.",3.4 CPT versus Alternatives,[0],[0]
"As our TNet involves multiple CPT layers, we investigate the effect of the layer number L. Specifically, we conduct experiments on the held-out training data of LAPTOP and vary L from 2 to 10, increased by 2.",3.5 Impact of CPT Layer Number,[0],[0]
The cases L=1 and L=15 are also included.,3.5 Impact of CPT Layer Number,[0],[0]
The results are illustrated in Figure 3.,3.5 Impact of CPT Layer Number,[0],[0]
We can see that both TNet-LF and TNetAS achieve the best results when L=2.,3.5 Impact of CPT Layer Number,[0],[0]
"While increasing L, the performance is basically becoming worse.",3.5 Impact of CPT Layer Number,[0],[0]
"For large L, the performance of TNet-AS
8We tried different attention mechanisms and report the best one here, namely, dot attention (Luong et al., 2015).
generally becomes more sensitive, it is probably because AS involves extra parameters (see Eq 9) that increase the training difficulty.",3.5 Impact of CPT Layer Number,[0],[0]
Table 4 shows some sample cases.,3.6 Case Study,[0],[0]
The input targets are wrapped in the brackets with true labels given as subscripts.,3.6 Case Study,[0],[0]
"The notations P, N and O in the table represent positive, negative and neutral respectively.",3.6 Case Study,[0],[0]
"For each sentence, we underline the target with a particular color, and the text of its corresponding most informative n-gram feature9 captured by TNet-AS (TNet-LF captures very similar features) is in the same color (so color printing is preferred).",3.6 Case Study,[0],[0]
"For example, for the target “resolution” in the first sentence, the captured feature is “Air has higher”.",3.6 Case Study,[0],[0]
"Note that as discussed above, the CNN layer of TNet captures such features with the size-three kernels, so that the features are trigrams.",3.6 Case Study,[0],[0]
"Each of the last features of the second and seventh sentences contains a padding token, which is not shown.
",3.6 Case Study,[0],[0]
Our TNet variants can predict target sentiment more accurately than RAM and BILSTM-ATT-G in the transitional sentences such as the first sentence by capturing correct trigram features.,3.6 Case Study,[0],[0]
"For the third sentence, its second and third most informative trigrams are “100% .",3.6 Case Study,[0],[0]
"PAD” and “’ s not”, being used together with “features make up”, our models can make correct predictions.",3.6 Case Study,[0],[0]
"Moreover, TNet can still make correct prediction when the explicit opinion is target-specific.",3.6 Case Study,[0],[0]
"For example,
9For each convolutional filter, only one n-gram feature in the feature map will be kept after the max pooling.",3.6 Case Study,[0],[0]
"Among those from different filters, the n-gram with the highest frequency will be regarded as the most informative n-gram w.r.t.",3.6 Case Study,[0],[0]
"the given target.
“long” in the fifth sentence is negative for “startup time”, while it could be positive for other targets such as “battery life” in the sixth sentence.",3.6 Case Study,[0],[0]
The sentiment of target-specific opinion word is conditioned on the given target.,3.6 Case Study,[0],[0]
"Our TNet variants, armed with the word-level feature transformation w.r.t.",3.6 Case Study,[0],[0]
"the target, is capable of handling such case.
",3.6 Case Study,[0],[0]
"We also find that all these models cannot give correct prediction for the last sentence, a commonly used subjunctive style.",3.6 Case Study,[0],[0]
"In this case, the difficulty of prediction does not come from the detection of explicit opinion words but the inference based on implicit semantics, which is still quite challenging for neural network models.",3.6 Case Study,[0],[0]
"Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis.",4 Related Work,[0],[0]
"The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis.",4 Related Work,[0],[0]
Dong et al. (2014) incorporate the target information into the feature learning using dependency trees.,4 Related Work,[0],[0]
"As observed in previous works, the performance heavily relies on the quality of dependency parsing.",4 Related Work,[0],[0]
Tang et al. (2016a) propose to split the context into two parts and associate target with contextual features separately.,4 Related Work,[0],[0]
"Similar to (Tang et al., 2016a), Zhang et al. (2016) develop a three-way gated neural network to model the in-
teraction between the target and its surrounding contexts.",4 Related Work,[0],[0]
"Despite the advantages of jointly modeling target and context, they are not capable of capturing long-range information when some critical context information is far from the target.",4 Related Work,[0],[0]
"To overcome this limitation, researchers bring in the attention mechanism to model target-context association (Tang et al., 2016a,b; Wang et al., 2016; Yang et al., 2017; Liu and Zhang, 2017; Ma et al., 2017; Chen et al., 2017; Zhang et al., 2017; Tay et al., 2017).",4 Related Work,[0],[0]
"Compared with these methods, our TNet avoids using attention for feature extraction so as to alleviate the attended noise.",4 Related Work,[0],[0]
"We re-examine the drawbacks of attention mechanism for target sentiment classification, and also investigate the obstacles that hinder CNN-based models to perform well for this task.",5 Conclusions,[0],[0]
Our TNet model is carefully designed to solve these issues.,5 Conclusions,[0],[0]
"Specifically, we propose target specific transformation component to better integrate target information into the word representation.",5 Conclusions,[0],[0]
"Moreover, we employ CNN as the feature extractor for this classification problem, and rely on the contextpreserving and position relevance mechanisms to maintain the advantages of previous LSTM-based models.",5 Conclusions,[0],[0]
The performance of TNet consistently dominates previous state-of-the-art methods on different types of data.,5 Conclusions,[0],[0]
"The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture.",5 Conclusions,[0],[0]
Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence.,abstractText,[0],[0]
"RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance.",abstractText,[0],[0]
"After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model to overcome these issues.",abstractText,[0],[0]
"Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer.",abstractText,[0],[0]
"Between the two layers, we propose a component to generate target-specific representations of words in the sentence, meanwhile incorporate a mechanism for preserving the original contextual information from the RNN layer.",abstractText,[0],[0]
Experiments show that our model achieves a new state-of-the-art performance on a few benchmarks.1,abstractText,[0],[0]
Transformation Networks for Target-Oriented Sentiment Classification,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2313–2318, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Transition-based parsing, one of the most prominent dependency parsing techniques, constructs a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure.",1 Introduction,[0],[0]
"Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence.
",1 Introduction,[0],[0]
"In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions.",1 Introduction,[0],[0]
Recent models include the greedy StackLSTM model of Dyer et al. (2015) and the globally normalized feed-forward networks of Andor et al. (2016).,1 Introduction,[0],[0]
"These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers.
",1 Introduction,[0],[0]
One common search technique is beam search.,1 Introduction,[0],[0]
"(Zhang and Clark, 2008; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015; Weiss et al., 2015; Yazdani and Henderson, 2015)",1 Introduction,[0],[0]
"In beamsearch, a fixed number of candidate transition sequences are generated, and the highest-scoring sequence is chosen as the answer.",1 Introduction,[0],[0]
One downside to beam search is that it often results in a significant amount of wasted predictions.,1 Introduction,[0],[0]
"A constant number of beams are explored at all points throughout the sentence, leading to some unnecessary exploration towards the beginning of the sentence, and potentially insufficient exploration towards the end.
",1 Introduction,[0],[0]
"One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013).",1 Introduction,[0],[0]
"When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams.",1 Introduction,[0],[0]
"Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam.
",1 Introduction,[0],[0]
Another approach to solving this issue is given by Choi and McCallum (2013).,1 Introduction,[0],[0]
"They introduced selectional branching, which involves performing an initial greedy parse, and then using confidence estimates on each prediction to spawn additional beams.",1 Introduction,[0],[0]
"Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up.
",1 Introduction,[0],[0]
"In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function (Pearl, 1984): a function which estimates
2313
the future cost of taking a particular decision.",1 Introduction,[0],[0]
"When paired with a good heuristic, heuristic backtracking maintains the property of reducing wasted predictions, but allows us to more fully explore the space of possible transition sequences (as compared to selectional branching).",1 Introduction,[0],[0]
"In this paper, we use a heuristic based on the confidence of transition predictions.
",1 Introduction,[0],[0]
We also introduce a new optimization: heuristic backtracking with cutoff.,1 Introduction,[0],[0]
"Since heuristic backtracking produces results incrementally, it is possible to stop the search early if we have found an answer that we believe to be the gold parse, saving time proportional to the number of backtracks remaining.
",1 Introduction,[0],[0]
"We compare the performance of these various decoding algorithms with the Stack-LSTM parser (Dyer et al., 2015), and achieve slightly higher accuracy than beam search, in significantly less time.",1 Introduction,[0],[0]
Our starting point is the model described by Dyer et al. (,2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"2015).1 The parser implements the arc-standard algorithm (Nivre, 2004) and it therefore makes use of a stack and a buffer.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"In (Dyer et al., 2015), the stack and the buffer are encoded with Stack-LSTMs, and a third sequence with the history of actions taken by the parser is encoded with another Stack-LSTM.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The three encoded sequences form the parser state pt defined as follows,
pt = max {0,W[st;bt;at] + d} , (1)
where W is a learned parameter matrix, bt, st and at are the stack LSTM encoding of buffer, stack and the history of actions, and d is a bias term.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The output pt (after a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011)) is then used to compute the probability of the parser action at time t as:
p(zt | pt) = exp
( g>ztpt + qzt ) ∑
z′∈A(S,B) exp",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"( g>z′pt + qz′ ) , (2)
where gz is a column vector representing the (output) embedding of the parser action z, and qz is a bias term for action z.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The set A(S,B) represents
1We refer to the original work for details.
",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
the valid transition actions that may be taken in the current state.,2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The objective function is:
Lθ(w, z) = |z|∑
t=1
log p(zt | pt) (3)
where z refers to parse transitions.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"Using the Stack-LSTM parsing model of Dyer et al. (2015) to predict each decision greedily yields very high accuracy; however, it can only explore one path, and it therefore can be improved by conducting a larger search over the space of possible parses.",3 Heuristic Backtracking,[0.9528233058076407],"['Any encoder– decoder model (Sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating “beliefs” induced by the encoding to explain what features of the input are being transmitted.']"
"To do this, we introduce a new algorithm, heuristic backtracking.",3 Heuristic Backtracking,[0],[0]
We also introduce a novel cutoff approach to further increase speed.,3 Heuristic Backtracking,[0],[0]
"We model the space of possible parses as a tree, where each node represents a certain parse state (with complete values for stack, buffer, and action history).",3.1 Decoding Strategy,[0],[0]
"Transitions connect nodes of the tree, and leaves of the tree represent final states.
",3.1 Decoding Strategy,[0],[0]
"During the first iteration, we start at the root of the tree, and greedily parse until we reach a leaf.",3.1 Decoding Strategy,[0],[0]
"That is, for each node, we use the Stack-LSTM model to calculate scores for each transition (as described in Section 2), and then execute the highest-scoring transition, generating a child node upon which we repeat the procedure.",3.1 Decoding Strategy,[0],[0]
"Additionally, we save an ordered list of the transition scores, and calculate the confidence of the node (as described in Section 3.2).
",3.1 Decoding Strategy,[0],[0]
"When we reach the leaf node, we backtrack to the location that is most likely to fix a mistake.",3.1 Decoding Strategy,[0],[0]
"To find this, we look at all explored nodes that still have at least one unexplored child, and choose the node with the lowest heuristic confidence (see Section 3.2).",3.1 Decoding Strategy,[0],[0]
"We rewind our stack, buffer, and action history to that state, and execute the highest-scoring transition from that node that has not yet been explored.",3.1 Decoding Strategy,[0],[0]
"At this point, we are again in a fully-unexplored node, and can greedily parse just as before until we reach another leaf.
",3.1 Decoding Strategy,[0],[0]
"Once we have generated b leaves, we score them all and return the transition sequence leading up to the highest-scoring leaf as the answer.",3.1 Decoding Strategy,[0],[0]
"Just as in previous studies (Collins and Roark, 2004), we use the
sum of the log probabilities of all individual transitions as the overall score for the parse.",3.1 Decoding Strategy,[0],[0]
"Let n indicate a node, which consists of a state, a buffer, and an action history.",3.2 Calculating Error Likelihood,[0],[0]
"We may refer to a specific node as nji , which means it has i actions in its action history and it is part of the history of the jth leaf (and possibly subsequent leaves).",3.2 Calculating Error Likelihood,[0],[0]
"Let the function T (n) represent a sorted vector containing all possible transitions from n, and S(n) represent a sorted vector containing the scores of all of these transitions, in terms of log probabilities of each score.",3.2 Calculating Error Likelihood,[0],[0]
"We can index the scores in order of value, so T1(n) is the highest-scoring transition and S1(n) is its score, T2(n) is the second-highest-scoring transition, etc.",3.2 Calculating Error Likelihood,[0],[0]
"Here, let un indicate the ranking of the transition leading to the first unexplored child of a node n. Also, let V (n) represent the total score of all nodes in the history of n, i.e. the sum of all the scores of individual transitions that allowed us to get to n.
To calculate the confidence of an individual node, Choi and McCallum (2013) simply found the score margin, or difference in probability between the topscoring transition and the second-highest scoring transition: C(n) = S1(n)",3.2 Calculating Error Likelihood,[0],[0]
− S2(n).,3.2 Calculating Error Likelihood,[0],[0]
"In selectional branching, the only states for which the confidence was relevant were the states in the first greedy parse, i.e. states n1i for all i. For heuristic backtracking, we wish to generalize this to any state nji for all i and j.
We do this in the following way:
H(nji )",3.2 Calculating Error Likelihood,[0],[0]
=,3.2 Calculating Error Likelihood,[0],[0]
(V (n 1 i ),3.2 Calculating Error Likelihood,[0],[0]
− V (nji )),3.2 Calculating Error Likelihood,[0],[0]
"+ (S(u
n",3.2 Calculating Error Likelihood,[0],[0]
j,3.2 Calculating Error Likelihood,[0],[0]
"i
)−1(n",3.2 Calculating Error Likelihood,[0],[0]
"j i ) + S(u
n j",3.2 Calculating Error Likelihood,[0],[0]
"i
)(n j i ))
",3.2 Calculating Error Likelihood,[0],[0]
"(4) Intuitively, this formula means that the node that will be explored first is the node that will yield a parse that scores as close to the greedy choice as possible.",3.2 Calculating Error Likelihood,[0],[0]
"The first term ensures that it has a history of good choices, and the second term ensures that the new child node being explored will be nearly as good as the prior child.",3.2 Calculating Error Likelihood,[0],[0]
"As discussed earlier, we use number of predictions made by the model as a proxy for the speed; execution speed may vary based on system and algorithmic implementation, but prediction count gives a good estimate of the overall work done by the algorithm.
",3.3 Number of Predictions,[0],[0]
"Consider a sentence of length l, which requires at most 2l transitions with the greedy decoder (Nivre, 2004).",3.3 Number of Predictions,[0],[0]
"The number of predictions required for heuristic backtracking for b leaves is guaranteed to be less than or equal to a beam search with b beams.
",3.3 Number of Predictions,[0],[0]
"When doing a beam search, the first transition will require 1 prediction, and then every subsequent transition will require 1 prediction per beam, or b predictions.",3.3 Number of Predictions,[0],[0]
This results in a total of b(2l,3.3 Number of Predictions,[0],[0]
"− 1) + 1 predictions.
",3.3 Number of Predictions,[0],[0]
"When doing heuristic backtracking, the first greedy search will require 2l predictions.",3.3 Number of Predictions,[0],[0]
"Every
subsequent prediction will require a number of predictions dependent on the target of the backtrack: backtracking to nji will require 2l − (i + 1) predictions.",3.3 Number of Predictions,[0],[0]
Note that 0,3.3 Number of Predictions,[0],[0]
<,3.3 Number of Predictions,[0],[0]
i < 2l.,3.3 Number of Predictions,[0],[0]
"Thus, each backtrack will require at maximum 2l − 1 predictions.",3.3 Number of Predictions,[0],[0]
"Therefore, the maximum total amount of predictions is 2l + (b− 1)(2l",3.3 Number of Predictions,[0],[0]
"− 1) = b(2l − 1) + 1.
",3.3 Number of Predictions,[0],[0]
"However, note that on average, there are significantly fewer.",3.3 Number of Predictions,[0],[0]
"Assuming that all parts of a sentence have approximately equal score distributions, the average backtrack will be where i = l, and reduce predictions by 50%.
",3.3 Number of Predictions,[0],[0]
An intuitive understanding of this difference can be gained by viewing the graphs of various decoding methods in Figure 1.,3.3 Number of Predictions,[0],[0]
"Beam search has many nodes which never yield children that reach an end-state; dynamic beam search has fewer, but still several.",3.3 Number of Predictions,[0],[0]
"Selectional branching has none, but suffers from the restriction that every parse candidate can be no more than one decision away from the greedy parse.",3.3 Number of Predictions,[0],[0]
"With heuristic backtracking, there is no such restriction, but yet every node explored is directly useful for generating a candidate parse.",3.3 Number of Predictions,[0],[0]
Another inefficiency inherent to beam search is the fact that all b beams are always fully explored.,3.4 Early Cutoff,[0],[0]
"Since the beams are calculated in parallel, this is inevitable.",3.4 Early Cutoff,[0],[0]
"However, with heuristic backtracking, the beams are calculated incrementally; this gives us the opportunity to cut off our search at any point.",3.4 Early Cutoff,[0],[0]
"In order to leverage this into more efficient parsing, we constructed a second Stack-LSTM model, which we call the cutoff model.",3.4 Early Cutoff,[0],[0]
"The cutoff model uses a single Stack-LSTM2 that takes as input the sequence of parser states (see Eq 1), and outputs a boolean variable predicting whether the entire parse is correct or incorrect.
",3.4 Early Cutoff,[0],[0]
"To train the cutoff model, we used stochastic gradient descent over the training set.",3.4 Early Cutoff,[0],[0]
"For each training example, we first parse it greedily using the StackLSTM parser.",3.4 Early Cutoff,[0],[0]
"Then, for as long as the parse has at least one mistake, we pass it to the cutoff model as a negative training example.",3.4 Early Cutoff,[0],[0]
"Once the parse is completely correct, we pass it to the cutoff model as a positive training example.",3.4 Early Cutoff,[0],[0]
"The loss function that we
22 layers and 300 dimensions.
use is:
Lθ = − log p(t | s) (5)
where s is the LSTM encoded vector and t is the truth (parse correct/incorrect).
",3.4 Early Cutoff,[0],[0]
"When decoding using early cutoff, we follow the exact same procedure as for normal heuristic backtracking, but after every candidate parse is generated, we use it as input to our cutoff model.",3.4 Early Cutoff,[0],[0]
"When our cutoff model returns our selection as correct, we stop backtracking and return it as the answer.",3.4 Early Cutoff,[0],[0]
"If we make b attempts without finding a correct parse, we follow the same procedure as before.",3.4 Early Cutoff,[0],[0]
"To test the effectiveness of heuristic backtracking, we compare it with other decoding techniques: greedy, beam search,3, dynamic beam search (Mejia-Lavalle and Ramos, 2013), and selectional branching (Choi and McCallum, 2013).",4 Experiments and Results,[0],[0]
"We then try heuristic backtracking (see Section 3.1), and heuristic backtracking with cutoff (see Section 3.4).",4 Experiments and Results,[0],[0]
"Note that beam search was not used for early-update training (Collins and Roark, 2004).",4 Experiments and Results,[0],[0]
"We use the same greedy training strategy for all models, and we only change the decoding strategy.
",4 Experiments and Results,[0],[0]
We tested the performance of these algorithms on the English SD and Chinese CTB.4,4 Experiments and Results,[0],[0]
"A single model was trained using the techniques described in Section 2, and used as the transition model for all decoding algorithms.",4 Experiments and Results,[0],[0]
"Each decoding technique was tested with varying numbers of beams; as b increased, both the predictions per sentence and accuracy trended upwards.",4 Experiments and Results,[0],[0]
"The results are summarized in Table 1.5 Note that we report results for only the highestaccuracy b (in the development set) for each.
",4 Experiments and Results,[0],[0]
We also report the results of the cutoff model in Table 2.,4 Experiments and Results,[0],[0]
"The same greedily-trained model as above was used to generate candidate parses and confidence estimates for each transition, and then the cutoff model was trained to use these confidence esti-
3Greedy and beam-search were already explored by Dyer et al. (2015)
4Using the exact same settings as Dyer et al. (2015) with pretrained embeddings and part-of-speech tags.
",4 Experiments and Results,[0],[0]
"5The development sets are used to set the model parameters; results on the development sets are similar to the ones obtained in the test sets.
",4 Experiments and Results,[0],[0]
mates to discriminate between correctly-parsed and incorrectly-parsed sentences.,4 Experiments and Results,[0],[0]
"In Table 1 we see that in both English and Chinese, the best heuristic backtracking performs approximately as well as the best beam search, while making less than half the predictions.",5 Discussion,[0],[0]
"This supports our hypothesis that heuristic backtracking can perform at the same level as beam search, but with increased efficiency.
",5 Discussion,[0],[0]
"Dynamic beam search also performed as well as full beam search, despite demonstrating a reduction in predictions on par with that of heuristic backtracking.",5 Discussion,[0],[0]
"Since the implementation of dynamic beam search is very straightforward for systems which have already implemented beam search, we believe this will prove to be a useful finding.
",5 Discussion,[0],[0]
"Heuristic backtracking with cutoff outperformed greedy decoding, and reduced transitions by an additional 50%.",5 Discussion,[0],[0]
"However, it increased accuracy slightly less than full heuristic backtracking.",5 Discussion,[0],[0]
"We believe this difference could be mitigated with an improved cutoff model; as can be seen in Table 2, the cutoff model was only able to discriminate between correct and incorrect parses around 75% of the time.",5 Discussion,[0],[0]
"Also, note that while predictions per sentence were low, the overall runtime was increased due to running the cutoff LSTM multiple times per sentence.",5 Discussion,[0],[0]
"Heuristic backtracking is most similar to the work of Choi and McCallum (2013), but is distinguished from theirs by allowing new beams to be initialized from any point in the parse, rather than only from points in the initial greedy parse.",6 Related Work,[0],[0]
"Heuristic backtracking also bears similarity to greedy-best-firstsearch (Pearl, 1984), but is unique in that it guarantees that b candidate solutions will be found within b(2l",6 Related Work,[0],[0]
− 1) + 1 predictions.,6 Related Work,[0],[0]
"Our work also relates to beam-search parsers (Zhang and Clark, 2008, inter alia).",6 Related Work,[0],[0]
"We have introduced a novel decoding algorithm, called heuristic backtracking, and presented evidence that it performs at the same level as beam search for decoding, while being significantly more efficient.",7 Conclusions,[0],[0]
"We have demonstrated this for both English and Chinese, using a parser with strong results with a greedy decoder.",7 Conclusions,[0],[0]
"We expect that heuristic backtracking could be applied to any other transition-based parser with similar benefits.
",7 Conclusions,[0],[0]
"We plan on experimenting with various heuristics and cutoff models, such as adapting the attentionbased models of Bahdanau et al. (2014) to act as a guide for both the heuristic search and cutoff.",7 Conclusions,[0],[0]
Miguel Ballesteros was supported by the European Commission under the contract numbers FP7ICT-610411 (project MULTISENSOR) and H2020RIA-645012 (project KRISTINA).,Acknowledgments,[0],[0]
We introduce a novel approach to the decoding problem in transition-based parsing: heuristic backtracking.,abstractText,[0],[0]
"This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search.",abstractText,[0],[0]
"This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions.",abstractText,[0],[0]
"When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese.",abstractText,[0],[0]
Transition-Based Dependency Parsing with Heuristic Backtracking,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 232–242 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1022
Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents’ messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1",text,[0],[0]
Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network.,1 Introduction,[0],[0]
"DCPs have been shown to solve a variety of coordination problems, including reference games (Lazaridou et al., 2016b), logic puzzles (Foerster et al., 2016), and simple control (Sukhbaatar et al., 2016).",1 Introduction,[0],[0]
"Appealingly, the agents’ communication protocol can be learned via direct
1 We have released code and data at http://github.",1 Introduction,[1.0000000343495692],"['Appealingly, the agents’ communication protocol can be learned via direct 1 We have released code and data at http://github.']"
"com/jacobandreas/neuralese.
",1 Introduction,[0],[0]
"backpropagation through the communication channel, avoiding many of the challenging inference problems associated with learning in classical decentralized decision processes (Roth et al., 2005).
",1 Introduction,[0],[0]
But analysis of the strategies induced by DCPs has remained a challenge.,1 Introduction,[0],[0]
"As an example, Figure 1 depicts a driving game in which two cars, which are unable to see each other, must both cross an intersection without colliding.",1 Introduction,[0],[0]
"In order to ensure success, it is clear that the cars must communicate with each other.",1 Introduction,[0],[0]
"But a number of successful communication strategies are possible—for example, they might report their exact (x, y) coordinates at every timestep, or they might simply announce whenever they are entering and leaving the intersection.",1 Introduction,[0],[0]
"If these messages were communicated in natural language, it would be straightforward to determine which strategy was being employed.",1 Introduction,[0],[0]
"However, DCP agents instead communicate with an automatically induced protocol of unstructured, real-valued recurrent state vectors—an artificial language we might call “neuralese,” which superficially bears little resemblance to natural language, and thus frustrates attempts at direct interpretation.
232
We propose to understand neuralese messages by translating them.",1 Introduction,[0.9886055898964694],"['However, DCP agents instead communicate with an automatically induced protocol of unstructured, real-valued recurrent state vectors—an artificial language we might call “neuralese,” which superficially bears little resemblance to natural language, and thus frustrates attempts at direct interpretation.']"
"In this work, we present a simple technique for inducing a dictionary that maps between neuralese message vectors and short natural language strings, given only examples of DCP agents interacting with other agents, and humans interacting with other humans.",1 Introduction,[0],[0]
"Natural language already provides a rich set of tools for describing beliefs, observations, and plans—our thesis is that these tools provide a useful complement to the visualization and ablation techniques used in previous work on understanding complex models (Strobelt et al., 2016; Ribeiro et al., 2016).
",1 Introduction,[0],[0]
"While structurally quite similar to the task of machine translation between pairs of human languages, interpretation of neuralese poses a number of novel challenges.",1 Introduction,[0],[0]
"First, there is no natural source of parallel data: there are no bilingual “speakers” of both neuralese and natural language.",1 Introduction,[1.0],"['First, there is no natural source of parallel data: there are no bilingual “speakers” of both neuralese and natural language.']"
"Second, there may not be a direct correspondence between the strategy employed by humans and DCP agents: even if it were constrained to communicate using natural language, an automated agent might choose to produce a different message from humans in a given state.",1 Introduction,[0],[0]
We tackle both of these challenges by appealing to the grounding of messages in gameplay.,1 Introduction,[0],[0]
"Our approach is based on one of the core insights in natural language semantics: messages (whether in neuralese or natural language) have similar meanings when they induce similar beliefs about the state of the world.
",1 Introduction,[0],[0]
"Based on this intuition, we introduce a translation criterion that matches neuralese messages with natural language strings by minimizing statistical distance in a common representation space of distributions over speaker states.",1 Introduction,[0],[0]
"We explore several related questions:
•",1 Introduction,[0],[0]
"What makes a good translation, and under what conditions is translation possible at all?",1 Introduction,[0],[0]
"(Section 4)
",1 Introduction,[0],[0]
• How can we build a model to translate between neuralese and natural language?,1 Introduction,[0],[0]
"(Section 5)
•",1 Introduction,[0],[0]
What kinds of theoretical guarantees can we provide about the behavior of agents communicating via this translation model?,1 Introduction,[0],[0]
"(Section 6)
",1 Introduction,[0],[0]
"Our translation model and analysis are general, and in fact apply equally to human–computer and
human–human translation problems grounded in gameplay.",1 Introduction,[0.9583927980677106],"['(Section 6) Our translation model and analysis are general, and in fact apply equally to human–computer and human–human translation problems grounded in gameplay.']"
"In this paper, we focus our experiments specifically on the problem of interpreting communication in deep policies, and apply our approach to the driving game in Figure 1 and two reference games of the kind shown in Figure 2.",1 Introduction,[0],[0]
We find that this approach outperforms a more conventional machine translation criterion both when attempting to interoperate with neuralese speakers and when predicting their state.,1 Introduction,[0],[0]
A variety of approaches for learning deep policies with communication were proposed essentially simultaneously in the past year.,2 Related work,[0],[0]
"We have broadly labeled these as “deep communicating policies”; concrete examples include Lazaridou et al. (2016b), Foerster et al. (2016), and Sukhbaatar et al. (2016).",2 Related work,[0],[0]
"The policy representation we employ in this paper is similar to the latter two of these, although the general framework is agnostic to low-level modeling details and could be straightforwardly applied to other architectures.",2 Related work,[0],[0]
"Analysis of communication strategies in all these papers has been largely adhoc, obtained by clustering states from which similar messages are emitted and attempting to manually assign semantics to these clusters.",2 Related work,[0],[0]
"The present work aims at developing tools for performing this analysis automatically.
",2 Related work,[0],[0]
"Most closely related to our approach is that of Lazaridou et al. (2016a), who also develop a model for assigning natural language interpretations to learned messages; however, this approach relies on supervised cluster labels and is targeted specifically towards referring expression games.",2 Related work,[0],[0]
"Here we attempt to develop an approach that can handle general multiagent interactions without assuming a prior discrete structure in space of observations.
",2 Related work,[0],[0]
"The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016).",2 Related work,[0],[0]
"This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al., 2013b).",2 Related work,[0],[0]
"All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference.
",2 Related work,[0],[0]
"Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game.",2 Related work,[0],[0]
"Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al. (2016).",2 Related work,[0],[0]
"On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013).
",2 Related work,[0],[0]
Another group of related approaches focuses on the development of more general machinery for interpreting deep models in which messages have no explicit semantics.,2 Related work,[0],[0]
"This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al., 2016; Vedantam et al., 2017).",2 Related work,[0],[0]
Games Consider a cooperative game with two players a and b of the form given in Figure 3.,3 Problem formulation,[0],[0]
"At every step t of this game, player a makes an observation x(t)a and receives a message z (t−1) b from b.",3 Problem formulation,[0],[0]
It then takes an action u(t)a and sends a message z (t) a to b.,3 Problem formulation,[0],[0]
(The process is symmetric for b.),3 Problem formulation,[0],[0]
"The distributions p(ua|xa, zb) and p(za|xa) together define a policy π which we assume is shared by both players, i.e. p(ua|xa, zb) = p(ub|xb, za) and p(za|xa) = p(zb|xb).",3 Problem formulation,[0],[0]
"As in a standard Markov decision process, the actions (u(t)a , u (t) b ) alter the world state, generating new observations for both players and a reward shared by both.
",3 Problem formulation,[0],[0]
"The distributions p(z|x) and p(u|x, z) may also be viewed as defining a language: they specify how a speaker will generate messages based on world states, and how a listener will respond to these mes-
sages.",3 Problem formulation,[0],[0]
Our goal in this work is to learn to translate between pairs of languages generated by different policies.,3 Problem formulation,[0],[0]
"Specifically, we assume that we have access to two policies for the same game: a “robot policy” πr and a “human policy” πh.",3 Problem formulation,[0],[0]
"We would like to use the representation of πh, the behavior of which is transparent to human users, in order to understand the behavior of πr (which is in general an uninterpretable learned model); we will do this by inducing bilingual dictionaries that map message vectors zr of πr to natural language strings zh of πh and vice-versa.
",3 Problem formulation,[0],[0]
Learned agents πr,3 Problem formulation,[0],[0]
Our goal is to present tools for interpretation of learned messages that are agnostic to the details of the underlying algorithm for acquiring them.,3 Problem formulation,[0],[0]
We use a generic DCP model as a basis for the techniques developed in this paper.,3 Problem formulation,[0],[0]
"Here each agent policy is represented as a deep recurrent Q network (Hausknecht and Stone, 2015).",3 Problem formulation,[0],[0]
This network is built from communicating cells of the kind depicted in Figure 4.,3 Problem formulation,[0],[0]
"At every timestep, this agent receives three pieces of information: an
observation of the current state of the world, the agent’s memory vector from the previous timestep, and a message from the other player.",3 Problem formulation,[0],[0]
"It then produces three outputs: a predicted Q value for every possible action, a new memory vector for the next timestep, and a message to send to the other agent.
",3 Problem formulation,[0],[0]
Sukhbaatar et al. (2016) observe that models of this form may be viewed as specifying a single RNN in which weight matrices have a particular block structure.,3 Problem formulation,[0],[0]
"Such models may thus be trained using the standard recurrent Q-learning objective, with communication protocol learned end-to-end.
",3 Problem formulation,[0],[0]
Human agents πh The translation model we develop requires a representation of the distribution over messages p(za|xa) employed by human speakers (without assuming that humans and agents produce equivalent messages in equivalent contexts).,3 Problem formulation,[0],[0]
"We model the human message generation process as categorical, and fit a simple multilayer perceptron model to map from observations to words and phrases used during human gameplay.",3 Problem formulation,[0],[0]
What does it mean for a message zh to be a “translation” of a message zr?,4 What’s in a translation?,[0],[0]
"In standard machine translation problems, the answer is that zh is likely to co-occur in parallel data with zr; that is, p(zh|zr) is large.",4 What’s in a translation?,[0],[0]
"Here we have no parallel data: even if we could observe natural language and neuralese messages produced by agents in the same state, we would have no guarantee that these messages actually served the same function.",4 What’s in a translation?,[0],[0]
Our answer must instead appeal to the fact that both natural language and neuralese messages are grounded in a common environment.,4 What’s in a translation?,[0],[0]
"For a given neuralese message zr, we will first compute a grounded representation of that message’s meaning; to translate, we find a natural-language message whose meaning is most similar.",4 What’s in a translation?,[0],[0]
The key question is then what form this grounded meaning representation should take.,4 What’s in a translation?,[0],[0]
"The existing literature suggests two broad approaches:
Semantic representation The meaning of a message za is given by its denotations: that is, by the set of world states of which za may be felicitously predicated, given the existing context available to a listener.",4 What’s in a translation?,[0],[0]
"In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(xa|za, xb) it induces over speaker states.",4 What’s in a translation?,[0],[0]
"Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016).
",4 What’s in a translation?,[0],[0]
Pragmatic representation,4 What’s in a translation?,[0],[0]
The meaning of a message za is given by the behavior it induces in a listener.,4 What’s in a translation?,[0],[0]
"In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(ub|za, xb) it induces over actions given the listener’s observation xb.",4 What’s in a translation?,[0],[0]
"Examples of this approach include Vogel et al. (2013a) and Gauthier and Mordatch (2016).
",4 What’s in a translation?,[0],[0]
These two approaches can give rise to rather different behaviors.,4 What’s in a translation?,[0],[0]
"Consider the following example:
square hexagon circle
few many many
The top language (in blue) has a unique name for every kind of shape, while the bottom language (in red) only distinguishes between shapes with few sides and shapes with many sides.",4 What’s in a translation?,[0],[0]
"Now imagine a simple reference game with the following form: player a is covertly assigned one of these three shapes as a reference target, and communicates that reference to b; b must then pull a lever labeled large or small depending on the size of the target shape.",4 What’s in a translation?,[0],[0]
"Blue language speakers can achieve perfect success at this game, while red language speakers can succeed at best two out of three times.
",4 What’s in a translation?,[0],[0]
How should we translate the blue word hexagon into the red language?,4 What’s in a translation?,[0],[0]
"The semantic approach suggests that we should translate hexagon as many: while many does not uniquely identify the hexagon, it produces a distribution over shapes that is closest to the truth.",4 What’s in a translation?,[0],[0]
"The pragmatic approach instead suggests that we should translate hexagon as few, as this is the only message that guarantees that the listener will pull the correct lever large.",4 What’s in a translation?,[0],[0]
"So in order to produce a correct listener action, the translator might have to “lie” and produce a maximally inaccurate listener belief.
",4 What’s in a translation?,[0],[0]
"If we were exclusively concerned with building a translation layer that allowed humans and DCP agents to interoperate as effectively as possible, it would be natural to adopt a pragmatic representation strategy.",4 What’s in a translation?,[0],[0]
"But our goals here are broader: we also want to facilitate understanding, and specifically to help users of learned systems form true beliefs about the systems’ computational processes and representational abstractions.",4 What’s in a translation?,[0],[0]
"The example above demonstrates that “pragmatically” optimizing directly for task performance can sometimes lead to translations that produce inaccurate beliefs.
",4 What’s in a translation?,[0],[0]
We instead build our approach around semantic representations of meaning.,4 What’s in a translation?,[0],[0]
"By preserving semantics, we allow listeners to reason accurately about the content and interpretation of messages.",4 What’s in a translation?,[0],[0]
"We might worry that by adopting a semantics-first view, we have given up all guarantees of effective interoperation between humans and agents using a translation layer.",4 What’s in a translation?,[0],[0]
"Fortunately, this is not so: as we will see in Section 6, it is possible to show that players communicating via a semantic translator perform only boundedly worse (and sometimes better!)",4 What’s in a translation?,[0],[0]
than pairs of players with a common language.,4 What’s in a translation?,[0],[0]
"In this section, we build on the intuition that messages should be translated via their semantics to define a concrete translation model—a procedure for constructing a natural language ↔ neuralese dictionary given agent and human interactions.
",5 Translation models,[0],[0]
"We understand the meaning of a message za to be represented by the distribution p(xa|za, xb) it induces over speaker states given listener context.",5 Translation models,[1.0],"['We understand the meaning of a message za to be represented by the distribution p(xa|za, xb) it induces over speaker states given listener context.']"
"We can formalize this by defining the belief distribution β for a message z and context xb as:
β(za, xb) = p(xa|za, xb) = p(za|xa)p(xb|xa)∑ x′a p(za|x′a)p(xb|x′a)
",5 Translation models,[0],[0]
"Here we have modeled the listener as performing a single step of Bayesian inference, using the listener state and the message generation model (by assumption shared between players) to compute the posterior over speaker states.",5 Translation models,[0.9673416766563662],"['We can formalize this by defining the belief distribution β for a message z and context xb as: β(za, xb) = p(xa|za, xb) = p(za|xa)p(xb|xa)∑ x′a p(za|x′a)p(xb|x′a) Here we have modeled the listener as performing a single step of Bayesian inference, using the listener state and the message generation model (by assumption shared between players) to compute the posterior over speaker states.']"
"While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).
",5 Translation models,[1.0000000552163941],"['While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).']"
"This provides a context-specific representation of belief, but for messages z and z′ to have the same semantics, they must induce the same belief over all contexts in which they occur.",5 Translation models,[0],[0]
"In our probabilistic formulation, this introduces an outer expectation over contexts, providing a final measure q of the quality of a translation from z to z′:
q(z, z′) =",5 Translation models,[0],[0]
E,5 Translation models,[0],[0]
"[ DKL(β(z,Xb) || β(z′, Xb))",5 Translation models,[0],[0]
"| z, z′ ]
= ∑
xa,xb
p(xa, xb|z, z′)DKL(β(z, xb) || β(z′, xb))
∝",5 Translation models,[0],[0]
"∑
xa,xb p(xa, xb) · p(z|xa) · p(z′|xa) ·",5 Translation models,[0],[0]
"DKL(β(z, xb) || β(z′, xb));",5 Translation models,[0],[0]
"(1)
Algorithm 1 Translating messages
given: a phrase inventory L function TRANSLATE(z)
return argminz′∈L q̂(z, z′)
function q̂(z, z′) // sample contexts and distractors xai, xbi ∼ p(Xa, Xb) for i = 1..n",5 Translation models,[0],[0]
x′ai ∼ p(Xa|xbi) //,5 Translation models,[0],[0]
compute context weights w̃i ← p(z|xai) ·,5 Translation models,[0],[0]
p(z′|xai),5 Translation models,[0],[0]
"wi ← w̃i/ ∑ j w̃j
// compute divergences ki ← ∑ x∈{xa,x′a} p(z|x) log p(z|x) p(z′|x)
return ∑
iwiki
recalling that in this setting
DKL(β || β′)",5 Translation models,[0],[0]
"= ∑
xa
p(xa|z, xb) log p(xa|z, xb) p(xa|z′, xb)
∝",5 Translation models,[0],[0]
"∑
xa
p(xa|xb)p(z|xa) log p(z|xa) p(z′|xa)
(2)
which is zero when the messages z and z′ give rise to identical belief distributions and increases as they grow more dissimilar.",5 Translation models,[0],[0]
"To translate, we would like to compute tr(zr) = argminzh q(zr, zh) and tr(zh) = argminzr q(zh, zr).",5 Translation models,[1.0],"['To translate, we would like to compute tr(zr) = argminzh q(zr, zh) and tr(zh) = argminzr q(zh, zr).']"
"Intuitively, Equation 1 says that we will measure the quality of a proposed translation z 7→ z′ by asking the following question: in contexts where z is likely to be used, how frequently does z′ induce the same belief about speaker states as z?
While this translation criterion directly encodes the semantic notion of meaning described in Section 4, it is doubly intractable: the KL divergence and outer expectation involve a sum over all observations xa and xb respectively; these sums are not in general possible to compute efficiently.",5 Translation models,[0],[0]
"To avoid this, we approximate Equation 1 by sampling.",5 Translation models,[0],[0]
"We draw a collection of samples (xa, xb) from the prior over world states, and then generate for each sample a sequence of distractors (x′a, xb) from p(x ′ a|xb)",5 Translation models,[0],[0]
(we assume access to both of these distributions from the problem representation).,5 Translation models,[0],[0]
"The KL term in Equation 1 is computed over each true sample and its distractors, which are then normalized and averaged to compute the final score.
",5 Translation models,[0],[0]
"Sampling accounts for the outer p(xa, xb) in Equation 1 and the inner p(xa|xb) in Equation 2.
",5 Translation models,[0],[0]
The only quantities remaining are of the form p(z|xa).,5 Translation models,[0],[0]
"In the case of neuralese, this distribution already is part of the definition of the agent policy πr and can be reused directly.",5 Translation models,[0],[0]
"For natural language, we use transcripts of human interactions to fit a model that maps from world states to a distribution over frequent utterances as discussed in Section 3.",5 Translation models,[0],[0]
"Details of these model implementations are provided in Appendix B, and the full translation procedure is given in Algorithm 1.",5 Translation models,[0],[0]
The translation criterion in the previous section makes no reference to listener actions at all.,6 Belief and behavior,[0],[0]
The shapes example in Section 4 shows that some model performance might be lost under translation.,6 Belief and behavior,[0],[0]
It is thus reasonable to ask whether this translation model of Section 5 can make any guarantees about the effect of translation on behavior.,6 Belief and behavior,[0],[0]
"In this section we explore the relationship between beliefpreserving translations and the behaviors they produce, by examining the effect of belief accuracy and strategy mismatch on the reward obtained by cooperating agents.
",6 Belief and behavior,[0],[0]
"To facilitate this analysis, we consider a simplified family of communication games with the structure depicted in Figure 5.",6 Belief and behavior,[0],[0]
"These games can be viewed as a subset of the family depicted in Figure 3; and consist of two steps: a listener makes an observation xa and sends a single message z to a speaker, which makes its own observation xb, takes a single action u, and receives a reward.",6 Belief and behavior,[0],[0]
"We emphasize that the results in this section concern the theoretical properties of idealized games, and are presented to provide intuition about high-level properties of our approach.",6 Belief and behavior,[0],[0]
"Section 8 investigates empirical behavior of this approach on real-world tasks where these ideal conditions do not hold.
",6 Belief and behavior,[0],[0]
"Our first result is that translations that minimize semantic dissimilarity q cause the listener to take near-optimal actions:2
2Proof is provided in Appendix A.
Proposition 1.",6 Belief and behavior,[0],[0]
Semantic translations reward rational listeners.,6 Belief and behavior,[0],[0]
"Define a rational listener as one that chooses the best action in expectation over the speaker’s state:
U(z, xb) = argmax u
∑
xa
p(xa|xb, z)r(xa, xb, u)
for a reward function r ∈",6 Belief and behavior,[0],[0]
"[0, 1] that depends only on the two observations and the action.3 Now let a be a speaker of a language r, b be a listener of the same language r, and b′ be a listener of a different language h.",6 Belief and behavior,[0],[0]
Suppose that we wish for a and b′ to interact via the translator tr :,6 Belief and behavior,[0],[0]
"zr 7→ zh (so that a produces a message zr, and b′ takes an action U(zh = tr(zr), xb′)).",6 Belief and behavior,[0],[0]
"If tr respects the semantics of zr, then the bilingual pair a and b′ achieves only boundedly worse reward than the monolingual pair a and",6 Belief and behavior,[0],[0]
"b. Specifically, if q(zr, zh) ≤ D, then
Er(Xa, Xb, U(tr(Z))
",6 Belief and behavior,[0],[0]
"≥ Er(Xa, Xb, U(Z))− √ 2D (3)
",6 Belief and behavior,[0],[0]
"So as discussed in Section 4, even by committing to a semantic approach to meaning representation, we have still succeeded in (approximately) capturing the nice properties of the pragmatic approach.
",6 Belief and behavior,[0],[0]
Section 4 examined the consequences of a mismatch between the set of primitives available in two languages.,6 Belief and behavior,[0],[0]
In general we would like some measure of our approach’s robustness to the lack of an exact correspondence between two languages.,6 Belief and behavior,[0],[0]
"In the case of humans in particular we expect that a variety of different strategies will be employed, many of which will not correspond to the behavior of the learned agent.",6 Belief and behavior,[0],[0]
It is natural to want some assurance that we can identify the DCP’s strategy as long as some human strategy mirrors it.,6 Belief and behavior,[0],[0]
"Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies:
Proposition 2.",6 Belief and behavior,[0],[0]
Semantic translations find hidden correspondences.,6 Belief and behavior,[0],[0]
"Consider a fixed robot policy πr and a set of human policies {πh1 , πh2 , . . . }",6 Belief and behavior,[0],[0]
"(recalling from Section 3 that each π is defined by distributions
3This notion of rationality is a fairly weak one: it permits many suboptimal communication strategies, and requires only that the listener do as well as possible given a fixed speaker— a first-order optimality criterion likely to be satisfied by any richly-parameterized model trained via gradient descent.
",6 Belief and behavior,[0],[0]
"p(z |xa) and p(u |z , xb)).",6 Belief and behavior,[0],[0]
"Suppose further that the messages employed by these human strategies are disjoint; that is, if phi(z |xa)",6 Belief and behavior,[0],[0]
"> 0, then phj (z |xa) = 0 for all j 6=",6 Belief and behavior,[0],[0]
i.,6 Belief and behavior,[0],[0]
"Now suppose that all q(zr , zh) = 0 for all messages in the support of some phi(z |xa) and > 0 for all j 6=",6 Belief and behavior,[0],[0]
i.,6 Belief and behavior,[0],[0]
"Then every message zr is translated into a message produced by πhi , and messages from other strategies are ignored.
",6 Belief and behavior,[0],[0]
"This observation follows immediately from the definition of q(zr, zh), but demonstrates one of the key distinctions between our approach and a conventional machine translation criterion.",6 Belief and behavior,[0],[0]
"Maximizing p(zh|zr) will produce the natural language message most often produced in contexts where zr is observed, regardless of whether that message is useful or informative.",6 Belief and behavior,[0],[0]
"By contrast, minimizing q(zh, zr) will find the zh that corresponds most closely to zr even when zh is rarely used.
",6 Belief and behavior,[0],[0]
"The disjointness condition, while seemingly quite strong, in fact arises naturally in many circumstances—for example, players in the driving game reporting their spatial locations in absolute vs. relative coordinates, or speakers in a color reference game (Figure 6) discriminating based on lightness vs. hue.",6 Belief and behavior,[0],[0]
"It is also possible to relax the above condition to require that strategies be only locally disjoint (i.e. with the disjointness condition holding for each fixed xa), in which case overlapping human strategies are allowed, and the recovered robot strategy is a context-weighted mixture of these.",6 Belief and behavior,[0],[0]
"In the remainder of the paper, we evaluate the empirical behavior of our approach to translation.",7.1 Tasks,[0],[0]
Our evaluation considers two kinds of tasks: reference games and navigation games.,7.1 Tasks,[0],[0]
"In a reference game (e.g. Figure 6a), both players observe a pair of candidate referents.",7.1 Tasks,[0],[0]
"A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target.",7.1 Tasks,[0],[0]
"In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds.",7.1 Tasks,[0],[0]
"For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al., 2010) with accom-
panying natural language descriptions (Reed et al., 2016).",7.1 Tasks,[0],[0]
"We use standard train / validation / test splits for both of these datasets.
",7.1 Tasks,[0],[0]
The final task we consider is the driving task (Figure 6c) first discussed in the introduction.,7.1 Tasks,[0],[0]
"In this task, two cars, invisible to each other, must each navigate between randomly assigned start and goal positions without colliding.",7.1 Tasks,[0],[0]
"This task takes a number of steps to complete, and potentially involves a much broader range of communication strategies.",7.1 Tasks,[0],[0]
"To obtain human annotations for this task, we recorded both actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other.",7.1 Tasks,[0],[0]
"We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set.",7.1 Tasks,[0],[0]
"A mechanism for understanding the behavior of a learned model should allow a human user both to correctly infer its beliefs and to successfully interoperate with it; we accordingly report results of both “belief” and “behavior” evaluations.
",7.2 Metrics,[0],[0]
"To support easy reproduction and comparison (and in keeping with standard practice in machine
translation), we focus on developing automatic measures of system performance.",7.2 Metrics,[0],[0]
"We use the available training data to develop simulated models of human decisions; by first showing that these models track well with human judgments, we can be confident that their use in evaluations will correlate with human understanding.",7.2 Metrics,[0],[0]
"We employ the following two metrics:
Belief evaluation This evaluation focuses on the denotational perspective in semantics that motivated the initial development of our model.",7.2 Metrics,[0],[0]
We have successfully understood the semantics of a message,7.2 Metrics,[0],[0]
"zr if, after translating zr 7→ zh, a human listener can form a correct belief about the state in which zr was produced.",7.2 Metrics,[0],[0]
"We construct a simple state-guessing game where the listener is presented with a translated message and two state observations, and must guess which state the speaker was in when the message was emitted.
",7.2 Metrics,[0],[0]
"When translating from natural language to neuralese, we use the learned agent model to directly guess the hidden state.",7.2 Metrics,[0],[0]
"For neuralese to natural language we must first construct a “model human listener” to map from strings back to state representations; we do this by using the training data to fit a simple regression model that scores (state, sentence) pairs using a bag-of-words sentence representation.",7.2 Metrics,[0],[0]
"We find that our “model human” matches the judgments of real humans 83% of the time on the colors task, 77% of the time on the birds task, and 77% of the time on the driving task.",7.2 Metrics,[0],[0]
"This gives us confidence that the model human gives a reasonably accurate proxy for human interpretation.
",7.2 Metrics,[0],[0]
Behavior evaluation This evaluation focuses on the cooperative aspects of interpretability: we measure the extent to which learned models are able to interoperate with each other by way of a translation layer.,7.2 Metrics,[0],[0]
"In the case of reference games, the goal of this semantic evaluation is identical to the goal of the game itself (to identify the hidden state of the speaker), so we perform this additional pragmatic evaluation only for the driving game.",7.2 Metrics,[0],[0]
We found that the most data-efficient and reliable way to make use of human game traces was to construct a “deaf” model human.,7.2 Metrics,[0],[0]
"The evaluation selects a full game trace from a human player, and replays both the human’s actions and messages exactly (disregarding any incoming messages); the evaluation measures the quality of the natural-language-toneuralese translator, and the extent to which the
learned agent model can accommodate a (real) human given translations of the human’s messages.
",7.2 Metrics,[0],[0]
"Baselines We compare our approach to two baselines: a random baseline that chooses a translation of each input uniformly from messages observed during training, and a direct baseline that directly maximizes p(z′|z) (by analogy to a conventional machine translation system).",7.2 Metrics,[0],[0]
This is accomplished by sampling from a DCP speaker in training states labeled with natural language strings.,7.2 Metrics,[0],[0]
"In all below, “R” indicates a DCP agent, “H” indicates a real human, and “H*” indicates a model human player.
",8 Results,[0],[0]
Reference games Results for the two reference games are shown in Table 1.,8 Results,[0],[0]
"The end-to-end trained model achieves nearly perfect accuracy in both
cases, while a model trained to communicate in natural language achieves somewhat lower performance.",8 Results,[0.9999999728277418],"['The end-to-end trained model achieves nearly perfect accuracy in both cases, while a model trained to communicate in natural language achieves somewhat lower performance.']"
"Regardless of whether the speaker is a DCP and the listener a model human or vice-versa, translation based on the belief-matching criterion in Section 5 achieves the best performance; indeed, when translating neuralese color names to natural language, the listener is able to achieve a slightly higher score than it is natively.",8 Results,[0],[0]
"This suggests that the automated agent has discovered a more effective strategy than the one demonstrated by humans in the dataset, and that the effectiveness of this strategy is preserved by translation.",8 Results,[0],[0]
"Example translations from the reference games are depicted in Figure 2 and Figure 7.
",8 Results,[0],[0]
"Driving game Behavior evaluation of the driving game is shown in Table 3, and belief evaluation is shown in Table 2.",8 Results,[0],[0]
"Translation of messages in the driving game is considerably more challenging than in the reference games, and scores are uniformly lower; however, a clear benefit from the beliefmatching model is still visible.",8 Results,[0],[0]
"Belief matching leads to higher scores on the belief evaluation in both directions, and allows agents to obtain a higher reward on average (though task completion rates remain roughly the same across all agents).",8 Results,[0],[0]
Some example translations of driving game messages are shown in Figure 8.,8 Results,[0],[0]
We have investigated the problem of interpreting message vectors from deep networks by translating them.,9 Conclusion,[0],[0]
"After introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conventional machine translation approach at recovering the content of message vectors and facilitating collaboration between humans and learned agents.
",9 Conclusion,[0],[0]
"While our evaluation has focused on understanding the behavior of deep communicating policies, the framework proposed in this paper could be much more generally applied.",9 Conclusion,[0],[0]
"Any encoder– decoder model (Sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating “beliefs” induced by the encoding to explain what features of the input are being transmitted.",9 Conclusion,[0],[0]
"The current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch.",9 Conclusion,[0],[0]
"More broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (Wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors.",9 Conclusion,[0],[0]
JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship.,Acknowledgments,[0],[0]
We are grateful to Lisa Anne Hendricks for assistance with the Caltech Birds dataset.,Acknowledgments,[0],[0]
Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel.,abstractText,[0],[0]
"While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge.",abstractText,[0],[0]
Here we propose to interpret agents’ messages by translating them.,abstractText,[0],[0]
"Unlike in typical machine translation problems, we have no parallel data to learn from.",abstractText,[0],[0]
Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener.,abstractText,[0],[0]
We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1,abstractText,[0],[0]
Translating Neuralese,title,[0],[0]
