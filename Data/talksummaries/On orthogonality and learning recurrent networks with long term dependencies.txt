18	20	We suspect that a strong constraint of orthogonality limits the model’s representational power, hindering its performance, and may make optimization more difficult.
19	19	We explore this hypothesis empirically by employing a factorization technique that allows us to limit the degree of deviation from the Stiefel manifold.
24	19	Following the above, the gradient in equation 2 can be fully decomposed into a recursive chain of matrix products: ∂L ∂θi = ∂ai ∂θi n∏ j=i (DjWj+1) ∂L ∂an+1 (4) In (Pascanu et al., 2013), it is shown that the 2-norm of ∂ai+1 ∂ai is bounded by the product of the norms of the nonlinearity’s Jacobian and transition matrix at time t (layer i ), as follows:∣∣∣∣∣∣∣∣∂at+1∂at ∣∣∣∣∣∣∣∣ ≤ ||Dt|| ||Wt|| ≤ λDt λWt = ηt, λDt , λWt ∈ R. (5) where λDt and λWt are the largest singular values of the non-linearity’s Jacobian Dt and the transition matrix Wt .
25	28	In RNNs, Wt is shared across time and can be simply denoted as W. Equation 5 shows that the gradient can grow or shrink at each layer depending on the gain of each layer’s linear transformation W and the gain of the Jacobian D. The gain caused by each layer is magnified across all time steps or layers.
27	25	The phenomena of extreme growth or contraction of the gradient across time steps or layers are known as the exploding and the vanishing gradient problems, respectively.
32	86	(6) By keeping our weight matrix W close to orthogonal, one can ensure that it is close to a norm-preserving transformation (where the spectral norm is equal to one, but the minimum gain is also one).
36	22	Similarly, the minimum gain or contractivity of a matrix can be obtained from the minimum singular value.
37	49	We can keep the bases U and V orthogonal via geodesic gradient descent along the set of weights that satisfy UTU = I and VTV = I respectively.
38	64	The submanifolds that satisfy these constraints are called Stiefel manifolds.
43	24	As such, we parameterize the transition matrix W in factorized form, as a singular value decomposition with orthogonal bases U and V updated by geodesic gradient descent using the Cayley transform approach above.
44	75	If W is an orthogonal matrix, the singular values in the diagonal matrix S are all equal to one.
45	16	However, in our formulation we allow these singular values to deviate from one and employ a sigmoidal parameterization to apply a hard constraint on the maximum and minimum amount of deviation.
46	19	Specifically, we define a margin m around 1 within which the singular values must lie.
66	23	Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993).
86	29	In this section, we experimentally explore the effect of loosening hard orthogonality constraints through loosening the spectral margin defined above for the hidden to hidden transition matrix.
97	47	As shown in Figure 1 we see an increase in the rate of convergence as we increase the spectral margin.
106	29	Using tanh, a short sequence length (T = 100) copy task required both a soft constraint that encourages orthogonality and thousands of epochs for training.
107	40	It is worth noting that in the unitary evolution recurrent neural network of Arjovsky et al. (2015), the non-linearity (referred to as the ”modReLU”) is actually initialized as an identity operation that is free to deviate from identity during training.
112	57	We also experimented with a trainable slope α, initialized to 0.7 and found that it converges to 0.96, further suggesting the optimal solution for the copy task is without a transition nonlinearity.
120	16	We trained the factorized RNN models with 128 hidden units for 120 epochs.
144	119	We can see that for a purely orthogonal parameterization of the transition matrix (when the margin is zero), the gradient norm is preserved across time steps, as expected.
145	18	We further observe that with increasing margin size, the number of update steps over which this norm preservation survives decreases, though surprisingly not as quickly as expected.
150	49	On the other hand, singular value distributions tend to drift away from one for PTB character prediction which may help explain why enforcing an orthogonality constraint can be helpful for this task, when modeling long sequences.
151	26	Interestingly, singular values spread out less for longer sequence lengths (nevertheless, the T=10000 copy task could not be solved with no sigmoid on the spectrum).
158	68	This is interesting because the cost function cannot take into account numerical issues such as vanishing or exploding gradients (or forward signals); we do not know what could make this deviation costly.
171	46	The second approach we explore replaces the sigmoidal margin parameterization with a mean one Gaussian prior on the singular values.
174	27	We see that strong priors lead to slow convergence.
178	15	These results further motivate the idea that parameterizations that deviate from orthogonality may perform better than purely orthogonal ones, as long as they are sufficiently constrained to avoid instability during training.
180	37	Our experiments indicate that while orthogonal initialization may be beneficial, maintaining hard constraints on orthogonality can be detrimental.
183	37	We thank the Natural Sciences and Engineeering Research Council (NSERC) of Canada and Samsung for supporting this research.
