7	23	We work within the framework of Meta-Learning / Learning-toLearn / Inductive Transfer (Thrun & Pratt, 1997; Vilalta & Drissi, 2002) 1 in which a ‘meta-learner’ extracts knowledge from several observed tasks to facilitate the learning of new tasks by a ‘base-learner’ (see Figure 1).
9	25	The performance is evaluated when learning related new tasks (which are unavailable to the meta-learner) .
10	17	As a motivational example, consider the case in which a meta-learner observes many image classification tasks of natural images, and uses a CNN to learn each task.
11	19	The meta-learner might learn a prior which fixes the lower layers of the network to extract generic image features, but allows variation in the higher layers to adapt to new classes.
12	21	Thus, new tasks can be learned using fewer examples than learning from scratch (e.g., Yosinski et al. (2014)).
20	12	The main contributions of this work are the following.
23	13	(iii) Empirical demonstration of the performance enhancement compared to naive approaches as well as recent methods in this field.
29	55	In the common setting for learning, a set of independent samples, S = {zi}mi=1, from a space of examples Z , is given, each sample drawn from an unknown probability distribution D, namely zi ∼ D. We will use the notation S ∼ Dm to denote the distribution over the full sample.
34	28	While in the standard approach to learning, described in the previous paragraph, one usually selects a single classifier (e.g., the one minimizing the empirical error), the PAC-Bayes framework, first formulated by McAllester (1999), considers the construction of a complete probability distribution overH, and the selection of a single hypothesis h ∈ H based on this distribution.
51	9	By choosing Q that minimizes the bound we obtain a learning algorithm with generalization guarantees.
70	41	The prior knowledge comes in the form of a distribution over hypotheses, P ∈ M. When learning a new task, the base learner uses the observed task’s data S and the prior P to output a posterior distribution Q(S, P ) overH.
80	11	When encountering a new task, the learner samples a prior from the hyper posterior Q(P ), and then use it for learning.
86	17	A hyper distribution would correspond in this case to a distribution over the mean and covariance of P .
92	18	Notice that the transfer error (2) is bounded by the empirical multi-task error (3) plus two complexity terms.
96	20	This term converges to zero if infinite number of tasks is observed from the task environment (n→∞).
98	28	The second step, similarly to Pentina & Lampert (2014), bounds the transfer-risk at the task-environment level (i.e, the error caused by observing only a finite number of tasks) by the average expected error in the observed tasks plus the environment-complexity term.
99	25	The first step differs from Pentina & Lampert (2014).
102	8	Therefore our bound is better adjusted the observed data set.
104	25	In section A.1 we use McAllester’s bound (Theorem 1), which is tighter than the lemma used in Pentina & Lampert (2014).
108	25	In Section 5 we will empirically evaluate the different bounds as meta-learning objectives and show that the improved tightness is critical for performance.
110	10	Since the bound holds uniformly for all Q, it is ensured to hold also for the minimizer of the objective Q∗.
116	15	Notice that Q appears in the bound (4) in two forms (i) divergence from the hyper-prior D(Q||P) and (ii) expectations over P ∼ Q.
117	48	By setting the hyper-prior as zero-mean isotropic Gaussian, P = N ( 0, κ2PINP×NP ) , where κP > 0 is another constant, we get a simple form for the KL-divergence term, D(Qθ||P) = 12κ2P ‖θ‖ 2 2 .
118	13	Note that the hyper-prior acts as a regularization term which prefers solutions with small L2 norm.
124	14	We will use a procedure which minimizes Ji(θ) due to the following advantages: (i) It minimizes a bound on the expected error of the observed task 6.
127	34	To formulate the single-task learning as an optimization problem, we choose a parametric form for the posterior of each task Qφi , φi ∈ RNQ (see section 4.3 for an explicit example).
130	11	(8) The optimization process is illustrated in Figure 2.
132	59	First, we define the hypothesis class H as a family of functions parameterized by a weight vector{ hw : w ∈ Rd } .
133	24	Given this parameterization, the posterior and prior are distributions over Rd.
137	24	Next we define the posteriors Qφi , i = 1, ..., n, and the prior Pθ as factorized Gaussian distributions8, Pθ(w) = d∏ k=1 N ( wk;µP,k, σ 2 P,k ) (9) Qφi(w) = d∏ k=1 N ( wk;µi,k, σ 2 i,k ) (10) where for each task, the posterior parameters vector φi = (µi, ρi) ∈ R2d is composed of the means and log-variances of each weight , µi,k and ρi,k = log σ2P,k, k = 1, ..., d. 9 The shared prior vector θ = (µP , ρP ) ∈ R2d has a similar structure.
