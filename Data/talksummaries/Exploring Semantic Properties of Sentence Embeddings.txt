3	18	Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector.
6	25	The SICK dataset (Marelli et al., 2014) was created to better benchmark the effectiveness of different models across a broad range of challenging lexical, syntactic, and semantic phenomena, in terms of both similarities and the ability to be predictive of entailment.
8	12	Adi et al. investigated to what extent different embedding methods are predictive of i) the occurrence of words in the original sentence, ii) the order of words in the original sentence, and iii) the length of the original sentence (Adi et al., 2016, 2017).
10	43	Wang et al. (2016) argue that the latent representations of advanced neural reading comprehension architectures encode information about predication.
12	12	Concurrently with our research, Conneau et al. (2018) investigated to what extent one can learn to classify specific syntactic and semantic properties of sentences using large amounts of training data (100,000 instances) for each property.
14	13	In this paper, we specifically focus on a few select aspects of sentence semantics and inspect to what extent prominent sentence embedding methods are able to capture them.
18	14	For instance, a sentence S such as A rabbit is jumping over the fence and a sentence S∗ such as A rabbit is not jumping over the fence diverge with respect to many of the inferences that they warrant.
20	28	Despite the semantic differences between the two sentences due to the negation, we still expect the cosine similarity between their respective embeddings to be fairly high, in light of their semantic relatedness in touching on similar themes.
34	19	A: A girl is cutting butter into two pieces.
41	18	Sentences that are expressed in active voice are changed to passive voice.
44	13	For sentences matching the structure “〈somebody〉 〈verb〉 〈somebody〉 to 〈do something〉”, we swap the subject and object of the original sentence A to generate a new sentence B.
52	15	Negation Detection: Original sentence, Synonym Substitution, Not-Negation With this dataset, we seek to explore how well sentence embeddings can distinguish sentences with similar structure and opposite meaning, while using Synonym Substitution as the contrast set.
54	17	Negation Variants: Quantifier-Negation, Not-Negation, Original sentence In the second dataset, we aim to investigate how well the sentence embeddings reflect negation quantifiers.
56	13	Clause Relatedness: Original sentence, Embedded Clause Extraction, Not-Negation In this third set, we want to explore whether the similarity between a sentence and its embedded clause is higher than between a sentence and its negation.
57	18	Argument Sensitivity: Original sentence, Passivization, Argument Reordering With this last test, we wish to ascertain whether the sentence embeddings succeed in distinguishing semantic information from structural information.
60	14	If the sentence embeddings focus more on semantic cues, then the similarity between S and S+ ought to be larger than that between S+ and S∗.
61	12	If the sentence embedding however is easily misled by matching sentence structures, the opposite will be the case.
62	18	Fixed Point Reorder: Original sentence, Semantically equivalent sentence, Fixed Point Inversion With this dataset, our objective is to explore how well the sentence embeddings account for shifts in meaning due to the word order in a sentence.
66	15	Additionally, we use the Fixed Point Inversion technique to generate a contrastive sentence.
67	19	The resulting sentence likely no longer adequately reflects the original meaning.
70	30	Using the aforementioned triplet generation methods, we create the evaluation datasets listed in Table 1, drawing on source sentences from SICK, Penn Treebank WSJ and MSR Paraphase corpus.
88	16	In contrast, both InferSent and SkipThought succeed in distinguishing unnegated sentences from negated ones.
102	20	Infersent also achieved better performance on sentences resembling a) compared with sentences resembling b), its accuracy on these two structures is 28.37% and 15.73% respectively.
104	14	Although recurrent architectures are able to consider the order of words, unfortunately, none of the analysed approaches prove adept at distinguishing the semantic information from structural information in this case.
106	39	As Table 6 indicates, sentence embeddings based on means (GloVe averages), weighted means (Sent2Vec), or concatenation of p-mean embeddings (P-Means) are unable to distinguish the fixed point inverted sentence from the semantically equivalent one, as they do not encode sufficient word order information into the sentence embeddings.
107	26	Sent2Vec does consider ngrams but these do not affect the results sufficiently.SkipThought and InferSent did well when the original sentence and its semantically equivalence share similar structure.
109	62	We find that both SkipThought and InferSent distinguish negation of a sentence from synonymy.
110	53	InferSent fares better at identifying semantic equivalence regardless of the order of words and copes better with quantifiers.
111	47	SkipThoughts is more suitable for tasks in which the semantics of the sentence corresponds to its structure, but it often fails to identify sentences with different word order yet similar meaning.
112	14	In almost all cases, dedicated sentence embeddings from hidden states a neural network outperform a simple averaging of word embeddings.
