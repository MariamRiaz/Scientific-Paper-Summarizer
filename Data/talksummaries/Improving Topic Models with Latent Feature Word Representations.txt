0	16	Topic modeling algorithms, such as Latent Dirichlet Allocation (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus, and predict the probabilities of each word in each document belonging to each topic (Teh et al., 2006; Newman et al., 2006; Toutanova and Johnson, 2008; Porteous et al., 2008; Johnson, 2010; Xie and Xing, 2013; Hingmire et al., 2013).
1	17	Conventional topic modeling algorithms such as these infer document-to-topic and topic-to-word distributions from the co-occurrence of words within documents.
3	34	Sahami and Heilman (2006) and Phan et al. (2011) show that it helps to exploit external knowledge to improve the topic representations.
29	27	In the Dirichlet Multinomial Mixture (DMM) model (Nigam et al., 2000), each document is assumed to only have one topic.
39	81	In general, LF-LDA and LF-DMM are formed by taking the original Dirichlet multinomial topic models LDA and DMM, and replacing their topic-to- word Dirichlet multinomial component that generates words from topics with a two-component mixture of a topic-to-word Dirichlet multinomial component and a latent feature component.
53	33	Algorithm 1: An approximate Gibbs sampling algorithm for the LF-LDA model Initialize the word-topic variables zdi using the LDA sampling algorithm for iteration iter = 1, 2, ... do for topic t = 1, 2, ..., T do τ t = arg maxτ t P(τ t | Z,S) for document d = 1, 2, ..., |D| do for word index i = 1, 2, ..., Nd do sample zdi and sdi from P(zdi = t, sdi | Z¬di ,S¬di , τ ,ω) Here, S denotes the distribution indicator variables for the whole document collection D. Instead of sampling τ t from the posterior, we perform MAP estimation as described in the section 3.5.
54	41	For sampling the topic zdi and the binary indicator variable sdi of the i th word wdi in the document d, we integrate out sdi in order to sample zdi and then sample sdi given zdi .
55	17	We sample the topic zdi using the conditional distribution as follows: P(zdi = t | Z¬di , τ ,ω) ∝ (N td¬i +Ktd¬i + α)( (1− λ) N t,wdi ¬di + β N t¬di + V β + λCatE(wdi | τ t ω>) ) (4) Then we sample sdi conditional on zdi = t with: P(sdi=s | zdi=t) ∝    (1− λ)N t,wdi ¬di +β Nt¬di +V β for s = 0 λ CatE(wdi |τ t ω>) for s = 1 (5) Notation: Due to the new models’ mixture architecture, we separate out the counts for each of the two components of each model.
56	35	We define the rank3 tensor Kt,wd as the number of times a word w in document d is generated from topic t by the latent feature component of the generative LF-LDA or LFDMM model.
57	19	We also extend the earlier definition of the tensor N t,wd as the number of times a word w in document d is generated from topic t by the Dirichlet multinomial component of our combined models, which in section 3.3 refers to the LF-LDA model, while in section 3.4 refers to the LF-DMM model.
60	23	For the LF-DMM model, we integrate out θ and φ, and then sample the topic zd and the distribution selection variables sd for document d using Gibbs sampling as outlined in Algorithm 2.
61	19	Algorithm 2: An approximate Gibbs sampling algorithm for the LF-DMM model Initialize the word-topic variables zdi using the DMM sampling algorithm for iteration iter = 1, 2, ... do for topic t = 1, 2, ..., T do τ t = arg maxτ t P(τ t | Z,S) for document d = 1, 2, ..., |D| do sample zd and sd from P(zd = t, sd | Z¬d,S¬d, τ ,ω) As before in Algorithm 1, we also use MAP estimation of τ as detailed in section 3.5 rather than sampling from the posterior.
62	26	The conditional distribution of topic variable and selection variables for document d is: P(zd = t, sd | Z¬d,S¬d, τ ,ω) ∝ λKd (1− λ)Nd (M t¬d + α) Γ(N t¬d + V β) Γ(N t¬d +Nd + V β) ∏ w∈W Γ(N t,w¬d +N w d + β) Γ(N t,w¬d + β) ∏ w∈W CatE(w | τ t ω>)K w d (6) Unfortunately the ratios of Gamma functions makes it difficult to integrate out sd in this distribution P. As zd and sd are not independent, it is computationally expensive to directly sample from this distribution, as there are 2(N w d +K w d ) different values of sd.
76	16	The document clustering and document classification tasks evaluate how useful the topics assigned to documents are in clustering and classification tasks.
130	24	Both sets of the pre-trained word vectors produce similar scores on the small and short Twitter dataset.
133	20	In table 5, topic 1 of the DMM model consists of words related to “nuclear crisis in Japan” together with other unrelated words.
141	22	We use two common metrics to evaluate clustering performance: Purity and normalized mutual information (NMI): see (Manning et al., 2008, Section 16.3) for details of these evaluations.
146	20	gain 2%+ absolute improvement13 on the two Purity and NMI against the baseline LDA model.
156	15	For the small value of T ≤ 7, on the large datasets of N20, TMN and TMNtitle, our models and baseline models obtain similar clustering results.
157	31	However, with higher values of T , our models perform better than the baselines on the short TMN and TMNtitle datasets, while on the N20 dataset, the baseline LDA model attains a slightly higher clustering results than ours.
167	21	Just as in the document clustering task, the mixture weight λ = 0.6 obtains the highest classification performances on the N20short dataset.
180	70	We found that using the latent feature word vectors produced significant performance improvements even when the domain of the topic-modeling corpus was quite different to that of the external corpus from which the word vectors were derived, as was the case in our experiments on Twitter data.
181	47	We found that using either the Google or the Stanford latent feature word vectors produced very similar results.
182	58	As far as we could tell, there is no reason to prefer either one of these in our topic modeling applications.
183	20	In this paper, we have shown that latent feature representations can be used to improve topic models.
186	113	In the topic coherence evaluation, our model outperformed the baseline models on all 6 experimental datasets, showing that our method for exploiting external information from very large corpora helps improve the topic-to-word mapping.
188	38	As an anonymous reviewer suggested, it would be interesting to identify exactly how the latent feature word vectors improve topic modeling performance.
189	54	We believe that they provide useful information about word meaning extracted from the large corpora that they are trained on, but as the reviewer suggested, it is possible that the performance improvements arise because the word vectors are trained on context windows of size 5 or 10, while the LDA and DMM models view documents as bags of words, and effectively use a context window that encompasses the entire document.
190	75	In preliminary experiments where we train latent feature word vectors from the topic-modeling corpus alone using context windows of size 10 we found that performance was degraded relative to the results presented here, suggesting that the use of a context window alone is not responsible for the performance improvements we reported here.
191	24	Clearly it would be valuable to investigate this further.
