0	101	Text summarization concerns the task of compressing a long sequence of text into a more concise form.
1	38	The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation.
4	21	∗ Work performed while at Salesforce Research.
5	25	A high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct.
6	26	Because it is difficult to gauge the correctness of the summary, evaluation metrics for summarization models use word overlap with the ground-truth summary in the form of ROUGE (Lin, 2004) scores.
7	62	However, word overlap metrics do not capture the abstractive nature of high quality human-written summaries: the use of paraphrases with words that do not necessarily appear in the source document.
9	16	We propose two general extensions to summarization models that improve the level of abstraction of the summary while preserving word overlap with the ground-truth summary.
11	15	The contextual network has the sole responsibility of extracting and compacting the source document whereas the language model is responsible for the generation of concise paraphrases.
14	62	We reward the policy with the ROUGE metric, which measures word overlap with the ground-truth summary, as well as a novel abstraction reward that encourages the generation of words not in the source document.
19	26	Our model obtains state-of-the-art ROUGE-L scores, and ROUGE-1 and ROUGE-2 performance comparable to state-of-the-art methods on the CNN/DailyMail dataset.
21	29	Table 1 shows a comparison of summaries generated by our model and previous abstractive models, showing less copying and more abstraction in our model.
22	50	The base model follows the encoder-decoder architecture with temporal attention and intraattention proposed by Paulus et al. (2017).
34	24	The policy learning loss is R̂ = R (ysam)−R (ygre) (14) Lpg = −E zsam ∼p(z), ysam ∼p(y|z) [R̂] (15) where we use greedy predictions by the model according to eq.
36	17	The policy gradient, as per Schulman et al. (2015), is ∇ΘLpg ≈ −R̂ m∑ t=1 ∇Θ log p (zsamt , ysamt ) (16) The final loss is a mixture between the maximum likelihood loss and the policy learning loss, weighted by a hyperparameter γ. L = (1− γ)Lml + γLpg (17)
37	62	The decoder is an essential component of the base model.
39	29	We decouple these two responsibilities by augmenting the decoder with an external language model.
42	35	The architecture of our language model is based on Merity et al. (2018).
43	15	We use a 3-layer unidirectional LSTM with weight-dropped LSTM units.
44	75	Let et denote the embedding of the word generated during time step t. The hidden state of the language model at the l-th layer is hlml,t = LSTM lm 3 ( et−1, h lm l,t−1 ) (18) At each time step t, we combine the hidden state of the last language model LSTM layer, hlm3,t, with rt defined in eq.
55	17	N (xgen, n) = ‖ng (xgen, n)− ng (xsrc, n)‖ ‖ng (xgen, n)‖ (23) To prevent the model for receiving high novelty rewards by outputting very short summaries, we normalize the metric by the length ratio of the generated and ground-truth summaries.
58	19	(15), alongside the original ROUGE-L metric.
59	17	In doing so, we encourage the model to generate summaries that both overlap with human written ground-truth summaries as well as incorporate novel words not in the source document: R (y) = λrouRrou (ysam) + λnovRnov (ysam) (25) where λrou and λnov are hyperparameters that control the weighting of each reward.
60	30	We train our model on the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016).
80	34	This baseline will intuitively have a higher percentage of novel n-grams than our base model outputs while being very similar to these original outputs, hence keeping the ROUGE score difference relatively small.
95	74	This indicates that the combination of the language model in the decoder and the novelty reward during training makes our model produce more novel unigrams while maintaining high ROUGE scores.
96	61	In order to understand the correlation between ROUGE and novel n-gram scores across different architectures, and to find the model type that gives the best trade-off between each of these metrics, we plot the ROUGE-1 and novel unigram scores for the five best iterations of each model type on the anonymized dataset, as well as the ROUGE-2 and novel bigram scores on a separate plot.
98	138	For each model type, we indicate the Pareto frontier by a line plot (Ben-Tal, 1980), illustrating which models of a given type give the best combination of ROUGE and novelty scores.
100	36	These plots show that there exist an inverse correlation between ROUGE and novelty scores in all model types, illustrating the challenge of choosing a model that performs well in both.
107	23	These results show that our model matches the relevance score of See et al. (2017) and Liu et al. (2018), but is slightly inferior to them in terms of readability.
134	28	Including mechanisms to promote paraphrase generation in the summary generator could be an interesting direction.
