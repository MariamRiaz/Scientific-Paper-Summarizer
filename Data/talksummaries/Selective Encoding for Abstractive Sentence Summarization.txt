3	12	We focus on abstractive sentence summarization task in this paper.
5	10	Rush et al. (2015) use autoconstructed sentence-headline pairs to train a neu- ∗Contribution during internship at Microsoft Research.
8	10	Chopra et al. (2016) extend their work by replacing the decoder with Recurrent Neural Network (RNN).
12	10	This approach achieves huge success in tasks like machine translation, where alignment between all parts of the input and output are required.
13	22	However, in abstractive sentence summarization, there is no explicit alignment relationship between the input sentence and the summary ex- 1095 cept for the extracted common words.
14	17	The challenge here is not to infer the alignment, but to select the highlights while filtering out secondary information in the input.
15	26	A desired work-flow for abstractive sentence summarization is encoding, selection, and decoding.
16	43	After selecting the important information from an encoded sentence, the decoder produces the output summary using the selected information.
56	65	As shown in Figure 2, our model consists of a sentence encoder using the Gated Recurrent Unit (GRU) (Cho et al., 2014), a selective gate network and an attention-equipped GRU decoder.
59	17	Then the selective gate selects and filters the word representations according to the sentence meaning representation to produce a tailored sentence word representation for abstractive sentence summarization task.
61	20	In the following sections, we introduce the sentence encoder, the selective mechanism, and the summary decoder respectively.
78	30	For each word xi, the selective gate network generates a gate vector sGatei using hi and s, then the tailored representation is constructed, i.e., h′i.
84	76	The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state st with each encoder hidden state h′i to get an importance score.
87	191	rt = Wrwt−1 +Urct +Vrst mt = [max{rt,2j−1, rt,2j}]>j=1,...,d p(yt|y1, .
94	17	Training Set For our training set, we use a parallel corpus which is constructed from the Annotated English Gigaword dataset (Napoles et al., 2012) as mentioned in Rush et al. (2015).
95	19	The parallel corpus is produced by pairing the first sentence and the headline in the news article with some heuristic rules.
126	14	Beam Search We use beam search to generate multiple summary candidates to get better results.
131	16	ABS+ Based on ABS model, Rush et al. (2015) further tune their model using DUC 2003 dataset, which leads to improvements on DUC 2004 test set.
134	22	Luong-NMT Neural machine translation model of Luong et al. (2015) with two-layer LSTMs for the encoder-decoder with 500 hidden units in each layer implemented in (Chopra et al., 2016).
135	117	s2s+att We also implement a sequence-tosequence model with attention as our baseline and denote it as “s2s+att”.
141	48	(2015), -m -n 2 -w 1.2 6The ROUGE evaluation option is, -m -n 2 -w 1.2 7The ROUGE evaluation option is, -m -b 75 -n 2 -w 1.2 English Gigaword We acquire the test set from Rush et al. (2015) so we can make fair comparisons to the baselines.
147	41	Compared to the highest CAs2s baseline, our model achieves 1.57 ROUGE-2 F1 improvement and passes the significant test according to the official ROUGE script.
150	21	DUC 2004 We evaluate our model using the ROUGE recall score since the reference summaries of the DUC 2004 test set are capped at 75 bytes.
154	23	MSR-ATC We report the full length ROUGE F1 score on the MSR-ATC test set in Table 6.
158	41	In this section, we first compare the performance of SEASS with the s2s+att baseline model to illustrate that the proposed method succeeds in selecting information and building tailored representation for abstractive sentence summarization.
166	32	Saliency Heat Map of Selective Gate Since the output of the selective gate network is a high dimensional vector, it is hard to visualize all the gate values.
168	21	Given sentence words x with associated output summary y, the trained model associates the pair (x, y) with a score Sy(x).
170	12	We approximate the Sy(g) by computing the first-order Taylor expansion since the score Sy(x) is a highly non-linear function in the deep neural network models: Sy(g) ≈ w(g)T g + b (19) where w(g) is first the derivative of Sy with respect to the gate g: w(g) = ∂(Sy) ∂g |g (20) We then draw the Euclidean norm of the first derivative of the output y with respect to the selective gate g associated with each input words.
171	24	Figure 3 shows an example of the first derivative heat map, in which most of the important words are selected by the selective gate such as “europe”, “slammed”, “unacceptable”, “conditions”, and “france”.
176	42	Experimental results show that the selective encoding model greatly improves the performance with respect to the state-of-theart methods on English Gigaword, DUC 2004 and MSR-ATC test sets.
