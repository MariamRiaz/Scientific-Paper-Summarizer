12	9	Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark.
14	48	Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work.
15	39	Task definition We formulate the coreference resolution task as a set of antecedent assignments yi for each of span i in the given document, following Lee et al. (2017).
18	27	Non-dummy antecedents represent coreference links between i and yi.
21	34	Baseline We describe the baseline model (Lee et al., 2017), which we will improve to address the modeling and computational limitations discussed previously.
22	33	The goal is to learn a distribution P (yi) over antecedents for each span i : P (yi) = es(i,yi)∑ y′∈Y(i) e s(i,y′) (1) where s(i, j) is a pairwise score for a coreference link between span i and span j.
23	31	The baseline model includes three factors for this pairwise coreference score: (1) sm(i), whether span i is a mention, (2) sm(j), whether span j is a mention, and (3) sa(i, j) whether j is an antecedent of i: s(i, j) = sm(i) + sm(j) + sa(i, j) (2) In the special case of the dummy antecedent, the score s(i, ) is instead fixed to 0.
24	14	A common component used throughout the model is the vector representations gi for each possible span i.
28	13	A beam of up to M potential mentions is computed (whereM is proportional to the document length) based on the spans with the highest mention scores sm(i).
30	13	Given supervision of gold coreference clusters, the model is learned by optimizing the marginal log-likelihood of the possibly correct antecedents.
33	7	First-order models are susceptible to consistency errors as demonstrated in Figure 1.
38	26	The refined span representations allow the model to also iteratively refine the antecedent distributions Pn(yi): Pn(yi) = es(g n i ,g n yi ) ∑ y∈Y(i) e s(gni ,g n y )) (5) where s is the coreference scoring function of the baseline architecture.
40	125	At each iteration, we first compute the expected antecedent representation ani of each span i by using the current antecedent distribution Pn(yi) as an attention mechanism: ani = ∑ yi∈Y(i) Pn(yi) · gnyi (6) The current span representation gni is then updated via interpolation with its expected antecedent representation ani : fni = σ(Wf[g n i ,a n i ]) (7) gn+1i = f n i ◦ gni + (1− fni ) ◦ ani (8) The learned gate vector fni determines for each dimension whether to keep the current span information or to integrate new information from its expected antecedent.
41	43	At iteration n, gni is an element-wise weighted average of approximately n span representations (assuming Pn(yi) is peaked), allowing Pn(yi) to softly condition on up to n other spans in the predicted cluster.
43	57	By iteratively refining the span representations and antecedent distributions, another way to interpret this model is that the joint distribution∏ i PN (yi) implicitly models every directed path of up to length N +1 in the latent antecedent tree.
44	6	The model described above scales poorly to long documents.
45	54	Despite heavy pruning of potential mentions, the space of possible antecedents for every surviving span is still too large to fully consider.
46	59	The bottleneck is in the antecedent score sa(i, j), which requires computing a tensor of size M ×M × (3|g|+ |φ|).
55	7	However, sc(i, j) is much more efficient to compute.
57	20	Therefore, we instead propose to use sc(i, j) to compute a rough sketch of likely antecedents.
59	22	The final inference procedure involves a three-stage beam search: First stage Keep the top M spans based on the mention score sm(i) of each span.
74	50	Therefore, our final model considers only 50 antecedents per span.
78	86	The main evaluation is the average F1 of the three metrics.
81	10	The baseline relative to our contributions is the span-ranking model from Lee et al. (2017) augmented with both ELMo and hyperparameter tuning, which achieves 72.3 F1.
83	10	Compared to the heuristic pruning with up to 250 antecedents, our coarse-to-fine model only computes the expensive scores sa(i, j) for 50 antecedents.
85	9	As a result, we observe a much higher recall when adopting the coarse-to-fine approach.
87	15	The improvement is largely driven by the overall increase in precision, which is expected since the higher-order inference mainly serves to rule out inconsistent clusters.
88	45	It is also consistent with findings from Martschat and Strube (2015) who report mainly improvements in precision when modeling latent trees to achieve a similar goal.
94	21	We presented a state-of-the-art coreference resolution system that models higher order interactions between spans in predicted clusters.
95	19	Additionally, our proposed coarse-to-fine approach alleviates the additional computational cost of higherorder inference, while maintaining the end-to-end learnability of the entire model.
