{"sections": [{"text": "ar X\niv :1\n80 2.\n06 09\n3v 4\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "Residual networks (He et al., 2016) are deep neural networks in which, roughly, subnetworks determine how a feature transformation should differ from the identity, rather than how it should differ from zero. After enabling the winning entry in the ILSVRC 2015 classification task, they have become established as a central idea in deep networks.\nHardt & Ma (2017) provided a theoretical analysis that shed light on residual networks. They showed that (a) any linear transformation with a positive determinant and a bounded condition number can be approximated by a \u201cdeep linear network\u201d of the form f(x) = \u0398L\u0398L\u22121...\u03981x, where,\nfor large L, each layer \u0398i is close to the identity, and (b) for networks that compose near-identity transformations this way, if the excess loss is large, then the gradient is steep. Bartlett et al. (2018) extended both results to the nonlinear case, showing that any smooth, bi-Lipschitz map can be represented as a composition of near-identity functions, and that a suboptimal loss in a composition of near-identity functions implies that the functional gradient of the loss with respect to a function in the composition cannot be small. These results are interesting because they suggest that, in many cases, this non-convex objective may be efficiently optimized through gradient descent if the layers stay close to the identity, possibly with the help of a regularizer.\nThis paper describes and analyzes such algorithms for linear regression with d input variables and d response variables with respect to the quadratic loss, the same setting analyzed by Hardt and Ma. We abstract away sampling issues by analyzing an algorithm that performs gradient descent with respect to the population loss. We focus on the case that the distribution on the input patterns is isotropic. (The data may be transformed through a preprocessing step to satisfy this constraint.)\nThe traditional analysis of convex optimization algorithms (see Boyd & Vandenberghe, 2004) provides a bound in terms of the quality of the initial solution, together with bounds on the eigenvalues of the Hessian of the loss. For the non-convex problem of this paper, we show that if gradient descent starts at the identity in each layer, and if the excess loss of that initial solution is bounded by a constant, then the Hessian remains well-conditioned enough throughout training for successful learning. Specifically, there is a constant c0 such that, if the excess loss of the identity (over the least squares linear map) is at most c0, then back-propagation initialized at the identity in each layer achieves loss within at most \u01eb of optimal in time polynomial in log(1/\u01eb), d, and L (Section 3). On the other hand, we show that there is a constant c1 and a least squares matrix \u03a6 such that the identity has excess loss c1 with respect to \u03a6, but backpropagation with identity initialization fails to learn \u03a6 (Section 6).\nWe also show that if the least squares matrix \u03a6 is symmetric positive definite then gradient descent with identity initialization achieves excess loss at most \u01eb in a number of steps bounded by a polynomial in log(d/\u01eb), L and the condition number of \u03a6 (Section 4).\nIn contrast, for any least squares matrix \u03a6 that is symmetric but has a negative eigenvalue, we show that no such guarantee is possible for a wide variety of algorithms of this type: the excess loss is forever bounded below by the square of this negative eigenvalue. This holds for step-and-project algorithms, and also algorithms that initialize to the identity and regularize by early stopping or penalizing \u2211\ni ||\u0398i \u2212 I|| 2 F (Section 6). Both this and the previous impossibility result can be\nproved using a least squares matrix \u03a6 with a positive determinant and a good condition number. Recall that such \u03a6 were proved by Hardt and Ma to have a good approximation as a product of near-identity matrices \u2013 we prove that gradient descent cannot learn them, even with the help of regularizers that reward near-identity representations.\nIn Section 5 we provide a convergence guarantee for a least squares matrix \u03a6 that may not be symmetric, but satisfies the positivity condition u\u22a4\u03a6u > \u03b3 for some \u03b3 > 0 that appears in the bounds. We call such matrices \u03b3-positive. Such \u03a6 include rotations by acute angles. In this case, we consider an algorithm that regularizes in addition to a near-identity initialization. After the gradient update, the algorithm performs what we call power projection, projecting its hypothesis \u0398L\u0398L\u22121...\u03981 onto the set of \u03b3-positive matrices. Second, it \u201cbalances\u201d \u03981, ...,\u0398L so that, informally, they contribute equally to \u0398L\u0398L\u22121...\u03981. (See Section 5 for the details.) We view this\nregularizer as a theoretically tractable proxy for regularizers that promote positivity and balance between layers by adding penalties.\nWhile, in practice, deep networks are non-linear, analysis of the linear case can provide a tractable way to gain insight through rigorous theoretical analysis (Saxe et al., 2013; Kawaguchi, 2016; Hardt & Ma, 2017). We might view back-propagation in the non-linear case as an approximation to a procedure that locally modifies the function computed by each layer in a manner that reduces the loss as fast as possible. If a non-linear network is obtained by composing transformations, each of which is chosen from a Hilbert space of functions (as in Daniely et al. (2016)), then a step in \u201cfunction space\u201d corresponds to a step in an (infinite-dimensional) linear space of functions.\nRelated work. The motivation for this work comes from the papers of Hardt & Ma (2017) and Bartlett et al. (2018). Saxe et al. (2013) studied the dynamics of a continuous-time process obtained by taking the step size of backpropagation applied to deep linear neural networks to zero. Kawaguchi (2016) showed that deep linear neural networks have no suboptimal local minima. In the case that L = 2, the problem studied here has a similar structure as problems arising from low-rank approximation of matrices, especially as regards algorithms that approximate a matrix A by iteratively improving an approximation of the form UV . For an interesting survey on the rich literature on these algorithms, please see Ge et al. (2017a); successful algorithms have included a regularizer that promotes balance in the sizes of U and V . Taghvaei et al. (2017) studied the properties of critical points on the loss when learning deep linear neural networks in the presence of a weight decay regularizer; they studied networks that transform the input to the output through a process indexed by a continuous variable, instead of through discrete layers. Lee et al. (2016) showed that, given regularity conditions, for a random initialization, gradient descent converges to a local minimizer almost surely; while their paper yields useful insights, their regularity condition does not hold for our problem. Many papers have analyzed learning of neural networks with non-linearities. The papers most closely related to this work analyze algorithms based on gradient descent. Some of these (Andoni et al., 2014; Brutzkus & Globerson, 2017; Ge et al., 2017b; Li & Yuan, 2017; Zhong et al., 2017; Zhang et al., 2018; Brutzkus et al., 2018; Ge et al., 2018) analyze constant-depth networks. Daniely (2017) showed that stochastic gradient descent learns a subclass of functions computed by log-depth networks in polynomial time; this class includes constant-degree polynomials with polynomially bounded coefficients. Other theoretical treatments of neural network learning algorithms include Lee et al. (1996); Arora et al. (2014); Livni et al. (2014); Janzamin et al. (2015); Safran & Shamir (2016); Zhang et al. (2016); Nguyen & Hein (2017); Zhang et al. (2017); Orhan & Pitkow (2018), although these are less closely related.\nOur three upper bound analyses combine a new upper bound on the operator norm of the Hessian of a deep linear network with the result of Hardt and Ma that gradients are lower bounded in terms of the loss for near-identity matrices. They otherwise have different outlines. The bound in terms of the loss of the initial solution proceeds by showing that the distance from each layer to the identity grows slowly enough that the loss is reduced before the layers stray far enough to harm the conditioning of the Hessian. The bound for symmetric positive definite matrices proceeds by showing that, in this case, all of the layers are the same, and each of their eigenvalues converges to the Lth root of a corresponding eigenvalue of \u03a6. As mentioned above, the bound for \u03b3-positive matrices \u03a6 is for an algorithm that achieves favorable conditioning through regularization.\nWe expect that the theoretical analysis reported here will inform the design of practical algorithms\nfor learning non-linear deep networks. One potential avenue for this arises from the fact that the leverage provided by regularizing toward the identity appears to already be provided by a weaker policy of promoting the property that the composition of layers is (potentially asymmetric) positive definite. Also, balancing singular values of the layers of the network aided our analysis; an analogous balancing of Jacobians associated with various layers may improve conditioning in practice in the non-linear case."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Setting", "text": "For a joint distribution P with support contained in \u211cd \u00d7 \u211cd and g : \u211cd \u2192 \u211cd, define \u2113P (g) = E(X,Y )\u223cP (||g(X) \u2212 Y || 2/2). We focus on the case that, for (X,Y ) drawn from P , the marginal on X is isotropic, with EXX\u22a4 = Id. For convenience, we assume that Y = \u03a6X for \u03a6 \u2208 \u211cd\u00d7d. This assumption is without loss of generality: if \u03a6 is the least squares matrix (so that f defined by f(X) = \u03a6X minimizes \u2113P (f) among linear functions), for any linear g we have\n\u2113P (g) = E\u2016g(X) \u2212 f(X)\u2016 2/2 + E\u2016f(X)\u2212 Y \u20162/2\n+ E ((g(X) \u2212 f(X))(f(X) \u2212 Y ))\n= E\u2016g(X) \u2212 f(X)\u20162/2 + E\u2016f(X)\u2212 Y \u20162/2\n= E\u2016g(X) \u2212 \u03a6X)\u20162/2 + E\u2016\u03a6X \u2212 Y \u20162/2,\nsince f is the projection of Y onto the set of linear functions ofX. So assuming Y = \u03a6X corresponds to setting \u03a6 as the least squares matrix and replacing the loss \u2113P (g) by the excess loss\nE\u2016g(X) \u2212 \u03a6X\u20162/2 = E\u2016g(X) \u2212 Y \u20162/2\u2212 E\u2016\u03a6X \u2212 Y \u20162/2.\nWe study algorithms that learn linear mappings parameterized by deep networks. The network with L layers and parameters \u0398 = (\u03981, . . . ,\u0398L) computes the parameterized function f\u0398(x) = \u0398L\u0398L\u22121 \u00b7 \u00b7 \u00b7\u03981x, where x \u2208 \u211cd and \u0398i \u2208 \u211cd\u00d7d.\nWe use the notation \u0398i:j = \u0398j\u0398j\u22121 \u00b7 \u00b7 \u00b7\u0398i for i \u2264 j, so that we can write f\u0398(x) = \u03981:Lx = \u0398i+1:L\u0398i\u03981:i\u22121x.\nWhen there is no possibility of confusion, we will sometimes refer to loss \u2113(f\u0398) simply as \u2113(\u0398). Because the distribution of X is isotropic, \u2113(\u0398) = 12 ||\u03981:L \u2212 \u03a6|| 2 F with respect to least squares matrix \u03a6. When \u0398 is produced by an iterative algorithm, will we also refer to loss of the tth iterate by \u2113(t).\nDefinition 1. For \u03b3 > 0, a matrix A \u2208 \u211cd\u00d7d is \u03b3-positive if, for all unit length u, we have u\u22a4Au > \u03b3."}, {"heading": "2.2 Tools and background", "text": "We use ||A||F for the Frobenius norm of matrix A, ||A||2 for its operator norm, and \u03c3min(A) for its least singular value. For vector v, we use ||v|| for its Euclidian norm.\nFor a matrix A and a matrix-valued function B, define DAB(A) to be the matrix with\n(DAB(A))i,j = \u2202vec(B(A))i \u2202vec(A)j ,\nwhere vec(A) is the column vector constructed by stacking the columns of A. We use Td,d to denote the d2 \u00d7 d2 permutation matrix mapping vec(A) to vec(A\u22a4) for A \u2208 \u211cd\u00d7d. For A \u2208 \u211cn\u00d7m and B \u2208 \u211cp\u00d7q, A\u2297B denotes the Kronecker product, that is, the np\u00d7mq matrix of n\u00d7m blocks, with the i, jth block given by AijB.\nWe will need the gradient and Hessian of \u2113. (The gradient, which can be computed using backprop, is of course well known.) The proof is in Appendix A.\nLemma 1.\nD\u0398i\u2113 (f\u0398)=(vec(Id)) \u22a4 (( \u0398\u22a41:i\u22121 \u2297 (\u03981:L\u2212\u03a6) \u22a4\u0398i+1:L ))\n= vec(G)\u22a4,\nwhere G is the d\u00d7 d matrix given by\nG def = \u0398\u22a4i+1:L (\u03981:L \u2212 \u03a6)\u0398 \u22a4 1:i\u22121. (1)\nFor i < j,\nD\u0398jD\u0398i\u2113 (f\u0398) = (Id2 \u2297 (vec(Id)) \u22a4) (Id \u2297 Td,d \u2297 Id)\n( vec(\u0398\u22a41:i\u22121)\u2297 Id2 )\n((\u0398\u22a4i+1:L\u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121)Td,d + (\u0398 \u22a4 i+1:j\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398j+1:L)).\nD\u0398iD\u0398i\u2113 (f\u0398) = (Id2 \u2297 (vec(Id)) \u22a4) (Id \u2297 Td,d \u2297 Id)\n( vec(\u0398\u22a41:i\u22121)\u2297 Id2 )\n(\n\u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121\n)\nTd,d."}, {"heading": "3 Targets near the identity", "text": "In this section, we prove an upper bound for gradient descent in terms of the loss of the initial solution."}, {"heading": "3.1 Procedure and upper bound", "text": "First, set \u0398(0) = (I, I, ..., I), and then iteratively update\n\u0398 (t+1) i = \u0398 (t) i \u2212 \u03b7(\u0398 (t) i+1:L)\n\u22a4 (\n\u0398 (t) 1:L \u2212 \u03a6\n)\n(\u0398 (t) 1:i\u22121) \u22a4.\nTheorem 1. There are positive constants c1 and c2 and polynomials p1 and p2 such that, if \u2113(\u0398 (0) 1:L) \u2264 c1, L \u2265 c2, and \u03b7 \u2264 1 p1(L,d,||\u03a6||2) , then the above gradient descent procedure achieves\n\u2113(f\u0398(t)) \u2264 \u01eb within t = p2\n(\n1 \u03b7\n) ln (\n\u2113(0) \u01eb\n)\niterations."}, {"heading": "3.2 Proof of Theorem 1", "text": "The following lemma, which is implicit in the proof of Theorem 2.2 in Hardt & Ma (2017), shows that the gradient is steep if the loss is large and the singular values of the layers are not too small.\nLemma 2 (Hardt & Ma 2017). Let \u2207\u0398\u2113(\u0398) be the gradient of \u2113(\u0398) with respect to any flattening of \u0398. If, for all layers i, \u03c3min(\u0398i) \u2265 1\u2212 a, then ||\u2207\u0398\u2113(\u0398)|| 2 \u2265 4\u2113(\u0398)L(1\u2212 a)2L.\nNext, we show that, if \u0398(t) and \u0398(t+1) are both close to the identity, then the gradient is not changing very fast between them, so that rapid progress continues to be made. We prove this through an upper bound on the operator norm of the Hessian that holds uniformly over members of a ball around the identity, which in turn can be obtained through a bound on the Frobenius norm. The proof is in Appendix B.\nLemma 3. Choose an arbitrary \u0398 with ||\u0398i||2 \u2264 1 + z for all i, and least squares matrix \u03a6 with ||\u03a6||2 \u2264 (1 + z)\nL. Let \u22072 be the Hessian of \u2113(f\u0398) with respect to an arbitrary flattening of the parameters of \u0398. We have\n||\u22072||F \u2264 3Ld 5(1 + z)2L.\nArmed with Lemmas 2 and 3, let us now analyze gradient descent. Very roughly, our strategy will be to show that the distance from the identity to the various layers grows slowly enough for the leverage from Lemmas 2 and 3 to enable successful learning. Let R(\u0398) = maxi ||\u0398i \u2212 I||2. From the update, we have\n||\u0398 (t+1) i \u2212 I||2 \u2264 ||\u0398 (t) i \u2212 I||2 + \u03b7||(\u0398 (t) i+1:L)\n\u22a4 (\n\u0398 (t) 1:L \u2212 \u03a6\n)\n(\u0398 (t) 1:i\u22121) \u22a4||2\n\u2264 ||\u0398 (t) i \u2212 I||2 + \u03b7(1 +R(\u0398 (t)))L||\u0398 (t) 1:L \u2212 \u03a6||2 \u2264 ||\u0398 (t) i \u2212 I||2 + \u03b7(1 +R(\u0398 (t)))L||\u0398 (t) 1:L \u2212 \u03a6||F .\nIf R(t) = maxs\u2264tR(\u0398(s)) (so R(0) = 0) and \u2113(t) = 12 ||\u0398 (t) 1:L \u2212 \u03a6|| 2 F , this implies\nR(t+ 1) \u2264 R(t) + \u03b7(1 +R(t))L \u221a 2\u2113(t). (2)\nBy Lemma 3, for all \u0398 on the line segment from \u0398(t) to \u0398(t+1), we have\n||\u22072\u0398||2 \u2264 ||\u2207 2 \u0398||F \u2264 3Ld 5 max{(1 +R(t+ 1))2L, ||\u03a6||22},\nso that\n\u2113(t+ 1) \u2264 \u2113(t)\u2212 \u03b7||\u2207\u0398(t) || 2 +\n3 2 \u03b72Ld5 max{(1 +R(t+ 1))2L, ||\u03a6||22}||\u2207\u0398(t) || 2.\nThus, if we ensure\n\u03b7 \u2264 1\n3Ld5 max{(1 +R(t+ 1))2L, ||\u03a6||22} , (3)\nwe have \u2113(t+ 1) \u2264 \u2113(t)\u2212 (\u03b7/2)||\u2207\u0398(t) || 2, which, using Lemma 2, gives\n\u2113(t+ 1) \u2264 ( 1\u2212 2\u03b7L(1 \u2212R(t))2L ) \u2113(t). (4)\nPick any c \u2265 1. Assume that L \u2265 (4/3) ln c = c2, \u2113(\u0398 (0) 1:L) \u2264\nln(c)2\n8c10 = c1 and \u03b7 \u2264 1\n3Ld5 max{c4,||\u03a6||22} .\nWe claim that, for all t \u2265 0,\n1. R(t) \u2264 \u03b7c \u221a 2\u2113(0) \u2211 0\u2264s<t exp ( \u2212 s\u03b7Lc4 )\n2. \u2113(t) \u2264 ( exp (\n\u22122t\u03b7L c4\n))\n\u2113(0).\nThe base case holds as R(0) = 0 and \u2113(0) = \u2113(0).\nBefore starting the inductive step, notice that for any t \u2265 0,\n\u03b7c \u221a 2\u2113(0) \u2211\n0\u2264s<t exp\n(\n\u2212 s\u03b7L\nc4\n)\n\u2264 \u03b7c \u221a 2\u2113(0)\u00d7 1\n1\u2212 exp (\n\u2212\u03b7L c4\n)\n\u2264 \u03b7c \u221a 2\u2113(0)\u00d7 2c4\n\u03b7L (since \u03b7Lc4 \u2264 1)\n= 2c5 \u221a 2\u2113(0)\nL \u2264\nln c\nL \u2264 3/4\nwhere the last two inequalities follow from the constraints on \u2113(0) and L.\nUsing (2),\nR(t+ 1) \u2264 R(t) + \u03b7(1 +R(t))L \u221a 2\u2113(t)\n\u2264 R(t) + \u03b7\n(\n1 + ln c\nL\n)L \u221a\n2\u2113(t)\n\u2264 R(t) + \u03b7c \u221a 2\u2113(t)\n\u2264 R(t) + \u03b7c \u221a 2\u2113(0) exp\n(\n\u2212 t\u03b7L\nc4\n)\n\u2264 \u03b7c \u221a 2\u2113(0) \u2211\n0\u2264s<t+1 exp\n(\n\u2212 s\u03b7L\nc4\n)\n.\nSince R(t+ 1) \u2264 ln cL , the choice of \u03b7 satisfies (3), so\n\u2113(t+ 1) \u2264 ( 1\u2212 2\u03b7L(1 \u2212R(t))2L ) \u2113(t).\nNow consider (1\u2212R(t))2L:\nln ( (1\u2212R(t))2L ) = 2L ln(1\u2212R(t))\n\u2265 2L(\u22122R(t)) since R(t) \u2208 [0, 3/4]\n\u2265 2L\n(\n\u22122 ln c\nL\n)\nsince R(t) \u2264 ln c\nL\n(1\u2212R(t))2L \u2265 1/c4.\nUsing this in the bound on \u2113(t+ 1):\n\u2113(t+ 1) \u2264 ( 1\u2212 2\u03b7L(1 \u2212R(t))2L ) \u2113(t)\n\u2264\n(\n1\u2212 2\u03b7L\nc4\n)\n\u2113(t)\n\u2264\n(\nexp\n(\n\u2212 2\u03b7L\nc4\n))(\nexp\n(\n\u2212 2t\u03b7L\nc4\n))\n\u2113(0)\n=\n(\nexp\n(\n\u2212 2(t+ 1)\u03b7L\nc4\n))\n\u2113(0).\nSolving \u2113(0) exp (\n\u22122t\u03b7L c4\n)\n\u2264 \u01eb for t and recalling that \u03b7 < 1/c4 completes the proof of the theorem."}, {"heading": "4 Symmetric positive definite targets", "text": "In this section, we analyze the procedure of Section 3.1 when the least squares matrix \u03a6 is symmetric and positive definite.\nTheorem 2. There is an absolute positive constant c3 such that, if \u03a6 is symmetric and \u03b3-positive with 0 < \u03b3 < 1, and L \u2265 c3 ln (||\u03a6||2/\u03b3), then for all \u03b7 \u2264\n1 L(1+||\u03a6||22) , gradient descent achieves\n\u2113(f\u0398(t)) \u2264 \u01eb in poly(L, ||\u03a6||2/\u03b3, 1/\u03b7) log(d/\u01eb) iterations.\nNote that a symmetric matrix is \u03b3-positive when its minimum eigenvalue is at least \u03b3."}, {"heading": "4.1 Proof of Theorem 2", "text": "Let \u03a6 be a symmetric, real, \u03b3-positive matrix with \u03b3 > 0, and let \u0398(0),\u0398(1), ... be the iterates of gradient descent with a step size 0 < \u03b7 \u2264 1\nL(1+||\u03a6||22) .\nDefinition 2. Symmetric matrices A \u2286 \u211cd\u00d7d are commuting normal matrices if there is a single unitary matrix U such that for all A \u2208 A, U\u22a4AU is diagonal.\nWe will use the following well-known facts about commuting normal matrices.\nLemma 4 (Horn & Johnson 2013). If A \u2286 \u211cd\u00d7d is a set of symmetric commuting normal matrices and A,B \u2208 A, the following hold:\n\u2022 AB = BA;\n\u2022 for all scalars \u03b1 and \u03b2, A \u222a {\u03b1A+ \u03b2B,AB} are commuting normal;\n\u2022 there is a unitary matrix U such that U\u22a4AU and U\u22a4BU are real and diagonal;\n\u2022 the multiset of singular values of A is the same as the multiset of magnitudes of its eigenvalues;\n\u2022 ||A\u2212 I||2 is the largest value of |z \u2212 1| for an eigenvalue z of A.\nLemma 5. The matrices {\u03a6} \u222a {\u0398 (t) i : i \u2208 {1, ..., L}, t \u2208 Z +} are commuting normal. For all t, \u0398 (t) 1 = ... = \u0398 (t) L .\nProof. The proof is by induction. The base case follows from the fact that \u03a6 and I are commuting normal.\nFor the induction step, the fact that\n{\u03a6} \u222a {\n\u0398 (s) i : i \u2208 {1, ..., L}, s \u2264 t\n} \u222a {\n\u0398 (s+1) i : i \u2208 {1, ..., L}, s \u2264 t\n}\nare commuting normal follows from Lemma 4. The update formula now reveals that \u0398 (t+1) 1 = ... = \u0398 (t+1) L .\nNow we are ready to analyze the dynamics of the learning process. Let \u03a6 = U\u22a4DLU be a diagonalization of \u03a6. Let \u0393 = max{1, ||\u03a6||2}. We next describe a sense in which gradient descent learns each eigenvalue independently.\nLemma 6. For each t, there is a real diagonal matrix D\u0302(t) such that, for all i, \u0398 (t) i = U \u22a4D\u0302(t)U and\nD\u0302(t+1) = D\u0302(t) \u2212 \u03b7(D\u0302(t))L\u22121((D\u0302(t))L \u2212DL). (5)\nProof. Lemma 5 implies that there is a single real U such that \u0398 (t) i = U \u22a4D\u0302(t)U for all i. Applying Lemma 1, recalling that \u0398 (t) 1 = ... = \u0398 (t) L , and applying the fact that \u0398 (t) i and \u03a6 commute, we get\n\u0398 (t+1) i = \u0398 (t) i \u2212 \u03b7(\u0398 (t) i )\nL\u22121 (\n(\u0398 (t) i )\nL \u2212 \u03a6 ) .\nReplacing each matrix by its diagonalization, we get\nU\u22a4D\u0302(t+1)U = U\u22a4D\u0302(t)U \u2212 \u03b7(U\u22a4(D\u0302(t))L\u22121U) ( U\u22a4(D\u0302(t))LU \u2212 U\u22a4DLU )\n= U\u22a4D\u0302(t)U \u2212 \u03b7U\u22a4(D\u0302(t))L\u22121 ( (D\u0302(t))L \u2212DL ) U,\nand left-multiplying by U and right-multiplying by U\u22a4 gives (5).\nWe will now analyze the convergence of each D\u0302 (t) kk to Dkk separately. Let us focus for now on an arbitrary single index k, let \u03bb = Dkk and \u03bb\u0302 (t) = D\u0302 (t) kk .\nRecalling that ||\u03a6||2 \u2264 \u0393, we have \u03b3 1/L \u2264 \u03bb \u2264 \u03931/L. Also, \u03931/L = e 1 L ln \u0393 \u2264 e1/a \u2264 1+2/a whenever a \u2265 1 and L \u2265 a ln \u0393. Similarly, \u03b31/L \u2265 1 \u2212 a whenever L \u2265 a ln(1/\u03b3). Thus, there are absolute constants c3 and c4 such that |1\u2212 \u03bb| \u2264 c4 ln(\u0393/\u03b3)\nL < 1 for all L \u2265 c3 ln(\u0393/\u03b3).\nWe claim that, for all t, \u03bb\u0302(t) lies between 1 and \u03bb inclusive, so that |\u03bb\u0302(t) \u2212 \u03bb| \u2264 c4 ln(\u0393/\u03b3)L . The base case holds because \u03bb\u0302(t) = 1 and |1 \u2212 \u03bb| \u2264 c4 ln(\u0393/\u03b3)L . Now let us work on the induction step. Applying (5) together with Lemma 1, we get\n\u03bb\u0302(t+1) = \u03bb\u0302(t) + \u03b7(\u03bb\u0302(t))L\u22121(\u03bbL \u2212 (\u03bb\u0302(t))L). (6)\nBy the induction hypothesis, we just need to show that sign(\u03bb\u0302(t+1) \u2212 \u03bb\u0302(t)) = sign(\u03bb \u2212 \u03bb\u0302(t)) and |\u03bb\u0302(t+1) \u2212 \u03bb\u0302(t)| \u2264 |\u03bb \u2212 \u03bb\u0302(t)| (i.e., the step is in the correct direction, and does not \u201covershoot\u201d). First, to see that the step is in the right direction, note that \u03bbL \u2265 (\u03bb\u0302(t))L if and only if \u03bb \u2265 (\u03bb\u0302(t)), and the inductive hypothesis implies that \u03bb\u0302(t), and therefore (\u03bb\u0302(t))L\u22121, is non-negative. To show that |\u03bb\u0302(t+1) \u2212 \u03bb\u0302(t)| \u2264 |\u03bb \u2212 \u03bb\u0302(t)|, it suffices to show that \u03b7(\u03bb\u0302(t))L\u22121 \u2223 \u2223 \u2223 \u03bbL \u2212 (\u03bb\u0302(t))L) \u2223 \u2223 \u2223 \u2264 |\u03bb \u2212 \u03bb\u0302(t)|,\nwhich, in turn would be implied by \u03b7 \u2264\n\u2223 \u2223 \u2223 \u2223 1 (\u03bb\u0302(t))L\u22121( \u2211L\u22121 i=0 (\u03bb\u0302 (t))i\u03bbL\u22121\u2212i) \u2223 \u2223 \u2223 \u2223 (since \u03bbL \u2212 (\u03bb\u0302(t))L = (\u03bb \u2212\n\u03bb\u0302(t)) \u2211L\u22121 i=0 (\u03bb\u0302 (t))i\u03bbL\u22121\u2212i), which follows from the inductive hypothesis and \u03b7 \u2264 1 L\u03932 . We have proved that each \u03bb\u0302(t) lies between \u03bb and 1, so that |1\u2212 \u03bb\u0302(t)| \u2264 |1\u2212 \u03bb| \u2264 c4 ln(\u0393/\u03b3).\nNow, since the step is in the right direction, and does not overshoot,\n|\u03bb\u0302(t+1) \u2212 \u03bb| \u2264 |\u03bb\u0302(t) \u2212 \u03bb| \u2212 \u03b7(\u03bb\u0302(t))L\u22121|\u03bbL \u2212 (\u03bb\u0302(t))L|\n\u2264 |\u03bb\u0302(t) \u2212 \u03bb|\n( 1\u2212 \u03b7(\u03bb\u0302(t))L\u22121 ( L\u22121 \u2211\ni=0\n(\u03bb\u0302(t))i\u03bbL\u22121\u2212i ))\n\u2264 |\u03bb\u0302(t) \u2212 \u03bb| ( 1\u2212 \u03b7L\u03b32 ) ,\nsince the fact that \u03bb\u0302(t) lies between 1 and \u03bb implies that \u03bb\u0302(t) \u2265 \u03b31/L. Thus, |\u03bb\u0302(t) \u2212 \u03bb| \u2264 (\n1\u2212 \u03b7L\u03b32 )t c4 ln(\u0393/\u03b3). This implies that, for any \u01eb \u2208 (0, 1), for any absolute constant c5, there is a\nconstant c6 such that, after c6 1 \u03b7L\u03b32 ln ( dL ln\u0393 \u03b3\u01eb )\nsteps, we have |\u03bb\u0302(t)\u2212\u03bb| \u2264 c5\u03b3 \u221a \u01eb\nL\u0393 \u221a d .Writing r = \u03bb\u0302(t)\u2212\u03bb,\nthis implies, if c5 is small enough, that\n((\u03bb\u0302(t))L \u2212 \u03bbL)2 = ((\u03bb+r)L\u2212\u03bbL)2\n\u2264 \u03932 ( ( 1+ r\n\u03bb\n)L \u22121\n)2\n\u2264 \u03932 ( 2c5rL\n\u03bb\n)2\n\u2264 \u03932 ( 2c5rL\n\u03b3\n)2\n\u2264 \u01eb\nd .\nThus, after O (\n1 \u03b7L\u03b32\nln (\ndL ln\u0393 \u03b3\u01eb\n))\nsteps, (Dkk \u2212 D\u0302 (t) kk ) 2 \u2264 \u01eb/d for all k, and therefore \u2113(\u0398(t)) \u2264 \u01eb,\ncompleting the proof."}, {"heading": "5 Asymmetric positive definite matrices", "text": "We have seen that if the least squares matrix is symmetric, \u03b3-positivity is sufficient for convergence of gradient descent. We shall see in Section 6 that positivity is also necessary for a broad family of gradient-based algorithms to converge to the optimal solution when the least squares matrix is symmetric. Thus, in the symmetric case, positivity characterizes the success of gradient methods.\nIn this section, we show that positivity suffices for the convergence of a gradient method even without the assumption that the least squares matrix is symmetric.\nNote that the set of \u03b3-positive (but not necessarily symmetric) matrices includes both rotations by an acute angle and \u201cpartial reflections\u201d of the form ax + b refl(x) where refl(\u00b7) is a lengthpreserving reflection and 0 \u2264 |b| < a. Since ( u\u22a4Au )\u22a4\n= u\u22a4A\u22a4u, a matrix A is \u03b3-positive if and only if u\u22a4(A+A\u22a4)u \u2265 2\u03b3 for all unit length u, i.e. A+A\u22a4 is positive definite with eigenvalues at least 2\u03b3."}, {"heading": "5.1 Balanced factorizations", "text": "The algorithm analyzed in this section uses a construction that is new, as far as we know, that we call a balanced factorization. This factorization may be of independent interest.\nRecall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP . The principal Lth root of a complex number whose expression in polar coordinates is re\u03b8i is r1/Le\u03b8i/L. The principal Lth root of a matrix A is the matrix B such that BL = A, and each eigenvalue of B is the principal Lth root of the corresponding eigenvalue of A.\nDefinition 3. If A be a matrix with polar decomposition RP , then A has the balanced factorization A = A1, ..., AL where for each i,\nAi = R 1/LPi, with Pi = R (L\u2212i)/LP 1/LR\u2212(L\u2212i)/L,\nand each of the Lth roots is the principal Lth root.\nThe motivation for balanced factorization is as follows. We want each factor to do a 1/L fraction of the total amount of rotation, and a 1/L fraction of the total amount of scaling. However, the scaling done by the ith factor should be done in directions that take account of the partial rotations done by the other factors. The following is the key property of the balanced factorization; its proof is in Appendix C.\nLemma 7. If \u03c31, ..., \u03c3d are the singular values of A, and A1, ..., AL is a balanced factorization of A, then the following hold: (a) A = \u220fL\ni=1Ai; (b) for each i \u2208 {1, ..., L}, \u03c3 1/L 1 , ..., \u03c3 1/L d are the singular\nvalues of Ai."}, {"heading": "5.2 Procedure and upper bound", "text": "The following is the power projection algorithm. It has a positivity parameter \u03b3 > 0, and uses H = {A : \u2200u s.t. ||u|| = 1, u\u22a4Au \u2265 \u03b3} as its \u201chypothesis space\u201d. First, it initializes \u0398(0)i = \u03b3 1/LI for all i \u2208 {1, ..., L}. Then, for each t, it does the following.\n\u2022 Gradient Step. For each i \u2208 {1, ..., L}, update:\n\u0398 (t+1/2) i = \u0398 (t) i \u2212 \u03b7(\u0398 (t) i+1:L)\n\u22a4 (\n\u0398 (t) 1:L \u2212 \u03a6\n)\n(\u0398 (t) 1:i\u22121) \u22a4.\n\u2022 Power Project. Compute the projection \u03a8(t+1/2) (w.r.t. the Frobenius norm) of \u0398 (t+1/2) 1:L\nonto H.\n\u2022 Factor. Let \u0398 (t+1) 1 , ...,\u0398 (t+1) L be the balanced factorization of \u03a8 (t+1/2), so that \u03a8(t+1/2) =\n\u0398 (t+1) 1:L .\nTheorem 3. For any \u03a6 such that u\u22a4\u03a6u > \u03b3 for all unit-length u, the power projection algorithm produces \u0398(t) with \u2113(\u0398(t)) \u2264 \u01eb in poly(d, ||\u03a6||F , 1 \u03b3 ) log(1/\u01eb) iterations."}, {"heading": "5.3 Proof of Theorem 3", "text": "Lemma 8. For all t, \u0398 (t) 1:L \u2208 H.\nProof. \u0398 (0) 1:L = \u03b3I \u2208 H, and, for all t, \u03a8 (t+1/2) is obtained by projection onto H, and \u0398 (t+1) 1:L = \u03a8(t+1/2).\nDefinition 4. The exponential of a matrix A is exp(A) def = \u2211\u221e\nk=0 1 k!A k, and B is a logarithm of A if A = exp(B).\nLemma 9 (Culver 1966). A real matrix has a real logarithm if and only if it is invertible and each Jordan block belonging to a negative eigenvalue occurs an even number of times.\nLemma 10. For all t, \u0398 (t) 1:L has a real Lth root.\nProof. Since \u0398 (t) 1:L \u2208 H implies u \u22a4\u0398(t)1:Lu > 0 for all u, \u0398 (t) 1:L does not have a negative eigenvalue and is invertible. By Lemma 9, \u0398 (t) 1:L has a real logarithm. Thus, its real Lth root can be constructed via exp(log(\u0398 (t) 1:L)/L).\nThe preceding lemma implies that the algorithm is well-defined, since all of the required roots can be calculated.\nLemma 11. H is convex.\nProof. Suppose A and B are in H and \u03bb \u2208 (0, 1). We have\nu\u22a4(\u03bbA+ (1\u2212 \u03bb)B)u = \u03bbu\u22a4Au+ (1\u2212 \u03bb)u\u22a4Bu \u2265 \u03b3.\nLemma 12. For all A \u2208 H, \u03c3min(A) \u2265 \u03b3.\nProof. Let u and v be singular vectors such that u\u22a4Av = \u03c3min(A).\n\u03b3 \u2264 v\u22a4Av = \u03c3min(A)v \u22a4u \u2264 \u03c3min(A).\nLemma 13. For all t, \u03c3min(\u0398 (t) i ) \u2265 \u03b3 1/L.\nProof. First, \u03c3min(\u0398 (0) i ) = \u03b3 1/L \u2265 \u03b31/L. Now consider t > 0. Since \u03a8(t\u22121/2) was projected into H, we have \u03c3min(\u03a8(t\u22121/2)) \u2265 \u03b3. Lemma 7 then completes the proof.\nDefine U(t) = max {\nmaxs\u2264tmaxi ||\u0398 (s) i ||2, ||\u03a6|| 1/L 2\n}\n, B(t) = mins\u2264tmini \u03c3min(\u0398 (s) i ), and recall that\n\u2113(t) = ||\u0398 (t) 1:L \u2212 \u03a6|| 2 F .\nArguing as in the initial portion of Section 3.2, as long as\n\u03b7 \u2264 1\n3Ld5U(t)2L (7)\nwe have \u2113(t + 1/2) \u2264 ( 1\u2212 \u03b7LB(t)2L ) \u2113(t) (see Equation 4). Lemma 13 gives B(t) \u2265 \u03b31/L, so \u2113(t+ 1/2) \u2264 ( 1\u2212 \u03b7L\u03b32 )\n\u2113(t). Since \u03a8(t+1/2) is the projection of \u0398 (t+1/2) 1:L onto a convex set H that\ncontains \u03a6, and \u0398 (t+1) 1:L = \u03a8 (t+1/2), (7) implies\n\u2113(t+ 1) \u2264 \u2113(t+ 1/2) \u2264 ( 1\u2212 \u03b7L\u03b32 ) \u2113(t). (8)\nNext, we prove an upper bound on U .\nLemma 14. For all t, U(t) \u2264 ( \u221a\n\u2113(t) + ||\u03a6||F\n)1/L .\nProof. Recall that \u2113(t) = ||\u0398 (t) 1:L\u2212\u03a6|| 2 F . By the triangle inequality, ||\u0398 (t) 1:L||F \u2264 \u221a \u2113(t)+ ||\u03a6||F . Thus ||\u0398 (t) 1:L||2 \u2264 \u221a \u2113(t) + ||\u03a6||F . By Lemma 7, for all i, we have ||\u0398 (t) i ||2 \u2264 ( \u221a \u2113(t) + ||\u03a6||F )1/L . Since ||\u03a6||2 \u2264 ||\u03a6||F , this completes the proof.\nNote that the triangle inequality implies that \u2113(0) \u2264 ||\u0398 (0) 1:L|| 2 F + ||\u03a6|| 2 F \u2264 \u03b3 2d + ||\u03a6||2F . Since \u03c3min(\u03a6) \u2265 \u03b3, we have ||\u03a6|| 2 F \u2265 \u03b3 2d, so \u2113(t) \u2264 2||\u03a6||2F and U(t) \u2264 (3||\u03a6||2) 1/L. Now, if we set \u03b7 = 1 cLd5||\u03a6||2\nF , for a large enough absolute constant c, then (7) is satisfied, so that (8) gives \u2113(t+1) \u2264 (\n1\u2212 \u03b3 2\ncd5||\u03a6||2 F\n)\n\u2113(t) and the power projection algorithm achieves \u2113(t+ 1) \u2264 \u01eb after\nO\n(\nd5||\u03a6||2F \u03b32 log\n(\n\u2113(0)\n\u01eb\n))\n=O\n(\nd5||\u03a6||2F \u03b32 log\n(\n||\u03a6||2F \u01eb\n))\nupdates."}, {"heading": "6 Failure", "text": "In this section, we show that positive definite \u03a6 are necessary for several gradient descent algorithms with different kinds of regularization to minimize the loss. One family of algorithms that we will\nanalyze is parameterized by a function \u03c8 mapping the number of inputs d and the number of layers L to a radius \u03c8(d, L), step sizes \u03b7t and initialization parameter \u03b3 \u2265 0. In particular, a \u03c8-step-and-project algorithm is any instantiation of the following algorithmic template.\nInitialize each \u0398 (0) i = \u03b3 1/LI for some \u03b3 \u2265 0 and iterate:\n\u2022 Gradient Step. For each i \u2208 {1, ..., L}, update:\n\u0398 (t+1/2) i = \u0398 (t) i \u2212 \u03b7t(\u0398 (t) i+1:L)\n\u22a4 (\n\u0398 (t) 1:L \u2212 \u03a6\n)\n(\u0398 (t) 1:i\u22121) \u22a4.\n\u2022 Project. Set each \u0398t+1i to the projection of \u0398 t+1/2 i onto {A : ||A\u2212 I||2 \u2264 \u03c8(d, L)}.\nWe will also show that Penalty Regularized Gradient Descent which uses gradient descent with any step sizes \u03b7t on the regularized objective \u2113(\u0398) + \u03ba 2 \u2211 i ||I \u2212\u0398|| 2 F also fails to minimize the loss.\nBoth results use the simple observation that when \u03981:L and \u03a6 are mutually diagonalizable then\n||\u03981:L \u2212 \u03a6|| 2 F = ||U \u22a4D\u0302U \u2212 U\u22a4DU ||2F = d \u2211\nj=1\n(D\u0302jj \u2212Djj) 2,\nwhere the Dii are the eigenvalues of \u03a6.\nTheorem 4. If the least squares matrix \u03a6 is symmetric then Penalty Regularized Gradient Descent produces hypotheses \u0398 (t) 1:L that are commuting normal with \u03a6. In addition, if \u03a6 has a negative eigenvalue \u2212\u03bb and L is even, then \u2113(\u0398(t)) \u2265 \u03bb2/2 for all t.\nProof. For all t, Penalty Regularized Gradient Descent produces \u0398 (t+1) i = (1 \u2212 \u03ba)\u0398 (t) i + \u03baI \u2212 \u03b7t(\u0398 (t) i+1:L) \u22a4 ( \u0398 (t) 1:L \u2212\u03a6 ) (\u0398 (t) 1:i\u22121) \u22a4. Thus, by induction, the \u0398(t)i are matrix polynomials of \u03a6, and therefore they are all commuting normal. As in Lemmas 5 and 6 each \u0398 (t) i is the same U \u22a4D\u0303(t)U and \u0398 (t) 1:L = U \u22a4(D\u0303(t))LU . Since L is even, each (D\u0303(t))Ljj \u2265 0, so \u2113(\u0398 (t)) = 12 ||\u0398 (t) 1:L\u2212\u03a6|| 2 F \u2265 \u03bb 2/2.\nTo analyze step-and-project algorithms, it is helpful to first characterize the project step (see also (Lefkimmiatis et al., 2013)).\nLemma 15. Let X be a symmetric matrix and let U\u22a4DU be its diagonalization.\nFor a > 0, let Y be the Frobenius norm projection of X onto Ba = {A : A is symmetric psd and ||A\u2212I||2 \u2264 a}. Then Y = U\n\u22a4D\u0303U where D\u0303 is obtained from D by projecting all of its diagonal elements onto [1\u2212 a, 1 + a].\nThus {X,Y } are symmetric commuting normal matrices.\nProof. First, if X \u2208 Ba, then Y = X and we are done. Assume X 6\u2208 Ba. Clearly U \u22a4D\u0303U \u2208 Ba, so we just need to show that any member of Ba is at least as far from X as U\u22a4D\u0303U is. Let \u039b be the multiset of eigenvalues of X (with repetitions) that are not in [1 \u2212 a, 1 + a], and for each \u03bb \u2208 \u039b, let e\u03bb be the adjustment to \u03bb necessary to bring it to [1\u2212 a, 1 + a]; i.e., so that \u03bb+ e\u03bb is the projection of \u03bb onto [1\u2212 a, 1 + a].\nIf u\u03bb is the eigenvector associated with \u03bb, we have U \u22a4D\u0303U \u2212X = \u2211 \u03bb\u2208\u039b e\u03bbu\u03bbu \u22a4 \u03bb , so that ||U \u22a4D\u0303U \u2212 X||2F = \u2211 \u03bb\u2208\u039b e 2 \u03bb. Let Z be an arbitrary member of Ba. We would like to show that ||Z \u2212 X|| 2 F \u2265 \u2211 \u03bb\u2208\u039b e 2 \u03bb. Since Z \u2208 Ba, we have ||Z \u2212 I||2 \u2264 a. ||Z \u2212 I||2 is the largest singular value of Z \u2212 I so, for any unit length vector, in particular some u\u03bb for \u03bb \u2208 \u039b, |u \u22a4 \u03bb (Z \u2212 I)u\u03bb| = |u \u22a4 \u03bb Zu\u03bb \u2212 1| \u2264 a, which implies u\u22a4\u03bbZu\u03bb \u2208 [1 \u2212 a, 1 + a]. Since U is unitary U \u22a4(X \u2212 Z)U has the same eigenvalues as X \u2212 Z, and, since the Frobenius norm is a function of the eigenvalues, ||U\u22a4(X \u2212 Z)U ||F = ||X \u2212 Z||F . But since u\u22a4\u03bbZu\u03bb \u2208 [1 \u2212 a, 1 + a] for all \u03bb \u2208 \u039b, just summing over the diagonal elements, we get ||U\u22a4(X \u2212 Z)U ||2F \u2265 \u2211 \u03bb\u2208\u039b e 2 \u03bb, completing the proof.\nTheorem 5. If the least squares matrix \u03a6 is symmetric then \u03c8-step-and-project algorithms produce hypotheses \u0398 (t) 1:L that are commuting normal with \u03a6. In addition, if \u03a6 has a negative eigenvalue \u2212\u03bb and either L is even or \u03c8(L, d) \u2264 1, then \u2113(\u0398(t)) \u2265 \u03bb2/2 for all t.\nProof. As in Lemmas 5 and 6, the \u0398 (t+1/2) i are identical and mutually diagonalizable with \u03a6. Lemma 15 shows that this is preserved by the projection step. Thus there is a real diagonal D\u0303(t) such that each \u0398 (t) i = U \u22a4D(t)i U , so \u0398 (t) 1:L = U \u22a4(D\u0303(t))LU . When L is even, each (D\u0303(t))L)j,j \u2265 0. When \u03c8(d, L) \u2264 1 then the projection ensures that the elements of D\u0303(t) are non-negative, and thus each (D\u0303(t))L)j,j \u2265 0. In either case, \u2113(\u0398 (t)) = 12 ||\u0398 (t) 1:L\u2212 \u03a6||2F \u2265 \u03bb 2/2.\nOne choice of \u03a6 that satisfies the requirements of Theorems 4 and 5 is \u03a6 = diag(\u2212\u03bb, 1, 1, ..., 1). For constant \u03bb, the loss of \u0398(0) = (I, I, ..., I) is a constant for this target. Another choice is \u03a6 = diag(\u2212\u03bb,\u2212\u03bb, 1, 1, ..., 1), which has a positive determinant.\nOur proof of failure to minimize the loss exploits the fact that the layers are initialized to multiples of the identity. Since the training process is a continuous function of the initial solution, this implies that any convergence to a good solution will be very slow if the initializations are sufficiently close to the identity."}, {"heading": "Acknowledgements", "text": "We thank Yair Carmon, Nigel Duffy, Matt Feiszli, Roy Frostig, Vineet Gupta, Moritz Hardt, Tomer Koren, Antoine Saliou, Hanie Sedghi, Yoram Singer and Kunal Talwar for valuable conversations.\nPeter Bartlett gratefully acknowledges the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS)."}, {"heading": "A Proof of Lemma 1", "text": "We rely on the following facts (Horn, 1986; Harville, 1997).\nLemma 16. For compatible matrices (and, where m,n, p, q, r, s are mentioned, A \u2208 \u211cm\u00d7n, B \u2208 \u211cp\u00d7q, X \u2208 \u211cr\u00d7s):\nA\u2297 (B \u2297 E) = (A\u2297B)\u2297E,\nAC \u2297BD = (A\u2297B)(C \u2297D),\n(A\u2297B)\u22a4 = A\u22a4 \u2297B\u22a4,\nvec(AXB) = (B\u22a4 \u2297A)vec(X),\nTm,nvec(A) def = vec(A\u22a4),\nTn,mTm,n = Imn,\nTm,n = T \u22a4 n,m,\nT1,n = Tn,1 = In,\nDX(A(B(X))) = DB(A(B(X)))DX (B(X)),\nDX(A(X)B(X)) = (B(X) \u22a4 \u2297 Im)DXA(X) + (Iq \u2297A(X))DXB(X),\nDX(A(X) T ) = Tn,mDX(A(X)),\nDX(AXB) = B \u22a4 \u2297A,\nDA(A\u2297B) = (In \u2297 Tq,m \u2297 Ip)(Imn \u2297 vec(B))\n= (Inq \u2297 Tm,p)(In \u2297 vec(B)\u2297 Im),\nDB(A\u2297B) = (In \u2297 Tq,m \u2297 Ip)(vec(A)\u2297 Ipq)\n= (Tp,q \u2297 Imn)(Iq \u2297 vec(A)\u2297 Ip).\nArmed with Lemma 16, we now prove Lemma 1. We have\nD\u0398if\u0398(x) = D\u0398i (\u0398i+1:L\u0398i\u03981:i\u22121x) = (\u03981:i\u22121x) \u22a4 \u2297\u0398i+1:L.\nAgain, from Lemma 16\nD\u0398i ( D\u0398jf\u0398(x) ) = D\u0398i\n(\n(\u03981:j\u22121x) \u22a4 \u2297\u0398j+1:L\n)\n= D\u03981:j\u22121x\n(\n(\u03981:j\u22121x) \u22a4 \u2297\u0398j+1:L\n)\nD\u0398i (\u03981:j\u22121x)\n(by the chain rule, since i < j)\n= D\u03981:j\u22121x\n(\n(\n(\u03981:j\u22121x)\u2297\u0398 \u22a4 j+1:L\n)\u22a4 ) (\n(\u03981:i\u22121x) \u22a4 \u2297\u0398i+1:j\u22121\n)\n. (9)\nDefine P = \u03981:j\u22121x and Q = \u0398j+1:L, so that P \u2208 \u211cd\u00d71 and Q \u2208 \u211cd\u00d7d. We have\nDP\n(\n( P \u2297Q\u22a4 )\u22a4 )\n= Td2,dDP\n( P \u2297Q\u22a4 )\n= Td2,d(I1 \u2297 Td,d \u2297 Id)(Id \u2297 vec(Q T )) = Td2,d(Td,d \u2297 Id)(Id \u2297 vec(Q \u22a4)).\nSubstituting back into (9), we get\nD\u0398i ( D\u0398jf\u0398(x) ) = Td2,d(Td,d \u2297 Id)(Id \u2297 vec(\u0398 \u22a4 j+1:L))\n(\n(\u03981:i\u22121x) \u22a4 \u2297\u0398i+1:j\u22121\n)\n.\nThe product rule in Lemma 16 gives, for each i,\nD\u0398i\u2113 (f\u0398) = E(D\u0398i(\u2113(f\u0398(X)))\n= E(D\u0398i( 1\n2 (f\u0398(X)\u2212 \u03a6X)\n\u22a4(f\u0398(X) \u2212\u03a6X)))\n= E(((\u03981:L \u2212 \u03a6)X) \u22a4D\u0398if\u0398(X)) = E ( ((\u03981:L \u2212 \u03a6)X) \u22a4 ( (\u03981:i\u22121X) \u22a4 \u2297\u0398i+1:L ))\n= E ( (I1 \u2297 ((\u03981:L \u2212 \u03a6)X) \u22a4) ( (\u03981:i\u22121X) \u22a4 \u2297\u0398i+1:L ))\n= E (( (\u03981:i\u22121X) \u22a4 \u2297 ((\u03981:L \u2212\u03a6)X) \u22a4\u0398i+1:L ))\n= E (( X\u22a4\u0398\u22a41:i\u22121 ) \u2297 ( X\u22a4(\u03981:L \u2212\u03a6) \u22a4\u0398i+1:L )) = E ( (X\u22a4 \u2297X\u22a4) (\n\u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L\n))\n= E ((X \u2297X)vec(1))\u22a4 ( \u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L )\n= E ( vec(XX\u22a4) )\u22a4 (\n\u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L\n)\n= (vec(Id)) T ( \u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L ) .\nHence,\n(D\u0398i\u2113 (f\u0398)) \u22a4 =\n(\n\u03981:i\u22121 \u2297\u0398 \u22a4 i+1:L(\u03981:L \u2212 \u03a6)\n)\n(vec(Id))\n= vec ( \u0398\u22a4i+1:L(\u03981:L \u2212 \u03a6)Id\u0398 \u22a4 1:i\u22121 ) .\nAlso, recalling that i < j, we have\nD\u0398jD\u0398i\u2113 (f\u0398) = D\u0398j\n( (vec(Id)) T ( \u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L ))\n= (Id2 \u2297 (vec(Id)) T )D\u0398j\n(\n\u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L\n)\n= (Id2 \u2297 (vec(Id)) T ) (Id \u2297 Td,d \u2297 Id)\n( vec(\u0398\u22a41:i\u22121)\u2297 Id2 ) D\u0398j ( (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L ) .\nContinuing with the subproblem,\nD\u0398j\n(\n(\u03981:L \u2212\u03a6) \u22a4\u0398i+1:L\n)\n= (\u0398\u22a4i+1:L \u2297 Id)D\u0398j\n( (\u03981:L \u2212 \u03a6) \u22a4 )\n+ (Id \u2297 (\u03981:L \u2212 \u03a6) \u22a4)D\u0398j (\u0398i+1:L)\n= (\u0398\u22a4i+1:L \u2297 Id)D\u0398j\n( \u0398\u22a41:L )\n+ (Id \u2297 (\u03981:L \u2212 \u03a6) \u22a4)D\u0398j (\u0398i+1:L)\n= (\u0398\u22a4i+1:L \u2297 Id) ( \u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121 ) D\u0398j (\u0398 \u22a4 j )\n+ (Id \u2297 (\u03981:L \u2212 \u03a6) \u22a4) ( \u0398\u22a4i+1:j\u22121 \u2297\u0398j+1:L )\n= (\u0398\u22a4i+1:L \u2297 Id) ( \u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121 ) Td,d\n+ (Id \u2297 (\u03981:L \u2212 \u03a6) \u22a4) ( \u0398\u22a4i+1:j\u22121 \u2297\u0398j+1:L )\n= ( \u0398\u22a4i+1:L\u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121 ) Td,d\n+ ( \u0398\u22a4i+1:j\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398j+1:L ) .\nFinally,\nD\u0398iD\u0398i\u2113 (f\u0398) = D\u0398i\n( (vec(Id)) T ( \u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L ))\n= (Id2 \u2297 (vec(Id)) T )D\u0398i\n(\n\u0398\u22a41:i\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L\n)\n= (Id2 \u2297 (vec(Id)) T ) (Id \u2297 Td,d \u2297 Id)\n( vec(\u0398\u22a41:i\u22121)\u2297 Id2 ) D\u0398i ( (\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L )\nand\nD\u0398i\n(\n(\u03981:L \u2212 \u03a6) \u22a4\u0398i+1:L\n)\n= (\u0398\u22a4i+1:L \u2297 Id)D\u0398i\n( (\u03981:L \u2212 \u03a6) \u22a4 )\n= (\u0398\u22a4i+1:L \u2297 Id)D\u0398i\n( \u0398\u22a41:L )\n= (\u0398\u22a4i+1:L \u2297 Id) ( \u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) D\u0398i(\u0398 \u22a4 i ) = (\u0398\u22a4i+1:L \u2297 Id) ( \u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) Td,d = (\n\u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121\n)\nTd,d."}, {"heading": "B Proof of Lemma 3", "text": "We have ||\u22072||2F = 2 \u2211\ni<j\n||D\u0398jD\u0398i\u2113(f\u0398)|| 2 F +\n\u2211\ni\n||D\u0398iD\u0398i\u2113(f\u0398)|| 2 F . (10)\nLet\u2019s start with the easier term. Choose \u0398 such that ||\u0398i \u2212 I||2 \u2264 z for all i. We have\n||D\u0398iD\u0398i\u2113 (f\u0398) ||F = \u2223 \u2223 \u2223 \u2223(Id2\u2297(vec(Id)) \u22a4) (Id\u2297Td,d\u2297Id)\n( vec(\u0398\u22a41:i\u22121)\u2297Id2 )\n(\n\u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121\n)\nTd,d \u2223 \u2223 \u2223 \u2223\nF\n\u2264 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 (Id2 \u2297 (vec(Id)) \u22a4) (Id \u2297 Td,d \u2297 Id) \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n\u00d7 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( vec(\u0398\u22a41:i\u22121)\u2297Id2 )( \u0398\u22a4i+1:L\u0398i+1:L\u2297\u0398 \u22a4 1:i\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n= d3/2 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( vec(\u0398\u22a41:i\u22121)\u2297 Id2 )\n(\n\u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121\n)\nTd,d\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n\u2264 d3/2 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( vec(\u0398\u22a41:i\u22121)\u2297 Id2 ) \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n\u00d7 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n= d7/2 \u2223 \u2223\n\u2223\n\u2223 \u2223\n\u2223 vec(\u0398\u22a41:i\u22121)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398i+1:L\u2297\u0398 \u22a4 1:i\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n= d7/2 ||\u03981:i\u22121||F\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n\u2264 d4 ||\u03981:i\u22121||2\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n\u2264 d4(1 + z)i\u22121 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n= d4(1 + z)i\u22121 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398i+1:L \u2297\u0398 \u22a4 1:i\u22121 ) \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n= d4(1 + z)i\u22121 \u2223 \u2223\n\u2223\n\u2223 \u2223\n\u2223 \u0398\u22a4i+1:L\u0398i+1:L\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 F \u00d7 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u0398\u22a41:i\u22121 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 F\n\u2264 d5(1 + z)i\u22121 \u2223 \u2223\n\u2223\n\u2223 \u2223\n\u2223 \u0398\u22a4i+1:L\u0398i+1:L\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 \u00d7 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u0398\u22a41:i\u22121 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n\u2264 d5(1 + z)2(L\u22121).\nSimilarly,\n||D\u0398jD\u0398i\u2113 (f\u0398) ||F = \u2223 \u2223 \u2223 \u2223(Id2\u2297(vec(I)) \u22a4) (Id\u2297Td,d\u2297Id)\n( vec(\u0398\u22a41:i\u22121)\u2297Id2 )\n(\n(\n\u0398\u22a4i+1:L\u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121\n)\nTd,d\n+ ( \u0398\u22a4i+1:j\u22121 \u2297 (\u03981:L \u2212\u03a6) \u22a4\u0398j+1:L )\n)\n\u2223 \u2223 \u2223 \u2223\nF\n\u2264 d4(1 + z)i\u22121 \u2223 \u2223 \u2223 \u2223\n(\n\u0398\u22a4i+1:L\u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121\n)\nTd,d\n+ ( \u0398\u22a4i+1:j\u22121 \u2297 (\u03981:L \u2212\u03a6) \u22a4\u0398j+1:L ) \u2223 \u2223 \u2223 \u2223\nF\n\u2264 d4(1 + z)i\u22121 (\u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( \u0398\u22a4i+1:L\u0398j+1:L \u2297\u0398 \u22a4 1:j\u22121 ) Td,d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n+ \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( \u0398\u22a4i+1:j\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398j+1:L )\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n)\n\u2264 d4(1 + z)i\u22121 ( d(1 + z)2L\u22121\u2212i\n+ \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 ( \u0398\u22a4i+1:j\u22121 \u2297 (\u03981:L \u2212 \u03a6) \u22a4\u0398j+1:L )\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n)\n= d4(1 + z)i\u22121 ( d(1 + z)2L\u22121\u2212i\n+ ||\u0398i+1:j\u22121||F \u00d7 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 (\u03981:L \u2212 \u03a6) \u22a4\u0398j+1:L \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\nF\n)\n\u2264 d4(1 + z)i\u22121 ( d(1 + z)2L\u22121\u2212i + 2d(1 + z)2L\u22121\u2212i ) = 3d5(1 + z)2L\u22122.\nPutting these together with (10), we get ||\u22072||2F \u2264 L 29d10(1 + z)4L, so that\n||\u22072||F \u2264 3Ld 5(1 + z)2L."}, {"heading": "C Proof of Lemma 7", "text": "Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .\nLemma 17 ((Horn & Johnson, 2013)). A is a unitary matrix if and only if all of the (complex) eigenvalues z of A have magnitude 1.\nLemma 18 ((Horn & Johnson, 2013)). If A is unitary then A is normal.\nLemma 19 ((Horn & Johnson, 2013)). If A is normal with eigenvalues \u03bb1, ..., \u03bbd, the singular values of A are |\u03bb1|, ..., |\u03bbd|.\nLemma 20. If A is unitary, then A1/L is unitary, and thus Ai/L is unitary for any non-negative integer i.\nLemma 21. If A is invertible and normal with singular values \u03c31, ..., \u03c3d, then, for any positive integer L, the singular values of A1/L are \u03c3 1/L 1 , ..., \u03c3 1/L d .\nProof. Follows from Lemma 19 together with the fact that raising a non-singular matrix to a power results in raising its eigenvalues to the same power.\nLemma 22 ((Horn & Johnson, 2013)). If A = RP is the polar decomposition of A, then the singular values of A are the same as the singular values of P .\nLemma 23. If \u03c31, ..., \u03c3d are the principal components of A, and A = \u220fL i=1Ai is a balanced factorization of A, then then \u03c3 1/L 1 , ..., \u03c3 1/L d are the principal components of Ai, for each i \u2208 {1, ..., L}.\nProof. The singular values of Ai = RiPi are the same as the singular values of Pi, which is similar to P 1/L, whose singular values are the Lth roots of the singular values of P , which are the same as the singular values of A.\nLemma 24. If A1, ..., AL is a balanced factorization of A, then\nA = L \u220f\ni=1\nAi.\nProof. We have\nA = RP\n= R1/LR1\u22121/LP 1/LP 1\u22121/L = R1/LR1\u22121/LP 1/LR\u2212(1\u22121/L)R1\u22121/LP 1\u22121/L = R1P1R 1\u22121/LP 1\u22121/L = A1R 1\u22121/LP 1\u22121/L = A1R 1/LR1\u22122/LP 1/LP 1\u22122/L\nand so on."}], "year": 2018, "references": [{"title": "Provable bounds for learning", "authors": ["Arora", "Sanjeev", "Bhaskara", "Aditya", "Ge", "Rong", "Ma", "Tengyu"], "year": 2014}, {"title": "Representing smooth functions", "authors": ["L. Peter", "Evans", "N. Steven", "Long", "M. Philip"], "venue": "International Conference on Machine Learning,", "year": 2014}, {"title": "Globally optimal gradient descent for a convnet with gaussian", "authors": ["A. Brutzkus", "A. Globerson"], "year": 2004}, {"title": "parameterized networks that provably generalize on linearly separable data", "authors": ["Culver", "Walter J"], "year": 2018}, {"title": "Daniely, A. SGD learns the conjugate kernel class of the network", "authors": ["A. Daniely", "R. Frostig", "Y. Singer"], "year": 1966}, {"title": "power of initialization and a dual view on expressivity", "authors": ["C. Jin", "Y. Zheng"], "venue": "In NIPS,", "year": 2016}, {"title": "Identity matters in deep learning", "authors": ["M. 2018. Hardt", "T. Ma"], "year": 1997}, {"title": "Topics in Matrix Analysis", "authors": ["A. R"], "year": 1986}, {"title": "Matrix analysis", "authors": ["ISBN 0-521-30587-X. Horn", "Roger A", "Johnson", "Charles R"], "year": 2013}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "authors": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "arXiv preprint arXiv:1506.08473,", "year": 2015}, {"title": "Deep learning without poor local minima", "authors": ["K. Kawaguchi"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Gradient descent only converges to minimizers", "authors": ["J.D. Lee", "M. Simchowitz", "M.I. Jordan", "B. Recht"], "venue": "In COLT,", "year": 2016}, {"title": "Efficient agnostic learning of neural networks with bounded fan-in", "authors": ["Lee", "Wee Sun", "Bartlett", "Peter L", "Williamson", "Robert C"], "venue": "IEEE Transactions on Information Theory,", "year": 1996}, {"title": "Hessian schatten-norm regularization for linear inverse problems", "authors": ["Lefkimmiatis", "Stamatios", "Ward", "John Paul", "Unser", "Michael"], "venue": "IEEE transactions on image processing,", "year": 2013}, {"title": "Convergence analysis of two-layer neural networks with relu activation", "authors": ["Li", "Yuanzhi", "Yuan", "Yang"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "On the computational efficiency of training neural networks", "authors": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "The loss surface of deep and wide neural networks", "authors": ["Nguyen", "Quynh", "Hein", "Matthias"], "venue": "In ICML,", "year": 2017}, {"title": "On the quality of the initial basin in overspecified neural networks", "authors": ["Safran", "Itay", "Shamir", "Ohad"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "authors": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "year": 2013}, {"title": "How regularization affects the critical points in linear networks", "authors": ["A. Taghvaei", "J.W. Kim", "P. Mehta"], "venue": "In NIPS,", "year": 2017}, {"title": "Electron-proton dynamics in deep learning", "authors": ["Q. Zhang", "R. Panigrahy", "S. Sachdeva"], "venue": "ITCS,", "year": 2018}, {"title": "l1-regularized neural networks are improperly learnable in polynomial time", "authors": ["Zhang", "Yuchen", "Lee", "Jason D", "Jordan", "Michael I"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "On the learnability of fullyconnected neural networks", "authors": ["Zhang", "Yuchen", "Lee", "Jason", "Wainwright", "Martin", "Jordan", "Michael"], "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,", "year": 2017}, {"title": "Recovery guarantees for one-hidden-layer neural networks", "authors": ["Zhong", "Kai", "Song", "Zhao", "Jain", "Prateek", "Bartlett", "Peter L", "Dhillon", "Inderjit S"], "venue": "In ICML,", "year": 2017}], "id": "SP:2d826b6725ca2cc10085997b94226e4180127a79", "authors": [], "abstractText": "We analyze algorithms for approximating a function f(x) = \u03a6xmapping R to R using deep linear neural networks, i.e. that learn a function h parameterized by matrices \u03981, ...,\u0398L and defined by h(x) = \u0398L\u0398L\u22121...\u03981x. We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic. We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix \u03a6, in the case where the initial hypothesis \u03981 = ... = \u0398L = I has excess loss bounded by a small enough constant. On the other hand, we show that gradient descent fails to converge for \u03a6 whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help. If \u03a6 is symmetric positive definite, we show that an algorithm that initializes \u0398i = I learns an \u01eb-approximation of f using a number of updates polynomial in L, the condition number of \u03a6, and log(d/\u01eb). In contrast, we show that if the least squares matrix \u03a6 is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge. We analyze an algorithm for the case that \u03a6 satisfies u\u03a6u > 0 for all u, but may not be symmetric. This algorithm uses two regularizers: one that maintains the invariant u\u22a4\u0398L\u0398L\u22121...\u03981u > 0 for all u, and another that \u201cbalances\u201d \u03981, ...,\u0398L so that they have the same singular values."}