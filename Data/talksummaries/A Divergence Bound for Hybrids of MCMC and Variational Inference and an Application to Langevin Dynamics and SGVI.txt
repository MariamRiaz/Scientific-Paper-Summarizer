19	27	Intuitively, both MCMC and variational methods can be thought of as trying to find a high-probability region of log p(z), with different strategies to encourage entropy to get coverage of the distribution.
20	22	In MCMC, entropy is created by randomness in the Markov chain, while in variational methods the KL-divergence directly measures the entropy of the variational distribution.
21	26	It is natural to think that hybrid methods might employ fractions of these two strategies.
22	70	This paper does so by defining a distribution over the parameters w of the approximating family q(z|w).
27	15	Second, one can “adjoin” some distribution p(w|z) to p(z), and bound Dtrue by the joint divergence D1 = KL (q(W,Z)kp(W,Z)).
36	12	Informally, at these extremes, the algorithm is “fast but approximate” and “slow but accurate”.
37	15	In the intermediate range, the algorithm exhibits new behavior with a fine-grained trade-off between speed and accuracy.
43	29	So, KL (q(Z|w)kp(Z|w)) = Eq(Z|w) log (q(Z|w)/p(Z|w)) is the divergence between q(z|w) and p(z|w) for a fixed w, while KL (q(Z|W )kp(Z|W )) = Eq(W,Z) log (q(Z|W )/p(Z|W )) is a standard conditional divergence.
44	41	This section derives a few results from an information theoretic viewpoint, without any particular regard for form of the target distribution, or how one might sample from it.
46	30	Let p(z) be the target distribution.
47	33	For simplicity, z is treated here as a continuous variable, although the results in this section remain true if it is discrete.
49	18	In principle, one might like to know how to set q(w) such that the resulting marginal distribution over z, is close to p(z), as measured by the KLdivergence, Dtrue = KL (q(Z)kp(Z)) = Eq(Z) log q(Z) p(Z) .
50	9	(4) This quantity is difficult to control directly, since the marginal q(z) typically cannot be evaluated in closed form.
52	18	The divergence from q(z) to p(z) is KL (q(Z)kp(Z)) = KL (q(Z|W )kp(Z))| {z } D0 Iq[W,Z], (5) where D0 = Eq(W,Z) log (q(Z|W )/p(Z)) is the conditional divergence and Iq = KL (q(Z,W )kq(Z)q(W )) denotes mutual information under q.
63	23	For fixed values of and p(w|z), the distribution q(w) that minimizes D is q⇤(w) = exp s(w) A) (8) A = log Z w exp s(w) s(w) = log p(w) KL (q(Z|w)kp(Z|w)) 1 1 KL (q(Z|w)kp(Z)) .
65	11	Since A is an upper-bound on the KLdivergence, A must be non-positive.
73	12	This may initially seem like a strange condition, given that p(w) results from both the target distribution p(z) and the adjoined distribution p(w|z).
78	11	Here, we assume for convenience that rz is a constant, not depending on z. Enforcing rz to be constant essentially means choosing r(w) in such a way that it doesn’t “favor” any z over any other since if q(w) / r(w) then q(z) is uniform over z. Lemma 5.
80	34	(12) This gives a distribution over q(w) that can be written in various equivalent ways, such as q⇤(w) / r(w) exp 1E(w) + ( 1 1)H(w) (13) =r(w) exp H(w) 1KL (qw(Z)kp(Z)) =r(w) exp1/ E qw(Z) [log p(Z) + ( 1) log qw(Z)] , where H(w) = Eqw(Z)[log qw(Z)] is the entropy of qw and E(w) = Eqw(z)[log p(Z)].
82	15	If one simply used r(w) = 1, then p(w|z) may not be well-defined.
91	14	This section considers algorithms to sample from the distribution defined by Eq.
96	28	The goal of probabilistic inference is to be able to evaluate expectations with respect to p(z).
97	25	The goal of MCMC methods is to obtain samples from the target distribution p(z).
98	20	Langevin dynamics sample by an extremely simple process of repeating gradient steps of log(z) with injected Gaussian noise.
99	66	Specifically, the iterate is z z + ✏ 2 r log p(z) + p ✏⌘, (15) where ⌘ is sampled from a standard Multivariate Gaussian distribution and ✏ is a step-size that may decay over time.
100	29	If Langevin dynamics are used as a proposal for a MetropolisHastings sampler, it can be shown that correct acceptance ratio is exp s(z0) s(z) + ✏ 8 krs(z)k2 ✏ 8 krs(z0)k2 + 1 2 (z z0) · (rs(z) +rs(z0)) , (16) where s(z) = log p(z) up to a constant factor, and z0 is the proposed point from Eq.
101	51	It is easy to see that as ✏ becomes small, this acceptance ratio will go to one, and so one can disregard the acceptance step with some bias in the results determined by the step size.
111	100	(19) While in some cases with specific p and q, one can derive exact updates of L (Ghahramani & Beal, 2000) in general one cannot exactly evaluate the expectation over Z in L. One line of approach to this (Ranganath et al., 2014; Salimans & Knowles, 2014) is to write the gradient as rL = Eqw(Z)[(log qw(Z) log p(Z))r log qw(Z)], and estimate this by drawing samples from qw(z).
112	10	This experimentally seems to result in gradients with large variance, but this can be reduced by two strategies.
113	45	Firstly, one can use control variates, based on either Taylor expansion (Paisley et al., 2012) or the fact that the expected value of rw log qw(Z) is zero.
