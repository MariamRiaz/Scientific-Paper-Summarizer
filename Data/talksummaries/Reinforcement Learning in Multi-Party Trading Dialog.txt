20	13	Recently, there has been an increasing amount of research on applying RL to negotiation dialog domains, which are generally more complex than slot-filling dialog because the system needs to consider its own goal as well as the user’s goal, and may need to keep track of more information, e.g., what has been accepted or rejected so far, proposals and arguments on the table, etc.
29	43	Note that in all of the previous work mentioned above, the focus was on negotiation dialog between two participants only, ignoring cases where negotiation takes place between more than two interlocutors.
31	31	In this paper, as a first study on multi-party negotiation, we apply RL to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents.
32	20	We experiment with different RL algorithms and reward functions.
34	24	In our experiments, we evaluate how the performance of the learner varies depending on the RL algorithm used and the number of traders.
35	16	To the best of our knowledge this is the first study that applies RL to multi-party (more than two participants) negotiation dialog management.
38	25	Section 3 describes our multi-party trading domain.
65	24	So at the end of the interaction each trader earns a number of points based on the items that it holds and the value of each item.
75	26	In our experiments, there are three types of items: apple, orange, and grape, and each trader may like, hate, or feel neutral about each type of fruit.
76	34	At the end of the dialog the trader earns 100 points for each fruit that he likes, 0 points for each fruit that he is neutral to, and -100 points for each fruit that he hates.
78	33	Furthermore, all traders can get a big payoff for having a fruit salad, i.e., the trader earns 500 additional points if he ends up with one fruit of each type.
79	14	Thus even hated fruits may sometimes be beneficial, but only if they can be part of a fruit salad.
80	26	Thus the outcome for a trader otr is calculated by Equation (1).
81	23	otr = Pay(appletr) ∗Num(appletr) + Pay(orangetr) ∗Num(orangetr) + Pay(grapetr) ∗Num(grapetr) + Pay(saladtr) (1) Pay(saladtr) =  500 if Num(appletr) ≥ 1 and Num(orangetr) ≥ 1 and Num(grapetr) ≥ 1 0 otherwise (2) where Pay is a function which takes as argument a fruit type and returns the value of that fruit type for the trader, and Num shows the number of items of a particular fruit type that the trader possesses.
89	19	The trader simulators are used as negotiation partners of the learner for both training and evaluating the learner’s policy (see Section 5).
91	38	Note that we use two kinds of rewards.
92	38	The first type of reward is based on Equation (3).
93	83	In this case, the learner is rewarded based on its outcome only at the end of the dialog.
95	62	The incremental reward at turn i is given by Equation (4), where otr(i) is the outcome for a trader applied at time point i. r ′ i = { γ ∗ otr(i)− otr(i− 1) if i > 0 0 if i = 0 (4) This equation represents the improvement on the outcome of the learner at turn i compared to its outcome at the previous turn i − 1.
96	93	Note that this implementation of the incremental reward function is basically the same as reward shaping, and has the following property (Ng et al., 1999; Asri et al., 2013): the policy learned by using Equation (4) maximizes the expectation of the cumulative reward given by Equation (3).
106	26	The dialog state is represented by binary variables (or features).
116	54	More concretely, this policy selects an action based on the following steps: 1.
120	32	Initially the learner has (O apples, 2 oranges, 1 grape) and Agent 1 has (1 apple, 0 oranges, 1 grape).
122	15	Trading Item requested Item given Occurrence partner by partner by partner binary value to learner (used as feature) Agent 1 apple orange 0 apple grape 0 orange apple 1 orange grape 0 grape apple 0 grape orange 0 Agent who Fruit type Number of fruits possesses fruits (used as feature) apple 0 learner orange 2 grape 1 apple 1 Agent 1 orange 0 grape 1 each item is represented by a card), given the role of the trader (Rich, Middle, Poor) and how many items there can be in the hand.
130	22	A plan is a sequence of trades (one item in hand for one item out of hand) that will lead to the goal.
135	42	The outcome will be the hand that results from the end state, or the state before the trade that fails.
142	71	This value of probability represents the fact that the simulator does not know a priori whether the trade will succeed or not.
149	16	In this section, we evaluate the learner’s policies learned with (1) different algorithms i.e., LinQ, LSPI, and NFQ (see Section 2), (2) different reward functions i.e., Equations 3 and 4 (see Section 4.1), and (3) different numbers of traders.
151	36	More specifically, there are 9 different setups: H: 2-party dialog, where the trader simulator follows a hand-crafted policy.
154	31	HxR: 3-party dialog, where one trader simulator follows a hand-crafted policy and the other one follows a random policy.
