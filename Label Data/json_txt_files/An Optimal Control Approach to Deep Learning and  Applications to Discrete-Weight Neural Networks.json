{"sections": [{"heading": "1. Introduction", "text": "The problem of training deep feed-forward neural networks is often studied as a nonlinear programming problem (Bazaraa et al., 2013; Bertsekas, 1999; Kuhn & Tucker, 2014)\nmin \u03b8 J(\u03b8)\nwhere \u03b8 represents the set of trainable parameters and J is the empirical loss function. In the general unconstrained case, necessary optimality conditions are given by the condition\u2207\u03b8J(\u03b8\u2217) = 0 for an optimal set of training parameters \u03b8\u2217. This is largely the basis for (stochastic) gradient-descent based optimization algorithms in deep learning (Robbins & Monro, 1951; Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2014). When there are additional constraints, e.g. on the trainable parameters, one can instead employ projected\n1Institute of High Performance Computing, Singapore. Correspondence to: Qianxiao Li <liqix@ihpc.a-star.edu.sg>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nversions of the above algorithms. More broadly, necessary conditions for optimality can be derived in the form of the Karush-Kuhn-Tucker conditions (Kuhn & Tucker, 2014). Such approaches are quite general and typically do not rely on the structures of the objectives encountered in deep learning. However, in deep learning, the objective function J often has a specific structure; It is derived from feeding a batch of inputs recursively through a sequence of trainable transformations, which can be adjusted so that the final outputs are close to some fixed target set. This process resembles an optimal control problem (Bryson, 1975; Bertsekas, 1995; Athans & Falb, 2013) that originates from the study of the calculus of variations.\nIn this paper, we exploit this optimal control viewpoint of deep learning. First, we introduce the discrete-time Pontryagin\u2019s maximum principle (PMP) (Halkin, 1966), which is an extension the central result in optimal control due to Pontryagin and coworkers (Boltyanskii et al., 1960; Pontryagin, 1987). This is an alternative set of necessary conditions characterizing optimality, and we discuss the extent of its validity in the context of deep learning. Next, we introduce the discrete method of successive approximations (MSA) based on the PMP to optimize deep neural networks. A rigorous error estimate is proved that elucidates the dynamics of the MSA, and aids us in designing optimization algorithms under rather general conditions. We apply our method to train a class of unconventional networks, i.e. those with discretevalued weights, to illustrate the usefulness of this approach. In the process, we discover that in the case of ternary networks, our training algorithm obtains trained models that are very sparse, which is an attractive feature in practice.\nThe rest of the paper is organized as follows: In Sec. 2, we introduce the optimal control viewpoint and the discretetime Pontryagin\u2019s maximum principle. We then introduce the method of successive approximation in Sec. 3 and prove our main estimate, Theorem 2. In Sec. 4, we derive algorithms based on the developed theory to train binary and ternary neural networks. Finally, we end with a discussion on related work and a conclusion in Sec. 5 and 6 respectively. Various details on proofs and algorithms are provided in Appendix A-D, which also contains a link to a software implementation of our algorithms that reproduces all exper-\niments in this paper.\nHereafter, we denote the usual Euclidean norm by \u2016 \u00b7 \u2016 and the corresponding induced matrix norm by \u2016 \u00b7 \u20162. The Frobenius norm is written as \u2016 \u00b7 \u2016F . Throughout this work, we use a bold-faced version of a variable to represent a collection of the same variable, but indexed additionally by t, e.g. \u03b8 := {\u03b8t : t = 0, . . . , T \u2212 1}."}, {"heading": "2. The Optimal Control Viewpoint", "text": "In this section, we formalize the problem of training a deep neural network as an optimal control problem. Let T \u2208 Z+ denote the number of layers and {xs,0 \u2208 Rd0 : s = 0, . . . , S} represent a collection of fixed inputs (images, time-series). Here, S \u2208 Z+ is the sample size. Consider the dynamical system\nxs,t+1 = ft(xs,t, \u03b8t), t = 0, 1, . . . , T \u2212 1, (1)\nwhere for each t, ft : Rdt \u00d7 \u0398t \u2192 Rdt+1 is a transformation on the state. For example, in typical neural networks, it can represent a trainable affine transformation or a nonlinear activation (in which case it is not trainable and ft does not depend on \u03b8). We assume that each trainable parameter set \u0398t is a subset of an Euclidean space. The goal of training a neural network is to adjust the weights \u03b8 := {\u03b8t : t = 0, . . . , T \u2212 1} so as to minimize some loss function that measures the difference between the final network output xs,T and some true targets ys of xs,0, which are fixed. Thus, we may define a family of real-valued functions \u03a6s : RdT \u2192 R acting on xs,T (ys are absorbed into the definition of \u03a6s) and the average loss function is\u2211 s \u03a6s(xs,T )/S. In addition, we may consider some regularization terms for each layer Lt : Rdt \u00d7 \u0398t \u2192 R that has to be simultaneously minimized. In typical applications, regularization is only performed for the trainable parameters so that Lt(x, \u03b8) \u2261 Lt(\u03b8), but here we will consider the slightly more general case where it is also possible to regularize the state at each layer. In summary, we wish to solve the following problem\nmin \u03b8\u2208\u0398\nJ(\u03b8) := 1\nS S\u2211 s=1 \u03a6s(xs,T ) + 1 S S\u2211 s=1 T\u22121\u2211 t=0 Lt(xs,t, \u03b8t)\nsubject to: xs,t+1 = ft(xs,t, \u03b8t), t = 0, . . . , T \u2212 1, s \u2208 [S] (2)\nwhere we have defined for shorthand \u0398 := {\u03980 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u0398T\u22121} and [S] := {1, . . . , S}. One may recognize problem (2) as a classical fixed-time, variable-terminal-state optimal control problem in discrete time (Ogata, 1995), in fact a special one with almost decoupled dynamics across samples in [S]."}, {"heading": "2.1. The Pontryagin\u2019s Maximum Principle", "text": "Maximum principles of the Pontryagin type (Boltyanskii et al., 1960; Pontryagin, 1987) usually consist of necessary conditions for optimality in the form of the maximization of a certain Hamiltonian function. The distinguishing feature is that it does not assume differentiability (or even continuity) of ft with respect to \u03b8. Consequently the optimality condition and the algorithms based on it need not rely on gradient-descent type updates. This is an attractive feature for certain classes of applications.\nLet \u03b8\u2217 = {\u03b80, . . . , \u03b8T\u22121} \u2208 \u0398 be a solution of (2). We now outline informally the Pontryagin\u2019s maximum principle (PMP) that characterizes \u03b8\u2217. First, for each t we define the Hamiltonian function Ht : Rdt \u00d7 Rdt+1 \u00d7\u0398t \u2192 R by\nHt(x, p, \u03b8) := p \u00b7 ft(x, \u03b8)\u2212 1SLt(x, \u03b8). (3)\nOne can show the following necessary conditions. Theorem 1 (Discrete PMP, Informal Statement). Let ft and \u03a6s, s = 1, . . . , S be sufficiently smooth in x. Assume further that for each t and x \u2208 Rdt , the sets {ft(x, \u03b8) : \u03b8 \u2208 \u0398t} and {Lt(x, \u03b8) : \u03b8 \u2208 \u0398t} are convex. Then, there exists co-state processes p\u2217s := {p\u2217s,t : t = 0, . . . , T}, such that following holds for t = 0, . . . , T \u2212 1 and s \u2208 [S]:\nx\u2217s,t+1 = \u2207pHt(x\u2217s,t, p\u2217s,t+1, \u03b8\u2217t ), x\u2217s,0 = xs,0 (4) p\u2217s,t = \u2207xHt(x\u2217s,t, p\u2217s,t+1, \u03b8\u2217t ), p\u2217s,T = \u2212 1S\u2207\u03a6s(x \u2217 s,T ) (5)\nS\u2211 s=1 Ht(x \u2217 s,t, p \u2217 s,t+1, \u03b8 \u2217 t ) \u2265 S\u2211 s=1 Ht(x \u2217 s,t, p \u2217 s,t+1, \u03b8), \u2200\u03b8 \u2208 \u0398t\n(6)\nThe full statement of Theorem 1 involve explicit smoothness assumptions and additional technicalities (such as the inclusion of an abnormal multiplier). In Appendix A, we state these assumptions and give a sketch of its proof based on Halkin (1966).\nLet us discuss the PMP in detail. The state equation (4) is simply the forward propagation equation (1) under the optimal parameters \u03b8\u2217. Eq. (5) defines the evolution of the co-state p\u2217s . To draw an analogy with nonlinear programming, the co-state can be interpreted as a set of Lagrange multipliers that enforces the constraint (1) when the optimization problem (2) is regarded as a joint optimization problem in \u03b8 and xs, s \u2208 [S]. In the optimal control and PMP viewpoint, it is perhaps more appropriate to think of the dynamics (5) as the evolution of the normal vector of a separating hyper-plane, which separates the set of reachable states and the set of states where the objective function takes on values smaller than the optimum (see Appendix A).\nThe Hamiltonian maximization condition (6) is the centerpiece of the PMP. It says that an optimal solution \u03b8\u2217 must globally maximize the (summed) Hamiltonian for each layer t = 0, . . . , T \u2212 1. Let us contrast this statement with usual\nfirst-order optimality conditions of the form \u2207\u03b8J(\u03b8\u2217) = 0. A key observation is that in Theorem 1, we did not make reference to the derivative of any quantity with respect \u03b8. In fact, the PMP holds even if ft is not differentiable, or even continuous, with respect to \u03b8, as long as the convexity assumptions are satisfied. On the other hand, if we assume for each t: 1) ft is differentiable with respect to \u03b8; 2) Ht is concave in \u03b8; and 3) \u03b8\u2217t lies in the interior of \u0398t, then the Hamiltonian maximization condition (6) is equivalent to the condition \u2207\u03b8 \u2211 sHt = 0 for all t, which one can then show is equivalent to \u2207\u03b8J = 0 (See Appendix C, proof of Prop. C.1). In other words, the PMP can be viewed as a stronger set of necessary conditions (at optimality, Ht is not just stationary, but globally maximized) and has meaning in more general scenarios, e.g. when stationarity with respect to \u03b8 is not achievable due to constraints, or not defined due to non-differentiability. Remark 1. It may occur that \u2211 sHt(x \u2217 s,t, p \u2217 s,t+1, \u03b8) is constant for all \u03b8 \u2208 \u0398t, in which case the problem is singular (Athans & Falb, 2013). In such cases, the PMP is trivially satisfied by any \u03b8 and so the PMP does not tell us anything useful. This may arise especially in the case where there are no regularization terms."}, {"heading": "2.2. The Convexity Assumption", "text": "The most crucial assumption in Theorem 1 is the convexity of the sets {ft(x, \u03b8) : \u03b8 \u2208 \u0398t} and {Lt(x, \u03b8) : \u03b8 \u2208 \u0398t} for each fixed x 1. We now discuss how restrictive these assumptions are with regard to deep neural networks. Let us first assume that the admissable sets \u0398t are convex. Then, the assumption with respect to Lt is not restrictive since most regularizers (e.g. `1, `2) satisfy it. Let us consider the convexity of {ft(x, \u03b8) : \u03b8 \u2208 \u0398t}. In classical feed-forward neural networks, there are two types of layers: trainable ones and non-trainable ones. Suppose layer t is non-trainable (e.g. f(xt, \u03b8t) = \u03c3(xt) where \u03c3 is a non-linear activation function), then for each x the set {ft(x, \u03b8) : \u03b8 \u2208 \u0398t} is a singleton, and hence trivially convex. On the other hand, in trainable layers ft is usually affine in \u03b8. This includes fully connected layers, convolution layers and batch normalization layers (Ioffe & Szegedy, 2015). In these cases, as long as the admissable set \u0398t is convex, we again satisfy the convexity assumption. Residual networks also satisfy the convexity constraint if one introduces auxiliary variables (see Appendix A.1). When the set \u0398t is not convex, then it is in general not true that the PMP constitute necessary conditions.\n1Note that this is in general unrelated to the convexity, in the sense of functions, of ft with respect to either x or \u03b8. For example, the scalar function f(x, \u03b8) = \u03b83 sin(x) is evidently non-convex in both arguments, but {f(x, \u03b8) : \u03b8 \u2208 R} is convex for each x. On the other hand {\u03b8x : \u03b8 \u2208 {\u22121, 1}} is non-convex because of a non-convex admissible set.\nFinally, we remark that in the original derivation of the PMP for continuous-time control systems (Boltyanskii et al., 1960) (i.e. x\u0307s,t = ft(xs,t, \u03b8t), t \u2208 [0, T ] in place of Eq. (1)), the convexity condition can be removed due to the \u201cconvexifying\u201d effect of integration with respect to time (Halkin, 1966; Warga, 1962). Hence, the convexity condition is purely an artifact of discrete-time dynamical systems."}, {"heading": "3. The Method of Successive Approximations", "text": "The PMP (Eq. (4)-(6)) gives us a set of necessary conditions an optimal solution to (2) must satisfy. However, it does not tell us how to find one such solution. The goal of this section is to discuss algorithms for solving (2) based on the maximum principle.\nOn closer inspection of Eq. (4)-(6), one can see that they each represent a manifold in solution space consisting of all possible \u03b8, {xs, s \u2208 [S]} and {ps, s \u2208 [S]}, and the intersection of these three manifolds must contain an optimal solution, if one exists. Consequently, an iterative projection method that successively projects a guessed solution onto each of the manifolds is natural. This is the method of successive approximations (MSA), which was first introduced to solve continuous-time optimal control problems (Krylov & Chernousko, 1962; Chernousko & Lyubushin, 1982). Let us now outline a discrete-time version.\nStart from an initial guess \u03b80 := {\u03b80t , t = 0, . . . , T \u2212 1}. For each sample s, we define x\u03b8 0\ns := {x\u03b8 0 s,t : t = 0, . . . , T} by the dynamics\nx\u03b8 0 s,t+1 = ft(x \u03b80 s,t, \u03b8 0 t ), x \u03b80 s,0 = xs,0, (7)\nfor t = 0, . . . , T \u2212 1. Intuitively, this is a projection onto the manifold defined by Eq. (4). Next, we perform the projection onto the manifold defined by Eq. (5), i.e. we define p\u03b8 0\ns := {p\u03b8 0 s,t : t = 0, . . . , T} by the backward dynamics\np\u03b8 0 s,t = \u2207xH(x\u03b8 0 s,t, p \u03b80 s,t+1, \u03b8 0 t ), p \u03b80 s,T = \u2212 1S\u2207\u03a6s(x \u03b80\ns,T ), (8)\nfor t = T \u2212 1, . . . , 0. Finally, we project onto manifold defined by Eq. (6) by performing Hamiltonian maximization to obtain \u03b81 := {\u03b81t : t = 0, . . . , T \u2212 1} with\n\u03b81t = arg max \u03b8\u2208\u0398t S\u2211 s=1 Ht(x \u03b80 s,t, p \u03b80 s,t+1, \u03b8). t = 0, . . . , T \u2212 1. (9) The steps (7)-(9) are then repeated until convergence. We summarize the basic MSA algorithm in Alg. 1.\nLet us contrast the MSA with gradient-descent based methods. Similar to the formulation of the PMP, at no point did we take the derivative of any quantity with respect to \u03b8. Hence, we can in principle apply this to problems that\nAlgorithm 1 Basic MSA Initialize: \u03b80 = {\u03b80t \u2208 \u0398t : t = 0 . . . , T \u2212 1}; for k = 0 to #Iterations do x\u03b8 k s,t+1 = ft(x \u03b8k s,t, \u03b8 k t ), x\u03b8 k\ns,0 = xs,0, \u2200s, t; p\u03b8 k s,t = \u2207xHt(x\u03b8 k s,t, p \u03b8k s,t+1, \u03b8 k t ), p \u03b8k\ns,T = \u2212 1S\u2207\u03a6s(xs,T ), \u2200s, t; \u03b8k+1t = arg max\u03b8\u2208\u0398t \u2211S s=1 Ht(x \u03b8k s,t, p \u03b8k\ns,t+1, \u03b8) for t = 0, . . . , T \u2212 1;\nend for\nare not differentiable with respect to \u03b8. However, the catch is that the Hamiltonian maximization step (9) may not be trivial to evaluate. Nevertheless, observe that the maximization step is decoupled across different layers of the neural network, and hence it is a much smaller problem than the original optimization problem, and its solution method can be parallelized. Alternatively, as seen in Sec. 4, one can exploit cases where the maximization step has explicit solutions.\nThe basic MSA (Alg. 1 can be shown to converge for problems where ft is linear and the costs \u03a6s, Lt are quadratic (Aleksandrov, 1968). In general, however, unless a good initial condition is given, the MSA may diverge. Let us understand the nature of such phenomena by obtaining rigorous error estimates per-iteration of Eq. (7)-(9)."}, {"heading": "3.1. An Error Estimate for the MSA", "text": "In this section, we derive a rigorous error estimate for the MSA, which can help us understand its dynamics. Let us define Wt := conv{x \u2208 Rdt : \u2203\u03b8 and s s.t. x\u03b8s,t = x}, where x\u03b8t is defined according to Eq. (7). This is the convex hull of all states reachable at layer t by some initial sample and some choice of the values for the trainable parameters. Let us now make the following assumptions:\n(A1) \u03a6s is twice continuously differentiable, with \u03a6s and \u2207\u03a6s satisfying a Lipschitz condition, i.e. there exists K > 0 such that for all x, x\u2032 \u2208WT and s \u2208 [S]\n|\u03a6s(x)\u2212 \u03a6s(x\u2032)|+ \u2016\u2207\u03a6s(x)\u2212\u2207\u03a6s(x\u2032)\u2016 \u2264 K\u2016x\u2212 x\u2032\u2016\n(A2) ft(\u00b7, \u03b8) and Lt(\u00b7, \u03b8) are twice continuously differentiable in x, with ft,\u2207xft, Lt,\u2207xLt satisfying Lipschitz conditions in x uniformly in t and \u03b8, i.e. there exists K > 0 such that\n\u2016ft(x, \u03b8)\u2212 ft(x\u2032, \u03b8)\u2016+ \u2016\u2207xft(x, \u03b8)\u2212\u2207xft(x\u2032, \u03b8)\u20162 + |Lt(x, \u03b8)\u2212 Lt(x\u2032, \u03b8)|+ \u2016\u2207xLt(x, \u03b8)\u2212\u2207xLt(x\u2032, \u03b8)\u2016 \u2264 K\u2016x\u2212 x\u2032\u2016\nfor all x, x\u2032 \u2208Wt, \u03b8 \u2208 \u0398t and t = 0, . . . , T \u2212 1.\nAgain, let us discuss these assumptions with respect to neural networks. Note that both assumptions are more easily\nsatisfied if each Wt is bounded, which is usually implied by the boundedness of \u0398t. Although this is not typically true in principle, we can safely assume this in practice by truncating weights that are too large in magnitude. Consequently, (A1) is not very restrictive, since many commonly employed loss functions (mean-square, soft-max with crossentropy) satisfy these assumptions. In (A2), the regularity assumption on Lt is again not an issue, because we mostly take Lt to be independent of x. On the other hand, the regularity of ft with respect to x is sometimes restrictive. For example, ReLU activations does not satisfy (A2) due to non-differentiability. Nevertheless, any suitably mollified version (like Soft-plus) does satisfy it. Moreover, tanh and sigmoid activations also satisfy (A2). Finally, unlike in Theorem 1, we do not assume the convexity of the sets {ft(x, \u03b8) : \u03b8 \u2208 \u0398t} and {Lt(x, \u03b8) : \u03b8 \u2208 \u0398t}, and hence the results in this section applies to discrete-weight neural networks considered in Sec. 4. With the above assumptions, we prove the following estimate. Theorem 2 (Error Estimate for Discrete MSA). Let assumptions (A1) and (A2) be satisfied. Then, there exists a constant C > 0, independent of S, \u03b8 and \u03c6, such that for any \u03b8,\u03c6 \u2208 \u0398, we have\nJ(\u03c6)\u2212 J(\u03b8)\n\u2264\u2212 T\u22121\u2211 t=0 S\u2211 s=1 Ht(x \u03b8 s,t, p \u03b8 s,t+1, \u03c6t)\u2212Ht(x\u03b8s,t, p\u03b8s,t+1, \u03b8t) (10)\n+ C\nS T\u22121\u2211 t=0 S\u2211 s=1 \u2016ft(x\u03b8s,t, \u03c6t)\u2212 ft(x\u03b8s,t, \u03b8t)\u20162 (11)\n+ C\nS T\u22121\u2211 t=0 S\u2211 s=1 \u2016\u2207xft(x\u03b8s,t, \u03c6t)\u2212\u2207xft(x\u03b8s,t, \u03b8t)\u201622, (12)\n+ C\nS T\u22121\u2211 t=0 S\u2211 s=1 \u2016\u2207xLt(x\u03b8s,t, \u03c6t)\u2212\u2207xLt(x\u03b8s,t, \u03b8t)\u20162, (13)\nwhere x\u03b8s , p \u03b8 s are defined by Eq. (7) and (8).\nProof. The proof follows from elementary estimates and a discrete Gronwall\u2019s lemma. See Appendix B.\nTheorem 2 relates the decrement of the total objective function J with respect to the iterative projection steps of the MSA. Intuitively, Theorem 2 says that the Hamiltonian maximization step (9) is generally the right direction, because a large magnitude of (10) results in higher loss improvement. However, whenever we change the parameters from \u03b8 to \u03c6 (e.g. during the maximization step (9)), we incur nonnegative penalty terms (11)-(13). Observe that these penalty terms vanish if \u03c6 = \u03b8, or more generally, when the state and co-state equations (Eq. (7), (8)) are still satisfied when \u03b8 is replaced by \u03c6. In other words, these terms measure the distance from manifolds defined by the state and co-state equations when the parameter changes. Alg. 1 diverges\nwhen these penalty terms dominate the gains from (10). This insight can point us in the right direction of developing convergent modifications of the basic MSA. We shall now discuss this in the context of some specific applications."}, {"heading": "4. Neural Networks with Discrete Weights", "text": "We now turn to the application of the theory developed in the previous section on the MSA, which is a PMP-based numerical method for training deep neural networks. As discussed previously, the main strength of the PMP and MSA formalism is that we do not rely on gradient-descent type updates. This is particularly useful when one considers neural networks with (some) trainable parameters that can only take values in a discrete set. Then, any small gradient update to the parameters will almost always be infeasible. In this section, we will consider two such cases: binary networks, where weights are restricted to {\u22121,+1}; and ternary networks, where weights are selected from {\u22121,+1, 0}. These networks are potentially useful for low-memory devices as storing the trained weights requires less memory. In this section, we will modify the MSA so that we can train these networks in a principled way."}, {"heading": "4.1. Binary Networks", "text": "Binary neural networks are those with binary trainable layers, e.g. in the fully connected case,\nft(x, \u03b8) = \u03b8x (14)\nwhere \u03b8 \u2208 \u0398t = {\u22121,+1}dt\u00d7dt+1 is a binary matrix. A similar form of ft holds for convolution neural networks after reshaping, except that \u0398t is now the set of Toeplitz binary matrices. Hereafter, we will consider the fully connected case for simplicity of exposition. It is also natural to set the regularization to 0 since there is in general no preference between +1 or \u22121. Thus, the Hamiltonian has the form\nHt(x, p, \u03b8) = p \u00b7 \u03b8x.\nConsequently, the Hamiltonian maximization step (9) has explicit solution, given by\narg max \u03b8\u2208\u0398t S\u2211 s=1 Ht(x \u03b8k s,t, p \u03b8k s,t+1, \u03b8) = sign(M \u03b8k t )\nwhere M\u03b8t := \u2211S s=1 p \u03b8 s,t+1(x \u03b8 s,t)\nT . Note that the sign function is applied element-wise. If [M\u03b8t ]ij = 0, then the argmax is arbitrary. Using Theorem 2 with the form of ft given\nby (14) and the fact that Lt \u2261 0, we get\nJ(\u03c6)\u2212 J(\u03b8) \u2264\u2212 T\u22121\u2211 t=0 S\u2211 s=1 Ht(x \u03b8k s,t, p \u03b8k s,t+1, \u03b8)\n+ C\nS T\u22121\u2211 t=0 (1 + S\u2211 s=1 \u2016x\u03b8s,t\u20162)\u2016\u03c6t \u2212 \u03b8t\u20162F ,\nNote that we have used the inequality \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u2016F . Assuming that \u2016x\u03b8s,t\u2016 is O(1), we may then decrease J by not only maximizing the Hamiltonian, but also penalizing the difference \u2016\u03c6t \u2212 \u03b8t\u2016F , i.e. for each k and t we set\n\u03b8k+1t = arg max \u03b8\u2208\u0398t [ S\u2211 s=1 Ht(x \u03b8k s,t, p \u03b8k s,t+1, \u03b8)\u2212 \u03c1k,t\u2016\u03b8 \u2212 \u03b8k\u20162F ] (15)\nfor some penalization parameters \u03c1k,t > 0. This again has the explicit solution\n[\u03b8k+1t ]ij =\n{ sign([M\u03b8 k\nt ]ij) |[M\u03b8 k t ]ij | \u2265 2\u03c1k,t [\u03b8kt ]ij otherwise (16)\nTherefore, we simply replace the parameter update step in Alg. 1 with (16). Furthermore, to deal with mini-batches, we keep a moving average of M\u03b8 k\nt across different minibatches and use the averaged value to update our parameters. It is found empirically that this further stabilizes the algorithm. Note that the assumption \u2016x\u03b8s,t\u2016 is O(1) can be achieved by normalization, e.g. batch-normalization (Ioffe & Szegedy, 2015). We summarize the algorithm in Alg. 2. Further algorithmic details are found in Appendix D, where we also discuss the choice of hyper-parameters and the convergence of the algorithm for a simple binary regression problem. A rigorous proof of convergence in the general case is beyond the scope of this work, but we demonstrate via experiments below that the algorithm performs well on the tested benchmarks.\nWe apply Alg. 2 to train binary neural networks on various benchmark datasets and compare the results from previous work on training binary-weight neural networks (Courbariaux et al., 2015). We consider a fully-connected neural network on MNIST (LeCun, 1998), as well as (shallow) convolutional networks on CIFAR-10 (Krizhevsky & Hinton, 2009) and SVHN (Netzer et al., 2011) datasets. The network structures are mostly identical to those considered in Courbariaux et al. (2015) for ease of comparison. Complete implementation and model details are found in Appendix D. The graphs of training/testing loss and error rates are shown in Fig. 1. We observe that our algorithm performs well in terms of an optimization algorithm, as measured by the training loss and error rates. For the harder datasets\nAlgorithm 2 Binary MSA Initialize: \u03b80, M 0 ;\nHyper-parameters: \u03c1k,t, \u03b1k,t; for k = 0 to #Iterations do x\u03b8 k s,t+1 = ft(x \u03b8k s,t, \u03b8 k t ) \u2200s, t\nwith x\u03b8 k\ns,0 = xs,0;\np\u03b8 k s,t = \u2207xHt(x\u03b8 k s,t, p \u03b8k s,t+1, \u03b8 k t ) \u2200s, t\nwith p\u03b8 k\ns,T = \u2212 1S\u2207\u03a6s(xs,T ); M k+1 t = \u03b1k,tM k t + (1\u2212 \u03b1k,t) \u2211S s=1 p \u03b8k s,t+1(x \u03b8k s,t) T\n[\u03b8k+1t ]ij =\n{ sign([M k+1 t ]ij) |[M k+1 t ]ij | \u2265 2\u03c1k,t\n[\u03b8kt ]ij otherwise \u2200t, i, and j;\nend for\n(CIFAR-10 and SVHN), we have rapid convergence but worse test loss and error rates at the end, possibly due to overfitting. We note that in (Courbariaux et al., 2015), many regularization strategies are performed. We expect that similar techniques must be employed to improve the testing performance. However, these issues are out of the scope of the optimization framework of this paper. Note that we also compared the results of BinaryConnect without regularization strategies such as stochastic binarization, but the results are similar in that our algorithm converges very fast with very low training losses, but sometimes overfits."}, {"heading": "4.2. Ternary Networks", "text": "We shall consider another case where the network weights are allowed to take on values in {\u22121,+1, 0}. In this case, our goal is to explore the sparsification of the network. To this end, we shall take Lt(x, \u03b8) = \u03bbt\u2016\u03b8\u20162F for some parameter \u03bbt. Note that since the weights are restricted to the ternary set, any component-wise `p regularization for p > 0 are identical. The higher the \u03bbt values, the more sparse the solution will be.\nAs in Sec. 4.1, we can write down the Hamiltonian for a fully connected ternary layer as\nHt(x, p, \u03b8) = p \u00b7 \u03b8x\u2212 1S\u03bbt\u2016\u03b8\u2016 2 F .\nThe derivation of the ternary algorithm then follows directly from those in Sec. 4.1, but with the new form of Hamiltonian above and that \u0398t = {\u22121,+1, 0}dt\u00d7dt+1 . Maximizing the augmented Hamiltonian (15) with Ht as defined above, we obtain the ternary update rule\n[\u03b8k+1t ]ij =  +1 [M\u03b8 k t ]ij \u2265 \u03c1k,t(1\u2212 2[\u03b8kt ]ij) + \u03bbt \u22121 [M\u03b8kt ]ij \u2264 \u2212\u03c1k,t(1 + 2[\u03b8kt ]ij)\u2212 \u03bbt 0 otherwise.\n(17) We replace the parameter update step in Alg. 2 by (17) to obtain the MSA algorithm for ternary networks. For completeness, we give the full ternary algorithm in Alg. 3. We\nnow test the ternary algorithm on the same benchmarks used in Sec. 4.1 and the results are shown in Fig. 2. Observe that the performance on training and testing datasets is similar to the binary case (Fig. 1), but the ternary networks achieve high degrees of sparsity in the weights, with only 0.5-2.5% of the trained weights being non-zero, depending on the dataset. This potentially offers significant memory savings compared to its binary or full floating precision counterparts."}, {"heading": "5. Discussion and Related Work", "text": "We begin with a discussion of the results presented thus far. We first introduced the viewpoint that deep learning can be regarded as a discrete-time optimal control problem. Consequently, an important result in optimal control theory, the Pontryagin\u2019s maximum principle, can be applied to give a set of necessary conditions for optimality. These are in general stronger conditions than the usual optimality conditions based on the vanishing of first-order partial derivatives. Moreover, they apply to broader contexts such as problems with constraints on the trainable parameters or problems that are non-differentiable in the trainable parameters. However, we note that specific assumptions regarding the convexity\nAlgorithm 3 Ternary MSA Initialize: \u03b80, M 0 ;\nHyper-parameters: \u03c1k,t, \u03b1k,t; for k = 0 to #Iterations do x\u03b8 k s,t+1 = ft(x \u03b8k s,t, \u03b8 k t ) \u2200s, t\nwith x\u03b8 k\ns,0 = xs,0;\np\u03b8 k s,t = \u2207xHt(x\u03b8 k s,t, p \u03b8k s,t+1, \u03b8 k t ) \u2200s, t\nwith p\u03b8 k\ns,T = \u2212 1S\u2207\u03a6s(xs,T ); M k+1 t = \u03b1k,tM k t + (1\u2212 \u03b1k,t) \u2211S s=1 p \u03b8k s,t+1(x \u03b8k s,t) T\n[\u03b8k+1t ]ij =  +1 [M k+1 t ]ij \u2265 \u03c1k,t(1\u2212 2[\u03b8kt ]ij) + \u03bbt\n\u22121 [Mk+1t ]ij \u2264 \u2212\u03c1k,t(1 + 2[\u03b8kt ]ij)\u2212 \u03bbt 0 otherwise.\n\u2200t, i, and j; end for\nof some sets must be satisfied. We showed that they are justified for conventional neural networks, but not necessarily so for all neural networks (e.g. binary, ternary networks).\nNext, based on the PMP, we introduced an iterative projection technique, the discrete method of successive approximations (MSA), to find an optimal solution of the learning problem. A rigorous error estimate (Theorem 2) is derived for the discrete MSA, which can be used to both understand its dynamics and to derive useful algorithms. This should be viewed as the main theoretical result of the present paper. Note that the usual back-propagation with gradient descent can be regarded as a simple modification of the MSA, if differentiability conditions are assumed (see Appendix C). Nevertheless, we note that Theorem 2 itself does not assume any regularity conditions with respect to the trainable parameters. Moreover, neither does it require the convexity conditions in Theorem 1, and hence applies to a wider range of neural networks, including those in Sec. 4. All results up to this point apply to general neural networks (assuming that the respective conditions are satisfied), and are not specific to the applications presented subsequently.\nIn the last part of this work, we apply our results to devise training algorithms for discrete-weight neural networks, i.e. those with trainable parameters that can only take values in a discrete set. Besides potential applications in model deployment in low-memory devices, the main reasons for choosing such applications are two-fold. First, gradientdescent updates are not applicable by itself because small updates to parameters are prohibited by the discrete equality constraint on the trainable parameters. However, our method based on the MSA is applicable since it does not perform gradient-descent updates. Second, in such applications the potentially expensive Hamiltonian maximization steps in the MSA have explicit solutions. This makes MSA an attractive optimization method for problems of this nature. In Sec 4, we demonstrate the effectiveness of our methods on various benchmark datasets. Interestingly, the ternary\nnetwork exhibits extremely sparse weights that perform almost as well as its binary counter-part (see Fig. 2). Also, the phenomena of overfitting in Fig. 1 and 2 are interesting as overfitting is generally less common in stochastic gradient based optimization approaches. This seems to suggest that the MSA based methods optimize neural networks in a rather different way.\nLet us now put our work in the context of the existing literature. First, the optimal control approach we adopt is quite different from the prevailing viewpoint of nonlinear programming (Bertsekas, 1999; Bazaraa et al., 2013; Kuhn & Tucker, 2014) and the analysis of the derived gradient-based algorithms (Moulines, 2011; Shamir & Zhang, 2013; Bach & Moulines, 2013; Xiao & Zhang, 2014; Shalev-Shwartz & Zhang, 2014) for the training of deep neural networks. In particular, the PMP (Thm. 1) and the MSA error estimate (Thm. 2) do not assume differentiability and do not characterize optimality via gradients (or sub-gradients) with respect to trainable parameters. In this sense, it is a stronger and more robust condition, albeit sometimes requiring different assumptions. The optimal control and dynamical\nsystems viewpoint has been discussed in the context of deep learning in E (2017); Li et al. (2018) and dynamical systems based discretization schemes has been introduced in Haber & Ruthotto (2017); Chang et al. (2017). Most of these works have theoretical basis in continuous-time dynamical systems. In particular, Li et al. (2018) analyzed continuous-time analogues of neural networks in the optimal control framework and derived MSA-based algorithms in continuous time. In contrast, the present work presents a discrete-time formulation, which is natural in the usual context of deep learning. The discrete PMP turns out to be more subtle, as it requires additional assumptions of convexity of reachable sets (Thm. 1). Note also that unlike the estimates derived in Li et al. (2018), Thm. 2 holds rigorously for discrete-time neural networks. The present method for stabilizing the MSA is also different from that in Li et al. (2018), where augmented Lagrangian type of modifications are employed. The latter would not be effective here because weights cannot be updated infinitesimally without violating the binary/ternary constraint. Moreover, the present methods that rely on explicit solutions of Hamiltonian maximization are fast (comparable to SGD) on a wall-clock basis.\nIn the deep learning literature, the connection between optimal control and deep learning has been qualitative discussed in LeCun (1988) and applied to the development of automatic differentiation and back-propagation (Bryson, 1975; Baydin et al., 2015). However, there are relatively fewer works relating optimal control algorithms to training neural networks beyond the classical gradient-descent with back-propagation. Optimal control based strategies in hyper-parameter tuning has been discussed in Li et al. (2017b).\nIn the continuous-time setting, the Pontryagin\u2019s maximum principle and the method of successive approximations have a long history, with a large body of relevant literature including, but not limited to Boltyanskii et al. (1960); Pontryagin (1987); Bryson (1975); Bertsekas (1995); Athans & Falb (2013); Krylov & Chernousko (1962); Aleksandrov (1968); Krylov & Chernousko (1972); Chernousko & Lyubushin (1982); Lyubushin (1982). The discrete-time PMP have been studied in Halkin (1966); Holtzman (1966a); Holtzman & Halkin (1966); Holtzman (1966b); Canon et al. (1970), where Theorem 1 and its extensions are proved. To the best of our knowledge, the discrete-time MSA and its quantitative analysis have not been performed in either the deep learning or the optimal control literature.\nSec. 4 concerns the application of the MSA, in particular Thm. 2, to develop training algorithms for binary and ternary neural networks. There are a number of prior work exploring the training of similar neural networks, such as Courbariaux et al. (2015); Hubara et al. (2016); Rastegari et al.\n(2016); Tang et al. (2017); Li et al. (2016); Zhu et al. (2016). Theoretical analysis for the case of convex loss functions is carried out in Li et al. (2017a). Our point of numerical comparison for the binary MSA algorithm is Courbariaux et al. (2015), where optimization of binary networks is based on shadow variables with full floating-point precision that is iteratively truncated to obtain gradients. We showed in Sec. 4.1 that the binary MSA is competitive as a training algorithm, but is in need of modifications to reduce overfitting for certain datasets. Training ternary networks has been discussed in Hwang & Fan (1967); Kim et al. (2014); Li et al. (2016); Zhu et al. (2016). The difference in our ternary formulation is that we explore the sparsification of networks using a regularization parameter. In this sense it is related to compression of neural networks (e.g. Han et al. (2015)), but our approach trains a network that is naturally ternary, and compression is achieved during training by a regularization term. Generally, a contrasting aspect of our approach from the aforementioned literature is that the theory of optimal control, together with Theorem. 2, provide a theoretical basis for the development of our algorithms. Nevertheless, further work is required to rigorously establish the convergence of these algorithms. We also mention a recent work (Yin et al., 2018) which analyzes quantized networks and develops algorithms based on relaxing the discrete-weight constraints into continuous regularizers. Lastly, there are also analyses of quantized networks from a statistical-mechanical viewpoint (Baldassi et al., 2015; 2016a;b; 2017)."}, {"heading": "6. Conclusion and Outlook", "text": "In this paper, we have introduced the discrete-time optimal control viewpoint of deep learning. In particular, the PMP and the MSA form an alternative theoretical and algorithmic basis for deep learning that may apply to broader contexts. As an application of our framework, we considered the training of binary and ternary neural networks, in which we develop effective algorithms based on optimal control.\nThere are certainly many avenues of future work. An interesting mathematical question is the applicability of the PMP for discrete-weight neural networks, which does not satisfy the convexity assumptions in Theorem 1. It will be desirable to find the condition under which rigorous statements can be made. Another question is to establish the convergence of the algorithms presented."}], "year": 2018, "references": [{"title": "On the accumulation of perturbations in the linear systems with two coordinates", "authors": ["V.V. Aleksandrov"], "venue": "Vestnik MGU,", "year": 1968}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "authors": ["F. Bach", "E. Moulines"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses", "authors": ["C. Baldassi", "A. Ingrosso", "C. Lucibello", "L. Saglietti", "R. Zecchina"], "venue": "Physical review letters,", "year": 2015}, {"title": "Learning may need only a few bits of synaptic precision", "authors": ["C. Baldassi", "F. Gerace", "C. Lucibello", "L. Saglietti", "R. Zecchina"], "venue": "Physical Review E,", "year": 2016}, {"title": "On the role of synaptic stochasticity in training low-precision neural networks", "authors": ["C. Baldassi", "F. Gerace", "H.J. Kappen", "C. Lucibello", "L. Saglietti", "E. Tartaglione", "R. Zecchina"], "venue": "arXiv preprint arXiv:1710.09825,", "year": 2017}, {"title": "Automatic differentiation in machine learning: a survey", "authors": ["A.G. Baydin", "B.A. Pearlmutter", "A.A. Radul", "J.M. Siskind"], "venue": "arXiv preprint arXiv:1502.05767,", "year": 2015}, {"title": "Nonlinear programming: theory and algorithms", "authors": ["M.S. Bazaraa", "H.D. Sherali", "C.M. Shetty"], "year": 2013}, {"title": "Dynamic programming and optimal control, volume 1", "authors": ["D.P. Bertsekas"], "venue": "Athena scientific Belmont, MA,", "year": 1995}, {"title": "The theory of optimal processes. I. The maximum principle", "authors": ["V.G. Boltyanskii", "R.V. Gamkrelidze", "L.S. Pontryagin"], "venue": "Technical report, TRW Space Tochnology Labs,", "year": 1960}, {"title": "Applied optimal control: optimization, estimation and control", "authors": ["A.E. Bryson"], "venue": "CRC Press,", "year": 1975}, {"title": "Theory of optimal control and mathematical programming", "authors": ["M.D. Canon", "C.D. Cullum Jr.", "E. Polak"], "venue": "McGrawHill Book Company,", "year": 1970}, {"title": "Reversible architectures for arbitrarily deep residual neural networks", "authors": ["B. Chang", "L. Meng", "E. Haber", "L. Ruthotto", "D. Begert", "E. Holtham"], "venue": "arXiv preprint arXiv:1709.03698,", "year": 2017}, {"title": "Method of successive approximations for solution of optimal control problems", "authors": ["F.L. Chernousko", "A.A. Lyubushin"], "venue": "Optimal Control Applications and Methods,", "year": 1982}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "authors": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "A proposal on machine learning via dynamical systems", "authors": ["E W"], "venue": "Communications in Mathematics and Statistics,", "year": 2017}, {"title": "Stable architectures for deep neural networks", "authors": ["E. Haber", "L. Ruthotto"], "venue": "arXiv preprint arXiv:1705.03341,", "year": 2017}, {"title": "A maximum principle of the pontryagin type for systems described by nonlinear difference equations", "authors": ["H. Halkin"], "venue": "SIAM Journal on control,", "year": 1966}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "authors": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "year": 2015}, {"title": "Convexity and the maximum principle for discrete systems", "authors": ["J. Holtzman"], "venue": "IEEE Transactions on Automatic Control,", "year": 1966}, {"title": "On the maximum priciple for nonlinear discrete-time systems", "authors": ["J. Holtzman"], "venue": "IEEE Transactions on Automatic Control,", "year": 1966}, {"title": "Discretional convexity and the maximum principle for discrete systems", "authors": ["J.M. Holtzman", "H. Halkin"], "venue": "SIAM Journal on Control,", "year": 1966}, {"title": "Binarized neural networks", "authors": ["I. Hubara", "M. Courbariaux", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "In Advances in neural information processing systems,", "year": 2016}, {"title": "A discrete version of pontryagin\u2019s maximum principle", "authors": ["C. Hwang", "L. Fan"], "venue": "Operations Research,", "year": 1967}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["S. Ioffe", "C. Szegedy"], "venue": "In International conference on machine learning,", "year": 2015}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "authors": ["J. Kim", "K. Hwang", "W. Sung"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "authors": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "authors": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical Report. University of Toronto,", "year": 2009}, {"title": "On the method of successive approximations for solution of optimal control problems", "authors": ["I.A. Krylov", "F.L. Chernousko"], "venue": "J. Comp. Mathem. and Mathematical Physics,", "year": 1962}, {"title": "An algorithm for the method of successive approximations in optimal control problems", "authors": ["I.A. Krylov", "F.L. Chernousko"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "year": 1972}, {"title": "Nonlinear programming", "authors": ["H.W. Kuhn", "A.W. Tucker"], "venue": "In Traces and emergence of nonlinear programming,", "year": 2014}, {"title": "A theoretical framework for back-propagation", "authors": ["Y. LeCun"], "venue": "In The Connectionist Models Summer School,", "year": 1988}, {"title": "The MNIST database of handwritten digits", "authors": ["Y. LeCun"], "venue": "http://yann.lecun.com/exdb/mnist/,", "year": 1998}, {"title": "Training quantized nets: A deeper understanding", "authors": ["H. Li", "S. De", "Z. Xu", "C. Studer", "H. Samet", "T. Goldstein"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Stochastic modified equations and adaptive stochastic gradient algorithms", "authors": ["Q. Li", "C. Tai"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "Maximum principle based algorithms for deep learning", "authors": ["Q. Li", "L. Chen", "C. Tai"], "venue": "Journal of Machine Learning Research,", "year": 2018}, {"title": "Modifications of the method of successive approximations for solving optimal control problems", "authors": ["A.A. Lyubushin"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "year": 1982}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "authors": ["Moulines", "Eric", "F. R"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"title": "Reading digits in natural images with unsupervised feature learning", "authors": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "year": 2011}, {"title": "Discrete-time control systems, volume 2", "authors": ["K. Ogata"], "venue": "Prentice Hall Englewood Cliffs, NJ,", "year": 1995}, {"title": "Mathematical theory of optimal processes", "authors": ["L.S. Pontryagin"], "venue": "CRC Press,", "year": 1987}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "authors": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "In European Conference on Computer Vision,", "year": 2016}, {"title": "A stochastic approximation method", "authors": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics,", "year": 1951}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "authors": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming,", "year": 2014}, {"title": "How to train a compact binary neural network with high accuracy", "authors": ["W. Tang", "G. Hua", "L. Wang"], "venue": "In AAAI,", "year": 2017}, {"title": "Relaxed variational problems", "authors": ["J. Warga"], "venue": "Journal of Mathematical Analysis and Applications,", "year": 1962}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "authors": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization,", "year": 2014}, {"title": "Binaryrelax: A relaxation approach for training deep neural networks with quantized weights", "authors": ["P. Yin", "S. Zhang", "J. Lyu", "S. Osher", "Y. Qi", "J. Xin"], "venue": "arXiv preprint arXiv:1801.06313,", "year": 2018}, {"title": "Adadelta: an adaptive learning rate method", "authors": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "year": 2012}], "id": "SP:89bd889635ea7938dbda8aa6ac952cdb64c162e6", "authors": [{"name": "Qianxiao Li", "affiliations": []}, {"name": "Shuji Hao", "affiliations": []}], "abstractText": "Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin\u2019s maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.", "title": "An Optimal Control Approach to Deep Learning and  Applications to Discrete-Weight Neural Networks"}