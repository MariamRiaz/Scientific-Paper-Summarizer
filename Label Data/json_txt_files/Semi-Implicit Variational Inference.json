{"sections": [{"heading": "1. Introduction", "text": "Variational inference (VI) is an optimization based method that is widely used for approximate Bayesian inference. It introduces variational distribution Q over the latent variables to approximate the posterior (Jordan et al., 1999), and its stochastic version is scalable to big data (Hoffman et al., 2013). VI updates the parameters of Q to move it closer to the posterior in each iteration, where the closeness is in general measured by the Kullback\u2013Leibler (KL) divergence from the posterior to Q, minimizing which is shown to be the same as maximizing the evidence lower bound (ELBO) (Jordan et al., 1999). To make it simple to climb the ELBO to a local optimum, one often takes the\n1Department of Statistics and Data Sciences, 2Department of IROM, McCombs School of Business, The University of Texas at Austin, Austin TX 78712, USA. Correspondence to: Mingzhang Yin <mzyin@utexas.edu>, Mingyuan Zhou <mingyuan.zhou@mccombs.utexas.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmean-field assumption that Q is factorized over the latent variables. The optimization problem is further simplified if each latent variable\u2019s distribution is in the same exponential family as its prior, which allows exploiting conditional conjugacy to derive closed-form coordinate-ascent update equations (Bishop & Tipping, 2000; Blei et al., 2017).\nDespite its popularity, VI has a well-known issue in underestimating the variance of the posterior, which is often attributed to the mismatch between the representation power of the variational family that Q is restricted to and the complexity of the posterior, and the use of KL divergence, which is asymmetric, to measure how different Q is from the posterior. This issue is often further amplified in mean-field VI (MFVI), due to the factorized assumption on Q that ignores the dependencies between different factorization components (Wainwright et al., 2008; Blei et al., 2017). While there exists a variety of methods that add some structure back to Q to partially restore the dependencies (Saul & Jordan, 1996; Jaakkola & Jordan, 1998; Hoffman & Blei, 2015; Giordano et al., 2015; Tran et al., 2015; 2016; Han et al., 2016; Ranganath et al., 2016; Maal\u00f8e et al., 2016; Gregor et al., 2015), it is still necessary for Q to have an analytic probability density function (PDF).\nTo further expand the variational family that Q belongs to, there has been significant recent interest in defining Q with an implicit model, which makes the PDF of Q become intractable (Husza\u0301r, 2017; Mohamed & Lakshminarayanan, 2016; Tran et al., 2017; Li & Turner, 2017; Mescheder et al., 2017; Shi et al., 2017). While using an implicit model could make Q more flexible, it makes it no longer possible to directly computing the log density ratio, as required for evaluating the ELBO. Thus, one often resorts to density ratio estimation, which, however, not only adds an additional level of complexity into each iteration of the optimization, but also is known to be a very difficult problem, especially in high-dimensional settings (Sugiyama et al., 2012).\nTo well characterize the posterior while maintaining simple optimization, we introduce semi-implicit VI (SIVI) that imposes a mixing distribution on the parameters of the original Q to expand the variational family with a semi-implicit hierarchical construction. The meaning of \u201csemi-implicit\u201d is twofold: 1) the original Q distribution is required to have an analytic PDF, but its mixing distribution is not subject to\nar X\niv :1\n80 5.\n11 18\n3v 1\n[ st\nat .M\nL ]\n2 8\nM ay\n2 01\n8\nsuch a constraint; and 2) even if both the original Q and its mixing distribution have analytic PDFs, it is common that the marginal of the hierarchy is implicit, that is, having a non-analytic PDF. Our intuition behind SIVI is that even if this marginal is not tractable, its density can be evaluated with Monte Carlo estimation under the semi-implicit hierarchical construction, an expansion that helps model skewness, kurtosis, multimodality, and other characteristics that are exhibited by the posterior but failed to be captured by the original variational family. For MFVI, a benefit of this expansion is restoring the dependencies between its factorization components, as the resulted Q distribution becomes conditionally independent but marginally dependent.\nSIVI makes three major contributions: 1) a reparameterizable implicit distribution can be used as a mixing distribution to effectively expand the richness of the variational family; 2) an analytic conditional Q distribution is used to sidestep the hard problem of density ratio estimation, and is not required to be reparameterizable in conditionally conjugate models; and 3) SIVI sandwiches the ELBO between a lower bound and an upper bound, and derives an asymptotically exact surrogate ELBO that is amenable to direct optimization via stochastic gradient ascent. With a flexible variational family and novel optimization, SIVI bridges the accuracy gap of posterior estimation between VI and Markov chain Monte Carlo (MCMC), which can accurately characterize the posterior using MCMC samples, as will be demonstrated in a variety of Bayesian inference tasks. Code is provided at https://github.com/mingzhang-yin/SIVI"}, {"heading": "2. Semi-Implicit Variational Inference", "text": "In VI, given observations x, latent variables z, model likelihood p(x | z), and prior p(z), we approximate the posterior p(z |x) with variational distribution q(z |\u03c8) that is often required to be explicit. We optimize the variational parameter \u03c8 to minimize KL(q(z |\u03c8)||p(z |x)), the KL divergence from p(z |x) to q(z |\u03c8). Since one may show log p(x) = ELBO + KL(q(z |\u03c8)||p(z |x)), where\nELBO = \u2212Ez\u223cq(z|\u03c8)[log q(z|\u03c8)\u2212 log p(x, z)], (1)\nminimizing KL(q(z |\u03c8)||p(z |x)) is equivalent to maximizing the ELBO (Bishop & Tipping, 2000; Blei et al., 2017). Rather than treating \u03c8 as the variational parameter to be inferred, SIVI regards\u03c8 \u223c q(\u03c8) as a random variable, as described below. Note that when q(\u03c8) degenerates to a point mass density, SIVI reduces to vanilla VI."}, {"heading": "2.1. Semi-Implicit Variational Family", "text": "Assuming \u03c8 \u223c q\u03c6(\u03c8), where \u03c6 denotes the distribution parameter to be inferred, the semi-implicit variational distri-\nbution for z can be defined in a hierarchical manner as\nz \u223c q(z |\u03c8), \u03c8 \u223c q\u03c6(\u03c8).\nMarginalizing the intermediate variable \u03c8 out, we can view z as a random variable drawn from distribution family H indexed by variational parameter \u03c6, expressed as\nH = { h\u03c6(z) : h\u03c6(z) = \u222b \u03c8 q(z |\u03c8)q\u03c6(\u03c8)d\u03c8 } .\nNote q(z |\u03c8) is required to be explicit, but the mixing distribution q\u03c6(\u03c8) is allowed to be implicit. Moreover, unless q\u03c6(\u03c8) is conjugate to q(z |\u03c8), the marginal Q distribution h\u03c6(z) \u2208 H is often implicit. These are the two reasons for referring to the proposed VI as semi-implicit VI (SIVI).\nSIVI requires q(z |\u03c8) to be explicit, and also requires it to either be reparameterizable, which means z \u223c q(z |\u03c8) can be generated by transforming random noise \u03b5 via function f(\u03b5,\u03c8), or allow the ELBO in (1) to be analytic. Whereas the mixing distribution q(\u03c8) is required to be reparameterizable but not necessarily explicit. In particular, SIVI draws from q(\u03c8) by transforming random noise via a deep neural network, which generally leads to an implicit distribution for q(\u03c8) due to a non-invertible transform.\nSIVI is related to the hierarchical variational model (Ranganath et al., 2016; Maal\u00f8e et al., 2016; Agakov & Barber, 2004) in using a hierarchical variational distribution, but, as discussed below, differs from it in allowing an implicit mixing distribution q\u03c6(\u03c8) and optimizing the variational parameter via an asymptotically exact surrogate ELBO. Note as long as q\u03c6(\u03c8) can degenerate to delta function \u03b4\u03c80(\u03c8) for arbitrary \u03c80, the semi-implicit variational family H is a strict expansion of the original Q = {q(z |\u03c80)} family, that is, Q \u2286 H. For MFVI that assumes q(z |\u03c8) =\u220f m q(zm |\u03c8m), this expansion significantly helps restore the dependencies between zm if \u03c8m are not imposed to be independent between each other."}, {"heading": "2.2. Implicit Mixing Distribution", "text": "While restricting q(z |\u03c8) to be explicit, SIVI introduces a mixing distribution q\u03c6(\u03c8) to enhance its representation power. In this paper, we construct q\u03c6(\u03c8) with an implicit distribution that generates its random samples via a stochastic procedure but may not allow a pointwise evaluable PDF. More specifically, an implicit distribution (Mohamed & Lakshminarayanan, 2016; Tran et al., 2017), consisting of a source of randomness q( ) for \u2208 Rg and a deterministic transform T\u03c6 : Rg \u2192 Rd, can be constructed as \u03c8 = T\u03c6( ), \u223c q( ), with PDF\nq\u03c6(\u03c8) = \u2202 \u2202\u03c81 . . . \u2202\u2202\u03c8d \u222b T\u03c6( )\u2264\u03c8 q( )d . (2)\nWhen T\u03c6 is invertible and the integration is tractable, the PDF of \u03c8 can be calculated with (2), but this is not the case\nin general and hence q\u03c6(\u03c8) is often implicit. When T\u03c6(\u00b7) is chosen as a deep neural network, thanks to its high modeling capacity, q\u03c6(\u03c8) can be highly flexible and the dependencies between the elements of \u03c8 can be well captured.\nPrevalently used in the study of thermodynamics, ecology, epidemiology, and differential equation systems, implicit distributions have only been recently introduced in VI to parameterize q(z |\u03c8) (Li & Turner, 2017; Mescheder et al., 2017; Husza\u0301r, 2017; Tran et al., 2017). Using implicit distributions with intractable PDF increases flexibility but substantially complicates the optimization problem for VI, due to the need to estimate log density ratios involving intractable PDFs, which is particularly challenging in high dimensions (Sugiyama et al., 2012). By contrast, taking a semi-implicit construction, SIVI offers the best of both worlds: constructing a highly flexible variational distribution, without sacrificing the key benefit of VI in converting posterior inference into an optimization problem that is simple to solve. Below we develop a novel optimization algorithm that exploits SIVI\u2019s semi-implicit construction."}, {"heading": "3. Optimization for SIVI", "text": "To optimize the variational parameters of SIVI, below we first derive for the ELBO a lower bound, climbing which, however, could drive the mixing distribution q\u03c6(\u03c8) towards a point mass density. To prevent degeneracy, we add a nonnegative regularization term, leading to a surrogate ELBO that is asymptotically exact, as can be further tightened by importance reweighting. To sandwich the ELBO, we also derive for the ELBO an upper bound, optimizing which, however, may lead to divergence. We further show that this upper bound can be corrected to a tighter upper bound that monotonically converges from above towards the ELBO."}, {"heading": "3.1. Lower Bound of ELBO", "text": "Theorem 1 (Cover & Thomas (2012)). The KL divergence from distribution p(z) to distribution q(z), expressed as KL(q(z)||p(z)), is convex in the pair (q(z), p(z)).\nFixing the distribution p(z) in Theorem 1, KL divergence can be viewed as a convex functional in q(z). As in Appendix A, with Theorem 1 and Jensen\u2019s inequality, we have\nKL(E\u03c8q(z |\u03c8)||p(z)) \u2264 E\u03c8KL(q(z |\u03c8)||p(z)). (3) Thus, using h\u03c6(z) = E\u03c8\u223cq\u03c6(\u03c8)q(z |\u03c8) as the variational distribution, SIVI has a lower bound for its ELBO as\nL(q(z |\u03c8), q\u03c6(\u03c8)) = E\u03c8\u223cq\u03c6(\u03c8)Ez\u223cq(z |\u03c8) log p(x,z)q(z |\u03c8) =\u2212 E\u03c8\u223cq\u03c6(\u03c8)KL(q(z |\u03c8)||p(z|x)) + log p(x) \u2264\u2212 KL(E\u03c8\u223cq\u03c6(\u03c8)q(z |\u03c8)||p(z|x)) + log p(x) = L = Ez\u223ch\u03c6(z) log p(x,z)h\u03c6(z) . (4)\nThe PDF of h\u03c6(z) is often intractable, especially if q\u03c6(\u03c8) is implicit, prohibiting a Monte Carlo estimation of the ELBO L. By contrast, a Monte Carlo estimation of L only requires q(z |\u03c8) to have an analytic PDF and q\u03c6(\u03c8) to be convenient to sample from. It is this separation of evaluation and sampling that allows SIVI to combine an explicit q(z |\u03c8) with an implicit q\u03c6(\u03c8) that is as powerful as needed, while maintaining tractable computation."}, {"heading": "3.2. Degeneracy and Regularization", "text": "A direct optimization of the lower bound L in (4), however, can suffer from degeneracy, as shown in the proposition below. All proofs are deferred to Appendix A. Proposition 1 (Degeneracy). Let us denote \u03c8\u2217 = arg max\u03c8 \u2212Ez\u223cq(z|\u03c8) log q(z |\u03c8)p(x,z) , then\nL(q(z |\u03c8), q\u03c6(\u03c8)) \u2264 \u2212Ez\u223cq(z |\u03c8\u2217) log q(z|\u03c8\u2217) p(x, z) ,\nwhere the equality is true if and only if q\u03c6(\u03c8) = \u03b4\u03c8\u2217(\u03c8).\nTherefore, if optimizing the variational parameter by climbing L(q(z |\u03c8), q\u03c6(\u03c8)), without stopping the optimization algorithm early, q\u03c6(\u03c8) could converge to a point mass density, making SIVI degenerate to vanilla VI.\nTo prevent degeneracy, we regularize L by adding BK = E\u03c8,\u03c8(1),...,\u03c8(K)\u223cq\u03c6(\u03c8)KL(q(z |\u03c8)||h\u0303K(z)), (5)\nwhere h\u0303K(z) = q(z |\u03c8)+\u2211Kk=1 q(z |\u03c8(k))\nK+1 .\nNote that BK \u2265 0, with BK = 0 if and only if K = 0 or q\u03c6(\u03c8) degenerates to a point mass density. Therefore, L0 = L and maximizing LK = L+BK withK \u2265 1 would encourage positive BK and drive q(\u03c8) away from degeneracy. Moreover, as limK\u2192\u221e h\u0303K(z) = E\u03c8\u223cq\u03c6(\u03c8)q(z |\u03c8) = h\u03c6(z) by the strong law of large numbers and\nlim K\u2192\u221e\nBK = E\u03c8\u223cq\u03c6(\u03c8)KL(q(z |\u03c8)||h\u03c6(z)) (6)\nby interchanging two limiting operations, as discussed in detail in Appendix A, we have the following proposition. Proposition 2. Suppose L and L are defined as in (4) and BK as in (5), the regularized lower bound LK = L+BK is an asymptotically exact ELBO that satisfies L0 = L and limK\u2192\u221e LK = L."}, {"heading": "3.3. Upper Bound of ELBO and Correction", "text": "Using the concavity of the logarithmic function, we have log h\u03c6(z) \u2265 E\u03c8\u223cq\u03c6(\u03c8) log q(z |\u03c8), and hence we can obtain an upper bound of SIVI\u2019s ELBO as\nL\u0304(q(z |\u03c8), q\u03c6(\u03c8)) = E\u03c8\u223cq\u03c6(\u03c8)Ez\u223ch\u03c6(z) log p(x,z)q(z |\u03c8) \u2265 L = Ez\u223ch\u03c6(z) log p(x,z)h\u03c6(z) . (7)\nComparing (4) and (7) shows that the lower bound L and upper bound L\u0304 only differ from each other in whether the expectation involving z is taken with respect to q(z |\u03c8) or h\u03c6(z). Moreover, one may show that L\u0304 \u2212 L is equal to\nE\u03c8\u223cq(\u03c8)[KL(q(z |\u03c8)||h\u03c6(z)) + KL(h\u03c6(z)||q(z |\u03c8))].\nSince L\u0304may not be bounded above by the evidence log p(x) and L\u0304 \u2212 L is not bounded from above, there is no convergence guarantee if maximizing L\u0304. For this reason, we subtract L\u0304 by a correction term as\nAK = E\u03c8\u223cq\u03c6(\u03c8)Ez\u223ch\u03c6(z)E\u03c8(1),...,\u03c8(K)\u223cq\u03c6(\u03c8) [\nlog (\n1 K \u2211K k=1 q(z |\u03c8(k)) ) \u2212 log q(z |\u03c8) ] . (8)\nAs E\u03c8(1)\u223cq\u03c6(\u03c8) log q(z |\u03c8 (1)) = E\u03c8\u223cq\u03c6(\u03c8) log q(z |\u03c8), we have A1 = 0. The following proposition shows that the corrected upper bound L\u0304K = L\u0304 \u2212 AK monotonically converges from above towards the ELBO as K \u2192\u221e. Proposition 3. Suppose L and L\u0304 are defined as in (7) and AK as in (8), then the corrected upper bound L\u0304K = L\u0304\u2212AK monotonically converges from the above towards the ELBO, satisfying L\u03041 = L\u0304, L\u0304K+1 \u2264 L\u0304K , and limK\u2192\u221e L\u0304K = L.\nThe relationship betweenLK = L+BK and L\u0304K = L\u0304\u2212AK , two different asymptotically exact ELBOs, can be revealed by rewriting them as\nLK = E\u03c8\u223cq\u03c6(\u03c8)Ez\u223cq(z |\u03c8)E\u03c8(1),...,\u03c8(K)\u223cq\u03c6(\u03c8) log p(x,z)\n1 K+1 [ q(z |\u03c8)+\u2211Kk=1 q(z |\u03c8(k)) ] , (9)\nL\u0304K = E\u03c8\u223cq\u03c6(\u03c8)Ez\u223cq(z |\u03c8)E\u03c8(1),...,\u03c8(K)\u223cq\u03c6(\u03c8) log p(x,z)1\nK \u2211K k=1 q(z |\u03c8(k)) . (10)\nThus LK differs from L\u0304K in whether q(z |\u03c8) participates in computing the log density ratio, which is analytic thanks to the semi-implicit construction, inside the expectations. When K is small, using LK as the surrogate ELBO for optimization is expected to have better numerical stability than using L\u0304K , as L0 = L relates to the ELBO as a lower bound while L\u03041 = L\u0304 does as an upper bound, but increasing K quickly diminishes the difference between LK and L\u0304K , which are both asymptotically exact. It is also instructive to note that as z \u223c q(z |\u03c8) is used for Monte Carlo estimation, if assuming {q(z |\u03c8), q(z |\u03c8(1)), . . . , q(z |\u03c8(K))} has a dominant element, then it is most likely that q(z |\u03c8) dominates all q(z |\u03c8(k)). Therefore, maximizing LK in (9) would become almost the same as maximizing L0, which would lead to degeneracy as in Proposition 1, which means \u03c8 = \u03c8(k) and q(z |\u03c8) = q(z |\u03c8(k)) for all k, contradicting the reasoning that q(z |\u03c8) dominates all q(z|\u03c8(k)). Using the importance reweighting idea, Burda et al. (2015) provides a lower bound LK\u0303 \u2265 ELBO that monotonically\nconverges from below to the evidence log p(x) as K\u0303 increases. Using the same idea, we may also tighten the asymptotically exact surrogate ELBO in (9) using\nLK\u0303K = E(z1,\u03c81),...,(zK\u0303 ,\u03c8K\u0303)\u223cq(z |\u03c8)q\u03c6(\u03c8)E\u03c8(1),...,\u03c8(K)\u223cq\u03c6(\u03c8) log 1\nK\u0303\n\u2211K\u0303 i=1\np(x,zi)\n1 K+1 [ q(zi |\u03c8i)+ \u2211K k=1 q(zi |\u03c8(k)) ] ,\nfor which limK\u2192\u221e LK\u0303K = LK\u0303 \u2265 ELBO and limK,K\u0303\u2192\u221e LK\u0303K = limK\u0303\u2192\u221e LK\u0303 = log p(x). Using LKt as the surrogate ELBO, where t indexes the number of iterations, Kt \u2208 {0, 1, . . .}, and Kt+1 \u2265 Kt, we describe the stochastic gradient ascent algorithm to optimize the variational parameter in Algorithm 1, in which we further introduce \u03be as the variational parameter of the conditional distribution q\u03be(z |\u03c8) that is not mixed with another distribution. For Monte Carlo estimation in Algorithm 1, we use a single random sample for each \u03c8(k), J random samples for \u03c8, and a single sample of z for each sample of \u03c8. One may further consult Rainforth et al. (2018) to help select K, J , and K\u0303 for SIVI. We denote z = f(\u03b5, \u03be,\u03c8), \u03b5 \u223c p(\u03b5) as the reparameterization for z \u223c q\u03be(z |\u03c8). As for \u03be, if \u03be 6= \u2205, one may learn it as in Algorithm 1, set it empirically, or fix it at the value learned by another algorithm such as MFVI. In summary, SIVI constructs a flexible variational distribution by mixing a (potentially) implicit distribution with an explicit one, while maintaining tractable optimization via the use of an asymptotically exact surrogate ELBO."}, {"heading": "3.4. Score Function Gradient in Conjugate Model", "text": "Let\u2019s consider the case that q(z |\u03c8) does not have a simple reparameterization but can benefit from conditional conjugacy. In particular, for a conditionally conjugate exponential family model, MFVI has an analytic ELBO, and its variational distribution can be directly used as the q\u03be(z |\u03c8) of SIVI. As in Appendix A, introducing a density ratio as\nr\u03be,\u03c6(z, , (1:K)) = q\u03be(z |T\u03c6( ))) q\u03be(z |T\u03c6( ))+\n\u2211K k=1 q\u03be(z |T\u03c6( (k))) K+1 ,\nwe approximate the gradient of LK with respect to \u03c6 as\n\u2207\u03c6LK \u2248 1J \u2211J j=1 { \u2212\u2207\u03c6Ez\u223cq\u03be(z |T\u03c6( j))[log q\u03be(z |T\u03c6( j)) p(x,z) ]\n+\u2207\u03c6 log r\u03be,\u03c6(zj , j , (1:K)) + [\u2207\u03c6 log q\u03be(zj |T\u03c6( j))] log r\u03be,\u03c6(zj , j , (1:K)) } , (11)\nwhere the first summation term is equivalent to the gradient of MFVI\u2019s ELBO, both the second and third terms correct the restrcitions of q\u03be(z |T\u03c6( j)), and log r\u03be,\u03c6(z, , (1:K)) in the third term is expected to be small regardless of convergence, effectively mitigating the variance of score function gradient estimation that is usually high in basic black-box VI; \u2207\u03beLK can be approximated in the same manner. For\nnon-conjugate models, we leave for future study the use of non-reparameterizable q\u03be(z |\u03c8), for which one may apply customized variance reduction techniques (Paisley et al., 2012; Ranganath et al., 2014; Mnih & Gregor, 2014; Ruiz et al., 2016; Kucukelbir et al., 2017; Naesseth et al., 2017)."}, {"heading": "4. Related Work", "text": "There exists a wide variety of VI methods that improve on MFVI. Examples include adding dependencies between latent variables (Saul & Jordan, 1996; Hoffman & Blei, 2015), using a mixture of variational distributions (Bishop et al., 1998; Gershman et al., 2012; Salimans & Knowles, 2013; Guo et al., 2016; Miller et al., 2017), introducing a copula to capture the dependencies between univariate marginals (Tran et al., 2015; Han et al., 2016), handling nonconjugacy (Paisley et al., 2012; Titsias & La\u0301zaro-Gredilla, 2014), and constructing a hierarchical variational distribution (Ranganath et al., 2016; Tran et al., 2017).\nTo increase the expressiveness of the PDF of a random variable, a simple but powerful idea is to transform it with complex deterministic and/or stochastic mappings. One successful application of this idea in VI is constructing the variational distribution with a normalizing flow, which transforms a simple random variable through a sequence of invertible differentiable functions with tractable Jacobians, to deterministically map a simple PDF to a complex one (Rezende & Mohamed, 2015; Kingma et al., 2016; Papamakarios et al., 2017). Normalizing flows help increase the flexibility of VI, but still require the mapping to be deterministic and invertible. Removing both restrictions, there have been several recent attempts to define highly flexible variational distributions with implicit models (Husza\u0301r, 2017; Mohamed & Lakshminarayanan, 2016; Tran et al., 2017; Li & Turner, 2017; Mescheder et al., 2017; Shi et al., 2017). A typical example is transforming random noise via a deep neural network, leading to a non-invertible highly nonlinear mapping and hence an implicit distribution.\nWhile an implicit variational distribution can be made highly flexible, it becomes necessary in each iteration to address the problem of density ratio estimation, which is often transformed into a problem related to learning generative adversarial networks (Goodfellow et al., 2014). In particular, a binary classifier, whose class probability is used for density ratio estimation, is trained in each iteration to discriminate the samples generated by the model from those by the variational distribution (Mohamed & Lakshminarayanan, 2016; Uehara et al., 2016; Mescheder et al., 2017). Controlling the bias and variance in density ratio estimation, however, is in general a very difficult problem, especially in highdimensional settings (Sugiyama et al., 2012).\nSIVI is related to the hierarchical variational model (HVM)\n(Ranganath et al., 2016; Maal\u00f8e et al., 2016) in having a hierarchical variational distribution, but there are two major distinctions: 1) the HVM restricts the mixing distribution in the hierarchy to have an explicit PDF, which can be constructed with a Markov chain (Salimans et al., 2015), a mixture model (Ranganath et al., 2016), or a normalizing flow (Ranganath et al., 2016; Louizos & Welling, 2017) but cannot come from an implicit model. By contrast, SIVI requires the conditional distribution q(z |\u03c8) to have an explicit PDF, but does not impose such a constraint on the mixing distribution q(\u03c8). In fact, any off-the-shelf reparameterizable implicit/explicit distribution can be used in SIVI, leading to considerably flexible h\u03c6(z) = E\u03c8\u223cq\u03c6(\u03c8)q(z |\u03c8). Moreover, SIVI does not require q(z |\u03c8) to be reparameterizable for conditionally conjugate models. 2) the HVM optimizes on a lower bound of the ELBO, constructed by adding a recursively estimated variational distribution that approximates q(\u03c8 | z) = q(z |\u03c8)q(\u03c8)/h(z). By contrast, SIVI sandwiches the ELBO between two bounds, and directly optimizes on an asymptotically exact surrogate ELBO, which involves only simple-to-compute analytic density ratios."}, {"heading": "5. Experiments", "text": "We implement SIVI in Tensorflow (Abadi et al., 2015) for a range of inference tasks. Note SIVI is a general purpose algorithm not relying on conjugacy, and has an inherent advantage over MCMC in being able to generate independent, and identically distributed (iid) posterior samples on the fly, this is, by feed-forward propagating iid random noises through the inferred semi-implicit hierarchy. The toy examples show SIVI captures skewness, kurtosis, and multimodality. A negative binomial model shows SIVI can accurately capture the dependencies between latent variables. A bivariate count distribution example shows for a conditionally conjugate model, SIVI can utilize a non-reparameterizable variational distribution, without being plagued by the high variance of score function gradient estimation. With Bayesian logistic regression, we demonstrate that SIVI can either work alone as a black-box inference procedure for correlated latent variables, or directly expand MFVI by adding a mixing distribution, leading to accurate uncertainty estimation on par with that of MCMC. Last but not least, moving beyond the canonical Gaussian based variational autoencoder (VAE), SIVI helps construct semi-implicit VAE (SIVAE) to improve unsupervised feature learning and amortized inference."}, {"heading": "5.1. Expressiveness of SIVI", "text": "We first show the expressiveness of SIVI by approximating various target distributions. As listed in Table 1, the conditional layer of SIVI is chosen to be as simple as an isotropic Gaussian (or log-normal) distribution N (0, \u03c320I). The implicit mixing layer is a multilayer perceptron (MLP), with\nlayer widths [30, 60, 30] and a ten dimensional isotropic Gaussian noise as its input. We fix \u03c320 = 0.1 and optimize the implicit layer to minimize KL(Eq\u03c6(\u03c8)q(z |\u03c8)||p(z)). As shown in Figure 1, despite having a fixed purposely misspecified explicit layer, by training a flexible implicit layer, the random samples from which are illustrated in Figure 8 of Appendix D, SIVI infers a sophisticated marginal variational distribution that accurately captures the skewness, kurtosis, and/or multimodality exhibited by the target one."}, {"heading": "5.2. Negative Binomial Model", "text": "We consider a negative binomial (NB) distribution with the gamma and beta priors (a = b = \u03b1 = \u03b2 = 0.01) as\nxi iid\u223c NB(r, p), r \u223c Gamma(a, 1/b), p \u223c Beta(\u03b1, \u03b2),\nfor which the posterior p(r, p | {xi}1,N ) is not tractable. In comparison to Gibbs sampling, it is shown in Zhou et al. (2012) that MFVI, which uses q(r, p) = Gamma(r; a\u0303, 1/b\u0303)Beta(p; \u03b1\u0303, \u03b2\u0303) to approximate the posterior, notably underestimates the variance. This caveat of MFVI motivates a semi-implicit variational distribution as\nq(r, p |\u03c8) = Log-Normal(r;\u00b5r, \u03c320)Logit-Normal(p;\u00b5p, \u03c320), \u03c8 = (\u00b5r, \u00b5p) \u223c q(\u03c8), \u03c30 = 0.1\nwhere and an MLP based implicit q(\u03c8), as in Section 5.1, is used by SIVI to capture the dependency between r and p.\nWe apply Gibbs sampling, MFVI, and SIVI to a real overdispersed count dataset of Bliss & Fisher (1953) that records the number of adult red mites on each of the 150 randomly selected apple leaves. With K = 1000, as shown in Figure 2, SIVI improves MFVI in closely matching the posterior inferred by Gibbs sampling. Moreover, the mixing distribution q(\u03c8) helps well capture the negative correla-\ntion between r and p, as totally ignored by MFVI. The twosample Kolmogorov-Smirnov (KS) distances, between 2000 posterior samples generated by SIVI and 2000 MCMC ones, are 0.0185 (p-value = 0.88) and 0.0200 (p-value = 0.81) for r and p, respectively. By contrast, for MFVI and MCMC, they are 0.2695 (p-value = 5.26 \u00d7 10\u221264) and 0.2965 (pvalue = 2.21\u00d7 10\u221277), which significantly reject the null hypothesis that the posterior inferred by MFVI matches that by MCMC. How the performance is related to K is shown in Figure 3 and Figures 9-10 of Appendix D, which suggests K = 20 achieves a nice compromise between complexity and accuracy, and as K increases, the posterior inferred by SIVI quickly approaches that inferred by MCMC."}, {"heading": "5.3. Non-reparameterizable Variational Distribution", "text": "To show that SIVI can use a non-reparameterizable q(z |\u03c8) in a conditionally conjugate model, as discussed in Section 3.4, we apply it to infer the two parameters of the Poissonlogarithmic bivariate count distribution as p(ni, li | r, p) = rlipni(1\u2212 p)r/Zi, where li \u2208 {0, . . . , ni} and Zi are normalization constants not related to r > 0 and p \u2208 (0, 1) (Zhou & Carin, 2015; Zhou et al., 2016). With r \u223c Gamma(a, 1/b) and p \u223c Beta(\u03b1, \u03b2), while the joint posterior p(r, p | {ni, li}1,N ) is intractable, the conditional posteriors of r and p follow the gamma and beta distributions,\nrespectively. Although neither the gamma nor beta distribution is reparameterizable, SIVI multiplies them to construct a semi-implicit variational distribution as\nq(r, p |\u03c8) = Gamma(r;\u03c81, \u03c82)Beta(p;\u03c83, \u03c84),\nwhere \u03c8 = (\u03c81, \u03c82, \u03c83, \u03c84) \u223c q(\u03c8) is similarly constructed as in Section 5.1. We set K = 200 for SIVI.\nAs shown in Figure 4, despite the need of score function gradient estimation that is notorious for variance control, by utilizing conjugacy as in (11), SIVI well controls the variance of its gradient estimation, achieving accurate posterior estimation without requiring q(z |\u03c8) to be reparameterizable. By contrast, MFVI, which uses only the first summation term in (11) for gradient estimation, ignores covariance structure and notably underestimates posterior uncertainty."}, {"heading": "5.4. Bayesian Logistic Regression", "text": "We compare SIVI with MFVI, Stein variational gradient descent (SVGD) of Liu & Wang (2016), and MCMC on Bayesian logistic regression, expressed as\nyi \u223c Bernoulli[(1 + e\u2212x \u2032 i\u03b2)\u22121], \u03b2 \u223c N (0, \u03b1\u22121IV+1),\nwhere xi = (1, xi1, . . . , xiV )\u2032 are covariates, yi \u2208 {0, 1} are binary response variables, and \u03b1 is set as 0.01. With the Po\u0301lya-Gamma data augmentation of Polson et al. (2013), we collect posterior MCMC samples of \u03b2 using Gibbs sampling. For MFVI, the variational distribution is chosen as a multivariate normal (MVN) N (\u03b2;\u00b5,\u03a3), with a diagonal or full covariance matrix. For SIVI, we treat \u03a3, diagonal or full, as a variational parameter, mix \u00b5 with an MLP based implicit distribution, and set K = 500. We consider three datasets: waveform, spam, and nodal. The details on datasets and inference are deferred to Appendix B. On waveform, the algorithm converges in about 500 iterations, which takes about 40 seconds on a 2.4 GHz CPU. Note the results of SIVI with K = 100 (or 50), which takes about 12 (or 8) seconds for 500 iterations, are almost identical to those\nshown in Figures 5-8 with K = 500. Given the posterior captured by the semi-implicit hierarchy, SIVI takes 0.92 seconds to generate 50, 000 iid 22-dimensional \u03b2\u2019s.\nWe collect \u03b2j for j = 1, . . . , 1000 to represent the inferred posterior p(\u03b2 | {xi, yi}1,N ). For each test data xN+i, we calculate the predictive probabilities 1/(1 + e\u2212x T N+i\u03b2j ) for all j and compute its sample mean, and sample standard deviation that measures the uncertainty of the predictive distribution p(yN+i = 1 |xN+i, {xi, yi}1,N ). As shown in Figure 5, even with a full covariance matrix, the MVN variational distribution inferred by MFVI underestimates the uncertainty in out-of-sample prediction, let alone with a diagonal one, whereas SIVI, mixing the MVN with an MLP based implicit distribution, closely matches MCMC in uncertainty estimation. As shown in Figure 6, the underestimation of predictive uncertainty by MFVI can be attributed to variance underestimation for both univariate marginal and pairwise joint posteriors, which are, by contrast, well agreed on between SIVI and MCMC. Further examining the correlation coefficients of \u03b2, shown in Figure 7, all the univariate marginals, shown in Figure 11 of Appendix D, and additional results, show in Figures 12-17 of Appendix D, it is revealed that SIVI well characterizes the posterior distribution of \u03b2 and is only slightly negatively affected if its\nexplicit layer is restricted with a diagonal covariance matrix, whereas MFVI with a diagonal/full covariance matrix and SVGD all misrepresent uncertainty. Note we have also tried modifying the code of variational boosting (Guo et al., 2016; Miller et al., 2017) for Bayesian logistic regression, but failed to obtain satisfactory results. We attribute the success of SIVI to its ability in better capturing the dependencies between \u03b2v and supporting a highly expressive non-Gaussian variational distribution by mixing an MVN with a flexible implicit distribution, whose parameters can be efficiently optimized via an asymptotically exact surrogate ELBO."}, {"heading": "5.5. Semi-Implicit Variational Autoencoder", "text": "Variational Auto-encoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), widely used for unsupervised feature learning and amortized inference, infers encoder parameter \u03c6 and decoder parameter \u03b8 to maximize the ELBO as\nL(\u03c6,\u03b8) = Ez\u223cq\u03c6(z |x)[log(p\u03b8(x |z))]\u2212 KL(q\u03c6(z |x)||p(z)).\nThe encoder distribution q\u03c6(z |x) is required to be reparameterizable and analytically evaluable, which usually restricts it to a small exponential family. In particular, a canonical encoder is q\u03c6(z |x) = N (z |\u00b5(x,\u03c6),\u03a3(x,\u03c6)), where the Gaussian parameters are deterministically transformed from the observations x, via non-probabilistic deep neural networks parameterized by \u03c6. Thus, given observation xi, its corresponding code zi is forced to follow a Gaussian distribution, no matter how powerful the deep neural networks are. The Gaussian assumption, however, is often too restrictive to model skewness, kurtosis, and multimodality.\nTo this end, rather than using a single-stochastic-layer encoder, we use SIVI that can add as many stochastic layers as needed, as long as the first stochastic layer q\u03c6(z |x) is reparameterizable and has an analytic PDF, and the layers added after are reparameterizable and simple to sample from. More specifically, we construct semi-implicit VAE (SIVAE)\nby using a hierarchical encoder that injects random noise at M different stochastic layers as\n`t = Tt(`t\u22121, t,x;\u03c6), t \u223c qt( ), t = 1, . . . ,M, \u00b5(x,\u03c6) = f(`M ,x;\u03c6), \u03a3(x,\u03c6) = g(`M ,x;\u03c6), q\u03c6(z |x,\u00b5,\u03a3) = N (\u00b5(x,\u03c6),\u03a3(x,\u03c6)), (12)\nwhere `0 = \u2205 and Tt, f , and g are all deterministic neural networks. Note given data xi, \u00b5(xi,\u03c6), \u03a3(xi,\u03c6) are now random variables rather than following vanilla VAE to assume deterministic values. This moves the encoder variational distribution beyond a simple Gaussian form.\nTo benchmark the performance of SIVAE, we consider the MNIST dataset that is stochastically binarized as in Salakhutdinov & Murray (2008). We use 55,000 for training and use the 10,000 observations in the testing set for performance evaluation. Similar to existing VAEs, we choose Bernoulli units, linked to a fully-connected neural network with two 500-unit hidden layers, as the decoder. Distinct from existing VAEs, whose encoders are often restricted to have a single stochastic layer, SIVI allows SIVAE to use an MVN as its first stochastic layer, and draw the parameters of the MVN from M = 3 stochastic layers, whose structure is described in detail in Appendix C. As shown in Table 2 of Appendix C, SIVAE achieves a negative log evidence of 84.07, which is further reduced to 83.25 if choosing importance reweighing with K\u0303 = 10. In comparison to other VAEs with a comparable single-stochastic-layer decoder, SIVAE achieves state-of-the-art performance by mixing an MVN with an implicit distribution defined as in (12) to construct a flexible encoder, whose marginal variational distribution is no longer restricted to the MVN distribution. We leave it for future study on further improving SIVAE by replacing the encoder MVN explicit layer with a normalizing flow, and adding convolution/autoregression to enrich the encoder\u2019s implicit distribution and/or the decoder."}, {"heading": "6. Conclusions", "text": "Combining the advantages of having analytic point-wise evaluable density ratios and tractable computation via Monte Carlo estimation, semi-implicit variational inference (SIVI) is proposed either as a black-box inference procedure, or to enrich mean-field variational inference with a flexible (implicit) mixing distribution. By designing a surrogate evidence lower bound that is asymptotically exact, SIVI establishes an optimization problem amenable to gradient ascent, without compromising the expressiveness of its semi-implicit variational distribution. Flexible but simple to optimize, SIVI approaches the accuracy of MCMC in quantifying posterior uncertainty in a wide variety of inference tasks, and is not constrained by conjugacy, often runs faster, and can generate iid posterior samples on the fly via the inferred stochastic variational inference network."}, {"heading": "Acknowledgements", "text": "The authors thank Texas Advanced Computing Center (TACC) for computational support."}, {"heading": "A. Proofs", "text": "Proof of Inequility (3). To prove a functional form of Jensen\u2019s Inequality, let h(z) = E\u03c8\u223cq\u03c6(\u03c8)q(z|\u03c8) and \u3008f, g\u3009L2 = \u222b f(z)g(z)dz. From Theorem 1, we have convexity, and according to Theorem 6.2.1. of Kurdila & Zabarankin (2005), we have an equivalent first-order definition for convexity as\nKL(q(z|\u03c8)||p(z)) \u2265KL(h(z)||p)+ \u3008q(z|\u03c8)\u2212 h(z),\u2207qKL(q||p)|h(z)\u3009L2\nTaking the expectation with respect to \u03c8 \u223c q\u03c6(\u03c8) on both sides, we have\nE\u03c8\u223cq\u03c6(\u03c8)KL(q(z|\u03c8)||p(z)) \u2265 KL(h(z)||p(z)) + E\u03c8\u223cq\u03c6(\u03c8)[\u3008q(z|\u03c8)\u2212 h(z),\u2207qKL(q||p)|h(z)\u3009L2 ] = KL(h(z)||p(z)) = KL(E\u03c8\u223cq\u03c6(\u03c8)q(z|\u03c8)||p(z)).\nProof of Proposition 1. We show that directly maximizing the lower bound L of ELBO in (4) may drive q(\u03c8) towards degeneracy. For VI that uses q(z |\u03c8) as its variational distribution, if supposing \u03c8\u2217 is the optimum variational parameter, which means\n\u03c8\u2217 = arg max \u03c8 \u2212Ez\u223cq(z|\u03c8) log q(z|\u03c8) p(x, z) ,\nthen we have\nL = \u2212E\u03c8\u223cq\u03c6(\u03c8)Ez\u223cq(z|\u03c8) log q(z|\u03c8) p(x, z)\n= \u222b q\u03c6(\u03c8)[\u2212Ez\u223cq(z|\u03c8) log\nq(z|\u03c8) p(x, z) ]d\u03c8\n\u2264 \u222b q\u03c6(\u03c8)d\u03c8[\u2212Ez\u223cq(z|\u03c8\u2217) log q(z|\u03c8\u2217) p(x, z) ] = \u2212Ez\u223cq(z|\u03c8\u2217) log q(z|\u03c8\u2217) p(x, z) .\nThe equality in the above equation is reached if and only if q(\u03c8) = \u03b4\u03c8\u2217(\u03c8), which means the mixing distribution degenerates to a point mass density and hence SIVI degenerates to vanilla VI.\nProof of Proposition 2. B0 = 0 is trivial. Denote \u03c8(0) = \u03c8v. For iid samples \u03c8(k) \u223c q\u03c6(\u03c8), when K \u2192 \u221e, by the strong law of large numbers, h\u0303K(z) = \u2211K k=0 q(z |\u03c8(k)) K+1 converges almost surely to Eq\u03c6(\u03c8)q(z |\u03c8) = h\u03c6(z). To prove (6), by the strong law of large numbers, we first rewrite it as the limit of a double sequence S(K,J), where K,J \u2208 {1, 2, . . . , }, and check the condition for the interchange of iterated limits (Rudin, 1964; Habil, 2016): i) The double limit exists; ii) Fixing one index of the double sequence, the one side limit exists for the other index .\nlim K\u2192\u221e E\u03c8(0),\u03c8(1),\u00b7\u00b7\u00b7 ,\u03c8(K)\u223cq(\u03c8) log \u2211K k=0 q(z |\u03c8(k)) K + 1\n= lim K\u2192\u221e lim J\u2192\u221e\n1\nJ\nJ\u2211\nj=1\nlog 1\nK + 1\nK\u2211\nk=0\nq(z |\u03c8(k)j )\n=\u2206 lim K\u2192\u221e lim J\u2192\u221e S(K,J).\nHere \u03c8(k)j are iid samples from q(\u03c8). For i) we show double limit limK,J\u2192\u221e S(K,J) = log h(z). For \u2200 > 0, \u2203N( ), when K,J > N( ), | log 1K+1 \u2211K k=0 q(z |\u03c8 (k) j )\u2212 log h(z)| < thanks to the law of large numbers, then \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211\nj=1\nlog 1\nK + 1\nK\u2211\nk=0\nq(z |\u03c8(k)j )\u2212 J log h(z) \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 J\u2211\nj=1\n\u2223\u2223\u2223\u2223\u2223log 1 K + 1 K\u2211\nk=0\nq(z |\u03c8(k)j )\u2212 log h(z) \u2223\u2223\u2223\u2223\u2223 \u2264 J .\nDeviding both sides by J we get |S(K,J) \u2212 log h(z)| \u2264 when K,J > N( ). By definition, we have limK,J\u2192\u221e S(K,J) = log h(z). ii) for each fixed J \u2208 N, limK\u2192\u221e S(K,J) = log h(z) exists; for each fixed K \u2208 N, limJ\u2192\u221e S(K,J) = E\u03c8(0),\u03c8(1),\u00b7\u00b7\u00b7 ,\u03c8(K)\u223cq(\u03c8) log \u2211K k=0 q(z |\u03c8(k))\nK+1 \u2264 log h(z) also exists. The limitation can then be interchanged and (6) is proved. Therefore, we have\nlim k\u2192\u221e\nLk = L+ E\u03c8KL(q(z |\u03c8)||h\u03c6(z))\n= E\u03c8\u223cq(\u03c8)Ez\u223cq(z |\u03c8) [ log\nq(z |\u03c8) h\u03c6(z) \u2212 log q(z |\u03c8) p(x, z)\n]\n=\u2212 E\u03c8\u223cq(\u03c8)Ez\u223cq(z |\u03c8) log h\u03c6(z)\np(x, z)\n=\u2212 Ez\u223ch\u03c6(z) log h\u03c6(z)\np(x, z) = L.\nProof of Proposition 3. Assume integer K > M > 0. Let I be the set that consists of all the subsets of {1, \u00b7 \u00b7 \u00b7 ,K} with cardinality M . Let I be a discrete uniform random variable and for element {i1, \u00b7 \u00b7 \u00b7 , iM} \u2208 I, P (I = {i1, \u00b7 \u00b7 \u00b7 , iM}) = 1(KM) . We have EI 1 M \u2211 i\u2208I q(z |\u03c8i) =\n1 K \u2211K i=1 q(z |\u03c8i). To show L\u0304K = L\u0304 \u2212 AK is monotonic decreasing, we only need to show AK is monotonic increasing:\nAK =E\u03c8\u223cq(\u03c8)Ez\u223ch\u03c6(z)E\u03c8(1),\u00b7\u00b7\u00b7 ,\u03c8(K)\u223cq(\u03c8)\nlog 1 K \u2211K i=1 q(z |\u03c8(i)) q(z |\u03c8)\n=E\u03c8\u223cq(\u03c8)Ez\u223ch\u03c6(z)E\u03c8(1),\u00b7\u00b7\u00b7 ,\u03c8(K)\u223cq(\u03c8)\nlogEI\n[ 1 M \u2211 i\u2208I q(z |\u03c8(i)) q(z |\u03c8) ]\n\u2265E\u03c8\u223cq(\u03c8)Ez\u223ch\u03c6(z)E\u03c8(1),\u00b7\u00b7\u00b7 ,\u03c8(K)\u223cq(\u03c8)\nEI log 1 M \u2211 i\u2208I q(z |\u03c8(i)) q(z |\u03c8)\n=E\u03c8\u223cq(\u03c8)Ez\u223ch\u03c6(z)E\u03c8(1),\u00b7\u00b7\u00b7 ,\u03c8(M)\u223cq(\u03c8)\nlog 1 M \u2211M i=1 q(z |\u03c8(i)) q(z |\u03c8)\n=AM .\nWe now show limK\u2192\u221e L\u0304K = L. Again, by the strong law of large numbers, 1K \u2211K i=1 q(z |\u03c8(i)) converges almost\nsurely to E\u03c8\u223cq\u03c6(\u03c8)q(z |\u03c8) = h\u03c6(z) and hence\nlim K\u2192\u221e\nL\u0304K = L\u0304+ E\u03c8KL(h\u03c6(z)||q(z |\u03c8))\n=\u2212 Ez\u223ch\u03c6(z)E\u03c8\u223cq(\u03c8) [ log q(z |\u03c8) p(x, z) + log h\u03c6(z) q(z |\u03c8) ]\n=L.\nProof of Equation (11). The gradient of BK with respect to \u03c6 can be expressed as\n\u2207\u03c6BK = \u2207\u03c6E\u03c8\u223cq\u03c6(\u03c8)E\u03c8(1),...,\u03c8(K)\u223cq\u03c6(\u03c8) [\nKL ( q(z |\u03c8) \u2223\u2223\u2223 \u2223\u2223\u2223 q(z |\u03c8)+ \u2211K k=1 q(z |\u03c8(k)\nK+1\n)]\n= E , (1),..., (K)\u223cp( )\u2207\u03c6Ez\u223cq(z |T\u03c6( )) [\nlog q(z |T\u03c6( ))\nq(z |T\u03c6( ))+ \u2211K k=1 q(z |T\u03c6( (k)))\nK+1\n]\n= E ,..., (K)\u2207\u03c6Ez\u223cq(z |T\u03c6( )) log [ r\u03c6(z, , (1:K)) ] = E ,..., (K)\u223cp( )Ez\u223cq(z |T\u03c6( )) {\nq(z |T\u03c6( ))\u2207\u03c6 log [ r\u03c6(z, , (1:K)) ]\n+ [ \u2207\u03c6 log q(z |T\u03c6( )) ] log [ r\u03c6(z, , (1:K)) ]} ."}, {"heading": "B. Bayesian Logistic Regression", "text": "We consider datesets waveform (n = 5000, V = 21, and 400/4600 for training/testing), spam (n = 3000, V = 2, and 2000/1000 for training/testing), and nodal (n = 53, V = 5, and 25/28 for training/testing). The training-setsize to feature-dimension ratio ntrain/V varies in these three datasets, and we expect the posterior uncertainty to be large if this ratio is small.\nThe contribution of observation i to the likelihood can be expressed as\nP (yi |xi,\u03b2) = eyx\n\u2032 i\u03b2\n1 + ex \u2032 i\u03b2\n\u221d e(y\u2212 12 )x\u2032i\u03b2E\u03c9i [ e\u2212 \u03c9i(x \u2032 i\u03b2) 2 2 ] ,\nwhere the expectation is taken respect to a Po\u0301lya-Gamma (PG) distribution (Polson et al., 2013) as \u03c9i \u223c PG(1, 0), and hence we have an augmented likelihood as\nP (yi, \u03c9i |xi,\u03b2) \u221d e(y\u2212 1 2 )x \u2032 i\u03b2\u2212 12\u03c9i(x\u2032i\u03b2)2 .\nB.1. Gibbs Sampling via Data Augmentation\nDenoting X = (x1, . . . ,xN )\u2032, y = (y1, . . . , yN )\u2032, A = diag(\u03b10, . . . , \u03b1V )\u2032, and \u2126 = diag(\u03c91, . . . , \u03c9N ), we have\n(\u03c9i | \u2212) \u223c PG(1,x\u2032i\u03b2), (\u03b2 | \u2212) \u223c N (\u00b5,\u03a3),\nwhere \u03a3 = (A + X\u2032\u2126X)\u22121 and \u00b5 = \u03a3X\u2032(y \u2212 1/2). To sample from the Po\u0301lya-Gamma distribution, a random sample from which can be generated as a weighted sum of an infinite number of iid gamma random variables, we follow Zhou (2016) to truncate the infinite sum to the summation of M gamma random variables, where the parameters of the M th gamma random variable are adjusted to match the mean and variance of the finite sum with those of the infinite sum. We set M = 5 in this paper.\nB.2. Mean-Field Variational Inference with Diagonal Covariance Matrix\nWe choose a fully factorized Q distribution as\nQ = [\u220f\ni q(\u03c9i) ] [\u220f v q(\u03b2v) ] .\nTo exploit conjugacy, defining\nq(\u03c9i) = PG (1, \u03bbi) ,\nq(\u03b2v) = N (\u00b5v, \u03c32v), we have closed-form coordinate ascent variational inference update equations as\n\u03bbi = \u221a E[(x\u2032i\u03b2)2] = \u221a x\u2032iE[\u03b2\u03b2 \u2032]xi, \u03c32v = ( E[\u03b1v] + \u2211 i E[\u03c9i]x2iv )\u22121\n\u00b5v = \u03c3 2 v \u2211 i xiv { yi \u2212 1/2\u2212 E[\u03c9i] \u2211 v\u0303 6=v xiv\u0303E[\u03b2v\u0303] } ,\nwhere the expectations with respect to the q distributions can be expressed as E[\u03b2\u03b2\u2032] = \u00b5\u00b5\u2032+diag(\u03c320 , . . . , \u03c32V ) and E[\u03c9i] = tanh(\u03bbi/2)/(2\u03bbi).\nB.3. Mean-Field Variational Inference with Full Covariance Matrix\nWe choose a fully factorized Q distribution as\nQ = [\u220f\ni q(\u03c9i)\n] q(\u03b2).\nTo exploit conjugacy, defining\nq(\u03c9i) = PG (1, \u03bbi) , q(\u03b2) = N (\u00b5,\u03a3),\nwe have closed-form coordinate ascent variational inference update equations as\n\u03bbi = \u221a E[(x\u2032i\u03b2)2] = \u221a x\u2032iE[\u03b2\u03b2 \u2032]xi, \u03a3 = (E[A] + X\u2032E[\u2126]X)\u22121, \u00b5 = \u03a3X\u2032(y \u2212 1/2),\nwhere the expectations with respect to the q distributions can be expressed as E[\u03b2\u03b2\u2032] = \u00b5\u00b5\u2032 + \u03a3 and E[\u03c9i] = tanh(\u03bbi/2)/(2\u03bbi). Note the update equations shown above are identical to those shown in Jaakkola & Jordan (2000).\nB.4. SIVI Configuration\nFor inputs in Algorithm 1, we choose a multi-layer perceptron with layer size [100, 200, 100] as T\u03c6 for \u03c8 = T\u03c6( ), as 50 dimensional isotropic Gaussian random variable and K = 100, J = 50. For the explicit layer, we choose an MVN as q\u03be(z |\u03c8) = N (z;\u03c8, \u03be). In this setting, \u03c8 is the mean variable mixed with implicit distribution q\u03c6(\u03c8) while \u03be is the covariance matrix which can be either diagonal or full. In the experiments, we update the neural network parameter \u03c6 by the Adam optimizer, with learning rate 0.01. We update \u03be by gradient ascent, with step size \u03b7t = 0.001 \u2217 0.9iteration/100. The implicit layer parameter \u03c6 and explicit layer parameter \u03be are updated iteratively."}, {"heading": "C. Experimental Settings and Results for SIVAE", "text": "We implement SIVI with M = 3 stochastic hidden layers, with the dimensions of hidden layers [`1, `2, `3] as [150, 150, 150] and with the dimensions of injected noises [ 1, 2, 3] as [150, 100, 50]. Between two adjacent stochastic layers there is a fully connected deterministic layer with size 500 and ReLU activation function. We choose binary pepper and salt noise (Im et al., 2017) for qt( ). The model is trained for 2000 epochs with mini-batch size 200 and step-size 0.001 \u2217 0.75epoch/100. Kt is gradually increased from 1 to 100 during the first 1500 epochs. The explicit and implicit layers are trained iteratively. Warm-up is used during the first 300 epochs as suggested by S\u00f8nderby et al. (2016) to gradually impose the prior regularization term KL(q\u03c6(z |x)||p(z)). The model is trained end-to-end using the Adam optimizer. After training process, as in Rezende et al. (2014) and Burda et al. (2015), we compute the marginal likelihood for test set by importance sampling with S = 2000:\nlog p(x) \u2248 log 1 S\nS\u2211\ns=1\np(x | zs)p(zs) h\u0302(zs |x) , zs \u223c h(zs|x),\nwhere\nh\u0302(zs|x) = 1\nM\nM\u2211\nk=1\nq(zs |\u03c8(k)), \u03c8(k) iid\u223c q\u03c6(\u03c8|x)\nis used to estimate h(zs |x); we set M = 100. The performance of SIVI and a comparison to reported results with other methods are provided in Table 2.\nD. Additional Figures"}], "year": 2018, "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org", "authors": ["Wicke", "Martin", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "year": 2015}, {"title": "An auxiliary variational method", "authors": ["F.V. Agakov", "D. Barber"], "venue": "In International Conference on Neural Information Processing,", "year": 2004}, {"title": "Variational relevance vector machines", "authors": ["Bishop", "Christopher M", "Tipping", "Michael E"], "venue": "In UAI,", "year": 2000}, {"title": "Approximating posterior distributions in belief networks using mixtures", "authors": ["Bishop", "Christopher M", "Lawrence", "Neil D", "Jaakkola", "Tommi", "Jordan", "Michael I"], "venue": "In NIPS,", "year": 1998}, {"title": "Variational inference: A review for statisticians", "authors": ["Blei", "David M", "Kucukelbir", "Alp", "McAuliffe", "Jon D"], "venue": "Journal of the American Statistical Association,", "year": 2017}, {"title": "Fitting the negative binomial distribution to biological data", "authors": ["Bliss", "Chester Ittner", "Fisher", "Ronald A"], "year": 1953}, {"title": "Importance weighted autoencoders", "authors": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "year": 2015}, {"title": "Elements of Information Theory", "authors": ["Cover", "Thomas M", "Thomas", "Joy A"], "year": 2012}, {"title": "NICE: Non-linear independent components estimation", "authors": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1410.8516,", "year": 2014}, {"title": "Nonparametric variational inference", "authors": ["Gershman", "Samuel J", "Hoffman", "Matthew D", "Blei", "David M"], "venue": "In ICML, pp", "year": 2012}, {"title": "Linear response methods for accurate covariance estimates from mean field variational bayes", "authors": ["Giordano", "Ryan J", "Broderick", "Tamara", "Jordan", "Michael I"], "venue": "In NIPS,", "year": 2015}, {"title": "Generative adversarial nets", "authors": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS,", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "authors": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo", "Wierstra", "Daan"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Boosting variational inference", "authors": ["Guo", "Fangjian", "Wang", "Xiangyu", "Fan", "Kai", "Broderick", "Tamara", "Dunson", "David B"], "venue": "arXiv preprint arXiv:1611.05559,", "year": 2016}, {"title": "Double sequences and double series", "authors": ["Habil", "Eissa D"], "venue": "IUG Journal of Natural Studies,", "year": 2016}, {"title": "Variational Gaussian copula inference", "authors": ["Han", "Shaobo", "Liao", "Xuejun", "Dunson", "David", "Carin", "Lawrence"], "venue": "In AISTATS,", "year": 2016}, {"title": "Stochastic structured variational inference", "authors": ["Hoffman", "Matthew", "Blei", "David"], "venue": "In AISTATS, pp", "year": 2015}, {"title": "Stochastic variational inference", "authors": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John"], "venue": "The Journal of Machine Learning Research,", "year": 2013}, {"title": "Variational inference using implicit distributions", "authors": ["Husz\u00e1r", "Ferenc"], "venue": "arXiv preprint arXiv:1702.08235,", "year": 2017}, {"title": "Denoising criterion for variational auto-encoding framework", "authors": ["Im", "Daniel Jiwoong", "Ahn", "Sungjin", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In AAAI,", "year": 2017}, {"title": "Improving the mean field approximation via the use of mixture distributions", "authors": ["Jaakkola", "Tommi S", "Jordan", "Michael I"], "venue": "In Learning in Graphical Models,", "year": 1998}, {"title": "Bayesian parameter estimation via variational methods", "authors": ["Jaakkola", "Tommi S", "Jordan", "Michael I"], "venue": "Statistics and Computing,", "year": 2000}, {"title": "An introduction to variational methods for graphical models", "authors": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "year": 1999}, {"title": "Auto-encoding variational Bayes", "authors": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "year": 2013}, {"title": "Improved variational inference with inverse autoregressive flow", "authors": ["Kingma", "Diederik P", "Salimans", "Tim", "Jozefowicz", "Rafal", "Chen", "Xi", "Sutskever", "Ilya", "Welling", "Max"], "venue": "In NIPS,", "year": 2016}, {"title": "Automatic differentiation variational inference", "authors": ["Kucukelbir", "Alp", "Tran", "Dustin", "Ranganath", "Rajesh", "Gelman", "Andrew", "Blei", "David M"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "Convex functional analysis (systems and control: Foundations and applications)", "authors": ["Kurdila", "Andrew J", "Zabarankin", "Michael"], "venue": "Switzerland: Birkha\u0308user,", "year": 2005}, {"title": "Gradient estimators for implicit models", "authors": ["Li", "Yingzhen", "Turner", "Richard E"], "venue": "arXiv preprint arXiv:1705.07107,", "year": 2017}, {"title": "Stein variational gradient descent: A general purpose Bayesian inference algorithm", "authors": ["Liu", "Qiang", "Wang", "Dilin"], "venue": "In NIPS,", "year": 2016}, {"title": "Multiplicative normalizing flows for variational Bayesian neural networks", "authors": ["Louizos", "Christos", "Welling", "Max"], "venue": "In ICML,", "year": 2017}, {"title": "Auxiliary deep generative models", "authors": ["Maal\u00f8e", "Lars", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "In ICML,", "year": 2016}, {"title": "Adversarial variational Bayes: Unifying variational autoencoders and generative adversarial networks", "authors": ["Mescheder", "Lars", "Nowozin", "Sebastian", "Geiger", "Andreas"], "venue": "In ICML,", "year": 2017}, {"title": "Variational boosting: Iteratively refining posterior approximations", "authors": ["Miller", "Andrew C", "Foti", "Nicholas J", "Adams", "Ryan P"], "venue": "In ICML,", "year": 2017}, {"title": "Neural variational inference and learning in belief networks", "authors": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In ICML, pp", "year": 2014}, {"title": "Learning in implicit generative models", "authors": ["Mohamed", "Shakir", "Lakshminarayanan", "Balaji"], "venue": "arXiv preprint arXiv:1610.03483,", "year": 2016}, {"title": "Reparameterization gradients through acceptancerejection sampling algorithms", "authors": ["Naesseth", "Christian", "Ruiz", "Francisco", "Linderman", "Scott", "Blei", "David"], "venue": "In AISTATS,", "year": 2017}, {"title": "Variational Bayesian inference with stochastic search", "authors": ["Paisley", "John", "Blei", "David M", "Jordan", "Michael I"], "venue": "In ICML,", "year": 2012}, {"title": "Masked autoregressive flow for density estimation", "authors": ["Papamakarios", "George", "Murray", "Iain", "Pavlakou", "Theo"], "venue": "In NIPS,", "year": 2017}, {"title": "Bayesian inference for logistic models using P\u00f3lya\u2013Gamma latent variables", "authors": ["Polson", "Nicholas G", "Scott", "James G", "Windle", "Jesse"], "venue": "Journal of the American statistical Association,", "year": 2013}, {"title": "Tighter variational bounds are not necessarily better", "authors": ["Rainforth", "Tom", "Kosiorek", "Adam R", "Le", "Tuan Anh", "Maddison", "Chris J", "Igl", "Maximilian", "Wood", "Frank", "Teh", "Yee Whye"], "venue": "arXiv preprint arXiv:1802.04537,", "year": 2018}, {"title": "Black box variational inference", "authors": ["Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David"], "venue": "In AISTATS, pp", "year": 2014}, {"title": "Hierarchical variational models", "authors": ["Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David"], "venue": "In ICML,", "year": 2016}, {"title": "Variational inference with normalizing flows", "authors": ["Rezende", "Danilo", "Mohamed", "Shakir"], "venue": "In ICML, pp. 1530\u20131538,", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "authors": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In ICML,", "year": 2014}, {"title": "Principles of Mathematical Analysis, volume 3. McGraw-hill", "authors": ["Rudin", "Walter"], "venue": "New York,", "year": 1964}, {"title": "The generalized reparameterization gradient", "authors": ["Ruiz", "Francisco J. R", "Titsias", "Michalis K", "Blei", "David M"], "venue": "In NIPS,", "year": 2016}, {"title": "On the quantitative analysis of deep belief networks", "authors": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In ICML, pp", "year": 2008}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "authors": ["Salimans", "Tim", "Knowles", "David A"], "venue": "Bayesian Analysis,", "year": 2013}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "authors": ["Salimans", "Tim", "Kingma", "Diederik", "Welling", "Max"], "venue": "In ICML,", "year": 2015}, {"title": "Exploiting tractable substructures in intractable networks", "authors": ["Saul", "Lawrence K", "Jordan", "Michael I"], "venue": "In NIPS, pp", "year": 1996}, {"title": "Implicit variational inference with kernel density ratio fitting", "authors": ["Shi", "Jiaxin", "Sun", "Shengyang", "Zhu", "Jun"], "venue": "arXiv preprint arXiv:1705.10119,", "year": 2017}, {"title": "Ladder variational autoencoders", "authors": ["S\u00f8nderby", "Casper Kaae", "Raiko", "Tapani", "Maal\u00f8e", "Lars", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "In NIPS,", "year": 2016}, {"title": "Density ratio estimation in machine learning", "authors": ["Sugiyama", "Masashi", "Suzuki", "Taiji", "Kanamori", "Takafumi"], "year": 2012}, {"title": "Doubly stochastic variational bayes for non-conjugate inference", "authors": ["Titsias", "Michalis", "L\u00e1zaro-Gredilla", "Miguel"], "venue": "In ICML, pp. 1971\u20131979,", "year": 2014}, {"title": "Copula variational inference", "authors": ["Tran", "Dustin", "Blei", "David", "Airoldi", "Edo M"], "venue": "In NIPS, pp", "year": 2015}, {"title": "The variational Gaussian process", "authors": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "In ICLR,", "year": 2016}, {"title": "Hierarchical implicit models and likelihood-free variational inference", "authors": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David"], "venue": "In NIPS,", "year": 2017}, {"title": "Generative adversarial nets from a density ratio estimation perspective", "authors": ["Uehara", "Masatoshi", "Sato", "Issei", "Suzuki", "Masahiro", "Nakayama", "Kotaro", "Matsuo", "Yutaka"], "venue": "arXiv preprint arXiv:1610.02920,", "year": 2016}, {"title": "Softplus regressions and convex polytopes", "authors": ["Zhou", "Mingyuan"], "year": 2016}, {"title": "Negative binomial process count and mixture modeling", "authors": ["Zhou", "Mingyuan", "Carin", "Lawrence"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2015}, {"title": "Lognormal and gamma mixed negative binomial regression", "authors": ["Zhou", "Mingyuan", "Li", "Lingbo", "Dunson", "David", "Carin", "Lawrence"], "venue": "In ICML, pp", "year": 2012}, {"title": "Priors for random count matrices derived from a family of negative binomial processes", "authors": ["Zhou", "Mingyuan", "Padilla", "Oscar Hernan Madrid", "Scott", "James G"], "venue": "J. Amer. Statist. Assoc.,", "year": 2016}, {"title": "From Theorem 1, we have convexity, and according to Theorem 6.2.1", "authors": ["\u222b f(z)g(z)dz"], "venue": "Zabarankin", "year": 2005}, {"title": "Bayesian Logistic Regression We consider datesets waveform (n = 5000, V = 21, and 400/4600 for training/testing), spam (n = 3000, V = 2, and 2000/1000 for training/testing), and nodal (n = 53", "authors": [], "year": 2000}], "id": "SP:3c1bbf2125e70131850182fbdb126971f0bb7b6c", "authors": [{"name": "Mingzhang Yin", "affiliations": []}, {"name": "Mingyuan Zhou", "affiliations": []}], "abstractText": "Semi-implicit variational inference (SIVI) is introduced to expand the commonly used analytic variational distribution family, by mixing the variational parameter with a flexible distribution. This mixing distribution can assume any density function, explicit or not, as long as independent random samples can be generated via reparameterization. Not only does SIVI expand the variational family to incorporate highly flexible variational distributions, including implicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an upper bound, and further derives an asymptotically exact surrogate ELBO that is amenable to optimization via stochastic gradient ascent. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior in a variety of Bayesian inference tasks.", "title": "Semi-Implicit Variational Inference"}