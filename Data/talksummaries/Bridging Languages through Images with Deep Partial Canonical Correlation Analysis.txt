0	14	Research in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005).
2	37	These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context.
4	12	While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling.
6	27	In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information.
7	37	This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup.
8	33	The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language.
9	41	We propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method (Rao, 1969).
11	20	In short, PCCA is a variant of CCA which learns maximally correlated linear projections of two views (e.g., two language-specific “text-based views”) conditioned on a shared third view (e.g., the “visual view”).
13	30	PCCA inherits one disadvantageous property from CCA: both methods compute estimates for covariance matrices based on all training data.
14	29	This would prevent feasible training of their deep nonlinear variants, since deep neural nets (DNNs) are predominantly optimized via stochastic optimization algorithms.
42	14	Given two image descriptions x and y in two languages and an image z that they refer to, the task is to learn a shared bilingual space such that similar descriptions obtain similar representations in the induced space.
47	60	In what follows, we first review the CCA model and its deep variant: DCCA.
54	48	A final linear layer is added to resemble the linear CCA projection.
55	15	The goal is to project the features of X and Y into a shared L-dimensional (1 ≤ L ≤ min(D′x, D ′ y)) space such that the canonical correlation of the final outputs F (X) = W Tf(X) and G(Y ) = V T g(Y ) is maximized.
60	11	(2) The main disadvantage of DCCA is its inability to support more than two views, and to learn conditioned on an additional shared view, which is why we introduce Deep Partial CCA.
63	30	The objective is to maximize the canonical correlation of the first two views X and Y conditioned on the shared third variable Z.
64	17	Following Rao (1969)’s work on Partial CCA, we first consider two multivariate linear multiple regression models: F (X) = AZ + F (X|Z), (3) G(Y ) = BZ +G(Y |Z).
65	9	(4) A,B ∈ RL×Dz are matrices of coefficients, and F (X|Z),G(Y |Z) ∈ RL×N are normal random error matrices: residuals.
104	35	Cross-lingual Image Description Retrieval The cross-lingual image description retrieval task is formulated as follows: taking an image description as a query in the source language, the system has to retrieve a set of relevant descriptions in the target language which describe the same image.
115	17	Each image is associated with one English and one German description.
117	29	Multilingual Word Similarity The word similarity task tests the correlation between automatic and human generated word similarity scores.
121	23	Each WIW entry is a triplet: an English word, its translation in DE/IT/RU, and a set of images relevant to the pair.
123	22	After removing stop words and punctuation, we extract the 6,000 most frequent words from the cleaned corpus not present in SimLex.
125	23	The images are crawled from the Bing search engine using MMFeat9 (Kiela, 2016) by querying the EN words only.
126	24	Following the suggestions from the study of Kiela et al. (2016), we save the top 20 images as relevant images.10 Table 1 provides a summary of the WIW dataset.
127	350	The dataset contains both concrete and abstract words, and words of different POS tags.11 This property has an influence on the image collection: similar to Kiela et al. (2014), we have noticed that images of more concrete concepts are less dispersed (see also examples from Figure 2).
131	19	For German we use the deWaC 1.7B corpus (Baroni et al., 2009) to obtain 500-dimensional German embeddings using the same word embedding model.
134	12	For the word similarity task, we average the visual vectors across all images of each word pair as done in, e.g., (Vulić et al., 2016), before the PCA step.
135	28	Baseline Models We consider a wide variety of multi-view CCA-based baselines.
136	85	First, we compare against the original (linear) CCA model (Hotelling, 1936), and its deep non-linear extension DCCA (Andrew et al., 2013).
