0,1,label2,summary_sentences
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 364–369 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
364",text,[0],[0]
"Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.”",1 Introduction,[0],[0]
"Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1).",1 Introduction,[0],[0]
"They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment.
",1 Introduction,[0],[0]
We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass.,1 Introduction,[0],[0]
"Our model builds on a recent coreference resolution model (Lee et al., 2017), by making central use of learned, contextualized span representations.",1 Introduction,[0],[0]
We use these representations to predict SRL graphs directly over text spans.,1 Introduction,[0],[0]
"Each edge is identified by independently predicting which role, if any, holds between every possible pair of text spans, while using aggressive beam
1Code and models: https://github.com/luheng/lsgn
pruning for efficiency.",1 Introduction,[0],[0]
"The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes).
",1 Introduction,[0],[0]
"Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1).",1 Introduction,[0],[0]
"The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015).
",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given.",1 Introduction,[0],[0]
"In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank.",1 Introduction,[0],[0]
"It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018).",1 Introduction,[0],[0]
"We consider the space of possible predicates to be all the tokens in the input sentence, and the space of arguments to be all continuous spans.",2 Model,[0],[0]
"Our model decides what relation exists between each predicate-argument pair (including no relation).
",2 Model,[0],[0]
"Formally, given a sequence X = w1, . . .",2 Model,[0],[0]
", wn, we wish to predict a set of labeled predicateargument relations Y ⊆ P ×",2 Model,[0],[0]
"A × L, where P = {w1, . . .",2 Model,[0],[0]
", wn} is the set of all tokens (predicates), A = {(wi, . . .",2 Model,[0],[0]
", wj) | 1 ≤",2 Model,[0],[0]
"i ≤ j ≤ n} contains all the spans (arguments), and L is the space of semantic role labels, including a null label indicating no relation.",2 Model,[0],[0]
"The final SRL output would be all the non-empty relations {(p, a, l) ∈ Y",2 Model,[0],[0]
"| l 6= }.
",2 Model,[0],[0]
"We then define a set of random variables, where each random variable yp,a corresponds to a predicate p ∈ P and an argument a ∈ A, taking value from the discrete label space L.",2 Model,[0],[0]
"The random variables yp,a are conditionally independent of each other given the input X:
P (Y | X) = ∏
p∈P,a∈A P (yp,a | X) (1)
P (yp,a = l | X) = exp(φ(p, a, l))∑
l′∈L exp(φ(p, a, l′))
",2 Model,[0],[0]
"(2)
Where φ(p, a, l) is a scoring function for a possible (predicate, argument, label) combination.",2 Model,[0],[0]
"φ is decomposed into two unary scores on the predicate and the argument (defined in Section 3), as well as a label-specific score for the relation:
φ(p, a, l) = Φa(a) + Φp(p) +",2 Model,[0],[0]
"Φ (l) rel (a, p) (3)
",2 Model,[0],[0]
"The score for the null label is set to a constant: φ(p, a, ) = 0, similar to logistic regression.
",2 Model,[0],[0]
"Learning For each input X , we minimize the negative log likelihood of the gold structure Y ∗:
",2 Model,[0],[0]
J (X) =,2 Model,[0],[0]
"− logP (Y ∗ | X) (4)
Beam pruning As our model deals with O(n2) possible argument spans and O(n) possible predicates, it needs to consider O(n3|L|) possible relations, which is computationally impractical.",2 Model,[0],[0]
"To overcome this issue, we define two beams Ba and Bp for storing the candidate arguments and predicates, respectively.",2 Model,[0],[0]
The candidates in each beam are ranked by their unary score (Φa or Φp).,2 Model,[0],[0]
The sizes of the beams are limited by λan and λpn.,2 Model,[0],[0]
"Elements that fall out of the beam do not participate
in computing the edge factors Φ(l)rel , reducing the overall number of relational factors evaluated by the model to O(n2|L|).",2 Model,[0],[0]
"We also limit the maximum width of spans to a fixed number W (e.g. W = 30), further reducing the number of computed unary factors to O(n).",2 Model,[0],[0]
"Our model builds contextualized representations for argument spans a and predicate words p based on BiLSTM outputs (Figure 2) and uses feedforward networks to compute the factor scores in φ(p, a, l) described in Section 2 (Figure 3).
",3 Neural Architecture,[0],[0]
"Word-level contexts The bottom layer consists of pre-trained word embeddings concatenated with character-based representations, i.e. for each token wi, we have xi = [WORDEMB(wi); CHARCNN(wi)].",3 Neural Architecture,[0],[0]
"We then contextualize each xi using an m-layered bidirectional LSTM with highway connections (Zhang et al., 2016), which we denote as x̄i.
Argument and predicate representation We build contextualized representations for all candidate arguments a ∈ A and predicates p ∈ P .",3 Neural Architecture,[0],[0]
"The argument representation contains the following: end points from the BiLSTM outputs (x̄START(a), x̄END(a)), a soft head word xh(a), and embedded span width features f(a), similar to Lee et al. (2017).",3 Neural Architecture,[0],[0]
"The predicate representation is simply the BiLSTM output at the position INDEX(p).
",3 Neural Architecture,[0],[0]
"g(a) =[x̄START(a); x̄END(a); xh(a); f(a)] (5)
g(p) =x̄INDEX(p) (6)
The soft head representation xh(a) is an attention mechanism over word inputs x in the argument span, where the weights e(a) are computed via a linear layer over the BiLSTM outputs x̄.
xh(a) = xSTART(a):END(a)e(s) ᵀ (7) e(a)",3 Neural Architecture,[0],[0]
"= SOFTMAX(wᵀe x̄START(a):END(a)) (8)
xSTART(a):END(a) is a shorthand for stacking a list of vectors xt, where START(a) ≤ t ≤ END(a).
",3 Neural Architecture,[0],[0]
"Scoring The scoring functions Φ are implemented with feed-forward networks based on the predicate and argument representations g:
Φa(a) =w ᵀ a MLPa(g(a))",3 Neural Architecture,[0],[0]
(9) Φp(p),3 Neural Architecture,[0],[0]
"=w ᵀ pMLPp(g(p)) (10)
Φ (l) rel (a, p) =w (l)ᵀ r MLPr([g(a); g(p)]) (11)",3 Neural Architecture,[0],[0]
"We experiment on the CoNLL 2005 (Carreras and Màrquez, 2005) and CoNLL 2012 (OntoNotes 5.0, (Pradhan et al., 2013)) benchmarks, using two SRL setups: end-to-end and gold predicates.",4 Experiments,[0],[0]
"In the end-to-end setup, a system takes a tokenized sentence as input, and predicts all the predicates and their arguments.",4 Experiments,[0],[0]
"Systems are evaluated on the micro-averaged F1 for correctly predicting (predicate, argument span, label) tuples.",4 Experiments,[0],[0]
"For comparison with previous systems, we also report results with gold predicates, in which the complete set of predicates in the input sentence is given as well.",4 Experiments,[0],[0]
"Other experimental setups and hyperparameteres are listed in Appendix A.1.
ELMo embeddings To further improve performance, we also add ELMo word representations (Peters et al., 2018) to the BiLSTM input (in the +ELMo rows).",4 Experiments,[0],[0]
"Since the contextualized representations ELMo provides can be applied to most previous neural systems, the improvement is orthogonal to our contribution.",4 Experiments,[0],[0]
"In Table 1 and 2, we organize all the results into two categories: the comparable single model systems, and the mod-
els augmented with ELMo or ensembling (in the PoE rows).
",4 Experiments,[0],[0]
"End-to-end results As shown in Table 1,2 our joint model outperforms the previous best pipeline system (He et al., 2017) by an F1 difference of anywhere between 1.3 and 6.0 in every setting.",4 Experiments,[0],[0]
"The improvement is larger on the Brown test set, which is out-of-domain, and the CoNLL 2012 test set, which contains nominal predicates.",4 Experiments,[0],[0]
"On all datasets, our model is able to predict over 40% of the sentences completely correctly.
",4 Experiments,[0],[0]
"Results with gold predicates To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only.",4 Experiments,[0],[0]
"As shown in Table 2, our model significantly out-performs He et al. (2017), but falls short of Tan et al. (2018), a very recent attention-based (Vaswani et al., 2017)",4 Experiments,[0],[0]
BIO-tagging model that was developed concurrently with our work.,4 Experiments,[0],[0]
"By adding the contextualized ELMo representations, we are able to out-perform all previous systems, including Peters et al. (2018), which applies ELMo to the SRL model introduced in He et al. (2017).",4 Experiments,[0],[0]
Our model’s architecture differs significantly from previous BIO systems in terms of both input and decision space.,5 Analysis,[0],[0]
"To better understand our model’s strengths and weaknesses, we perform three analyses following Lee et al. (2017) and He et al. (2017), studying (1) the effectiveness of beam
2For the end-to-end setting on CoNLL 2012, we used a subset of the train/dev data from previous work due to noise in the dataset; the dev result is not directly comparable.",5 Analysis,[0],[0]
"See Appendix A.2 for detailed explanation.
",5 Analysis,[0],[0]
"pruning, (2) the ability to capture long-range dependencies, (3) agreement with syntactic spans, and (4) the ability to predict globally consistent SRL structures.",5 Analysis,[0],[0]
The analyses are performed on the development sets without using ELMo embeddings.,5 Analysis,[0],[0]
"3
Effectiveness of beam pruning Figure 4 shows the predicate and argument spans kept in the beam, sorted with their unary scores.",5 Analysis,[0],[0]
"Our model efficiently prunes unlikely argument spans and predicates, significantly reduces the number of edges it needs to consider.",5 Analysis,[0],[0]
Figure 5 shows the recall of predicate words on the CoNLL 2012 development set.,5 Analysis,[0],[0]
"By retaining λp = 0.4 predicates per word, we are able to keep over 99.7% argument-bearing predicates.",5 Analysis,[0],[0]
"Compared to having a part-of-speech tagger (POS:X in Figure 5), our joint beam pruning allowing the model to have a soft trade-off between efficiency and recall.4
Long-distance dependencies Figure 6 shows the performance breakdown by binned distance between arguments to the given predicates.",5 Analysis,[0],[0]
"Our model is better at accurately predicting arguments that are farther away from the predicates, even
3For comparability with prior work, analyses (2)-(4) are performed on the CoNLL 05 dev set with gold predicates.
",5 Analysis,[0],[0]
"4The predicate ID accuracy of our model is not comparable with that reported in He et al. (2017), since our model does not predict non-argument-bearing predicates.
",5 Analysis,[0],[0]
"compared to an ensemble model (He et al., 2017) that has a higher overall F1.",5 Analysis,[0],[0]
"This is very likely due to architectural differences; in a BIO tagger, predicate information passes through many LSTM timesteps before reaching a long-distance argument, whereas our architecture enables direct connections between all predicates-arguments pairs.
Agreement with syntax As mentioned in He et al. (2017), their BIO-based SRL system has good agreement with gold syntactic span boundaries (94.3%) but falls short of previous syntaxbased systems (Punyakanok et al., 2004).",5 Analysis,[0],[0]
"By directly modeling span information, our model achieves comparable syntactic agreement (95.0%) to Punyakanok et al. (2004) without explicitly modeling syntax.
",5 Analysis,[0],[0]
"Global consistency On the other hand, our model suffers from global consistency issues.",5 Analysis,[0],[0]
"For example, on the CoNLL 2005 test set, our model has lower complete-predicate accuracy (62.6%) than the BIO systems (He et al., 2017; Tan et al., 2018) (64.3%-66.4%).",5 Analysis,[0],[0]
"Table 3 shows its viola-
tions of global structural constraints5 compared to previous systems.",5 Analysis,[0],[0]
Our model made more constraint violations compared to previous systems.,5 Analysis,[0],[0]
"For example, our model predicts duplicate core arguments6 (shown in the U column in Table 3) more often than previous work.",5 Analysis,[0],[0]
"This is due to the fact that our model uses independent classifiers to label each predicate-argument pair, making it difficult for them to implicitly track the decisions made for several arguments with the same predicate.
",5 Analysis,[0],[0]
"The Ours+decode row in Table 3 shows SRL performance after enforcing the U-constraint using dynamic programming (Täckström et al., 2015) at decoding time.",5 Analysis,[0],[0]
"Constrained decoding at test time is effective at eliminating all the core-role inconsistencies (shown in the U-column), but did not bring significant gain on the end result (shown
5Punyakanok et al. (2008) described a list of global constraints for SRL systems, e.g., there can be at most one core argument of each type for each predicate.
6Arguments with labels ARG0,ARG1,. . .",5 Analysis,[0],[0]
",",5 Analysis,[0],[0]
"ARG5 and AA.
in SRL F1), which only evaluates the piece-wise predicate-argument structures.",5 Analysis,[0],[0]
"We proposed a new SRL model that is able to jointly predict all predicates and argument spans, generalized from a recent coreference system (Lee et al., 2017).",6 Conclusion and Future Work,[0],[0]
"Compared to previous BIO systems, our new model supports joint predicate identification and is able to incorporate span-level features.",6 Conclusion and Future Work,[0],[0]
"Empirically, the model does better at longrange dependencies and agreement with syntactic boundaries, but is weaker at global consistency, due to our strong independence assumption.
",6 Conclusion and Future Work,[0],[0]
"In the future, we could incorporate higher-order inference methods (Lee et al., 2018) to relax this assumption.",6 Conclusion and Future Work,[0],[0]
"It would also be interesting to combine our span-based architecture with the selfattention layers (Tan et al., 2018; Strubell et al., 2018) for more effective contextualization.",6 Conclusion and Future Work,[0],[0]
"This research was supported in part by the ARO (W911NF-16-1-0121), the NSF (IIS-1252835, IIS-1562364), a gift from Tencent, and an Allen Distinguished Investigator Award.",Acknowledgments,[0],[0]
"We thank Eunsol Choi, Dipanjan Das, Nicholas Fitzgerald, Ariel Holtzman, Julian Michael, Noah Smith, Swabha Swayamdipta, and our anonymous reviewers for helpful feedback.",Acknowledgments,[0],[0]
"Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features.",abstractText,[0],[0]
"We propose an endto-end approach for jointly predicting all predicates, arguments spans, and the relations between them.",abstractText,[0],[0]
"The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision.",abstractText,[0],[0]
Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.1,abstractText,[0],[0]
Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 401–406 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
401",text,[0],[0]
"Neural NER trains a deep neural network for the NER task and has become quite popular as they minimize the need for hand-crafted features and, learn feature representations from the training data itself.",1 Introduction,[0],[0]
"Recently, multilingual learning has been shown to benefit Neural NER in a resource-rich language setting (Gillick et al., 2016; Yang et al., 2017).",1 Introduction,[0],[0]
Multilingual learning aims to improve the NER performance on the language under consideration (primary language) by adding training data from one or more assisting languages.,1 Introduction,[0],[0]
The neural network is trained on the combined data of the primary (DP ) and the assisting languages (DA).,1 Introduction,[0],[0]
"The neural network has a combination of languagedependent and language-independent layers, and, the network learns better cross-lingual features via these language-independent layers.
∗This work began when the second author was a research scholar at IIT Bombay
Existing approaches add all training sentences from the assisting language to the primary language and train the neural network on the combined data.",1 Introduction,[0],[0]
"However, data from assisting languages can introduce a drift in the tag distribution for named entities, since the common named entities from the two languages may have vastly divergent tag distributions.",1 Introduction,[0],[0]
"For example, the entity China appears in training split of Spanish (primary) and English (assisting) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with the corresponding tag frequencies, Spanish = { Loc : 20, Org : 49, Misc : 1 } and English = { Loc : 91, Org : 7 }.",1 Introduction,[0],[0]
"By adding English data to Spanish, the tag distribution of China is skewed towards Location entity in Spanish.",1 Introduction,[0],[0]
This leads to a drop in named entity recognition performance.,1 Introduction,[0],[0]
"In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language.
",1 Introduction,[0],[0]
"The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task.",1 Introduction,[0],[0]
"For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017).",1 Introduction,[0],[0]
"For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011).",1 Introduction,[0],[0]
The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data.,1 Introduction,[0],[0]
"Out-of-domain sentences most similar to the in-domain data are added.
",1 Introduction,[0],[0]
"Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the train-
ing instances.",1 Introduction,[0],[0]
"Adding assisting language sentences with lower divergence reduces the possibility of entity drift enabling the multilingual model to learn better cross-lingual features.
",1 Introduction,[0],[0]
Following are the contributions of the paper: (a) We present a simple approach to select assisting language sentences based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages.,1 Introduction,[0],[0]
"We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences.",1 Introduction,[0],[0]
"To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER.",1 Introduction,[0],[0]
"For every assisting language sentence, we calculate the sentence score based on the average symmetric KL-Divergence score of overlapping entities present in that sentence.",2 Judicious Selection of Assisting Language Sentences,[0],[0]
"By overlapping entities, we mean entities whose surface form appears in both the languages’ training data.",2 Judicious Selection of Assisting Language Sentences,[0],[0]
"The symmetric KL-Divergence SKL(x), of a named entity x, is defined as follows,
SKL(x) =",2 Judicious Selection of Assisting Language Sentences,[0],[0]
[ KL( Pp(x) || Pa(x) ),2 Judicious Selection of Assisting Language Sentences,[0],[0]
+KL( Pa(x) ||,2 Judicious Selection of Assisting Language Sentences,[0],[0]
Pp(x) ) ] /2,2 Judicious Selection of Assisting Language Sentences,[0],[0]
"(1)
where Pp(x) and Pa(x) are the probability distributions for entity x in the primary (p) and the assisting (a) languages respectively.",2 Judicious Selection of Assisting Language Sentences,[0],[0]
"KL refers to the standard KL-Divergence score between the two probability distributions.
",2 Judicious Selection of Assisting Language Sentences,[0],[0]
KL-Divergence calculates the distance between the two probability distributions.,2 Judicious Selection of Assisting Language Sentences,[0],[0]
"Lower the KLDivergence score, higher is the tag agreement for an entity in both the languages thereby, reducing the possibility of entity drift in multilingual learning.",2 Judicious Selection of Assisting Language Sentences,[0],[0]
Assisting language sentences with the sentence score below a threshold value are added to the primary language data for multilingual learning.,2 Judicious Selection of Assisting Language Sentences,[0],[0]
"If an assisting language sentence contains no overlapping entities, the corresponding sentence score is zero resulting in its selection.
",2 Judicious Selection of Assisting Language Sentences,[0],[0]
"Network Architecture
Several deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature.",2 Judicious Selection of Assisting Language Sentences,[0],[0]
"Apart from the model by Collobert et al. (2011), remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs.",2 Judicious Selection of Assisting Language Sentences,[0],[0]
The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models.,2 Judicious Selection of Assisting Language Sentences,[0],[0]
"We choose the model by Murthy and Bhattacharyya (2016)1 in our experiments.
",2 Judicious Selection of Assisting Language Sentences,[0],[0]
"Multilingual Learning
We consider two parameter sharing configurations for multilingual learning (i) sub-word feature extractors shared across languages (Yang et al., 2017)",2 Judicious Selection of Assisting Language Sentences,[0],[0]
(Sub-word) (ii) the entire network trained in a language independent way (All).,2 Judicious Selection of Assisting Language Sentences,[0],[0]
"As Murthy and Bhattacharyya (2016) use CNNs to extract sub-word features, only the character-level CNNs are shared for the Sub-word configuration.
",2 Judicious Selection of Assisting Language Sentences,[0],[0]
1The code is available here: https://github.com/ murthyrudra/NeuralNER,2 Judicious Selection of Assisting Language Sentences,[0],[0]
In this section we list the datasets used and the network configurations used in our experiments.,3 Experimental Setup,[0],[0]
The Table 1 lists the datasets used in our experiments along with pre-trained word embeddings used and other dataset statistics.,3.1 Datasets,[0],[0]
"For German NER, we use ep-96-04-16.conll to create train and development splits, and use ep-96-04-15.conll as test split.",3.1 Datasets,[0],[0]
"As Italian has a different tag set compared to English, Spanish and Dutch, we do not share output layer for All configuration in multilingual experiments involving Italian.",3.1 Datasets,[0],[0]
"Even though the languages considered are resource-rich languages, we consider German and Italian as primary languages due to their relatively lower number of train tokens.",3.1 Datasets,[0],[0]
"The German NER data followed IO notation and for all experiments involving German, we converted other language data to IO notation.",3.1 Datasets,[0],[0]
"Similarly, the Italian NER data followed IOBES notation and for all experiments involving Italian, we converted other language data to IOBES notation.
",3.1 Datasets,[0],[0]
"For low-resource language setup, we consider the following Indian languages: Hindi, Marathi2, Bengali, Tamil and Malayalam.",3.1 Datasets,[0],[0]
Except for Hindi all are low-resource languages.,3.1 Datasets,[0],[0]
"We consider only Person, Location and Organization tags.",3.1 Datasets,[0],[0]
"Though the scripts of these languages are different, they share the same set of phonemes making script mapping across languages easier.",3.1 Datasets,[0],[0]
"We convert Tamil, Bengali and Malayalam data to the Devanagari script using the Indic NLP li-
2Data is available here: http://www.cfilt.iitb.",3.1 Datasets,[0],[0]
"ac.in/ner/annotated_corpus/
brary3 (Kunchukuttan et al., 2015)",3.1 Datasets,[0],[0]
"thereby, allowing sharing of sub-word features across the Indian languages.",3.1 Datasets,[0],[0]
"For Indian languages, the annotated data followed the IOB format.",3.1 Datasets,[0],[0]
"With the exception of English, Spanish and Dutch, remaining language datasets did not have official train and development splits provided.",3.2 Network Hyper-parameters,[0],[0]
We randomly select 70% of the train split for training the model and remaining as development split.,3.2 Network Hyper-parameters,[0],[0]
"The threshold for sentence score SKL, is selected based on cross-validation for every language pair.",3.2 Network Hyper-parameters,[0],[0]
The dimensions of the Bi-LSTM hidden layer are 200 and 400 for the monolingual and multilingual experiments respectively.,3.2 Network Hyper-parameters,[0],[0]
"We extract 20 features per convolution filter, with width varying from 1 to 9.",3.2 Network Hyper-parameters,[0],[0]
The initial learning rate is 0.4 and multiplied by 0.7 when validation error increases.,3.2 Network Hyper-parameters,[0],[0]
The training is stopped when the learning rate drops below 0.002.,3.2 Network Hyper-parameters,[0],[0]
"We assign a weight of 0.1 to assisting language sentences and oversample primary language sentences to match the assisting language sentence count in all multilingual experiments.
",3.2 Network Hyper-parameters,[0],[0]
"For European languages, we have performed hyper-parameter tuning for both the monolingual and multilingual learning (with all assisting language sentences) configurations.",3.2 Network Hyper-parameters,[0],[0]
The best hyperparameter values for the language pair involved were observed to be within similar range.,3.2 Network Hyper-parameters,[0],[0]
"Hence, we chose the same set of hyper-parameter values for all languages.
",3.2 Network Hyper-parameters,[0],[0]
3https://github.com/anoopkunchukuttan/ indic_nlp_library,3.2 Network Hyper-parameters,[0],[0]
We now present the results on both resource-rich and resource-poor languages.,4 Results,[0],[0]
Table 2 presents the results for German and Italian NER.,4.1 Resource-Rich Languages,[0],[0]
"We consistently observe improvements for German and Italian NER using our data selection strategy, irrespective of whether only subword features are shared (Sub-word) or the entire network (All) is shared across languages.
",4.1 Resource-Rich Languages,[0],[0]
Adding all Spanish/Dutch sentences to Italian data leads to drop in Italian NER performance when all layers are shared.,4.1 Resource-Rich Languages,[0],[0]
Label drift from overlapping entities is one of the reasons for the poor results.,4.1 Resource-Rich Languages,[0],[0]
This can be observed by comparing the histograms of English and Spanish sentences ranked by the SKL scores for Italian multilingual learning (Figure 1).,4.1 Resource-Rich Languages,[0],[0]
Most English sentences have lower SKL scores indicating higher tag agreement for overlapping entities and lower drift in tag distribution.,4.1 Resource-Rich Languages,[0],[0]
"Hence, adding all English sentences improves Italian NER accuracy.",4.1 Resource-Rich Languages,[0],[0]
"In contrast, most Spanish sentences have larger SKL
scores and adding these sentences adversely impacts Italian NER performance.",4.1 Resource-Rich Languages,[0],[0]
"By judiciously selecting assisting language sentences, we eliminate sentences which are responsible for drift occurring during multilingual learning.
",4.1 Resource-Rich Languages,[0],[0]
"To understand how overlapping entities impact the NER performance, we study the statistics of overlapping named entities between ItalianEnglish and Italian-Spanish pairs.",4.1 Resource-Rich Languages,[0],[0]
911 and 916 unique entities out of 4061 unique Italian entities appear in the English and Spanish data respectively.,4.1 Resource-Rich Languages,[0],[0]
We had hypothesized that entities with divergent tag distribution are responsible for hindering the performance in multilingual learning.,4.1 Resource-Rich Languages,[0],[0]
If we sort the common entities based on their SKL divergence value.,4.1 Resource-Rich Languages,[0],[0]
We observe that 484 out of 911 common entities in English and 535 out of 916 common entities in Spanish have an SKL score greater than 1.0. 162 out of 484 common entities in English-Italian data having SKL divergence value greater than 1.0 also appear more than 10 times in the English corpus.,4.1 Resource-Rich Languages,[0],[0]
"Similarly, 123 out of 535 common entities in Spanish-Italian data having SKL divergence value greater than 1.0 also appear more than 10 times in the Spanish corpus.",4.1 Resource-Rich Languages,[0],[0]
"However, these common 162 entities have a combined frequency of 12893 in English, meanwhile the 123 common entities have a combined frequency of 34945 in Spanish.",4.1 Resource-Rich Languages,[0],[0]
"To summarize, although the number of overlapping entities is comparable in English and Spanish sentences, entities with larger SKL divergence score appears more frequently in Spanish sentences compared to English sentences.",4.1 Resource-Rich Languages,[0],[0]
"As a consequence, adding all Spanish sentences leads to significant drop in Italian NER performance which is not the case when all English sentences are added.",4.1 Resource-Rich Languages,[0],[0]
"As Indian languages exhibit high lexical overlap (Kunchukuttan and Bhattacharyya, 2016) and syntactic relatedness (V Subbãrão, 2012), we share all layers of the network across languages.",4.2 Resource-Poor Languages,[0],[0]
Table 3 presents the results.,4.2 Resource-Poor Languages,[0],[0]
"Bengali, Malayalam, and Tamil (low-resource languages) benefits from our data selection strategy.",4.2 Resource-Poor Languages,[0],[0]
"Hindi and Marathi NER performance improves when the other is used as assisting language.
",4.2 Resource-Poor Languages,[0],[0]
"Bengali, Malayalam, and Tamil have weaker baselines compared to Hindi and Marathi, and are benefited from our approach irrespective of the assisting language chosen.",4.2 Resource-Poor Languages,[0],[0]
"However, Hindi and Marathi are not benefited from multilingual learning with Bengali, Malayalam and Tamil.",4.2 Resource-Poor Languages,[0],[0]
Malayalam and Tamil being morphologically rich have low entity overlap (surface level) with Hindi and Marathi.,4.2 Resource-Poor Languages,[0],[0]
"As a result, only 2-3% of Malayalam and Tamil sentences are eliminated from our approach, leading to no gains from multilingual learning.",4.2 Resource-Poor Languages,[0],[0]
Hindi and Marathi are negatively impacted by noisy Bengali data.,4.2 Resource-Poor Languages,[0],[0]
"Bengali has less training sentences compared to other languages and, choosing a low SKL threshold results in selecting very few Bengali sentences for multilingual learning.",4.2 Resource-Poor Languages,[0],[0]
"Here, we study the influence of SKL score threshold on the NER performance.",4.3 Influence of SKL Threshold,[0],[0]
We run experiments for Italian NER by adding Spanish training sentences and sharing all layers except for output layer across languages.,4.3 Influence of SKL Threshold,[0],[0]
"We vary the threshold value from 1.0 to 9.0 in steps of 1, and select sentences with score less than the threshold.",4.3 Influence of SKL Threshold,[0],[0]
"A threshold of 0.0 indicates monolingual training and threshold greater than 9.0 indicates all assist-
ing language sentences considered.",4.3 Influence of SKL Threshold,[0],[0]
The plot of Italian test F-Score against SKL score is shown in the Figure 2.,4.3 Influence of SKL Threshold,[0],[0]
Italian test F-Score increases initially as we add more and more Spanish sentences and then drops due to influence of drift becoming significant.,4.3 Influence of SKL Threshold,[0],[0]
"Finding the right SKL threshold is important, hence we use a validation set to tune the SKL threshold.",4.3 Influence of SKL Threshold,[0],[0]
"In this paper, we address the problem of divergence in tag distribution between primary and assisting languages for multilingual Neural NER.",5 Conclusion,[0],[0]
We show that filtering out the assisting language sentences exhibiting significant divergence in the tag distribution can improve NER accuracy.,5 Conclusion,[0],[0]
We propose to use the symmetric KL-Divergence metric to measure the tag distribution divergence.,5 Conclusion,[0],[0]
We observe consistent improvements in multilingual Neural NER performance using our data selection strategy.,5 Conclusion,[0],[0]
"The strategy shows benefits for extremely low resource primary languages too.
",5 Conclusion,[0],[0]
"This problem of drift in data distribution may not be unique to multilingual NER, and we plan to study the influence of data selection for multilingual learning on other NLP tasks like sentiment analysis, question answering, neural machine translation, etc.",5 Conclusion,[0],[0]
"We also plan to explore more metrics for multilingual learning, specifically for morphologically rich languages.",5 Conclusion,[0],[0]
"We thank Gajanan Rane and Geetanjali Rane for annotating the Marathi data, which was created as part of the CLIA project.",Acknowledgements,[0],[0]
Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages.,abstractText,[0],[0]
"Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages.",abstractText,[0],[0]
We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning.,abstractText,[0],[0]
"To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language.",abstractText,[0],[0]
"We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.",abstractText,[0],[0]
Judicious Selection of Training Data in Assisting Language for Multilingual Neural NER,title,[0],[0]
The key challenge of drug discovery is to find target molecules with desired chemical properties.,1. Introduction,[0],[0]
"Currently, this task takes years of development and exploration by expert chemists and pharmacologists.",1. Introduction,[0],[0]
Our ultimate goal is to automate this process.,1. Introduction,[0],[0]
"From a computational perspective, we decompose the challenge into two complementary subtasks: learning to represent molecules in a continuous manner that facilitates the prediction and optimization of their properties (encoding); and learning to map an optimized continuous representation back into a molecular graph with improved properties (decoding).",1. Introduction,[0],[0]
"While deep learning has been extensively investigated for molecular graph encoding (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017), the harder combinatorial task of molecular graph generation from latent representation remains under-explored.
",1. Introduction,[0],[0]
1MIT Computer Science & Artificial Intelligence Lab.,1. Introduction,[0],[0]
"Correspondence to: Wengong Jin <wengong@csail.mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Prior work on drug design formulated the graph generation task as a string generation problem (Gómez-Bombarelli et al., 2016; Kusner et al., 2017) in an attempt to side-step direct generation of graphs.",1. Introduction,[0],[0]
"Specifically, these models start by generating SMILES (Weininger, 1988), a linear string notation used in chemistry to describe molecular structures.",1. Introduction,[0],[0]
"SMILES strings can be translated into graphs via deterministic mappings (e.g., using RDKit (Landrum, 2006)).",1. Introduction,[0],[0]
"However, this design has two critical limitations.",1. Introduction,[0],[0]
"First, the SMILES representation is not designed to capture molecular similarity.",1. Introduction,[0],[0]
"For instance, two molecules with similar chemical structures may be encoded into markedly different SMILES strings (e.g., Figure 1).",1. Introduction,[0],[0]
This prevents generative models like variational autoencoders from learning smooth molecular embeddings.,1. Introduction,[0],[0]
"Second, essential chemical properties such as molecule validity are easier to express on graphs rather than linear SMILES representations.",1. Introduction,[0],[0]
"We hypothesize that operating directly on graphs improves generative modeling of valid chemical structures.
",1. Introduction,[0],[0]
Our primary contribution is a new generative model of molecular graphs.,1. Introduction,[0],[0]
While one could imagine solving the problem in a standard manner – generating graphs node by node – the approach is not ideal for molecules.,1. Introduction,[0],[0]
"This is because creating molecules atom by atom would force the model to generate chemically invalid intermediaries (see, e.g., Figure 2), delaying validation until a complete graph is generated.",1. Introduction,[0],[0]
"Instead, we propose to generate molecular graphs in two phases by exploiting valid subgraphs as components.",1. Introduction,[0],[0]
"The overall generative approach, cast as a junction tree variational autoencoder, first generates a tree structured object (a junction tree) whose role is to represent the scaffold of subgraph components and their coarse relative arrangements.",1. Introduction,[0],[0]
The components are valid chemical substructures automatically extracted from the training set using tree decomposition and are used as building blocks.,1. Introduction,[0],[0]
"In the sec-
ond phase, the subgraphs (nodes in the tree) are assembled together into a coherent molecular graph.
",1. Introduction,[0],[0]
We evaluate our model on multiple tasks ranging from molecular generation to optimization of a given molecule according to desired properties.,1. Introduction,[0],[0]
"As baselines, we utilize state-of-the-art SMILES-based generation approaches (Kusner et al., 2017; Dai et al., 2018).",1. Introduction,[0],[0]
"We demonstrate that our model produces 100% valid molecules when sampled from a prior distribution, outperforming the top performing baseline by a significant margin.",1. Introduction,[0],[0]
"In addition, we show that our model excels in discovering molecules with desired properties, yielding a 30% relative gain over the baselines.",1. Introduction,[0],[0]
"Our approach extends the variational autoencoder (Kingma & Welling, 2013) to molecular graphs by introducing a suitable encoder and a matching decoder.",2. Junction Tree Variational Autoencoder,[0],[0]
"Deviating from previous work (Gómez-Bombarelli et al., 2016; Kusner et al., 2017), we interpret each molecule as having been built from subgraphs chosen out of a vocabulary of valid components.",2. Junction Tree Variational Autoencoder,[0],[0]
These components are used as building blocks both when encoding a molecule into a vector representation as well as when decoding latent vectors back into valid molecular graphs.,2. Junction Tree Variational Autoencoder,[0],[0]
"The key advantage of this view is that the decoder can realize a valid molecule piece by piece by utilizing the collection of valid components and how they interact, rather than trying to build the molecule atom by atom through chemically invalid intermediaries (Figure 2).",2. Junction Tree Variational Autoencoder,[0],[0]
"An aromatic bond, for example, is chemically invalid on its own unless the entire aromatic ring is present.",2. Junction Tree Variational Autoencoder,[0],[0]
"It would be therefore challenging to learn to build rings atom by atom rather than by introducing rings as part of the basic vocabulary.
",2. Junction Tree Variational Autoencoder,[0],[0]
"Our vocabulary of components, such as rings, bonds and individual atoms, is chosen to be large enough so that a given molecule can be covered by overlapping components or clusters of atoms.",2. Junction Tree Variational Autoencoder,[0],[0]
"The clusters serve the role analogous to cliques in graphical models, as they are expressive enough that a molecule can be covered by overlapping clusters without forming cluster cycles.",2. Junction Tree Variational Autoencoder,[0],[0]
"In this sense, the clusters serve as cliques in a (non-optimal) triangulation of the molecular graph.",2. Junction Tree Variational Autoencoder,[0],[0]
We form a junction tree of such clusters and use it as the tree representation of the molecule.,2. Junction Tree Variational Autoencoder,[0],[0]
"Since our choice of cliques is constrained a priori, we cannot guarantee that a junction tree exists with such clusters for an arbitrary
molecule.",2. Junction Tree Variational Autoencoder,[0],[0]
"However, our clusters are built on the basis of the molecules in the training set to ensure that a corresponding junction tree can be found.",2. Junction Tree Variational Autoencoder,[0],[0]
"Empirically, our clusters cover most of the molecules in the test set.
",2. Junction Tree Variational Autoencoder,[0],[0]
The original molecular graph and its associated junction tree offer two complementary representations of a molecule.,2. Junction Tree Variational Autoencoder,[0],[0]
We therefore encode the molecule into a two-part latent representation z,2. Junction Tree Variational Autoencoder,[0],[0]
=,2. Junction Tree Variational Autoencoder,[0],[0]
"[zT , zG] where zT encodes the tree structure and what the clusters are in the tree without fully capturing how exactly the clusters are mutually connected.",2. Junction Tree Variational Autoencoder,[0],[0]
zG encodes the graph to capture the fine-grained connectivity.,2. Junction Tree Variational Autoencoder,[0],[0]
Both parts are created by tree and graph encoders q(zT |T ) and q(zG|G).,2. Junction Tree Variational Autoencoder,[0],[0]
The latent representation is then decoded back into a molecular graph in two stages.,2. Junction Tree Variational Autoencoder,[0],[0]
"As illustrated in Figure 3, we first reproduce the junction tree using a tree decoder p(T |zT )",2. Junction Tree Variational Autoencoder,[0],[0]
based on the information in zT .,2. Junction Tree Variational Autoencoder,[0],[0]
"Second, we predict the fine grain connectivity between the clusters in the junction tree using a graph decoder p(G|T , zG) to realize the full molecular graph.",2. Junction Tree Variational Autoencoder,[0],[0]
"The junction tree approach allows us to maintain chemical feasibility during generation.
",2. Junction Tree Variational Autoencoder,[0],[0]
"Notation A molecular graph is defined as G = (V,E) where V is the set of atoms (vertices) and E the set of bonds (edges).",2. Junction Tree Variational Autoencoder,[0],[0]
Let N(x) be the neighbor of x. We denote sigmoid function as σ(·) and ReLU function as τ(·).,2. Junction Tree Variational Autoencoder,[0],[0]
"We use i, j, k for nodes in the tree and u, v, w for nodes in the graph.",2. Junction Tree Variational Autoencoder,[0],[0]
A tree decomposition maps a graph G into a junction tree by contracting certain vertices into a single node so that G becomes cycle-free.,2.1. Junction Tree,[0],[0]
"Formally, given a graph G, a junction tree TG = (V, E ,X ) is a connected labeled tree whose node set is V = {C1, · · · , Cn} and edge set is E .",2.1. Junction Tree,[0],[0]
"Each node or cluster Ci = (Vi, Ei) is an induced subgraph of G, satisfying the following constraints:
1.",2.1. Junction Tree,[0],[0]
"The union of all clusters equals G. That is, ⋃ i Vi = V
and ⋃ iEi = E.
2.",2.1. Junction Tree,[0],[0]
"Running intersection: For all clusters Ci, Cj and Ck, Vi ∩ Vj ⊆ Vk if Ck is on the path from Ci to Cj .
",2.1. Junction Tree,[0],[0]
"Viewing induced subgraphs as cluster labels, junction trees are labeled trees with label vocabulary X .",2.1. Junction Tree,[0],[0]
"By our molecule tree decomposition, X contains only cycles (rings) and single edges.",2.1. Junction Tree,[0],[0]
"Thus the vocabulary size is limited (|X | = 780 for a standard dataset with 250K molecules).
",2.1. Junction Tree,[0],[0]
"Tree Decomposition of Molecules Here we present our tree decomposition algorithm tailored for molecules, which finds its root in chemistry (Rarey & Dixon, 1998).",2.1. Junction Tree,[0],[0]
Our cluster vocabulary X includes chemical structures such as bonds and rings (Figure 3).,2.1. Junction Tree,[0],[0]
"Given a graphG, we first find all its simple cycles, and its edges not belonging to any cycles.",2.1. Junction Tree,[0],[0]
"Two simple rings are merged together if they have more than two overlapping atoms, as they constitute a specific structure called bridged compounds (Clayden et al., 2001).",2.1. Junction Tree,[0],[0]
Each of those cycles or edges is considered as a cluster.,2.1. Junction Tree,[0],[0]
"Next, a cluster graph is constructed by adding edges between all intersecting clusters.",2.1. Junction Tree,[0],[0]
"Finally, we select one of its spanning trees as the junction tree of G (Figure 3).",2.1. Junction Tree,[0],[0]
"As a result of ring merging, any two clusters in the junction tree have at most two atoms in common, facilitating efficient inference in the graph decoding phase.",2.1. Junction Tree,[0],[0]
The detailed procedure is described in the supplementary.,2.1. Junction Tree,[0],[0]
"We first encode the latent representation of G by a graph message passing network (Dai et al., 2016; Gilmer et al., 2017).",2.2. Graph Encoder,[0],[0]
"Each vertex v has a feature vector xv indicating the atom type, valence, and other properties.",2.2. Graph Encoder,[0],[0]
"Similarly, each edge (u, v) ∈ E has a feature vector xuv indicating its bond type, and two hidden vectors νuv and νvu denoting the message from u to v and vice versa.",2.2. Graph Encoder,[0],[0]
"Due to the loopy structure of the graph, messages are exchanged in a loopy belief propagation fashion:
ν(t)uv = τ(W",2.2. Graph Encoder,[0],[0]
"g 1xu +W g 2xuv +W g 3 ∑ w∈N(u)\v ν(t−1)wu ) (1)
where ν(t)uv is the message computed in t-th iteration, initialized with ν(0)uv = 0.",2.2. Graph Encoder,[0],[0]
"After T steps of iteration, we aggregate
those messages as the latent vector of each vertex, which captures its local graphical structure:
hu =",2.2. Graph Encoder,[0],[0]
"τ(U g 1xu + ∑ v∈N(u) Ug2ν (T ) vu ) (2)
",2.2. Graph Encoder,[0],[0]
The final graph representation is hG = ∑ i hi/|V |.,2.2. Graph Encoder,[0],[0]
The mean µG and log variance logσG of the variational posterior approximation are computed from hG with two separate affine layers.,2.2. Graph Encoder,[0],[0]
"zG is sampled from a Gaussian N (µG,σG).",2.2. Graph Encoder,[0],[0]
We similarly encode TG with a tree message passing network.,2.3. Tree Encoder,[0],[0]
Each cluster Ci is represented by a one-hot encoding xi representing its label type.,2.3. Tree Encoder,[0],[0]
"Each edge (Ci, Cj) is associated with two message vectors mij and mji.",2.3. Tree Encoder,[0],[0]
We pick an arbitrary leaf node as the root and propagate messages in two phases.,2.3. Tree Encoder,[0],[0]
"In the first bottom-up phase, messages are initiated from the leaf nodes and propagated iteratively towards root.",2.3. Tree Encoder,[0],[0]
"In the top-down phase, messages are propagated from the root to all the leaf nodes.",2.3. Tree Encoder,[0],[0]
"Message mij is updated as:
mij = GRU(xi, {mki}k∈N(i)\j) (3)
where GRU is a Gated Recurrent Unit (Chung et al., 2014; Li et al., 2015) adapted for tree message passing:
sij = ∑
k∈N(i)\j mki (4)
",2.3. Tree Encoder,[0],[0]
zij = σ(W zxi +U zsij + b z) (5) rki = σ(W rxi +U rmki + b r),2.3. Tree Encoder,[0],[0]
"(6)
m̃ij = tanh(Wxi +U ∑
k∈N(i)\j
rki mki) (7)
mij = (1− zij) sij + zij m̃ij (8)
The message passing follows the schedule where mij is computed only when all its precursors {mki | k ∈ N(i)\j} have been computed.",2.3. Tree Encoder,[0],[0]
"This architectural design is motivated by the belief propagation algorithm over trees and is thus different from the graph encoder.
",2.3. Tree Encoder,[0],[0]
"After the message passing, we obtain the latent representation of each node hi by aggregating its inward messages:
hi = τ(W oxi + ∑ k∈N(i) Uomki) (9)
",2.3. Tree Encoder,[0],[0]
"The final tree representation is hTG = hroot, which encodes a rooted tree (T , root).",2.3. Tree Encoder,[0],[0]
"Unlike the graph encoder, we do not apply node average pooling because it confuses the tree decoder which node to generate first.",2.3. Tree Encoder,[0],[0]
zTG is sampled in a similar way as in the graph encoder.,2.3. Tree Encoder,[0],[0]
"For simplicity, we abbreviate zTG as zT from now on.
",2.3. Tree Encoder,[0],[0]
This tree encoder plays two roles in our framework.,2.3. Tree Encoder,[0],[0]
"First, it is used to compute zT , which only requires the bottom-up phase of the network.",2.3. Tree Encoder,[0],[0]
"Second, after a tree T̂ is decoded
from zT , it is used to compute messages m̂ij over the entire T̂ , to provide essential contexts of every node during graph decoding.",2.3. Tree Encoder,[0],[0]
This requires both top-down and bottom-up phases.,2.3. Tree Encoder,[0],[0]
We will elaborate this in section 2.5.,2.3. Tree Encoder,[0],[0]
We decode a junction tree T from its encoding zT with a tree structured decoder.,2.4. Tree Decoder,[0],[0]
The tree is constructed in a top-down fashion by generating one node at a time.,2.4. Tree Decoder,[0],[0]
"As illustrated in Figure 4, our tree decoder traverses the entire tree from the root, and generates nodes in their depth-first order.",2.4. Tree Decoder,[0],[0]
"For every visited node, the decoder first makes a topological prediction: whether this node has children to be generated.",2.4. Tree Decoder,[0],[0]
"When a new child node is created, we predict its label and recurse this process.",2.4. Tree Decoder,[0],[0]
Recall that cluster labels represent subgraphs in a molecule.,2.4. Tree Decoder,[0],[0]
"The decoder backtracks when a node has no more children to generate.
",2.4. Tree Decoder,[0],[0]
"At each time step, a node receives information from other nodes in the current tree for making those predictions.",2.4. Tree Decoder,[0],[0]
The information is propagated through message vectors hij when trees are incrementally constructed.,2.4. Tree Decoder,[0],[0]
"Formally, let Ẽ = {(i1, j1), · · · , (im, jm)} be the edges traversed in a depth first traversal over T = (V, E), where m = 2|E| as each edge is traversed in both directions.",2.4. Tree Decoder,[0],[0]
The model visits node it at time t. Let Ẽt be the first t edges in Ẽ .,2.4. Tree Decoder,[0],[0]
"The message hit,jt is updated through previous messages:
hit,jt = GRU(xit , {hk,it}(k,it)∈Ẽt,k 6=jt) (10)
where GRU is the same recurrent unit as in the tree encoder.
",2.4. Tree Decoder,[0],[0]
"Topological Prediction When the model visits node it, it makes a binary prediction on whether it still has children to be generated.",2.4. Tree Decoder,[0],[0]
"We compute this probability by combining
Algorithm 1 Tree decoding at sampling time Require:",2.4. Tree Decoder,[0],[0]
"Latent representation zT
1: Initialize: Tree T̂ ← ∅ 2: function SampleTree(i, t) 3:",2.4. Tree Decoder,[0],[0]
Set Xi ← all cluster labels that are chemically compatible with node i and its current neighbors.,2.4. Tree Decoder,[0],[0]
4: Set dt ← expand with probability pt. .,2.4. Tree Decoder,[0],[0]
Eq.(11) 5:,2.4. Tree Decoder,[0],[0]
if dt = expand and Xi 6= ∅,2.4. Tree Decoder,[0],[0]
then 6: Create a node j and add it to tree T̂ .,2.4. Tree Decoder,[0],[0]
"7: Sample the label of node j from Xi .. Eq.(12) 8: SampleTree(j, t+ 1) 9: end if
10: end function
zT , node features xit and inward messages hk,it via a one hidden layer network followed by a sigmoid function:
pt = σ(u d ·τ(Wd1xit+Wd2zT +Wd3 ∑ (k,it)∈Ẽt hk,it) (11)
Label Prediction When a child node j is generated from its parent i, we predict its node label with
qj = softmax(Ulτ(Wl1zT",2.4. Tree Decoder,[0],[0]
+,2.4. Tree Decoder,[0],[0]
W l 2hij)),2.4. Tree Decoder,[0],[0]
"(12)
where qj is a distribution over label vocabulary X .",2.4. Tree Decoder,[0],[0]
"When j is a root node, its parent i is a virtual node and hij = 0.
",2.4. Tree Decoder,[0],[0]
Learning The tree decoder aims to maximize the likelihood p(T |zT ).,2.4. Tree Decoder,[0],[0]
"Let p̂t ∈ {0, 1} and q̂j be the ground truth topological and label values, the decoder minimizes the following cross entropy loss:1
Lc(T ) =",2.4. Tree Decoder,[0],[0]
"∑
t Ld(pt, p̂t)",2.4. Tree Decoder,[0],[0]
"+ ∑ j Ll(qj , q̂j) (13)
",2.4. Tree Decoder,[0],[0]
"Similar to sequence generation, during training we perform teacher forcing: after topological and label prediction at each step, we replace them with their ground truth so that the model makes predictions given correct histories.
",2.4. Tree Decoder,[0],[0]
Decoding & Feasibility Check Algorithm 1 shows how a tree is sampled from zT .,2.4. Tree Decoder,[0],[0]
The tree is constructed recursively guided by topological predictions without any external guidance used in training.,2.4. Tree Decoder,[0],[0]
"To ensure the sampled tree could be realized into a valid molecule, we define set Xi to be cluster labels that are chemically compatible with node i and its current neighbors.",2.4. Tree Decoder,[0],[0]
"When a child node j is generated from node i, we sample its label from Xi with a renormalized distribution qj over Xi by masking out invalid labels.",2.4. Tree Decoder,[0],[0]
"The final step of our model is to reproduce a molecular graph G that underlies the predicted junction tree T̂ = (V̂, Ê).
",2.5. Graph Decoder,[0],[0]
1The node ordering is not unique as the order within sibling nodes is ambiguous.,2.5. Graph Decoder,[0],[0]
"In this paper we train our model with one ordering and leave this issue for future work.
",2.5. Graph Decoder,[0],[0]
Note that this step is not deterministic since there are potentially many molecules that correspond to the same junction tree.,2.5. Graph Decoder,[0],[0]
The underlying degree of freedom pertains to how neighboring clusters Ci and Cj are attached to each other as subgraphs.,2.5. Graph Decoder,[0],[0]
"Our goal here is to assemble the subgraphs (nodes in the tree) together into the correct molecular graph.
",2.5. Graph Decoder,[0],[0]
Let G(T ) be the set of graphs whose junction tree is T .,2.5. Graph Decoder,[0],[0]
"Decoding graph Ĝ from T̂ = (V̂, Ê) is a structured prediction:
Ĝ = arg max G′∈G(T̂ )
",2.5. Graph Decoder,[0],[0]
"fa(G′) (14)
where fa is a scoring function over candidate graphs.",2.5. Graph Decoder,[0],[0]
We only consider scoring functions that decompose across the clusters and their neighbors.,2.5. Graph Decoder,[0],[0]
"In other words, each term in the scoring function depends only on how a cluster Ci is attached to its neighboring clusters",2.5. Graph Decoder,[0],[0]
"Cj , j ∈ NT̂ (i) in the tree T̂ .",2.5. Graph Decoder,[0],[0]
The problem of finding the highest scoring graph Ĝ – the assembly task – could be cast as a graphical model inference task in a model induced by the junction tree.,2.5. Graph Decoder,[0],[0]
"However, for efficiency reasons, we will assemble the molecular graph one neighborhood at a time, following the order in which the tree itself was decoded.",2.5. Graph Decoder,[0],[0]
"In other words, we start by sampling the assembly of the root and its neighbors according to their
scores.",2.5. Graph Decoder,[0],[0]
"Then we proceed to assemble the neighbors and their associated clusters (removing the degrees of freedom set by the root assembly), and so on.
",2.5. Graph Decoder,[0],[0]
It remains to be specified how each neighborhood realization is scored.,2.5. Graph Decoder,[0],[0]
"Let Gi be the subgraph resulting from a particular merging of cluster Ci in the tree with its neighbors Cj , j ∈ NT̂ (i).",2.5. Graph Decoder,[0],[0]
We score Gi as a candidate subgraph by first deriving a vector representation hGi and then using fai (Gi) = hGi · zG as the subgraph score.,2.5. Graph Decoder,[0],[0]
"To this end, let u, v specify atoms in the candidate subgraph Gi and let αv = i",2.5. Graph Decoder,[0],[0]
if v ∈ Ci and αv = j if v ∈,2.5. Graph Decoder,[0],[0]
Cj \ Ci.,2.5. Graph Decoder,[0],[0]
"The indices αv are used to mark the position of the atoms in the junction tree, and to retrieve messages m̂i,j summarizing the subtree under i along the edge (i, j) obtained by running the tree encoding algorithm.",2.5. Graph Decoder,[0],[0]
"The neural messages pertaining to the atoms and bonds in subgraph Gi are obtained and aggregated into hGi , similarly to the encoding step, but with different (learned) parameters:
µ(t)uv = τ(W a 1xu +W a 2xuv +W a 3µ̃ (t−1) uv ) (15)
µ̃(t−1)uv =
{∑ w∈N(u)\v µ (t−1) wu",2.5. Graph Decoder,[0],[0]
"αu = αv
m̂αu,αv + ∑ w∈N(u)\v µ",2.5. Graph Decoder,[0],[0]
(t−1) wu,2.5. Graph Decoder,[0],[0]
"αu 6= αv
The major difference from Eq.",2.5. Graph Decoder,[0],[0]
"(1) is that we augment the model with tree messages m̂αu,αv derived by running the tree encoder over the predicted tree T̂ .",2.5. Graph Decoder,[0],[0]
"m̂αu,αv provides a tree dependent positional context for bond (u, v) (illustrated as subtree A in Figure 5).
",2.5. Graph Decoder,[0],[0]
"Learning The graph decoder parameters are learned to maximize the log-likelihood of predicting correct subgraphs Gi of the ground true graph G at each tree node:
Lg(G)",2.5. Graph Decoder,[0],[0]
= ∑ i fa(Gi)− log ∑ G′i∈Gi exp(fa(G′i))  ,2.5. Graph Decoder,[0],[0]
"(16) where Gi is the set of possible candidate subgraphs at tree node i. During training, we again apply teacher forcing, i.e. we feed the graph decoder with ground truth trees as input.
",2.5. Graph Decoder,[0],[0]
Complexity,2.5. Graph Decoder,[0],[0]
"By our tree decomposition, any two clusters share at most two atoms, so we only need to merge at most two atoms or one bond.",2.5. Graph Decoder,[0],[0]
"By pruning chemically invalid subgraphs and merging isomorphic graphs, |Gi| ≈ 4 on average when tested on a standard ZINC drug dataset.",2.5. Graph Decoder,[0],[0]
"The computational complexity of JT-VAE is therefore linear in the number of clusters, scaling nicely to large graphs.",2.5. Graph Decoder,[0],[0]
Our evaluation efforts measure various aspects of molecular generation.,3. Experiments,[0],[0]
"The first two evaluations follow previously proposed tasks (Kusner et al., 2017).",3. Experiments,[0],[0]
"We also introduce a third task — constrained molecule optimization.
",3. Experiments,[0],[0]
"• Molecule reconstruction and validity We test the VAE models on the task of reconstructing input molecules from their latent representations, and decoding valid molecules when sampling from prior distribution.",3. Experiments,[0],[0]
(Section 3.1) •,3. Experiments,[0],[0]
"Bayesian optimization Moving beyond generating valid
molecules, we test how the model can produce novel molecules with desired properties.",3. Experiments,[0],[0]
"To this end, we perform Bayesian optimization in the latent space to search molecules with specified properties.",3. Experiments,[0],[0]
"(Section 3.2)
• Constrained molecule optimization The task is to modify given molecules to improve specified properties, while constraining the degree of deviation from the original molecule.",3. Experiments,[0],[0]
"This is a more realistic scenario in drug discovery, where development of new drugs usually starts with known molecules such as existing drugs (Besnard et al., 2012).",3. Experiments,[0],[0]
"Since it is a new task, we cannot compare to any existing baselines.",3. Experiments,[0],[0]
"(Section 3.3)
",3. Experiments,[0],[0]
"Below we describe the data, baselines and model configuration that are shared across the tasks.",3. Experiments,[0],[0]
"Additional setup details are provided in the task-specific sections.
",3. Experiments,[0],[0]
"Data We use the ZINC molecule dataset from Kusner et al. (2017) for our experiments, with the same training/testing split.",3. Experiments,[0],[0]
"It contains about 250K drug molecules extracted from the ZINC database (Sterling & Irwin, 2015).
",3. Experiments,[0],[0]
"Baselines We compare our approach with SMILES-based baselines: 1) Character VAE (CVAE) (Gómez-Bombarelli et al., 2016) which generates SMILES strings character by character; 2) Grammar VAE (GVAE) (Kusner et al., 2017) that generates SMILES following syntactic constraints given
by a context-free grammar; 3) Syntax-directed VAE (SDVAE) (Dai et al., 2018) that incorporates both syntactic and semantic constraints of SMILES via attribute grammar.",3. Experiments,[0],[0]
"For molecule generation task, we also compare with GraphVAE (Simonovsky & Komodakis, 2018) that directly generates atom labels and adjacency matrices of graphs.
",3. Experiments,[0],[0]
"Model Configuration To be comparable with the above baselines, we set the latent space dimension as 56, i.e., the tree and graph representation hT and hG have 28 dimensions each.",3. Experiments,[0],[0]
Full training details and model configurations are provided in the appendix.,3. Experiments,[0],[0]
Setup The first task is to reconstruct and sample molecules from latent space.,3.1. Molecule Reconstruction and Validity,[0],[0]
"Since both encoding and decoding process are stochastic, we estimate reconstruction accuracy by Monte Carlo method used in (Kusner et al., 2017):",3.1. Molecule Reconstruction and Validity,[0],[0]
Each molecule is encoded 10 times and each encoding is decoded 10 times.,3.1. Molecule Reconstruction and Validity,[0],[0]
"We report the portion of the 100 decoded molecules that are identical to the input molecule.
",3.1. Molecule Reconstruction and Validity,[0],[0]
"To compute validity, we sample 1000 latent vectors from the prior distribution N (0, I), and decode each of these vectors 100 times.",3.1. Molecule Reconstruction and Validity,[0],[0]
We report the percentage of decoded molecules that are chemically valid (checked by RDKit).,3.1. Molecule Reconstruction and Validity,[0],[0]
"For ablation study, we also report the validity of our model without validity check in decoding phase.
",3.1. Molecule Reconstruction and Validity,[0],[0]
"Results Table 1 shows that JT-VAE outperforms previous models in molecule reconstruction, and always pro-
duces valid molecules when sampled from prior distribution.",3.1. Molecule Reconstruction and Validity,[0],[0]
"When validity check is removed, our model could still generates 93.5% valid molecules.",3.1. Molecule Reconstruction and Validity,[0],[0]
This shows our method does not heavily rely on prior knowledge.,3.1. Molecule Reconstruction and Validity,[0],[0]
"As shown in Figure 6, the sampled molecules have non-trivial structures such as simple chains.",3.1. Molecule Reconstruction and Validity,[0],[0]
We further sampled 5000 molecules from prior and found they are all distinct from the training set.,3.1. Molecule Reconstruction and Validity,[0],[0]
"Thus our model is not a simple memorization.
",3.1. Molecule Reconstruction and Validity,[0],[0]
Analysis We qualitatively examine the latent space of JTVAE by visualizing the neighborhood of molecules.,3.1. Molecule Reconstruction and Validity,[0],[0]
"Given a molecule, we follow the method in Kusner et al. (2017) to construct a grid visualization of its neighborhood.",3.1. Molecule Reconstruction and Validity,[0],[0]
Figure 6 shows the local neighborhood of the same molecule visualized in Dai et al. (2018).,3.1. Molecule Reconstruction and Validity,[0],[0]
"In comparison, our neighborhood does not contain molecules with huge rings (with more than 7 atoms), which rarely occur in the dataset.",3.1. Molecule Reconstruction and Validity,[0],[0]
We also highlight two groups of closely resembling molecules that have identical tree structures but vary only in how clusters are attached together.,3.1. Molecule Reconstruction and Validity,[0],[0]
This demonstrates the smoothness of learned molecular embeddings.,3.1. Molecule Reconstruction and Validity,[0],[0]
Setup The second task is to produce novel molecules with desired properties.,3.2. Bayesian Optimization,[0],[0]
"Following (Kusner et al., 2017), our target chemical property y(·) is octanol-water partition coefficients (logP) penalized by the synthetic accessibility (SA) score and number of long cycles.2 To perform Bayesian optimization (BO), we first train a VAE and associate each molecule with a latent vector, given by the mean of the variational encoding distribution.",3.2. Bayesian Optimization,[0],[0]
"After the VAE is learned, we train a sparse Gaussian process (SGP) to predict y(m) given its latent representation.",3.2. Bayesian Optimization,[0],[0]
"Then we perform five iterations of batched BO using the expected improvement heuristic.
",3.2. Bayesian Optimization,[0],[0]
"For comparison, we report 1) the predictive performance of SGP trained on latent encodings learned by different VAEs, measured by log-likelihood (LL) and root mean square error (RMSE) with 10-fold cross validation.",3.2. Bayesian Optimization,[0],[0]
"2) The top-3 molecules found by BO under different models.
2y(m) = logP (m) − SA(m)",3.2. Bayesian Optimization,[0],[0]
"− cycle(m) where cycle(m) counts the number of rings that have more than six atoms.
",3.2. Bayesian Optimization,[0],[0]
"Results As shown in Table 2, JT-VAE finds molecules with significantly better scores than previous methods.",3.2. Bayesian Optimization,[0],[0]
Figure 7 lists the top-3 best molecules found by JT-VAE.,3.2. Bayesian Optimization,[0],[0]
"In fact, JT-VAE finds over 50 molecules with scores over 3.50 (the second best molecule proposed by SD-VAE).",3.2. Bayesian Optimization,[0],[0]
"Moreover, the SGP yields better predictive performance when trained on JT-VAE embeddings (Table 3).",3.2. Bayesian Optimization,[0],[0]
Setup The third task is to perform molecule optimization in a constrained scenario.,3.3. Constrained Optimization,[0],[0]
"Given a molecule m, the task is to find a different molecule m′ that has the highest property value with the molecular similarity sim(m,m′)",3.3. Constrained Optimization,[0],[0]
≥ δ for some threshold δ.,3.3. Constrained Optimization,[0],[0]
"We use Tanimoto similarity with Morgan fingerprint (Rogers & Hahn, 2010) as the similarity metric, and penalized logP coefficient as our target chemical property.",3.3. Constrained Optimization,[0],[0]
"For this task, we jointly train a property predictor F (parameterized by a feed-forward network) with JT-VAE to predict y(m) from the latent embedding of m. To optimize a molecule m, we start from its latent representation, and apply gradient ascent in the latent space to improve the predicted score F (·), similar to (Mueller et al., 2017).",3.3. Constrained Optimization,[0],[0]
"After applying K = 80 gradient steps, K molecules are decoded from resulting latent trajectories, and we report the molecule with the highest F (·) that satisfies the similarity constraint.",3.3. Constrained Optimization,[0],[0]
"A modification succeeds if one of the decoded molecules satisfies the constraint and is distinct from the original.
",3.3. Constrained Optimization,[0],[0]
"To provide the greatest challenge, we selected 800 molecules with the lowest property score y(·) from the test set.",3.3. Constrained Optimization,[0],[0]
"We report the success rate (how often a modification succeeds), and among success cases the average improvement y(m′)− y(m) and molecular similarity sim(m,m′) between the original and modified molecules m and m′.
Results Our results are summarized in Table 4.",3.3. Constrained Optimization,[0],[0]
"The unconstrained scenario (δ = 0) has the best average improvement, but often proposes dissimilar molecules.",3.3. Constrained Optimization,[0],[0]
"When we tighten the constraint to δ = 0.4, about 80% of the time our model finds similar molecules, with an average improvement 0.84.",3.3. Constrained Optimization,[0],[0]
This also demonstrates the smoothness of the learned latent space.,3.3. Constrained Optimization,[0],[0]
Figure 8 illustrates an effective modification resulting in a similar molecule with great improvement.,3.3. Constrained Optimization,[0],[0]
Molecule Generation Previous work on molecule generation mostly operates on SMILES strings.,4. Related Work,[0],[0]
GómezBombarelli,4. Related Work,[0],[0]
et al. (2016); Segler et al. (2017) built generative models of SMILES strings with recurrent decoders.,4. Related Work,[0],[0]
"Unfortunately, these models could generate invalid SMILES that do not result in any molecules.",4. Related Work,[0],[0]
"To remedy this issue, Kusner et al. (2017); Dai et al. (2018) complemented the decoder with syntactic and semantic constraints of SMILES by context free and attribute grammars, but these grammars do not fully capture chemical validity.",4. Related Work,[0],[0]
"Other techniques such as active learning (Janz et al., 2017) and reinforcement learning (Guimaraes et al., 2017) encourage the model to generate valid SMILES through additional training signal.",4. Related Work,[0],[0]
"Very recently, Simonovsky & Komodakis (2018) proposed to generate molecular graphs by predicting their adjacency matrices, and Li et al. (2018) generated molecules node by node.",4. Related Work,[0],[0]
"In comparison, our method enforces chemical validity and is more efficient due to the coarse-to-fine generation.
",4. Related Work,[0],[0]
Graph-structured Encoders,4. Related Work,[0],[0]
"The neural network formulation on graphs was first proposed by Gori et al. (2005); Scarselli et al. (2009), and later enhanced by Li et al. (2015) with gated recurrent units.",4. Related Work,[0],[0]
"For recurrent architectures over
graphs, Lei et al. (2017) designed Weisfeiler-Lehman kernel network inspired by graph kernels.",4. Related Work,[0],[0]
"Dai et al. (2016) considered a different architecture where graphs were viewed as latent variable graphical models, and derived their model from message passing algorithms.",4. Related Work,[0],[0]
"Our tree and graph encoder are closely related to this graphical model perspective, and to neural message passing networks (Gilmer et al., 2017).",4. Related Work,[0],[0]
"For convolutional architectures, Duvenaud et al. (2015) introduced a convolution-like propagation on molecular graphs, which was generalized to other domains by Niepert et al. (2016).",4. Related Work,[0],[0]
Bruna et al. (2013); Henaff et al. (2015) developed graph convolution in spectral domain via graph Laplacian.,4. Related Work,[0],[0]
"For applications, graph neural networks are used in semisupervised classification (Kipf & Welling, 2016), computer vision (Monti et al., 2016), and chemical domains (Kearnes et al., 2016; Schütt et al., 2017; Jin et al., 2017).
",4. Related Work,[0],[0]
Tree-structured Models,4. Related Work,[0],[0]
"Our tree encoder is related to recursive neural networks and tree-LSTM (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).",4. Related Work,[0],[0]
These models encode tree structures where nodes in the tree are bottom-up transformed into vector representations.,4. Related Work,[0],[0]
"In contrast, our model propagates information both bottom-up and top-down.
",4. Related Work,[0],[0]
"On the decoding side, tree generation naturally arises in natural language parsing (Dyer et al., 2016; Kiperwasser & Goldberg, 2016).",4. Related Work,[0],[0]
"Different from our approach, natural language parsers have access to input words and only predict the topology of the tree.",4. Related Work,[0],[0]
"For general purpose tree generation, Vinyals et al. (2015); Aharoni & Goldberg (2017) applied recurrent networks to generate linearized version of trees, but their architectures were entirely sequence-based.",4. Related Work,[0],[0]
Dong & Lapata (2016); Alvarez-Melis & Jaakkola (2016) proposed tree-based architectures that construct trees top-down from the root.,4. Related Work,[0],[0]
"Our model is most closely related to Alvarez-Melis & Jaakkola (2016) that disentangles topological prediction from label prediction, but we generate nodes in a depth-first order and have additional steps that propagate information bottom-up.",4. Related Work,[0],[0]
"This forward-backward propagation also appears in Parisotto et al. (2016), but their model is node based whereas ours is based on message passing.",4. Related Work,[0],[0]
In this paper we present a junction tree variational autoencoder for generating molecular graphs.,5. Conclusion,[0],[0]
Our method significantly outperforms previous work in molecule generation and optimization.,5. Conclusion,[0],[0]
"For future work, we attempt to generalize our method for general low-treewidth graphs.",5. Conclusion,[0],[0]
"We thank Jonas Mueller, Chengtao Li, Tao Lei and MIT NLP Group for their helpful comments.",Acknowledgement,[0],[0]
This work was supported by the DARPA Make-It program under contract ARO W911NF-16-2-0023.,Acknowledgement,[0],[0]
We seek to automate the design of molecules based on specific chemical properties.,abstractText,[0],[0]
"In computational terms, this task involves continuous embedding and generation of molecular graphs.",abstractText,[0],[0]
"Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs.",abstractText,[0],[0]
"Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network.",abstractText,[0],[0]
This approach allows us to incrementally expand molecules while maintaining chemical validity at every step.,abstractText,[0],[0]
We evaluate our model on multiple tasks ranging from molecular generation to optimization.,abstractText,[0],[0]
"Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.",abstractText,[0],[0]
Junction Tree Variational Autoencoder for Molecular Graph Generation,title,[0],[0]
"by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the popular Bradley–Terry model, under natural assumptions on the parameters. Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items. This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost.",text,[0],[0]
"The problem of recovering a ranking over n items from noisy outcomes of pairwise comparisons has attracted, in the last century, much research interest, driven by applications in sports (Elo, 1978), social sciences (Thurstone, 1927; Salganik & Levy, 2015) and—more recently—recommender systems (Houlsby et al., 2012).",1 Introduction,[0],[0]
"Whereas pairwise comparison models and related inference algorithms have been extensively studied, the issue of which pairwise comparisons to sample, also known as active learning, has received significantly less attention.",1 Introduction,[0],[0]
"To understand the potential benefits of adaptively selecting samples, consider the case where comparison outcomes are noiseless, i.e., consistent with a linear order on a set of n items.",1 Introduction,[0],[0]
"If pairs of items are selected at random, it is necessary to collect Ω(n2) comparisons to recover the ranking (Alon et al., 1994).",1 Introduction,[0],[0]
"In contrast, by using an efficient sorting algorithm, O(n log n) adaptively
1School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland.",1 Introduction,[0],[0]
"Correspondence to: Lucas Maystre <lucas.maystre@epfl.ch>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
chosen comparisons are sufficient.",1 Introduction,[0],[0]
"In this work, we demonstrate that sorting algorithms can also be helpful in the noisy setting, where some comparison outcomes are inconsistent with the ranking: despite errors, sorting algorithms tend to select informative samples.",1 Introduction,[0],[0]
"We focus on the Bradley–Terry (BT) model, a widely-used probabilistic model of comparison outcomes.",1 Introduction,[0],[0]
"In this model, each item is associated with a parameter on the real line, and the probability of observing an incorrect outcome decreases as the distance between the items’ parameters increases.
",1 Introduction,[0],[0]
"First, we study the output of a single execution of Quicksort when comparison outcomes are generated from a BT model, under the assumption that the distance between adjacent parameters is (stochastically) uniform across the ranking.",1 Introduction,[0],[0]
"We measure the quality of a ranking estimate by its displacement with respect to the ground truth, i.e., the sum of rank differences.",1 Introduction,[0],[0]
We show that Quicksort’s output is a good approximation to the ground-truth ranking: no method comparing every pair of items at most once can do better (up to constant factors).,1 Introduction,[0],[0]
"Furthermore, we show that by aggregating O(log5 n) independent runs of Quicksort, it is possible to recover the exact rank for all but a vanishing fraction of the items.",1 Introduction,[0],[0]
"These theoretical results suggest that adaptive sampling is able to bring a substantial acceleration to the learning process.
",1 Introduction,[0],[0]
"Second, we propose a practical active-learning (AL) strategy that consists of repeatedly sorting the items.",1 Introduction,[0],[0]
We evaluate our sorting-based method on three datasets and compare it to existing AL methods.,1 Introduction,[0],[0]
We observe that all the strategies that we consider lead to better ranking estimates noticeably faster than random sampling.,1 Introduction,[0],[0]
"However, most strategies are challenging to operate and computationally expensive, thus hindering wider adoption (Schein & Ungar, 2007).",1 Introduction,[0],[0]
"In this regard, sorting-based AL stands out, as a) it is computationally-speaking as inexpensive as random sampling, b) it is trivial to implement, and c)",1 Introduction,[0],[0]
it requires no tuning of hyperparameters.,1 Introduction,[0],[0]
"We consider n items that are represented by consecutive integers [n] = {1, . . .",1.1 Preliminaries and Notation,[0],[0]
", n}.",1.1 Preliminaries and Notation,[0],[0]
"Without loss of generality, we
assume that the items are ranked by increasing preference1, i.e., i < j means that j is (in expectation) preferred to i.",1.1 Preliminaries and Notation,[0],[0]
"When j is preferred to i as a result of a pairwise comparison, we denote the observation by i ≺ j.",1.1 Preliminaries and Notation,[0],[0]
"If i < j, we say that i ≺ j is a consistent outcome and j ≺ i an inconsistent (incorrect) outcome.",1.1 Preliminaries and Notation,[0],[0]
"In most of the paper, pairwise comparison outcomes follow a Bradley–Terry model with parameters θ = [ θ1 · · · θn ]
∈ Rn, denoted BT(θ).",1.1 Preliminaries and Notation,[0],[0]
"The parameters θ1 < · · · < θn represent the utilities of items 1, . . .",1.1 Preliminaries and Notation,[0],[0]
",",1.1 Preliminaries and Notation,[0],[0]
"n, and the probability of observing the outcome",1.1 Preliminaries and Notation,[0],[0]
"i ≺ j is
p(i ≺ j | θ) = 1
1 + exp[−(θj − θi)] .
",1.1 Preliminaries and Notation,[0],[0]
The probability of observing an inconsistent comparison decreases with the distance between the items.,1.1 Preliminaries and Notation,[0],[0]
"This captures the intuitive notion that some pairs of items are easy to compare and some are more difficult (Zermelo, 1928; Bradley & Terry, 1952).
",1.1 Preliminaries and Notation,[0],[0]
"A ranking σ is a function that maps an item to its rank, i.e., σ(i) = rank of item i.",1.1 Preliminaries and Notation,[0],[0]
"The (ground-truth) identity ranking is denoted by id, i.e. id(i) = i. To measure the quality of a ranking σ with respect to the ground-truth, we consider the displacement
∆(σ) =
n ∑
i=1
|σ(i)− i|,
also known as Spearman’s footrule distance.",1.1 Preliminaries and Notation,[0],[0]
"Another metric widely used in practice is the Kendall–Tau distance, defined as K(σ) =",1.1 Preliminaries and Notation,[0],[0]
"∑
i<j 1 {σ(i) > σ(j)}.",1.1 Preliminaries and Notation,[0],[0]
"Both metrics are equiv-
alent up to a factor of two2, such that bounds on ∆(σ) also hold for K(σ) up to constant factors.
",1.1 Preliminaries and Notation,[0],[0]
"Finally, we say that an event A holds with high probability if P [A] → 1 as n → ∞. For a random variable X and a sequence of numbers an, we say that X = O(an) with high probability if P",1.1 Preliminaries and Notation,[0],[0]
[|X| ≤ can]→,1.1 Preliminaries and Notation,[0],[0]
"1 as n→∞ for some constant c that does not depend on n.
Outline of the paper.",1.1 Preliminaries and Notation,[0],[0]
We begin by briefly reviewing related literature in Section 2.,1.1 Preliminaries and Notation,[0],[0]
"Next, in Section 3, we study the displacement of Quicksort’s output under noisy comparisons.",1.1 Preliminaries and Notation,[0],[0]
"In Section 4, we empirically evaluate several AL strategies on three datasets.",1.1 Preliminaries and Notation,[0],[0]
"Finally, we conclude in Section 5.",1.1 Preliminaries and Notation,[0],[0]
Passive setting.,2 Related Work,[0],[0]
"Recently, there have been a number of results on the sample complexity of the BT model, based on
1 This convention greatly simplifies the notation throughout the paper, but differs from that used in most of the preference learning literature.",2 Related Work,[0],[0]
"In our paper, the item with rank 1 is the worst.
2∆(σ)/2 ≤ K(σ)",2 Related Work,[0],[0]
"≤ ∆(σ) (Diaconis & Graham, 1977).
",2 Related Work,[0],[0]
"the assumption that all pairs of items are chosen before any comparison outcome is revealed (Negahban et al., 2012; Hajek et al., 2014; Rajkumar & Agarwal, 2014; Vojnovic & Yun, 2016).",2 Related Work,[0],[0]
"In general, these results reveal that choosing pairs of items uniformly at random is essentially optimal.",2 Related Work,[0],[0]
"Furthermore, they suggest that the ranking induced by the BT model cannot be recovered with less than Ω(n2) comparisons.",2 Related Work,[0],[0]
"Our work shows that by adaptively selecting pairs based on observed outcomes, we observe substantial gains.
",2 Related Work,[0],[0]
Active preference learning.,2 Related Work,[0],[0]
AL approaches for learning a ranking based on noisy comparison outcomes have been studied under various assumptions.,2 Related Work,[0],[0]
"Braverman & Mossel (2008) examine a model where outcomes of pairwise comparisons are flipped with a small, constant probability.",2 Related Work,[0],[0]
"Ailon (2012) considers an adversarial setting (comparison outcomes can be arbitrary) and investigates AL in the context of finding a ranking that minimizes the number of inconsistent outcomes, also known as the minimum feedback-arc set problem on tournaments (MFAST).",2 Related Work,[0],[0]
"These theoretical studies imply, in their respective settings, that O(n logk n) comparison outcomes are enough to recover a near-optimal ranking.",2 Related Work,[0],[0]
"Jamieson & Nowak (2011) propose an efficient active-ranking algorithm that is applicable if items can be embedded in Rd (e.g., using d features) and assuming that admissible rankings satisfy some geometric constraints.",2 Related Work,[0],[0]
Wang et al. (2014) study a collaborative preference-learning problem and show that a variant of uncertainty sampling (a well-known AL strategy) works well for their problem.,2 Related Work,[0],[0]
"In this work, we assume that we do not have access to item features and that comparison outcomes follow a single BT model.
",2 Related Work,[0],[0]
Bayesian methods.,2 Related Work,[0],[0]
"From a practical standpoint, Bayesian methods provide an effective way to select informative samples (MacKay, 1992).",2 Related Work,[0],[0]
"However, they can be difficult to scale if the number of items is large.",2 Related Work,[0],[0]
"Work on Bayesian active preference learning includes Chu & Ghahramani (2005), Houlsby et al. (2012), Salimans et al. (2012) and Chen et al. (2013).",2 Related Work,[0],[0]
"We compare our AL strategy to these methods in Section 4.
",2 Related Work,[0],[0]
Multi-armed bandit.,2 Related Work,[0],[0]
"The dueling bandit problem (Yue et al., 2009) is somewhat related to our work.",2 Related Work,[0],[0]
"In this problem, the goal is to identify the best item based on noisy comparison outcomes, using as few adaptively chosen samples as possible.",2 Related Work,[0],[0]
Two recent papers also extend the problem to that of recovering the entire ranking (instead of only the top element).,2 Related Work,[0],[0]
"The work of Szörényi et al. (2015) is the closest to ours, as it also uses the BT model.",2 Related Work,[0],[0]
"One of their results is similar to our Theorem 2: They show that a quasi-linear number of comparisons is sufficient to recover the true ranking, under some conditions on θ.",2 Related Work,[0],[0]
Heckel et al. (2016) investigate a non-parametric model and develop some theoretical guarantees.,2 Related Work,[0],[0]
"In contrast to these works, our paper studies practical
Algorithm 1 Quicksort
Require: set of items V 1: if |V | < 2 then return list(V ) ⊲",2 Related Work,[0],[0]
Terminating case.,2 Related Work,[0],[0]
"2: L← ∅, R← ∅ 3: p← element of V selected uniformly at random 4: for i ∈ V \ {p} do 5: if i ≺ p then ⊲",2 Related Work,[0],[0]
Pairwise comparison.,2 Related Work,[0],[0]
"6: L← L ∪ {i} 7: else
8: R← R ∪ {i}
9: return Quicksort(L) · p · Quicksort(R)
comparison budgets: we give theoretical guarantees for the output obtained from a single call to Quicksort, and in our experiments we never exceed ≈ 10 calls.
",2 Related Work,[0],[0]
Quicksort.,2 Related Work,[0],[0]
"The Quicksort algorithm (Hoare, 1962) is one of the most widely studied sorting procedures.",2 Related Work,[0],[0]
Quicksort has been shown to produce useful rankings beyond classic sorting problems.,2 Related Work,[0],[0]
"For example, Ailon et al. (2008) show that Quicksort produces (in expectation) a 3-approximation to the MFAST problem.",2 Related Work,[0],[0]
"Quicksort combined with BT comparison outcomes has also been proposed as a probabilistic ranking model (Ailon, 2008).",2 Related Work,[0],[0]
We take advantage of some of the properties of this ranking model in order to derive the theoretical results of Section 3.,2 Related Work,[0],[0]
"In this section, we begin by studying the behavior and output of Quicksort under inconsistent comparison outcomes, without any assumptions on the noise generating process.",3 Theoretical Results,[0],[0]
"Then, starting in Section 3.1, we focus on comparison outcomes generated by the BT model.",3 Theoretical Results,[0],[0]
"Due to limited space, most full proofs are deferred to the supplementary material (Section A).
",3 Theoretical Results,[0],[0]
Quicksort (Algorithm 1) is best described as a recursive procedure.,3 Theoretical Results,[0],[0]
"At each step of the recursion, a pivot item p is chosen uniformly at random (line 3).",3 Theoretical Results,[0],[0]
"Then, during the partition operation (lines 4–8), every other item is compared to p and added to the set L or R, depending on the outcome.",3 Theoretical Results,[0],[0]
"If all comparison outcomes are consistent, it is well-known that Quicksort terminates after sampling O(n log n) comparisons with high probability.",3 Theoretical Results,[0],[0]
What happens if we drop the consistency assumption?,3 Theoretical Results,[0],[0]
"The following two lemmas state that these key properties remain valid, no matter which (and how many) comparison outcomes are inconsistent.
",3 Theoretical Results,[0],[0]
Lemma 1.,3 Theoretical Results,[0],[0]
"Quicksort always terminates and samples each of the n(n−1)/2 possible comparisons at most once.
",3 Theoretical Results,[0],[0]
Proof.,3 Theoretical Results,[0],[0]
The proof is identical to the consistent setting.,3 Theoretical Results,[0],[0]
"Consider the state of L and R at the end of a partition operation.
",3 Theoretical Results,[0],[0]
"Because |L|+ |R| = |V |−1, the recursive calls are made on sets of items of strictly decreasing cardinality, and the algorithm terminates after a finite number of steps.",3 Theoretical Results,[0],[0]
"Furthermore, suppose that Quicksort samples an outcome for the pair (i, j).",3 Theoretical Results,[0],[0]
Then either i or j is the pivot in a partition operation.,3 Theoretical Results,[0],[0]
"In either case, the pivot is not included in the recursive calls, which ensures that (i, j) cannot be compared again.
",3 Theoretical Results,[0],[0]
Lemma 2.,3 Theoretical Results,[0],[0]
"Quicksort samples O(n log n) comparisons w.h.p.
",3 Theoretical Results,[0],[0]
Proof (sketch).,3 Theoretical Results,[0],[0]
"We follow a standard analysis of Quicksort (see, e.g., Dubhashi & Panconesi, 2009, Section 3.3.3).",3 Theoretical Results,[0],[0]
"With high probability, we choose a “good” pivot (i.e., one that results in a balanced partition) a constant fraction of the time.",3 Theoretical Results,[0],[0]
"In this case, the depth of the call tree is O(log n).",3 Theoretical Results,[0],[0]
"As there are at most n comparisons at each level of the call tree, we conclude that Quicksort uses O(n log n) comparisons in total.",3 Theoretical Results,[0],[0]
"With respect to the standard proof, we need some additional work to formalize the notion of “good” pivot to the setting where comparison outcomes are not consistent with a linear order.
",3 Theoretical Results,[0],[0]
"Lemma 2 complements Theorem 3 in Ailon & Mohri (2010), which states that Quicksort samples O(n log n) in expectation.",3 Theoretical Results,[0],[0]
These results might suggest that all properties of Quicksort carry over to the noisy setting.,3 Theoretical Results,[0],[0]
This is not the case.,3 Theoretical Results,[0],[0]
"For example, although Quicksort uses approximately 2n lnn comparisons on average in the noiseless setting (Sedgewick & Wayne, 2011), this number can be distinctly different with inconsistent comparison outcomes3.
Quicksort (and efficient sorting algorithms in general) infer most pairs of items’ relative position by transitivity and thus rely heavily on the consistency of comparison outcomes.",3 Theoretical Results,[0],[0]
"In the noisy case, it is therefore important to precisely understand the effect of an inconsistent outcome on the output of the algorithm; this effect extends beyond the pair of items whose comparison outcome was inconsistent.",3 Theoretical Results,[0],[0]
"For this purpose, the next Lemma bounds the displacement of Quicksort’s output as a function of the inconsistent outcomes.
",3 Theoretical Results,[0],[0]
Lemma 3.,3 Theoretical Results,[0],[0]
Let E be the set of pairs sampled by Quicksort and whose outcome is inconsistent with id. Let σ be the output.,3 Theoretical Results,[0],[0]
"Then,
∆(σ) ≤ 2 ∑
(i,j)∈E
|i− j|
Proof (sketch).",3 Theoretical Results,[0],[0]
"Consider the first partition operation, with pivot p, resulting in partitions L and R. Denote the errors
3E.g., if comparison outcomes are uniformly random, all items are “good” pivots w.h.p., and the average number of comparisons will be closer to n log
2 n on average, for large n.
made during this partition operation by E1.",3 Theoretical Results,[0],[0]
"We can show that the displacement is bounded by
∆(σ) ≤ ∆L(σ) + ∆R(σ) + 2 ∑
(i,j)∈E1
|i− j|,
where ∆L(σ) and ∆R(σ) represent the displacement of the ordering induced by σ on L and R, respectively.",3 Theoretical Results,[0],[0]
"In other words, the total displacement can be decomposed into a term that represents the “local” displacement due to the partition operation and into two terms that account for errors in the recursive calls.",3 Theoretical Results,[0],[0]
"We obtain the desired result by recursively bounding ∆L(σ) and ∆R(σ).
",3 Theoretical Results,[0],[0]
"Informally, Lemma 3 states that the displacement can be bounded by a sum of “local shifts” due to the inconsistent outcomes and that the price to pay for any information inferred by transitivity is bounded by a factor two.",3 Theoretical Results,[0],[0]
"Lemma 3 is a crucial component of our subsequent analysis of BT noise, and we believe that it can be useful in order to investigate Quicksort under a wide variety of other noise generating processes.",3 Theoretical Results,[0],[0]
"From here on, we assume that comparison outcomes are generated from BT(θ).",3.1 Displacement in the Poisson Model,[0],[0]
"Clearly, any results on the displacement of a ranking estimated from samples of a BT model will depend on θ; it is easy to construct a model instance for which it is arbitrarily hard to recover the ranking, by choosing parameters sufficiently close to each other.",3.1 Displacement in the Poisson Model,[0],[0]
Our approach is as follows.,3.1 Displacement in the Poisson Model,[0],[0]
"We postulate a family of distributions over θ, and we give bounds on the displacement that hold with high probability.
",3.1 Displacement in the Poisson Model,[0],[0]
"We suppose that comparison outcomes are (in expectation) uniformly noisy across the ranking: i.e., comparing two elements at the bottom is (a priori) as difficult as comparing two elements at the top or in the middle.",3.1 Displacement in the Poisson Model,[0],[0]
"This means that the probability distribution over parameters θ1, . . .",3.1 Displacement in the Poisson Model,[0],[0]
", θn results in (random) distances |θi+k−θi| that depend only on k. One such distribution arises if the parameters are drawn from a Poisson point process of rate λ.",3.1 Displacement in the Poisson Model,[0],[0]
"That is,
i.i.d.",3.1 Displacement in the Poisson Model,[0],[0]
"x1, . . .",3.1 Displacement in the Poisson Model,[0],[0]
", xn−1 ∼ Exp(λ), θi =
i−1 ∑
k=1
xk.",3.1 Displacement in the Poisson Model,[0],[0]
"(1)
The average distance between two items separated by k positions in the ordering is E",3.1 Displacement in the Poisson Model,[0],[0]
[θi+k,3.1 Displacement in the Poisson Model,[0],[0]
− θi] = k/λ.,3.1 Displacement in the Poisson Model,[0],[0]
"Although the distance between adjacent items is constant in expectation, we allow some parameters to be arbitrarily close4.",3.1 Displacement in the Poisson Model,[0],[0]
"The parameter λ controls the expected level of noise; a large λ is
4 In particular, the expected minimum distance between two items (i.e., the min of n exponential r.v.s) decreases as (nλ)−1 as n increases.
likely to result in a larger number of inconsistent outcomes.",3.1 Displacement in the Poisson Model,[0],[0]
"Although the precise choice of this Poisson model is driven by tractability concerns, in Section 3.2 we argue that it is essentially equivalent to choosing the parameters independently and uniformly at random in the interval",3.1 Displacement in the Poisson Model,[0],[0]
"[0, (n+1)/λ], when λ is fixed and n is large.",3.1 Displacement in the Poisson Model,[0],[0]
"We are now ready to state our main result.
",3.1 Displacement in the Poisson Model,[0],[0]
Theorem 1.,3.1 Displacement in the Poisson Model,[0],[0]
Let θ be sampled from a Poisson point process of rate λ.,3.1 Displacement in the Poisson Model,[0],[0]
Let σ be the output of Quicksort using comparison outcomes sampled from BT(θ).,3.1 Displacement in the Poisson Model,[0],[0]
"Then, w.h.p.,
∆(σ) = O(λ2n), (2)
max i |σ(i)− i| = O(λ log n).",3.1 Displacement in the Poisson Model,[0],[0]
"(3)
Proof (sketch).",3.1 Displacement in the Poisson Model,[0],[0]
"Let zij be the indicator random variable of the event “the comparison between i and j results in an error”, and let dij = |θi",3.1 Displacement in the Poisson Model,[0],[0]
− θj |.,3.1 Displacement in the Poisson Model,[0],[0]
"The distance dij is a sum of |i − j| exponential random variables, i.e., dij ∼ Gamma(|i− j|, λ), and we can show that
E",3.1 Displacement in the Poisson Model,[0],[0]
"[zij ] = E
[
1
1 + exp(dij)
]
≤ E",3.1 Displacement in the Poisson Model,[0],[0]
[exp(−dij)],3.1 Displacement in the Poisson Model,[0],[0]
"= (1 + 1/λ) −|i−j|.
Using Lemma 3 and the fact that every pair of items is compared at most once, we find
E [∆] ≤ 2 ∑
i<j
|i−",3.1 Displacement in the Poisson Model,[0],[0]
j|E,3.1 Displacement in the Poisson Model,[0],[0]
"[zij ]
≤ 2n
∞ ∑
k=0
k(1 + 1/λ)−k = 2nλ(λ+ 1).
",3.1 Displacement in the Poisson Model,[0],[0]
"The random variables {zij} are not unconditionally independent (they are independent when conditioned on θ) but, with some more work, we can show that Var [∆] = O(n).",3.1 Displacement in the Poisson Model,[0],[0]
"By using a Chebyshev bound, (2) follows.
",3.1 Displacement in the Poisson Model,[0],[0]
"In order to prove (3), we take advantage of a theorem due to Ailon (2008) which states that
P",3.1 Displacement in the Poisson Model,[0],[0]
[σ(i) < σ(j),3.1 Displacement in the Poisson Model,[0],[0]
"| θ] = p(i ≺ j | θ),
even if i and j were not directly compared with each other.",3.1 Displacement in the Poisson Model,[0],[0]
We use a Chernoff bound on dij to show that the relative order between any two items separated by at least O(λ log n) positions is correct with high probability.,3.1 Displacement in the Poisson Model,[0],[0]
"The second part of the claim follows easily.
",3.1 Displacement in the Poisson Model,[0],[0]
"Note that any method that compares each pair of items at most once results in a ranking estimate τ with displacement ∆(τ) = Ω(n) with high probability: As there is only a single (possibly inconsistent) comparison outcome between each pair of adjacent items, it is likely that a constant fraction of the items will be ranked incorrectly, resulting in a
Algorithm 2 Multisort
Require: set of items V , number of iterations m",3.1 Displacement in the Poisson Model,[0],[0]
"1: S ← ∅ 2: for k = 1, . . .",3.1 Displacement in the Poisson Model,[0],[0]
",m do 3: σ ← Quicksort(V ) 4: S ← S ∪ {σ}
5: return Copeland aggregation of S
displacement that grows linearly in n. Hence, our bound on ∆(σ) shows that Quicksort is order-optimal (in n).
",3.1 Displacement in the Poisson Model,[0],[0]
"In light of Theorem 1, a natural question to ask is as follows.",3.1 Displacement in the Poisson Model,[0],[0]
How many comparisons are needed in order to find the correct ranking?,3.1 Displacement in the Poisson Model,[0],[0]
"Clearly, finding the exact ranking is difficult: in fact, Ω(n) comparison outcomes are necessary to discriminate the closest pair of items reliably (see supplementary material, Section B).",3.1 Displacement in the Poisson Model,[0],[0]
"As such, we will focus on finding a ranking that matches the ground truth everywhere, except at a vanishing fraction of the items.
",3.1 Displacement in the Poisson Model,[0],[0]
"Multiple runs of Quicksort likely produce different outputs, because of the noisy comparison outcomes and because the algorithm itself is randomized (the pivot selection is random).",3.1 Displacement in the Poisson Model,[0],[0]
"By aggregating m independent outputs of Quicksort, is it possible to produce a better ranking estimate?",3.1 Displacement in the Poisson Model,[0],[0]
"Similarly to Szörényi et al. (2015), we combine the m outputs σ1, . . .",3.1 Displacement in the Poisson Model,[0],[0]
", σm into an aggregate ranking σ̂ using Copeland’s method.",3.1 Displacement in the Poisson Model,[0],[0]
"The method assigns, to each item, a score that corresponds to the number of items that it beats in a majority of the rankings, and it then ranks the items by increasing score (Copeland, 1951).",3.1 Displacement in the Poisson Model,[0],[0]
"We call the procedure Multisort and describe it in Algorithm 2.
Theorem 2.",3.1 Displacement in the Poisson Model,[0],[0]
Let θ be sampled from a Poisson point process of rate λ.,3.1 Displacement in the Poisson Model,[0],[0]
Let σ̂ be the output of Multisort using m = O(λ2 log5 n) and comparison outcomes sampled from BT(θ).,3.1 Displacement in the Poisson Model,[0],[0]
"Then, w.h.p.,
∆(σ̂) = o(λn).
",3.1 Displacement in the Poisson Model,[0],[0]
Proof (sketch).,3.1 Displacement in the Poisson Model,[0],[0]
"We use results on the order statistics of the distances x1, . . .",3.1 Displacement in the Poisson Model,[0],[0]
", xn−1 between successive items, as defined in (1), to partition the items into two disjoint subsets B and G. The set B contains a vanishing (1/ log2 n)-fraction of “bad” items that are difficult to order.",3.1 Displacement in the Poisson Model,[0],[0]
The set G is such that the smallest distance dij from any item,3.1 Displacement in the Poisson Model,[0],[0]
i ∈ G to any other item j ∈,3.1 Displacement in the Poisson Model,[0],[0]
[n] is bounded from below by c/(λ log2 n).,3.1 Displacement in the Poisson Model,[0],[0]
"We can show that with m = O(λ2 log5 n), for any i ∈ G and j ∈",3.1 Displacement in the Poisson Model,[0],[0]
[n] we have i < j ⇐⇒ σ(i) < σ(j) in a majority of the Quicksort outputs (with high probability).,3.1 Displacement in the Poisson Model,[0],[0]
This implies that σ̂(i),3.1 Displacement in the Poisson Model,[0],[0]
= i for all i ∈ G with high probability.,3.1 Displacement in the Poisson Model,[0],[0]
"Using (3) for items in B, we have
∆(σ̂) = |B| ·O(λ log n) = O(λn/ log n)
with high probability.
",3.1 Displacement in the Poisson Model,[0],[0]
Theorem 2 states that all but a vanishing fraction of items are correctly ranked using O(λ2n log6 n) comparisons.,3.1 Displacement in the Poisson Model,[0],[0]
"This result should be compared to the Ω(n2) comparisons needed if samples are selected uniformly at random.
",3.1 Displacement in the Poisson Model,[0],[0]
Empirical validation.,3.1 Displacement in the Poisson Model,[0],[0]
"In Figure 1, we illustrate the results of Theorems 1 and 2 by running simulations for increasing n and different values of λ.",3.1 Displacement in the Poisson Model,[0],[0]
"The bound on ∆(σ) is tight in n, but the dependence on λ appears to be linear rather than quadratic.",3.1 Displacement in the Poisson Model,[0],[0]
The bound on maxi|σ(i)− i| appears to be tight in n and λ.,3.1 Displacement in the Poisson Model,[0],[0]
"Finally, we compare the Copeland aggregation of m outputs of Quicksort with the ranking induced by the maximum-likelihood (ML) estimate, inferred from the outcomes of all the pairwise comparisons sampled by the m runs.",3.1 Displacement in the Poisson Model,[0],[0]
"Although the ranking induced by the ML estimate does not benefit from the guarantees of Theorem 2, it performs better in practice.",3.1 Displacement in the Poisson Model,[0],[0]
We will make use of this observation in Section 4.,3.1 Displacement in the Poisson Model,[0],[0]
A different (perhaps more natural) assumption on the parameters θ is to consider that they are drawn independently and uniformly at random over some interval.,3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"That is,
i.i.d. θ̄1, . . .",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
", θ̄n ∼ U(0, (n+ 1)/λ),
with θ1, . . .",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
", θn the order statistics of θ̄, i.e., the random variables arranged in increasing order.",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"From some elementary results on the joint distribution of order statistics (see, e.g., Arnold et al., 2008), we see that
|θi+k",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"− θi| ∼ (n+ 1)/λ · Beta(k, n− k + 1),
i.e., a Beta random variable rescaled between 0 and (n + 1)/λ.",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"Letting fk,n(x) be the probability density of |θi+k",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"− θi|, we have, for any fixed k and λ,
fk,n(x) ∝",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"x k−1
[
1− λx
n+ 1
]n−k n→∞ −−−−→ xk−1e−λx.
",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"We recognize the functional form of the density of a Gamma(k, λ) distribution.",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"Hence, the Poisson model and the i.i.d. uniform model are essentially equivalent for fixed λ and large n, and we can expect the results developed in Section 3.1 to hold under this distribution as well.",3.2 Independent Uniformly-Distributed Parameters,[0],[0]
"In practice, the comparison budget for estimating a ranking from noisy data might typically be larger than that for a single call to Quicksort, and it might not exactly match the number of comparisons required to run a given number of calls to Quicksort to completion.",4 Experimental Results,[0],[0]
"Building upon the observations made at the end of Section 3.1, we suggest the following practical active-learning strategy: for a budget of
c pairwise comparisons, run the sorting procedure repeatedly until the budget is depleted (the last call might have to be truncated).",4 Experimental Results,[0],[0]
"Then, retain only the set of c comparison pairs and their outcomes and discard the rankings produced by the sorting procedure.",4 Experimental Results,[0],[0]
"The final ranking estimate is then induced from the ML estimate over the set of c comparison outcomes.
",4 Experimental Results,[0],[0]
"In this section, we demonstrate the effectiveness of this sampling strategy on synthetic and real-world data.",4 Experimental Results,[0],[0]
"In particular, we show that it is comparable to existing AL strategies at a minuscule fraction of the computational cost.",4 Experimental Results,[0],[0]
"To assess the relative merits of our sorting-based strategy, we consider three strategies that we believe are representative of the state of the art in active preference learning.
",4.1 Competing Sampling Strategies,[0],[0]
Uncertainty sampling.,4.1 Competing Sampling Strategies,[0],[0]
"Developed in the context of classification tasks, this popular active-learning heuristic suggests to greedily sample the point that lies closest to the decision boundary (Settles, 2012).",4.1 Competing Sampling Strategies,[0],[0]
"In the context of a ranking task, this corresponds to sampling the pair of items whose relative order is most uncertain.",4.1 Competing Sampling Strategies,[0],[0]
"After t observations, given an estimate of model parameters θt, the strategy selects the (t+1)-st pair uniformly at random in
argmin i 6=j
|θti − θ t j |.
",4.1 Competing Sampling Strategies,[0],[0]
This set can be computed in time O(n log n) by sorting the parameters.,4.1 Competing Sampling Strategies,[0],[0]
"The parameters themselves need to be estimated, e.g., using (penalized) ML inference that in practice can be the dominating cost.
",4.1 Competing Sampling Strategies,[0],[0]
Bayesian methods.,4.1 Competing Sampling Strategies,[0],[0]
"If we have access to a full posterior distribution qt(θ) instead of a point estimate θt, we can take advantage of the extra information on the uncertainty of the parameters to improve the selection strategy.",4.1 Competing Sampling Strategies,[0],[0]
"A principled approach to AL consists of sampling the point that
maximizes the expected information gain (MacKay, 1992).",4.1 Competing Sampling Strategies,[0],[0]
"That is, the pair of items at iteration t+ 1 is selected in
argmax i 6=j
H(qt)−E",4.1 Competing Sampling Strategies,[0],[0]
"[ H(qt+1) ] , (4)
where H(·) denotes the entropy function.",4.1 Competing Sampling Strategies,[0],[0]
A conceptually similar but slightly different selection strategy is given by Chen et al. (2013).,4.1 Competing Sampling Strategies,[0],[0]
"Letting qij be the marginal distribution of (θi, θj), the pair is selected in
argmax i 6=j
E",4.1 Competing Sampling Strategies,[0],[0]
"[ KL(qt+1ij ‖q t ij) ] , (5)
where KL(·) denotes the Kullback–Leibler divergence.",4.1 Competing Sampling Strategies,[0],[0]
"Computing the exact posterior is not analytically tractable for the BT model, but a Gaussian approximation can be found in time O(n3).",4.1 Competing Sampling Strategies,[0],[0]
Criteria (4) and (5) can be computed in constant time for each pair of items.,4.1 Competing Sampling Strategies,[0],[0]
"The dominating cost is again that of estimating θ (or, in this case, q(θ)).
",4.1 Competing Sampling Strategies,[0],[0]
"In addition to these existing AL strategies, we also include in our experiments a variation of our sorting-based strategy that uses Mergesort instead of Quicksort.",4.1 Competing Sampling Strategies,[0],[0]
"In the noiseless setting, Mergesort is known to use on average≈ 39 % fewer comparisons than Quicksort per run (Knuth, 1998), but it does not benefit from the theoretical guarantees developed in Section 3.",4.1 Competing Sampling Strategies,[0],[0]
"In this section, we briefly discuss the running time of the methods.",4.2 Running Time,[0],[0]
We implement ML and Bayesian approximate inference algorithms for the BT model as a Python library5.,4.2 Running Time,[0],[0]
"For ML inference, we find that the fastest running time is achieved by a truncated Newton algorithm (even for large n).",4.2 Running Time,[0],[0]
"For approximate Bayesian inference, we use a variant of the expectation-propagation algorithm outlined by Chu & Ghahramani (2005).",4.2 Running Time,[0],[0]
"All experiments are performed on
5See: http://lucas.maystre.ch/choix.
a server with a 12-core Xeon X5670 processor running at 2.93 GHz.",4.2 Running Time,[0],[0]
"Numerical computations take advantage of the Intel Math Kernel Library.
",4.2 Running Time,[0],[0]
We illustrate the running time of AL strategies as follows.,4.2 Running Time,[0],[0]
"For n ∈ {102, 103, 104}, we generate outcomes for n comparisons pairs chosen uniformly at random among n items.",4.2 Running Time,[0],[0]
"For each strategy, we then measure the time it takes to select the (n+1)-st pair of items adaptively.",4.2 Running Time,[0],[0]
The results are presented in Table 1.,4.2 Running Time,[0],[0]
"Note that these numbers are intended to be considered as orders of magnitude, rather than exact values, as they depend on the particular combination of software and hardware that we use.",4.2 Running Time,[0],[0]
The running time of the Bayesian AL strategies exceed 10 hours for n = 104 and the calls were stopped ahead of completion.,4.2 Running Time,[0],[0]
"Our sorting-based methods, like random sampling, are the only AL strategies whose running time is constant for increasing n",4.2 Running Time,[0],[0]
(and for increasing c).,4.2 Running Time,[0],[0]
"In fact, their running time is negligible in comparison to the other strategies, including uncertainty sampling.",4.2 Running Time,[0],[0]
"We now investigate three datasets and measure the displacement of rankings estimated from adaptively-chosen samples, as a function of the budget c. Note that in order to use uncertainty sampling and Bayesian methods, it is necessary to choose a regularization strength or prior variance in the inference step.",4.3 Empirical Evaluation,[0],[0]
"Different values can result in drastically different outcomes (in particular for uncertainty sampling) and, in practice, choosing a good value can be a significant challenge6.",4.3 Empirical Evaluation,[0],[0]
"In the following, we report results for the values that worked best a posteriori.
",4.3 Empirical Evaluation,[0],[0]
Synthetic dataset.,4.3 Empirical Evaluation,[0],[0]
We generate n i.i.d.,4.3 Empirical Evaluation,[0],[0]
"parameters θ1, . .",4.3 Empirical Evaluation,[0],[0]
.,4.3 Empirical Evaluation,[0],[0]
", θn uniformly in [0, (n + 1)/λ] and draw samples from BT(θ).",4.3 Empirical Evaluation,[0],[0]
The ground-truth ranking is the one induced by the parameters.,4.3 Empirical Evaluation,[0],[0]
Figure 2 presents results for n = 200 and λ,4.3 Empirical Evaluation,[0],[0]
"= 5 (plots for different values of λ are presented in the supplementary material, Section C, and are qualitatively
6Observe that our sorting-based approach is entirely parameterfree and is therefore not affected by this issue.
similar).",4.3 Empirical Evaluation,[0],[0]
"In comparison to random sampling, AL is very effective and results in significantly better ranking estimates for any given number of comparisons.",4.3 Empirical Evaluation,[0],[0]
"The two Bayesian methods, though being the most computationally expensive, perform the best for all values of c, but are nearly indistinguishable from uncertainty sampling.",4.3 Empirical Evaluation,[0],[0]
The two sorting-based strategies perform similarly (with a small edge for Mergesort).,4.3 Empirical Evaluation,[0],[0]
"They are slightly worse than the Bayesian methods but are still able to reap most of the benefits of active learning.
",4.3 Empirical Evaluation,[0],[0]
Sushi dataset.,4.3 Empirical Evaluation,[0],[0]
"Next, we consider a dataset of Sushi preferences (Kamishima & Akaho, 2009).",4.3 Empirical Evaluation,[0],[0]
"In this dataset, 5000 respondents give a strict ordering over 10 different types of sushi.",4.3 Empirical Evaluation,[0],[0]
These 10 sushi are chosen among a larger set of n = 100 items.,4.3 Empirical Evaluation,[0],[0]
"To suit our purposes, we decompose each 10-way partial ranking into pairwise comparisons, resulting in 225 000 comparison outcomes.",4.3 Empirical Evaluation,[0],[0]
"We use all comparisons to fit a BT model that induces a ground-truth ranking7.
",4.3 Empirical Evaluation,[0],[0]
"The comparisons are dense, and there is at least one comparison outcome for almost all pairs.",4.3 Empirical Evaluation,[0],[0]
"When an outcome for pair (i, j) is requested, we sample uniformly at random over all outcomes observed for this pair.",4.3 Empirical Evaluation,[0],[0]
"In the rare case where no outcome is available, we return i ≺ j with probability 1/2.",4.3 Empirical Evaluation,[0],[0]
"This enables us to compare sampling strategies in a realistic setting, where the assumptions of the BT model do not necessarily hold anymore.
",4.3 Empirical Evaluation,[0],[0]
Results are shown in Figure 3 (left).,4.3 Empirical Evaluation,[0],[0]
"Once again, active learning performs noticeably better than random sampling.",4.3 Empirical Evaluation,[0],[0]
"On this real-world dataset, the performance of our sorting-based strategies is indistinguishable from that of the Bayesian
7 The BT-induced ranking is almost the same as that obtained using the Copeland score.",4.3 Empirical Evaluation,[0],[0]
"The results are very similar if the Copeland aggregation is used as ground truth.
methods, after completing one entire call to the sorting procedure (slightly less than 1000 comparisons).",4.3 Empirical Evaluation,[0],[0]
"This result should be interpreted in light of the time needed to select all 104 pairs: a fraction of a second for sorting-based strategies, and several hours for the Bayesian methods.",4.3 Empirical Evaluation,[0],[0]
"Finally, we observe that the performance of uncertainty sampling progressively degrades as c increases.",4.3 Empirical Evaluation,[0],[0]
"A detailed analysis reveals that uncertainty sampling increasingly focuses on a small set of hard-to-discriminate pairs, symptomatic of a well-known issue (Settles, 2012).
GIFGIF dataset.",4.3 Empirical Evaluation,[0],[0]
GIFGIF8 is a project of the MIT Media Lab that aims at explaining the emotions communicated by a collection of animated GIF images.,4.3 Empirical Evaluation,[0],[0]
"Users of the website are shown a prompt with two images and a question, “Which better expresses x?” where x is one of 17 emotions.",4.3 Empirical Evaluation,[0],[0]
"The users can click on either image, or use a third option, neither.",4.3 Empirical Evaluation,[0],[0]
"To date, over three million comparison outcomes have been collected.",4.3 Empirical Evaluation,[0],[0]
"For the purpose of our experiment, we restrict ourselves to a single emotion, happiness; and we ignore outcomes that resulted in neither.",4.3 Empirical Evaluation,[0],[0]
"We consider 106 887 comparison outcomes over n = 6120 items—a significant increase in scale compared to the Sushi dataset.
",4.3 Empirical Evaluation,[0],[0]
"As the data, despite a relatively large number of comparisons, remains sparse (less than 20 comparisons per item on average), we proceed as follows.",4.3 Empirical Evaluation,[0],[0]
We fit a BT model by using all the available comparisons and use the induced ranking as ground truth.,4.3 Empirical Evaluation,[0],[0]
"We then generate new, synthetic comparison outcomes from the BT model.",4.3 Empirical Evaluation,[0],[0]
"In this sense, the experiment enables us to compare sampling strategies by using a large BT model with realistic parameters.",4.3 Empirical Evaluation,[0],[0]
"The large number of items makes uncertainty sampling and the two Bayesian
8See http://www.gif.gf/.",4.3 Empirical Evaluation,[0],[0]
"Data available at http:// lucas.maystre.ch/gifgif-data.
methods prohibitively expensive.",4.3 Empirical Evaluation,[0],[0]
"We try a simplified, computationally less expensive version of uncertainty sampling where, at every iteration, each item is compared to its two closest neighbors, but this heuristic fails spectacularly: The resulting displacement is over 5× larger than random sampling for c = 106, and is therefore not reported here (see supplementary material, Section C).
",4.3 Empirical Evaluation,[0],[0]
Figure 3 (right) compares the displacement of random sampling to that of the two sorting-based sampling strategies for increasing c.,4.3 Empirical Evaluation,[0],[0]
The adaptive sampling approaches perform systematically better.,4.3 Empirical Evaluation,[0],[0]
"After 106 comparisons, the displacement of random sampling is 14 % and 23 % larger than that of Quicksort and Mergesort, respectively.",4.3 Empirical Evaluation,[0],[0]
"Conversely, in order to reach any target displacement, Mergesort requires approximately 2× fewer comparisons than random sampling.",4.3 Empirical Evaluation,[0],[0]
"In this work, we demonstrate that active learning can substantively speed up the task of learning a ranking from noisy comparisons gains—both in theory and in practice.",5 Conclusion,[0],[0]
"With the advent of large-scale crowdsourced ranking surveys, exemplified by GIFGIF and wiki surveys (Salganik & Levy, 2015), there is a clear need for practical AL strategies.",5 Conclusion,[0],[0]
"However, existing methods are complex and computationally expensive to operate even for a reasonable number of items (a few thousands).",5 Conclusion,[0],[0]
"We show that a deceptively simple idea—repeatedly sorting the items—is able to bring in all the benefits of active learning, is trivial to implement, and is computationally no more expensive that random sampling.",5 Conclusion,[0],[0]
"Therefore, we believe that our method can be broadly useful for machine-learning practitioners interested in ranking problems.",5 Conclusion,[0],[0]
"We thank Holly Cogliati-Bauereis, Ksenia Konyushkova, Brunella Spinelli and the anonymous reviewers for careful proofreading and helpful comments.",Acknowledgments,[0],[0]
We address the problem of learning a ranking by using adaptively chosen pairwise comparisons.,abstractText,[0],[0]
Our goal is to recover the ranking accurately but to sample the comparisons sparingly.,abstractText,[0],[0]
"If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort.",abstractText,[0],[0]
But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking?,abstractText,[0],[0]
"We give favorable guarantees for Quicksort for the popular Bradley–Terry model, under natural assumptions on the parameters.",abstractText,[0],[0]
"Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items.",abstractText,[0],[0]
This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost.,abstractText,[0],[0]
Just Sort It! A Simple and Effective Approach to Active Preference Learning,title,[0],[0]
K-means clustering is a classical clustering problems and has been studied for several decades.,1. Introduction,[0],[0]
The goal of K-Means clustering is to find a set of k cluster centers for a dataset such that the sum of squared distances of each point to its closest cluster center is minimized.,1. Introduction,[0],[0]
"While it is known that k-means clustering is an NP hard optimization problem even for k = 2 (Dasgupta, 2008), in practice a local search heuristic due to Lloyd (Lloyd, 1982) is widely used for solving K-means clustering problem.",1. Introduction,[0],[0]
"Lloyd’s iterative
1Department of Electrical Engineering & Computer Science, Wichita State University, KS, USA.",1. Introduction,[0],[0]
"Correspondence to: Kaushik Sinha <kaushik.sinha@wichita.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
algorithm begins with k arbitrary “cluster centers”, and in each iteration, each point is assigned to the nearest cluster center, and each cluster center is recomputed as the center of mass of all points assigned to it.",1. Introduction,[0],[0]
These last two steps are repeated until the process stabilizes.,1. Introduction,[0],[0]
"Lloyd’s algorithm for k-means clustering is known to be one of the top ten data mining tools of the last fifty years (Wu, 2008).
",1. Introduction,[0],[0]
"K-means clustering is typically performed on a data matrix A ∈ Rn×d, consisting of n data points each having d attributes/features and per iteration computational cost of Lloyd’s algorithm is O(nkd).",1. Introduction,[0],[0]
In recent years there has been a series of work towards reducing this computational cost and speeding up k-means clustering computation.,1. Introduction,[0],[0]
Most of these works can broadly be classified into three categories.,1. Introduction,[0],[0]
"In the first category, K-means clustering algorithm is accelerated by avoiding unnecessary distance calculations by applying various forms of triangular inequality and by keeping track of lower and upper bounds for distances between points and cluster centers (Elkan, 2003; Hamerly, 2010; Drake & Hamerly, 2012; Ding et al., 2015; Newling & Fleuret, 2016; Bottesch et al., 2016).",1. Introduction,[0],[0]
All these algorithms ensure the exact same clustering result that would have been obtained had one applied Lloyd’s heuristic from the same set of initial cluster centers without applying any distance inequality bounds.,1. Introduction,[0],[0]
"In the second category, various dimension reduction techniques are applied to data matrix A to reduce data dimensionality from d to d′ (d′ d), where d′ is independent of n and d, so that optimal k-means clustering solution of dimensionality reduced dataset A′ ∈",1. Introduction,[0],[0]
"Rn×d′ ensures an approximately optimal k-means clustering objective function of A. Most prominent among these is the random projection based dimensionality reduction technique that reduces data dimensionality from d to Ω(k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al., 2010; 2015; Cohen et al., 2015) and also from d to O(log(k)/ 2) resulting in (9 + ) approximation of the optimal k-means objective function (Cohen et al., 2015).",1. Introduction,[0],[0]
"Recently, (Liu et al., 2017) demonstrated the the random projection step can be performed by multiplying with a sparse matrix that yields the same (1 + ) approximation guarantee.",1. Introduction,[0],[0]
"Additionally, random feature selection method reduces data dimensionality from d to Ω(k log k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al.,
2015; Cohen et al., 2015).",1. Introduction,[0],[0]
"In the third category, a smaller subset of n data points called coresets, are constructed so that optimal weighted k-means clustering objective function performed on this coreset is (1 + ) approximation of the optimal k-means objective function performed on the original dataset (Feldman & Langberg, 2011; Feldman et al., 2013).",1. Introduction,[0],[0]
"In k-means clustering using Lloyd’s heuristic, a major computational bottleneck arises from Euclidean distance computation between each data point and k cluster centers in every iteration.",1. Introduction,[0],[0]
"For a data point a ∈ Rd and a cluster center µ ∈ Rd, the Euclidean distance can be represented as, ‖a",1. Introduction,[0],[0]
− µ‖2 = ‖a‖2 + ‖µ‖2,1. Introduction,[0],[0]
"− 2a>µ. Note that ‖a‖2 needs be computed for each data point only once over all iterations of Lloyd’s heuristics (and can be done off-line), ‖µ‖2 needs be computed once for each cluster center in each iteration, while the dot product needs to be computed for every possible data point, cluster center pair in every single iteration.",1. Introduction,[0],[0]
"In fact, the dot product between a cluster center µ and all n data points can be computed by a simple matrix-vector multiplication:",1. Introduction,[0],[0]
Aµ.,1. Introduction,[0],[0]
"If the data matrix A is significantly sparse (i.e., number of non-zero entries is reasonable small) the above matrix vector multiplication can be performed reasonably fast.",1. Introduction,[0],[0]
"A key question that is not addressed in the literature is, To what extent the data matrix A can be made sparse without significantly affecting optimal k-means clustering objective?",1. Introduction,[0],[0]
"In this paper we show that under mild conditions, we can randomly sparsify data matrix A to obtain a sparse data matrix Ã such that optimal k-means clustering solution of Ã yields an approximately optimal k-means clustering objective of A with high probability.",1. Introduction,[0],[0]
Note that such a sparsification scheme can be extremely useful in practice.,1. Introduction,[0],[0]
"If the original data matrix is reasonably dense, then such sparsification results in fast matrix-vector multiplication, thereby speeding up k-means clustering.",1. Introduction,[0],[0]
"However, it may seem at first that for many real world high dimensional datasets that are very sparse to begin with, such as text datasets represented in “bag of word” format, such sparsification scheme may not be useful.",1. Introduction,[0],[0]
"But, note that instead of directly working with high dimensional data, typically a random projection step is often applied first to reduce data dimensionality since it is known that optimal k-means clustering solution of this randomly projected dataset results in approximately optimal k-means clustering objective of the original high dimensional dataset (Boutsidis et al., 2010; 2015; Cohen et al., 2015; Liu et al., 2017).",1. Introduction,[0],[0]
"Unfortunately, such a random projection step results in a dense projected data matrix.",1. Introduction,[0],[0]
"Interestingly, our sparsification method can now be applied on this projected dense data matrix to reap further computational benefit in addition to the computational benefit already achieved by random projection step (see Figure 1).
",1. Introduction,[0],[0]
"To quantify the approximation factor as well as the level of sparsity of our proposed method, we use ideas from
(Achlioptas & Mcsherry, 2007) which establishes that random matrix sparsification approximately preserves low rank matrix structure with high probability and also ideas from (Cohen et al., 2015) which establishes to what extent an approximately optimal low rank matrix serves as a projection cost preserving sketch.",1. Introduction,[0],[0]
"To the best of our knowledge, this is the first result that quantifies how random matrix sparsification affects k-means clustering.",1. Introduction,[0],[0]
"In particular, we make the following contributions in this paper.
",1. Introduction,[0],[0]
"• We show that for any ∈ (0, 1), a dense data matrix A ∈ Rn×d can be randomly sparsified to yield a data matrix Ã ∈",1. Introduction,[0],[0]
"Rn×d, such that, Ã contains O(nk/ 9 + d log4 n) non-zero entries in expectation, and optimal k-means clustering solution of Ã results in (1 + ) approximation of optimal k-means objective of A with high probability.
",1. Introduction,[0],[0]
"• We show that for any ∈ (0, 1), a dense data matrix A ∈ Rn×d can be randomly sparsified to yield a data matrix Ã ∈",1. Introduction,[0],[0]
"Rn×d, such that, Ã contains O(nk/ 9 + d log4 n) non-zero entries in expectation, and any approximately optimal k-means clustering solution of Ã, having (1 + ) approximation of optimal k-means objective of Ã, results in (1 +O( )) approximation of optimal k-means objective of A with high probability.
",1. Introduction,[0],[0]
"• We present experimental results on three real world datasets to demonstrate effect of our proposed random sparsification scheme on k-means clustering solution.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
In section 2 we present k-means clustering problem in matrix notation and introduce uniform and non-uniform sampling strategies for random matrix sparsification.,1. Introduction,[0],[0]
We propose an algorithm for k-means clustering using random matrix sparsification in section 3 and present its analysis in section 4.,1. Introduction,[0],[0]
Empirical evaluations are presented in section 5.,1. Introduction,[0],[0]
"Finally, we conclude and point out a few open questions in section 6.",1. Introduction,[0],[0]
We use bold lower case letters to denote vectors and bold upper case letters to denote matrices.,2.1. Notation and linear algebra basics,[0],[0]
"For any n and d, consider a matrix A ∈ Rn×d with rank r = rank(A).",2.1. Notation and linear algebra basics,[0],[0]
"Using singular value decomposition A can be written as A = UΣV>, where U ∈ Rn×r contains r left singular vectors u1,u2, . . .",2.1. Notation and linear algebra basics,[0],[0]
",ur ∈ Rn, V contains r right singular vectors v1,v2, . . .",2.1. Notation and linear algebra basics,[0],[0]
",vr ∈ Rd, and Σ ∈",2.1. Notation and linear algebra basics,[0],[0]
Rr×r is a positive diagonal matrix containing the singular values of A : σ1(A),2.1. Notation and linear algebra basics,[0],[0]
≥ σ2(A),2.1. Notation and linear algebra basics,[0],[0]
≥ · · · ≥ σr(A).,2.1. Notation and linear algebra basics,[0],[0]
A can also be written as A = ∑r i=1,2.1. Notation and linear algebra basics,[0],[0]
σi(A)uiv >,2.1. Notation and linear algebra basics,[0],[0]
i .,2.1. Notation and linear algebra basics,[0],[0]
"For any k ≤ r,
Ak = ∑k i=1",2.1. Notation and linear algebra basics,[0],[0]
σi(A)uiv >,2.1. Notation and linear algebra basics,[0],[0]
"i is the best rank k approximation to A for any unitarily invariant norm, including Frobenious
and spectral norm (Mirsky, 1960).",2.1. Notation and linear algebra basics,[0],[0]
Note that,2.1. Notation and linear algebra basics,[0],[0]
A = Ak + Ar−k,2.1. Notation and linear algebra basics,[0],[0]
where Ar−k =,2.1. Notation and linear algebra basics,[0],[0]
∑r,2.1. Notation and linear algebra basics,[0],[0]
i=k+1 σi(A)uiv >,2.1. Notation and linear algebra basics,[0],[0]
i .,2.1. Notation and linear algebra basics,[0],[0]
"Therefore, Ar−k = A−Ak.",2.1. Notation and linear algebra basics,[0],[0]
"Square Frobenious norm of A is given by ‖A‖2F = ∑ i,jA(i, j) 2 = trace(AA>)",2.1. Notation and linear algebra basics,[0],[0]
= ∑,2.1. Notation and linear algebra basics,[0],[0]
i σ 2,2.1. Notation and linear algebra basics,[0],[0]
i (A).,2.1. Notation and linear algebra basics,[0],[0]
The spectral norm of A is given by ‖A‖2 = σ1(A).,2.1. Notation and linear algebra basics,[0],[0]
Ak satisfies ‖A −Ak‖F =,2.1. Notation and linear algebra basics,[0],[0]
"minB,rank(B)=k ‖A",2.1. Notation and linear algebra basics,[0],[0]
"− B‖F and ‖A−Ak‖2 = minB,rank(B)=k ‖A−B‖2.",2.1. Notation and linear algebra basics,[0],[0]
"The objective of k-means clustering is to partition n data points in Rd, {a1, . . .",2.2. K-means clustering,[0],[0]
",an}, into k non-overlapping clusters C = {C1, . . .",2.2. K-means clustering,[0],[0]
", Ck} such that points that are close to each other belong to the same cluster and points that are far from each other belong to to different clusters.",2.2. K-means clustering,[0],[0]
"Let µi be the centroid of cluster Ci and for any data point ai, let C(ai) be the index of the cluster to which ai is assigned to.",2.2. K-means clustering,[0],[0]
"The goal of k-means clustering is to minimize the objective function
k∑ i=1",2.2. K-means clustering,[0],[0]
∑ aj∈Ci ‖aj −µi‖22,2.2. K-means clustering,[0],[0]
"= n∑ j=1 ‖aj −µC(aj)‖ 2 2 (1)
",2.2. K-means clustering,[0],[0]
"Let A ∈ Rn×d be a data matrix containing the n data points {a1, . . .",2.2. K-means clustering,[0],[0]
",an} as rows and for any clustering C, let XC ∈ Rn×k be the cluster indicator matrix, with XC(i, j) = 1/ √",2.2. K-means clustering,[0],[0]
"|Cj | if ai is assigned to Cj and XC(i, j) = 0",2.2. K-means clustering,[0],[0]
otherwise.,2.2. K-means clustering,[0],[0]
"The k-means objective function given in equation 1 can now be represented in the matrix notation as,
",2.2. K-means clustering,[0],[0]
"‖A−XCX>CA‖2F = n∑ j=1 ‖aj −µC(aj)‖ 2 2 (2)
",2.2. K-means clustering,[0],[0]
"By construction, the columns of XC have disjoint support and are orthonormal vectors and XCX>C is an orthogonal projection matrix of rank k.",2.2. K-means clustering,[0],[0]
Let S be the set of all possible rank k cluster projection matrices of the form,2.2. K-means clustering,[0],[0]
XCX>C .,2.2. K-means clustering,[0],[0]
"The objective of k-means clustering is to find an optimal clustering of A that minimizes the objective function in equation
2, that is, to find XCopt such that,
XCopt = argmin XCX>C ∈S
‖A−XCX>CA‖2F
As mentioned earlier, finding XCopt is an NP-hard problem.",2.2. K-means clustering,[0],[0]
"Any cluster indicator matrix Xγ is called an γapproximation for the k-means clustering problem (γ ≥ 1) for data matrix A if it satisfies,
‖A−XγX>γA‖2F ≤",2.2. K-means clustering,[0],[0]
γ min,2.2. K-means clustering,[0],[0]
XCX>C ∈S,2.2. K-means clustering,[0],[0]
"‖A−XCX>CA‖2F
= γ‖A−XCoptX>CoptA‖ 2 F",2.2. K-means clustering,[0],[0]
"Given a data matrix A ∈ Rn×d, the basic idea of random matrix sparsification is to randomly sparsify the entries of A to get a matrix Ã ∈ Rn×d such that Ã contains fewer nonzero entries compared to A. Such a sparse matrix Ã speeds up matrix-vector multiplication by decreasing the number of arithmetic operations.",2.3. Random matrix sparsification,[0],[0]
"Let us write Ã = A+N, where N ∈ Rn×d.",2.3. Random matrix sparsification,[0],[0]
"A fundamental result of random matrix theory is that, as long as N is a random matrix whose entries are zero mean, independent random variables with bounded variance, no low dimensional subspace accommodates N well, i.e., ‖Nm‖2 and ‖Nm‖F are small for small m. In fact, optimal rank m approximation to Ã approximates A nearly as well as Am as long as ‖Am‖ ‖Nm‖ (for both Frobenious and spectral norm) and the quantity ‖Nm‖ bounds the influence that N may exert on the optimal rank m approximation to Ã.",2.3. Random matrix sparsification,[0],[0]
"Next we describe a simple random uniform sampling scheme as well as a simple non-uniform sampling scheme for generating sparse Ã that were proposed in (Achlioptas & Mcsherry, 2007) along with bounds on ‖Nm‖.",2.3. Random matrix sparsification,[0],[0]
"In the following, we set b to be b = max(i,j) |A(i, j)|.",2.3. Random matrix sparsification,[0],[0]
"In random matrix sparsificaion using uniform sampling scheme, p fraction of entries of A are set to zero to obtain a sparse Ã.",2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
"In particular,
Ã(i, j) = { A(i, j)/p with probability p 0 otherwise
(3)
It was shown in (Achlioptas & Mcsherry, 2007) that as long as p is bounded from below, with high probability, ‖Nm‖ is bounded as shown below (a simplified version of a result from (Achlioptas & Mcsherry, 2007)).
",2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
Theorem 1.,2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
"[Theorem 2 of (Achlioptas & Mcsherry, 2007)]",2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
"For p ≥ (8 log n)4/n, let Ã be the random sparse matrix obtained by applying uniform sampling scheme (equation 3).",2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
"Then with probability at least (1 − 1/n19 log3 n), for any m ≤ min{n, d}, N satisfies ‖Nm‖2 ≤",2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
"4b √ n/p and
‖Nm‖F ≤",2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
4b,2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
√ mn/p.,2.3.1. UNIFORM SAMPLING SCHEME,[0],[0]
Random sparsification using uniform sampling can be improved by retaining entries with probability that depends on their magnitude.,2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
"For any p > 0 define τij = p(A(i, j)/b)2
and let pij = max { τij , √ τij × (8 log n)4/n } .",2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
"Then a
sparse Ã can be obtained from A using the following nonuniform sampling scheme.
",2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
"Ã(i, j) = { A(i, j)/pij with probability pij 0",2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
"otherwise (4)
Such non-uniform sampling scheme yields greater sparsification when entry magnitudes vary, without increasing error bound of Theorem 1.
",2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
Theorem 2.,2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
"[Theorem 3 of (Achlioptas & Mcsherry, 2007)] Let Ã be the random sparse matrix obtained by applying non-uniform sampling scheme (equation 4).",2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
"Then with probability at least (1− 1/n19 log3 n), for any m ≤ min{n, d}, N satisfies ‖Nm‖2 ≤",2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
4b √ n/p and ‖Nm‖F ≤ 4b √ mn/p.,2.3.2. NON-UNIFORM SAMPLING SCHEME,[0],[0]
most p(‖A‖F /b)2,"In addition, expected number of non-zero entries in Ã is at",[0],[0]
"+ d(8 log n)4.
","In addition, expected number of non-zero entries in Ã is at",[0],[0]
"For any s > 0, setting p = s(b/‖A‖F )2 in the above Theorem ensures that expected number of non-zero entries in Ã is at most s+ d(8 log n)4.","In addition, expected number of non-zero entries in Ã is at",[0],[0]
"While the goal of k-means clustering is to well approximate each row of A with its cluster center, as can be seen from equation 1, an equivalent formulation in equation 2 shows that the problem actually amounts to finding an optimal rank
Algorithm 1 K-means clustering using random sparsification Input : Data matrix A ∈ Rn×d, number of clusters k, a positive scalar p and a γ-approximation k-means algorithm.",3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
"Output : Cluster indicator matrix Xγ̃ determining a k partition of the rows of A.
1: Compute Ã using non-uniform sampling scheme (equation 4).",3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
2: Run the γ-approximation k-means algorithm on Ã to obtain Xγ̃ .,3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
"3: Return Xγ̃ .
",3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
"k subspace for approximating the columns of A. Moreover, the choice of subspace is constrained because it must be spanned by the columns of a cluster indicator matrix.",3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
"The random sampling schemes presented in the previous section yields a sparse Ã whose optimal rank m approximation Ãm approximates Am reasonably well for small m. For appropriate choice of m, if such Am approximates optimal rank k subspace for approximating the columns of A well, then a reasonable strategy for k-means clustering that will reduce number of arithmetic operations is to perform kmeans clustering on Ã, instead of A, and hope that optimal k-means clustering solution of Ã will be close to optimal kmeans clustering solution of A. We propose such a strategy in Algorithm 1.",3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
In the next section we present an analysis of this algorithm and prove that an optimal k-means clustering solution of Ã indeed results in an approximately optimal k-means objective of A.,3. An algorithm for k-means clustering using random matrix sparsification,[0],[0]
In this section we present an analysis of Algorithm 1.,4. Analysis of algorithm,[0],[0]
For all our results we have assumed that n,4. Analysis of algorithm,[0],[0]
≥,4. Analysis of algorithm,[0],[0]
d.,4. Analysis of algorithm,[0],[0]
"The main intuition for the technical part of the proof is that even though A and Ã look very different because of the enforced sparse structure, if their appropriate low-rank structures are similar, that is enough to argue that optimal k-means solution of Ã is close to optimal k-means solution of A. We use the notion of projection cost preserving sketch1 as a useful mathematical object for our proof.",4. Analysis of algorithm,[0],[0]
"If B is a rank k projection-cost preserving sketch for A with error 1, then it implies (can be easily shown) that optimal k-means solution of B is (1 + 1) optimal k-means solution of A (Cohen et al., 2015).",4. Analysis of algorithm,[0],[0]
"It turns out that for appropriate choice of m = m(k, 1), the best rankm approximation of A, namely Am, constructed by the m largest SVD structure form a rank k projection-cost preserving sketch for A with error 1.
1B ∈ Rn×d ′
is a rank k projection-cost preserving sketch of A ∈ Rn×d, with error 0 ≤ ≤ 1 if, for all rank k orthogonal projection matrices P ∈ Rn×n it holds",4. Analysis of algorithm,[0],[0]
that (1− )‖A−PA‖2F ≤,4. Analysis of algorithm,[0],[0]
‖B − PB‖2F + c ≤ (1 + ),4. Analysis of algorithm,[0],[0]
‖A,4. Analysis of algorithm,[0],[0]
"− PA‖2F for some fixed nonnegative constant c that may depend on A and B but is independent of P.
Similarly, Ãm is a rank k projection-cost preserving sketch for Ã.",4. Analysis of algorithm,[0],[0]
"We show that optimal k-means solution of Ã is close to optimal k-mean solution of A in two steps.
1.",4. Analysis of algorithm,[0],[0]
"First, we show the reverse direction of the implication of rank k projection-cost preserving sketch also holds.",4. Analysis of algorithm,[0],[0]
"In particular, we show that an optimal k-means solution of Ã is also (1 +O( 1)) optimal for Ãm.
2.",4. Analysis of algorithm,[0],[0]
Then we show that Ãm is also rank k projection-cost preserving sketch for A with a different error 2.,4. Analysis of algorithm,[0],[0]
"(this is where we quantify amount of sparsity to k-means error)
",4. Analysis of algorithm,[0],[0]
"Combining these two facts and properly choosing 1 and 2, we conclude that optimal k-means solution of Ã is (1 + )",4. Analysis of algorithm,[0],[0]
optimal k-means solution of A. We lay out the necessary details in the following subsections.,4. Analysis of algorithm,[0],[0]
"Ã and Ãm
We first show that close to optimal cluster indicator matrix obtained by solving k-means clustering problem on Ã yields a close to optimal k-means objective of Ãm.
",4.1. Relation between clustering objective functions of,[0],[0]
Lemma 1.,4.1. Relation between clustering objective functions of,[0],[0]
"For any 0 < ≤ 1/2, let m = dk/ e.",4.1. Relation between clustering objective functions of,[0],[0]
For any Ã ∈ Rn×d with rank r ≥ dk/,4.1. Relation between clustering objective functions of,[0],[0]
"e + k, let Ãm be its best rank m approximation.",4.1. Relation between clustering objective functions of,[0],[0]
"For any set S of rank k cluster projection matrices, let P̃∗ = argminP∈S ‖Ã",4.1. Relation between clustering objective functions of,[0],[0]
− PÃ‖2F and P̃∗m = argminP∈S ‖Ãm−PÃm‖2F .,4.1. Relation between clustering objective functions of,[0],[0]
"For any γ ≥ 1, if ‖Ã− P̂Ã‖2F ≤ γ‖Ã− P̃∗Ã‖2F , then, ‖Ãm− P̂Ãm‖2F ≤ γ‖Ãm",4.1. Relation between clustering objective functions of,[0],[0]
− P̃∗mÃm‖2F,4.1. Relation between clustering objective functions of,[0],[0]
+ (γ − 1)‖Ã − Ãm‖2F + ‖P̂(Ã − Ãm)‖2F .,4.1. Relation between clustering objective functions of,[0],[0]
"In particular, the following holds.
",4.1. Relation between clustering objective functions of,[0],[0]
(i),4.1. Relation between clustering objective functions of,[0],[0]
"If γ = 1, then, ‖Ãm − P̃∗Ãm‖2F ≤ (1 + 2 )‖Ãm",4.1. Relation between clustering objective functions of,[0],[0]
− P̃∗mÃm‖2.,4.1. Relation between clustering objective functions of,[0],[0]
(ii),4.1. Relation between clustering objective functions of,[0],[0]
"If γ = 1 + 1, for any 0 < 1 < 1 satisfying 1 ∑r i=m+1 σ 2 i (Ã) ≤ ∑m+k",4.1. Relation between clustering objective functions of,[0],[0]
"i=m+1 σ 2 i (Ã), then, ‖Ãm",4.1. Relation between clustering objective functions of,[0],[0]
− P̂Ãm‖2F ≤ (1 + 1 + 4 ),4.1. Relation between clustering objective functions of,[0],[0]
‖Ãm,4.1. Relation between clustering objective functions of,[0],[0]
"− P̃∗mÃm‖2.
",4.1. Relation between clustering objective functions of,[0],[0]
"Proof of the above lemma, which follows from repeated applications of linear algebra basics, choice of m, definition of P̂ and optimality of P̃∗, is long and technical and is differed to the supplementary material for better readability.",4.1. Relation between clustering objective functions of,[0],[0]
"Note that on the left hand side of the first inequality above (for γ = 1), we have used P̃∗ instead of P̂ since P̂ and P̃∗ are identical for γ",4.1. Relation between clustering objective functions of,[0],[0]
= 1.,4.1. Relation between clustering objective functions of,[0],[0]
Now we show that optimal cluster indicator matrix obtained by solving k-means clustering problem on Ãm results in close to optimal k-means objective of A. Let P∗,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"=
argminP∈S",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
‖A − PA‖2F and P̃∗m =,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
argminP∈S,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
‖Ãm,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
− PÃm‖2F .,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Our goal is to show that ‖A−P̃∗mA‖2F is close to ‖A−P∗A‖2F .,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"In fact, we prove a stronger result showing that Ãm is a rank k projection cost preserving sketch2 of A.",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
We do this in multiple steps.,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"First we show that for small k, Ãm is approximately best rank m subspace of A. Next, we show that such an Ãm is a rank k projection cost preserving sketch for A, which in turn ensures the required guarantee.
",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
We start with the following lemma which is a consequence of Theorem 2.,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Lemma 2.,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Fix any m ≥ 1 and let 0 < 2,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
< 1/,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"√ m. Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = 16nb 2
22‖A‖2F .",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"Then Ã contains O ( n 22 + d(log n)4 ) non-zero entries in expectation and with probability at least (1− 1/n19 log3 n),
‖A− Ãm‖F ≤ ‖A−Am‖F + 3 √ 2m 1/4‖A‖F
We tailor the above result to show that under mild conditions Ãm approximates Am reasonably well.
",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Lemma 3.,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"Fix any 3, where 0 < 3 < 1.",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Let rank of A be ρ.,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Fix any k that satisfies ∑k/ 3 i=1 σ,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
2 i (A) ≤ 12,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
∑ρ i=1,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
σ 2,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"i (A), and let m = dk/ 3e.",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k
93‖A‖2F
) .",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"Then Ã contains
O ( nk 93 + d(log n)4 ) non-zero entries in expectation and with probability at least (1− 1/n19 log3 n),
‖A− Ãm‖F ≤ (1 + 23)‖A−Am‖F
Next, we use a result from (Cohen et al., 2015) to show that if Ãm is close to best rank approximation of A, then Ãm is rank k projection cost preserving sketch for A.
Theorem 3.",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"[Theorem 9 of (Cohen et al., 2015)] Let m = dk/ 3e.",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"For any A ∈ Rn×d, 0 ≤ 4 ≤ 1 and any B ∈ Rn×d with rank(B) = m satisfying ‖A",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"− B‖2F ≤ (1 + 24)‖A−Am‖2F , the sketch B is a projection cost preserving sketch for A. Specifically, for all rank k orthogonal projections P,
(1− 2 4)‖A−PA‖2F ≤",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
‖B−PB‖2F,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
+ c ≤,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"(1 + 2 3 + 5 4)‖A−PA‖2F
where c is a non-negative scalar.
",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"From Lemma 3 we see that with high probability, ‖A − Ãm‖2F ≤ (1 + 2 23 + 43)‖A −Am‖2F = (1 + 3 23)‖A −
2If B is a rank k projection cost preserving sketch of A, then optimal k-means clustering solution of B results in approximately optimal k-means clustering objective of A (Cohen et al., 2015).
",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
Am‖2F .,4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
"Setting 4 = √
3 3 and B = Ãm, it follows from Theorem 3 that Ãm is a rank k projection cost preserving sketch of A.",4.2. Relation between clustering objective functions of Ãm and A,[0],[0]
We combine these results from previous two subsections to present the main result of this paper.,4.3. Main result,[0],[0]
Theorem 4.,4.3. Main result,[0],[0]
"Fix any , where 0 < < 1/4.",4.3. Main result,[0],[0]
Let rank of A be ρ.,4.3. Main result,[0],[0]
For any k that satisfies ∑k/ i=1,4.3. Main result,[0],[0]
σ,4.3. Main result,[0],[0]
2,4.3. Main result,[0],[0]
i (A) ≤ 1 2 ∑ρ i=1,4.3. Main result,[0],[0]
σ 2,4.3. Main result,[0],[0]
"i (A), and let m = dk/ e. Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k
9‖A‖2F
) .",4.3. Main result,[0],[0]
"For any
set S of rank k cluster projection matrices, let P∗",4.3. Main result,[0],[0]
= argminP∈S,4.3. Main result,[0],[0]
"‖A−PA‖2F , P̃∗ = argminP∈S ‖Ã−PÃ‖2F and P̃∗m = argminP∈S",4.3. Main result,[0],[0]
‖Ãm,4.3. Main result,[0],[0]
− PÃm‖2F .,4.3. Main result,[0],[0]
"For any γ ≥ 1, if ‖Ã − P̂Ã‖2F ≤ γ‖Ã",4.3. Main result,[0],[0]
"− P̃∗Ã‖2F , then Ã contains O ( nk 9 + d(log n) 4 )
non-zero entries in expectation and with probability at least (1 − 1/n19 log3 n)",4.3. Main result,[0],[0]
"the following holds,
(i)",4.3. Main result,[0],[0]
"If γ = 1, then ‖A−P̂A‖2F ≤ (1+2 )(1+11 ) 1−4 ‖A−P ∗A‖2
(ii) If γ = 1 + 1, for any 0 < 1 < 1 satisfying 1 ∑r i=m+1 σ 2 i (Ã) ≤ ∑m+k",4.3. Main result,[0],[0]
"i=m+1 σ 2 i (Ã), then ‖A − P̂A‖2F ≤ (1+ 1+4 )(1+11 ) 1−4 ‖A−P ∗A‖2
Proof.",4.3. Main result,[0],[0]
Set 3 = .,4.3. Main result,[0],[0]
"Then from Lemma 3 we get, ‖A − Ãm‖2F ≤ (1 + 3 2)‖A −Am‖2F .",4.3. Main result,[0],[0]
"Now setting B = Ãm and 4 = √ 3 in Theorem 3, for any rank k orthogonal projection P we get, (1− 2 √
3 )‖A−PA‖2F ≤",4.3. Main result,[0],[0]
‖Ãm −PÃm‖2F + c ≤,4.3. Main result,[0],[0]
"(1 + 2 + 5 √ 3 )‖A−PA‖2F
or simplifying,
(1− 4 )‖A−PA‖2F ≤",4.3. Main result,[0],[0]
‖Ãm −PÃm‖2F + c ≤,4.3. Main result,[0],[0]
(1 + 11 )‖A−PA‖2F,4.3. Main result,[0],[0]
"(5)
Let γ1 = (1 + 2 ) if γ = 1 and γ1 = (1 + 1 + 4 ) if γ ≥ 1.",4.3. Main result,[0],[0]
Then from lemma 1 we get ‖Ãm,4.3. Main result,[0],[0]
− P̂Ãm‖2F ≤ γ1‖Ãm,4.3. Main result,[0],[0]
− P̃∗mÃm‖2.,4.3. Main result,[0],[0]
"Using this result and repeated application of Equation 5 we get,
‖A− P̂A‖2F
≤ 1 1− 4
{",4.3. Main result,[0],[0]
"‖Ãm − P̂Ãm‖2F + c } ≤ 1
1− 4
{ γ1‖Ãm − P̃∗mÃm‖2F + c } ≤ 1
1− 4
{ γ1‖Ãm −P∗Ãm‖2F + c } ≤ 1
1− 4 { γ1",4.3. Main result,[0],[0]
[ (1 + 11 ),4.3. Main result,[0],[0]
‖A−P∗A‖2F,4.3. Main result,[0],[0]
− c ],4.3. Main result,[0],[0]
"+ c }
≤",4.3. Main result,[0],[0]
γ1(1 + 11 ),4.3. Main result,[0],[0]
"1− 4 ‖A−P∗A‖2F
Substituting appropriate value of γ1 yields the result.
",4.3. Main result,[0],[0]
The above result (Theorem 4) is obtained by stitching together many intermediate results.,4.3. Main result,[0],[0]
"To make sure that everything works at the end, we have different ranges for in Lemma 1 and Theorem 4.
",4.3. Main result,[0],[0]
A simple consequence of the above theorem is the following result which ensures (1 + ′) approximation for any 0,4.3. Main result,[0],[0]
< ′,4.3. Main result,[0],[0]
"< 1.
",4.3. Main result,[0],[0]
Corollary 1.,4.3. Main result,[0],[0]
"Fix any ′, where 0 < ′",4.3. Main result,[0],[0]
< 1.,4.3. Main result,[0],[0]
Let rank of A be ρ.,4.3. Main result,[0],[0]
Let m = O(k/ ′) and fix any k that satisfies∑m i=1 σ,4.3. Main result,[0],[0]
2,4.3. Main result,[0],[0]
i (A) ≤ 12,4.3. Main result,[0],[0]
∑ρ i=1,4.3. Main result,[0],[0]
σ 2,4.3. Main result,[0],[0]
i (A).,4.3. Main result,[0],[0]
"Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k
( ′)9‖A‖2F
) .",4.3. Main result,[0],[0]
"For any set S of rank
k cluster projection matrices, let P∗",4.3. Main result,[0],[0]
= argminP∈S ‖A− PA‖2F and P̃∗ = argminP∈S ‖Ã,4.3. Main result,[0],[0]
− PÃ‖2F .,4.3. Main result,[0],[0]
For any 1 ≤ γ ≤ 2 satisfying (γ − 1) ∑r,4.3. Main result,[0],[0]
"i=m+1 σ
2 i (Ã) ≤∑m+k
i=m+1 σ 2 i (Ã), if ‖Ã− P̂Ã‖2F ≤ γ‖Ã−",4.3. Main result,[0],[0]
"P̃∗Ã‖2F , then Ã contains O (
nk ( ′)9 + d(log n)
4 )
non-zero entries in ex-
pectation and with probability at least (1−1/n19 log3 n)",4.3. Main result,[0],[0]
"the following holds,
‖A− P̂A‖2F ≤",4.3. Main result,[0],[0]
γ(1 + ′)‖A−P∗A‖2,4.3. Main result,[0],[0]
"In this section we present empirical evaluation of our proposed algorithm on three real world datasets: USPS, RCV1 and TDT2.",5. Empirical evaluations,[0],[0]
"The USPS dataset (Hull, 1994) contains 9298 handwritten digit images, where each 16 × 16 image is represented by a feature vector of length 256.",5. Empirical evaluations,[0],[0]
"We seek to find k = 10 clusters, one for each of the ten digits.",5. Empirical evaluations,[0],[0]
"The RCV1 dataset (Lewis et al., 2004) is an archive of over 800, 000 manually categorized news articles recently made available by Reuters.",5. Empirical evaluations,[0],[0]
"We use a smaller subset of this dataset available from LIBSVM webpage (LIB) containing 15, 564 news articles from 53 categories.",5. Empirical evaluations,[0],[0]
"Each such news article is represented by a feature vector of length 47, 236.",5. Empirical evaluations,[0],[0]
"We seek to find k = 53 clusters, one for each news article category.",5. Empirical evaluations,[0],[0]
"The TDT2 dataset (Cieri et al., 1999) consists of 11201 text documents which are classified into 96 semantic categories.",5. Empirical evaluations,[0],[0]
"We use a smaller subset of this dataset available from Deng Cai’s webpage3 where those documents appearing in two or more categories are removed, and only the largest 30 categories are kept, resulting in 9, 394 documents in total.",5. Empirical evaluations,[0],[0]
"Each such document is represented by a feature vector of length 36, 771.",5. Empirical evaluations,[0],[0]
"We seek to find k = 30 clusters, one for each news article category.",5. Empirical evaluations,[0],[0]
"For USPS dataset, all (100%) data matrix entries are non-zero.",5. Empirical evaluations,[0],[0]
"However, for TDT2 dataset, only 0.35% entries of the 9394 × 36771 data matrix are
3http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html
non-zero, while for RCV1 dataset, only 0.14% entries of the 15564 × 47236 data matrix are non-zero.",5. Empirical evaluations,[0],[0]
Since these two later datasets are already very sparse we reduce data dimensionality to 1000 using random projection in both cases by multiplying original data matrices with a random projection matrix (of appropriate size) whose entries are standard i.i.d.,5. Empirical evaluations,[0],[0]
normals4.,5. Empirical evaluations,[0],[0]
"After this random projection step, resulting projected data matrices become dense matrices, each containing 100% non-zero entries.",5. Empirical evaluations,[0],[0]
"As we will demonstrate next, for these two dense projected matrices our proposed sparsification method finds k-means clustering solution without severely affecting cluster quality.
",5. Empirical evaluations,[0],[0]
"To apply Lloyd’s heuristic for k-means clustering we use Matlab’s kmeans function which, by default, uses kmeans++ algorithm (Arthur & Vassilvitskii, 2007) for cluster center initialization.",5. Empirical evaluations,[0],[0]
"We repeat this clustering 30 times, each time initializing cluster center using k-means++ and selecting the final clustering as the one with lowest k-means objective.",5. Empirical evaluations,[0],[0]
"We demonstrate the effect of random sparsification obtained by uniform and non-uniform sampling on k-means clustering by reporting the following quantities, (a) the ratio h1(q) = ‖A−XqX>q",5. Empirical evaluations,[0],[0]
A‖2F /‖A−XX,5. Empirical evaluations,[0],[0]
">A‖2F , (b) cluster quality h2(q), measured by normalized mutual information (with respect to ground truth cluster labels of A) of a sparse data matrix Ã whose q fracation of entries are non-zero, and (c) normalized objective function h3(q) =",5. Empirical evaluations,[0],[0]
‖A − XqX>q A‖2F /‖A‖2F,5. Empirical evaluations,[0],[0]
",",5. Empirical evaluations,[0],[0]
as we vary q.,5. Empirical evaluations,[0],[0]
"In the above description, Xq is the cluster indicator matrix obtained by running k-means on sparse data matrix Ã whose q fraction of entries are non-zero and X is the cluster indicator matrix obtained by running k-means on A.
For uniform sampling, p simply indicates that p fraction
4It has been shown (Cohen et al., 2015) that such dimensionality reduction introduces (1 + ) relative error to optimal k-means objective.",5. Empirical evaluations,[0],[0]
"We chose projected dimension to be 1000 since increasing it further did not increase normalized mutual information significantly.
of entries of Ã are non-zero (in expectation, when A is dense matrix).",5. Empirical evaluations,[0],[0]
"For non-uniform sampling, number of nonzero entries in Ã can only be guaranteed by Theorem 2, which typically holds for large n.",5. Empirical evaluations,[0],[0]
In our empirical evaluation we use a slightly different strategy for non-uniform sampling than what is presented in section 2.3.2.,5. Empirical evaluations,[0],[0]
"However, this modified strategy, in principle, is still similar to what is presented in section 2.3.2.",5. Empirical evaluations,[0],[0]
"For any fixed value of p, note that τij = p(A(i, j)/b)
2.",5. Empirical evaluations,[0],[0]
"Now, instead of using pij in terms of τij as given in section 2.3.2, we use,
pij = { τij if τij ≥ p× f√ τij × p× f",5. Empirical evaluations,[0],[0]
"otherwise
where, f > 1, is to be chosen later.",5. Empirical evaluations,[0],[0]
"Therefore, for τij < p×f , pij = √ τij × p× f = p×(|A(i, j)|/b)× √ f .",5. Empirical evaluations,[0],[0]
"The basic idea is still same as before, i.e., when τij is small, instead of setting pij ∝",5. Empirical evaluations,[0],[0]
"(A(i, j))2, we set pij ∝",5. Empirical evaluations,[0],[0]
"|A(i, j)|.",5. Empirical evaluations,[0],[0]
"Now consider the case when τij < p× f , for all i, j.",5. Empirical evaluations,[0],[0]
"The expected number of non-zero entries is ∑ i,j p ×",5. Empirical evaluations,[0],[0]
√ f ×,5. Empirical evaluations,[0],[0]
"(|A(i, j)|/b) = pnd×",5. Empirical evaluations,[0],[0]
"√ f×Avg(|A(i, j)|/b).",5. Empirical evaluations,[0],[0]
"Therefore, if we choose5 f = 1/(Avg(|A(i, j)|/b))2, expected number of non-zero entries in Ã is pnd.",5. Empirical evaluations,[0],[0]
"In general, when the condition τij < p× f does not hold for all i, j, the expected fraction will be even less since pij < p, for τij ≥ p× f .",5. Empirical evaluations,[0],[0]
"In our experimental setting, we set f = 1/(Avg(|A(i, j)|/b))2.",5. Empirical evaluations,[0],[0]
"This ensures for any p, non-uniform sampling results in at most p fraction of non-zero entries in Ã. In our experiments, we vary p from 0.01 to 1.0 in steps of 0.01 and for each value of p, the number of of non-zero entries in Ã obtained due to non-uniform sampling is denoted by q and is plotted in Figure 2 for all three datasets.",5. Empirical evaluations,[0],[0]
"Observe that for all values of p, q = p for uniform sampling and q ≤ p for non-uniform sampling.
",5. Empirical evaluations,[0],[0]
"Next, in Figure 3 we show how random sparsification affects k-means clustering quality with increasing q.",5. Empirical evaluations,[0],[0]
"As can be seen from Figure 3, with increasing q, h1(q) and h3(q) decrease, while h2(q) increases as one would expect.",5. Empirical evaluations,[0],[0]
"In fact, h1(q) decreases quickly towards its optimal value 1 and corresponding h2(q) value quickly increases towards optimal k-means normalized mutual information of A. The normalized k-means objective h3(q)",5. Empirical evaluations,[0],[0]
"also shows steady decrease with increasing q. As can be seen from Figure 3, for all three datasets, non-uniform sampling yields better k-means clustering performance compared to uniform sampling.",5. Empirical evaluations,[0],[0]
"This makes perfect sense since non-uniform sampling, unlike uniform sampling, enforces sparsity by retaining entries with probability that depends on their magnitude.",5. Empirical evaluations,[0],[0]
"In fact, for TDT2 and RCV1 datasets, non-uniform sampling results in significant improvement in k-means clustering performance compared to uniform sampling.
",5. Empirical evaluations,[0],[0]
"5Avg(|A(i, j)|) represents average over all entries |A(i, j)|.
USPS
TDT2
RCV1",5. Empirical evaluations,[0],[0]
In this paper we proposed a simple algorithm for k-means clustering using random matrix sparsification and presented its analysis.,6. Conclusion,[0],[0]
"We proved that under mild condition, for any ∈ (0, 1), a dense data matrix A ∈ Rn×d can be randomly sparsified to yield a data matrix Ã ∈ Rn×d containing O(nk/ 9 + d log4 n) non-zero entries in expectation, such that an (1+ )-approximate k-mean clustering solution of Ã results in (1 +O( ))-approximate clustering solution of A with high probability.",6. Conclusion,[0],[0]
"Empirical results on three real world datasets demonstrated that k-means clustering solution of Ã was indeed very close to k-means clustering solution of A. Moreover, sparsification obtained by non-uniform sampling resulted in better cluster quality compared to uniform sampling.",6. Conclusion,[0],[0]
Empirical results also seem to suggest that the O(1/ 9) dependence on the number of non-zero entries in Ã is possibly a bit loose.,6. Conclusion,[0],[0]
We conclude this paper with two possible open questions: (a) Is it possible to analytically provide a better estimate (by improving dependence on 1/ ) of the number of non-zero entries in the sparse matrix Ã that ensures (1 + ) approximation guarantee?,6. Conclusion,[0],[0]
"and, (b) Using different proof technique, is it possible to show that γ-approximate clustering solution of Ã will result in γ(1 + )-approximate solution of A as shown in Corollary 1, even for γ > 2?",6. Conclusion,[0],[0]
"In other words, is the restriction on γ
in Corollary 1 a limitation of the proof technique or does it indicate computational hardness of the problem?",6. Conclusion,[0],[0]
K-means clustering algorithm using Lloyd’s heuristic is one of the most commonly used tools in data mining and machine learning that shows promising performance.,abstractText,[0],[0]
"However, it suffers from a high computational cost resulting from pairwise Euclidean distance computations between data points and cluster centers in each iteration of Lloyd’s heuristic.",abstractText,[0],[0]
"Main contributing factor of this computational bottle neck is a matrix-vector multiplication step, where the matrix contains all the data points and the vector is a cluster center.",abstractText,[0],[0]
In this paper we show that we can randomly sparsify the original data matrix resulting in a sparse data matrix which can significantly speed up the above mentioned matrix vector multiplication step without significantly affecting cluster quality.,abstractText,[0],[0]
"In particular, we show that optimal k-means clustering solution of the sparse data matrix, obtained by applying random matrix sparsification, results in an approximately optimal k-means clustering objective of the original data matrix.",abstractText,[0],[0]
Our empirical studies on three real world datasets corroborate our theoretical findings and demonstrate that our proposed sparsification method can indeed achieve satisfactory clustering performance.,abstractText,[0],[0]
K-means clustering using random matrix sparsification,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1470–1480 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a nontrivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.",text,[0],[0]
"Knowledge graph (Dong et al., 2014) is a powerful graph structure that can provide direct access of knowledge to users via various applications such as structured search, question answering, and intelligent virtual assistant.",1 Introduction,[0],[0]
"A common representation of knowledge graph beliefs is in the
form of a discrete relational triple such as LocatedIn(NewOrleans,Louisiana).
",1 Introduction,[1.0000000043507873],"['A common representation of knowledge graph beliefs is in the form of a discrete relational triple such as LocatedIn(NewOrleans,Louisiana).']"
A main challenge for using discrete representation of knowledge graph is the lack of capability of accessing the similarities among different entities and relations.,1 Introduction,[0],[0]
"Knowledge graph embedding (KGE) techniques (e.g., RESCAL (Nickel et al., 2011), TRANSE (Bordes et al., 2013), DISTMULT (Yang et al., 2015), and COMPLEX (Trouillon et al., 2016)) have been proposed in recent years to deal with the issue.",1 Introduction,[0],[0]
"The main idea is to represent the entities and relations in a vector space, and one can use machine learning technique to learn the continuous representation of the knowledge graph in the latent space.
",1 Introduction,[0.9999999788769537],"['The main idea is to represent the entities and relations in a vector space, and one can use machine learning technique to learn the continuous representation of the knowledge graph in the latent space.']"
"However, even steady progress has been made in developing novel algorithms for knowledge graph embedding, there is still a common challenge in this line of research.",1 Introduction,[1.0],"['However, even steady progress has been made in developing novel algorithms for knowledge graph embedding, there is still a common challenge in this line of research.']"
"For space efficiency, common knowledge graphs such as Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and NELL (Mitchell et al., 2015) by default only stores beliefs, rather than disbeliefs.",1 Introduction,[1.0],"['For space efficiency, common knowledge graphs such as Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and NELL (Mitchell et al., 2015) by default only stores beliefs, rather than disbeliefs.']"
"Therefore, when training the embedding models, there is only the natural presence of the positive examples.",1 Introduction,[1.0],"['Therefore, when training the embedding models, there is only the natural presence of the positive examples.']"
"To use negative examples, a common method is to remove the correct tail entity, and randomly sample from a uniform distribution (Bordes et al., 2013).",1 Introduction,[1.0],"['To use negative examples, a common method is to remove the correct tail entity, and randomly sample from a uniform distribution (Bordes et al., 2013).']"
"Unfortunately, this approach is not ideal, because the sampled entity could be completely unrelated to the head and the target relation, and thus the quality of randomly generated negative examples is often poor (e.g, LocatedIn(NewOrleans,BarackObama)).",1 Introduction,[1.0],"['Unfortunately, this approach is not ideal, because the sampled entity could be completely unrelated to the head and the target relation, and thus the quality of randomly generated negative examples is often poor (e.g, LocatedIn(NewOrleans,BarackObama)).']"
"Other approach might leverage external ontological constraints such as entity types (Krompaß et al., 2015) to generate negative examples, but such resource does not always exist or accessible.
",1 Introduction,[0],[0]
"In this work, we provide a generic solution to improve the training of a wide range of knowl-
1470
edge graph embedding models.",1 Introduction,[0],[0]
"Inspired by the recent advances of generative adversarial deep models (Goodfellow et al., 2014), we propose a novel adversarial learning framework, namely, KBGAN, for generating better negative examples to train knowledge graph embedding models.",1 Introduction,[0],[0]
"More specifically, we consider probabilitybased, log-loss embedding models as the generator to supply better quality negative examples, and use distance-based, margin-loss embedding models as the discriminator to generate the final knowledge graph embeddings.",1 Introduction,[1.0],"['More specifically, we consider probabilitybased, log-loss embedding models as the generator to supply better quality negative examples, and use distance-based, margin-loss embedding models as the discriminator to generate the final knowledge graph embeddings.']"
"Since the generator has a discrete generation step, we cannot directly use the gradient-based approach to backpropagate the errors.",1 Introduction,[0],[0]
"We then consider a onestep reinforcement learning setting, and use a variance-reduction REINFORCE method to achieve this goal.",1 Introduction,[0],[0]
"Empirically, we perform experiments on three common KGE datasets (FB15K-237, WN18 and WN18RR), and verify the adversarial learning approach with a set of KGE models.",1 Introduction,[0],[0]
"Our experiments show that across various settings, this adversarial learning mechanism can significantly improve the performance of some of the most commonly used translation based KGE methods.",1 Introduction,[0],[0]
"Our contributions are three-fold:
•",1 Introduction,[0],[0]
"We are the first to consider adversarial learning to generate useful negative training examples to improve knowledge graph embedding.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"This adversarial learning framework applies to a wide range of KGE models, without the need of external ontologies constraints.
",1 Introduction,[0],[0]
• Our method shows consistent performance gains on three commonly used KGE datasets.,1 Introduction,[0],[0]
"A large number of knowledge graph embedding models, which represent entities and relations in a knowledge graph with vectors or matrices, have been proposed in recent years.",2.1 Knowledge Graph Embeddings,[0],[0]
"RESCAL (Nickel et al., 2011) is one of the earliest studies on matrix factorization based knowledge graph embedding models, using a bilinear form as score function.",2.1 Knowledge Graph Embeddings,[0],[0]
"TRANSE (Bordes et al., 2013) is the first model to introduce translation-based embedding.",2.1 Knowledge Graph Embeddings,[0],[0]
"Later variants, such as TRANSH (Wang et al., 2014), TRANSR (Lin et al., 2015) and TRANSD (Ji et al., 2015), extend TRANSE by projecting the embedding vectors of entities into various spaces.",2.1 Knowledge Graph Embeddings,[0],[0]
"DISTMULT (Yang et al., 2015) simplifies RESCAL by only using a diagonal matrix, and COMPLEX (Trouillon et al., 2016) extends DISTMULT into the complex number field.",2.1 Knowledge Graph Embeddings,[0],[0]
"(Nickel et al., 2015) is a comprehensive survey on these models.
",2.1 Knowledge Graph Embeddings,[0],[0]
Some of the more recent models achieve strong performances.,2.1 Knowledge Graph Embeddings,[0],[0]
"MANIFOLDE (Xiao et al., 2016) embeds a triple as a manifold rather than a point.",2.1 Knowledge Graph Embeddings,[0],[0]
"HOLE (Nickel et al., 2016) employs circular correlation to combine the two entities in a triple.",2.1 Knowledge Graph Embeddings,[0],[0]
"CONVE (Dettmers et al., 2017) uses a convolutional neural network as the score function.",2.1 Knowledge Graph Embeddings,[0],[0]
"However, most of these studies use uniform sampling to generate negative training examples (Bordes et al., 2013).",2.1 Knowledge Graph Embeddings,[0],[0]
"Because our framework is independent of the concrete form of models, all these models can be potentially incorporated into our framework, regardless of the complexity.",2.1 Knowledge Graph Embeddings,[0],[0]
"As a proof of principle, our work focuses on simpler models.",2.1 Knowledge Graph Embeddings,[0],[0]
Table 1 summarizes the score functions and dimensions of all models mentioned above.,2.1 Knowledge Graph Embeddings,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) was originally proposed for generating samples in a continuous space such as images.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"A GAN consists of two parts, the generator and the discriminator.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
The generator accepts a noise input and outputs an image.,2.2 Generative Adversarial Networks and its Variants,[0],[0]
The discriminator is a classifier which classifies images as “true” (from the ground truth set) or “fake” (generated by the generator).,2.2 Generative Adversarial Networks and its Variants,[0],[0]
"When training a GAN, the generator and the discriminator play a minimax game, in which the generator tries to generate “real” images to deceive the discriminator, and the discriminator tries to tell them apart from ground truth images.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"GANs are also capable of generating samples satisfying certain requirements, such as conditional GAN (Mirza and Osindero, 2014).
",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"It is not possible to use GANs in its original form for generating discrete samples like natural language sentences or knowledge graph triples, because the discrete sampling step prevents gradients from propagating back to the generator.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"SEQGAN (Yu et al., 2017) is one of the first successful solutions to this problem by using reinforcement learning—It trains the generator using policy gradient and other tricks.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"IRGAN (Wang et al., 2017) is a recent work which combines two categories of information retrieval models into a discrete GAN framework.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"Likewise, our framework relies on policy gradient to train the generator which provides discrete negative triples.
",2.2 Generative Adversarial Networks and its Variants,[0],[0]
The discriminator in a GAN is not necessarily a classifier.,2.2 Generative Adversarial Networks and its Variants,[0],[0]
"Wasserstein GAN or WGAN (Arjovsky et al., 2017) uses a regressor with clipped parameters as its discriminator, based on solid analysis about the mathematical nature of GANs.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"GOGAN (Juefei-Xu et al., 2017) further replaces the loss function in WGAN with marginal loss.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"Although originating from very different fields, the form of loss function in our framework turns out to be more closely related to the one in GOGAN.",2.2 Generative Adversarial Networks and its Variants,[0],[0]
"In this section, we first define two types of training objectives in knowledge graph embedding models to show how KBGAN can be applied.",3 Our Approaches,[0],[0]
"Then, we demonstrate a long overlooked problem about negative sampling which motivates us to propose KBGAN to address the problem.",3 Our Approaches,[0],[0]
"Finally, we dive into the mathematical, and algorithmic details of
KBGAN.",3 Our Approaches,[0],[0]
"For a given knowledge graph, let E be the set of entities, R be the set of relations, and T be the set of ground truth triples.",3.1 Types of Training Objectives,[0],[0]
"In general, a knowledge graph embedding (KGE) model can be formulated as a score function f(h, r, t), h, t ∈ E , r ∈ R which assigns a score to every possible triple in the knowledge graph.",3.1 Types of Training Objectives,[0],[0]
"The estimated likelihood of a triple to be true depends only on its score given by the score function.
",3.1 Types of Training Objectives,[0],[0]
"Different models formulate their score function based on different designs, and therefore interpret scores differently, which further lead to various training objectives.",3.1 Types of Training Objectives,[0],[0]
"Two common forms of training objectives are particularly of our interest: Marginal loss function is commonly used by a large group of models called translation-based models, whose score function models distance between points or vectors, such as TRANSE, TRANSH, TRANSR, TRANSD and so on.",3.1 Types of Training Objectives,[0],[0]
"In these models, smaller distance indicates a higher likelihood of truth, but only qualitatively.",3.1 Types of Training Objectives,[0],[0]
"The marginal loss function takes the following form:
Lm = ∑
(h,r,t)∈T [f(h, r, t)− f(h′, r, t′) +",3.1 Types of Training Objectives,[0],[0]
"γ]+ (1)
where γ is the margin, [·]+ = max(0, ·) is the hinge function, and (h′, r, t′) is a negative triple.",3.1 Types of Training Objectives,[0],[0]
"The negative triple is generated by replacing the head entity or the tail entity of a positive triple with a random entity in the knowledge graph, or formally (h′, r, t′) ∈ {(h′, r, t)|h′ ∈ E} ∪ {(h, r, t′)|t′ ∈ E}.",3.1 Types of Training Objectives,[0],[0]
Log-softmax loss function is commonly used by models whose score function has probabilistic interpretation.,3.1 Types of Training Objectives,[0],[0]
"Some notable examples are RESCAL, DISTMULT, COMPLEX.",3.1 Types of Training Objectives,[0],[0]
"Applying the softmax function on scores of a given set of triples gives the probability of a triple to be the best one among them: p(h, r, t) = exp f(h,r,t)∑
(h′,r,t′) exp f(h ′,r,t′) .",3.1 Types of Training Objectives,[0],[0]
"The loss
function is the negative log-likelihood of this probabilistic model:
",3.1 Types of Training Objectives,[0],[0]
"Ll = ∑
(h,r,t)∈T − log exp f(h, r, t)∑ exp f(h′, r, t′)
(h′, r, t′) ∈ {(h, r, t)} ∪Neg(h, r, t) (2) where Neg(h, r, t) ⊂ {(h′, r, t)|h′ ∈ E} ∪ {(h, r, t′)|t′ ∈ E} is a set of sampled corrupted triples.
",3.1 Types of Training Objectives,[0],[0]
"Other forms of loss functions exist, for example CONVE uses a triple-wise logistic function to model how likely the triple is true, but by far the two described above are the most common.",3.1 Types of Training Objectives,[0],[0]
"Also, softmax function gives an probabilistic distribution over a set of triples, which is necessary for a generator to sample from them.",3.1 Types of Training Objectives,[0],[0]
"Most previous KGE models use uniform negative sampling for generating negative triples, that is, replacing the head or tail entity of a positive triple with any of the entities in E , all with equal probability.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"Most of the negative triples generated in this way contribute little to learning an effective embedding, because they are too obviously false.
",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"To demonstrate this issue, let us consider the following example.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"Suppose we have a ground truth triple LocatedIn(NewOrleans,Louisiana), and corrupt it by replacing its tail entity.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"First, we remove the tail entity, leaving LocatedIn(NewOrleans,?).",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"Because the relation LocatedIn constraints types of its entities, “?” must be a geographical region.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"If we fill “?” with a random entity e ∈ E , the probability of e having a wrong type is very high, resulting in ridiculous triples like LocatedIn(NewOrleans,BarackObama) or LocatedIn(NewOrleans,StarTrek).",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"Such triples are considered “too easy”, because they can be eliminated solely by types.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"In contrast, LocatedIn(NewOrleans,Florida) is a very useful negative triple, because it satisfies type constraints, but it cannot be proved wrong without detailed knowl-
edge of American geography.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"If a KGE model is fed with mostly “too easy” negative examples, it would probably only learn to represent types, not the underlying semantics.
",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"The problem is less severe to models using logsoftmax loss function, because they typically samples tens or hundreds of negative triples for one positive triple in each iteration, and it is likely to have a few useful negatives among them.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"For instance, (Trouillon et al., 2016) found that a 100:1 negative-to-positive ratio results in the best performance for COMPLEX.",3.2 Weakness of Uniform Negative Sampling,[1.0],"['For instance, (Trouillon et al., 2016) found that a 100:1 negative-to-positive ratio results in the best performance for COMPLEX.']"
"However, for marginal loss function, whose negative-to-positive ratio is always 1:1, the low quality of uniformly sampled negatives can seriously damage their performance.",3.2 Weakness of Uniform Negative Sampling,[0],[0]
"Inspired by GANs, we propose an adversarial training framework named KBGAN which uses a KGE model with softmax probabilities to provide high-quality negative samples for the training of a KGE model whose training objective is marginal loss function.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"This framework is independent of the score functions of these two models, and therefore possesses some extent of universality.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Figure 1 illustrates the overall structure of KBGAN.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"In parallel to terminologies used in GAN literature, we will simply call these two models generator and discriminator respectively in the rest of this paper.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"We use softmax probabilistic models as the generator because they can adequately model the “sampling from a probability distribu-
Algorithm 1: The KBGAN algorithm Data: training set of positive fact triples T = {(h, r, t)}",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Input: Pre-trained generator G with parameters θG and score function fG(h, r, t), and pre-trained discriminator D with
parameters θD and score function fD(h, r, t)",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Output: Adversarially trained discriminator
1 b←− 0; // baseline for policy gradient 2 repeat 3 Sample a mini-batch of data Tbatch from T ; 4 GG ←− 0, GD ←− 0; // gradients of parameters of G and D 5 rsum ←− 0; // for calculating the baseline 6 for (h, r, t) ∈",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Tbatch do 7 Uniformly randomly sample Ns negative triples Neg(h, r, t) =",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"{(h′i, r, t′i)}i=1...",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
Ns ; 8 Obtain their probability of being generated: pi = exp fG(h ′,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"i,r,t ′",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
i)∑Ns,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"j=1 exp fG(h ′ j ,r,t ′ j)
;
9 Sample one negative triple (h′s, r, t′s) from Neg(h, r, t) according to {pi}i=1...",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
Ns .,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Assume its probability to be ps;
10 GD ←− GD +∇θD",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"[fD(h, r, t)− fD(h′s, r, t′s) + γ]+; // accumulate gradients for D 11 r ←− −fD(h′s, r, t′s), rsum ←− rsum + r; // r is the reward 12 GG ←− GG",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
+ (r − b)∇θG log ps; // accumulate gradients for G 13 end 14 θG ←−,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
θG,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"+ ηGGG,",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"θD ←− θD − ηDGD; // update parameters 15 b← rsum/|Tbatch|; // update baseline 16 until convergence;
tion” process of discrete GANs, and we aim at improving discriminators based on marginal loss because they can benefit more from high-quality negative samples.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Note that a major difference between GAN and our work is that, the ultimate goal of our framework is to produce a good discriminator, whereas GANS are aimed at training a good generator.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"In addition, the discriminator here is not a classifier as it would be in most GANs.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Intuitively, the discriminator should assign a relatively small distance to a high-quality negative sample.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"In order to encourage the generator to generate useful negative samples, the objective of the generator is to minimize the distance given by discriminator for its generated triples.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"And just like the ordinary training process, the objective of the discriminator is to minimize the marginal loss between the positive triple and the generated negative triple.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"In an adversarial training setting, the generator and the discriminator are alternatively trained towards their respective objectives.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Suppose that the generator produces a probability distribution on negative triples pG(h
′, r, t′|h, r, t) given a positive triple (h, r, t), and generates negative triples (h′, r, t′) by sampling from this distribution.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Let fD(h, r, t) be the score function of the discriminator.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[1.0],"['Let fD(h, r, t) be the score function of the discriminator.']"
"The objective of the discriminator can be formulated as
minimizing the following marginal loss function:
LD = ∑
(h,r,t)∈T [fD(h, r, t)− fD(h′, r, t′) + γ]+
(h′, r, t′) ∼ pG(h′, r, t′|h, r, t) (3)
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"The only difference between this loss function and Equation 1 is that it uses negative samples from the generator.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"The objective of the generator can be formulated as maximizing the following expectation of negative distances:
RG = ∑
(h,r,t)∈T E[−fD(h′, r, t′)]
(h′, r, t′) ∼ pG(h′, r, t′|h, r, t) (4)
RG involves a discrete sampling step, so we cannot find its gradient with simple differentiation.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"We use a simple special case of Policy Gradient Theorem1 (Sutton et al., 2000) to obtain the gradient of RG with respect to parameters of the generator:
∇GRG = ∑
(h,r,t)∈T E(h′,r,t′)∼pG(h′,r,t′|h,r,t)
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"[−fD(h′, r, t′)∇G log pG(h′, r, t′|h, r, t)]
' ∑
(h,r,t)∈T
1
N
∑
(h′i,r,t ′",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"i)∼pG(h′,r,t′|h,r,t),i=1...",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"N
[−fD(h′, r, t′)∇G log pG(h′, r, t′|h, r, t)]",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"(5) 1A proof can be found in the supplementary material
where the second approximate equality means we approximate the expectation with sampling in practice.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Now we can calculate the gradient of RG and optimize it with gradient-based algorithms.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Policy Gradient Theorem arises from reinforcement learning (RL), so we would like to draw an analogy between our model and an RL model.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
The generator can be viewed as an agent which interacts with the environment by performing actions and improves itself by maximizing the reward returned from the environment in response of its actions.,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Correspondingly, the discriminator can be viewed as the environment.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Using RL terminologies, (h, r, t) is the state (which determines what actions the actor can take), pG(h′, r, t′|h, r, t) is the policy (how the actor choose actions), (h′, r, t′) is the action, and −fD(h′, r, t′) is the reward.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"The method of optimizing RG described above is called REINFORCE (Williams, 1992) algorithm in RL.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Our model is a simple special case of RL, called one-step RL.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"In a typical RL setting, each action performed by the agent will change its state, and the agent will perform a series of actions (called an epoch) until it reaches certain states or the number of actions reaches a certain limit.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"However, in the analogy above, actions does not affect the state, and after each action we restart with another unrelated state, so each epoch consists of only one action.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"To reduce the variance of REINFORCE algorithm, it is common to subtract a baseline from the reward, which is an arbitrary number that only depends on the state, with-
out affecting the expectation of gradients.2 In our case, we replace −fD(h′, r, t′) with −fD(h′, r, t′)",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0.9940479222894703],"['To reduce the variance of REINFORCE algorithm, it is common to subtract a baseline from the reward, which is an arbitrary number that only depends on the state, with- out affecting the expectation of gradients.2 In our case, we replace −fD(h′, r, t′) with −fD(h′, r, t′) − b(h, r, t) in the equation above to introduce the baseline.']"
"− b(h, r, t) in the equation above to introduce the baseline.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"To avoid introducing new parameters, we simply let b be a constant, the average reward of the whole training set: b =∑
(h,r,t)∈T E(h′,r,t′)∼pG(h′,r,t′|h,r,t)[−fD(h′, r, t′)].",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"In practice, b is approximated by the mean of rewards of recently generated negative triples.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0.9999999922466583],"['In practice, b is approximated by the mean of rewards of recently generated negative triples.']"
"Let the generator’s score function to be fG(h, r, t), given a set of candidate negative triples Neg(h, r, t) ⊂ {(h′, r, t)|h′ ∈ E}∪{(h, r, t′)|t′ ∈ E}, the probability distribution pG is modeled as:
pG(h ′, r, t′|h, r, t)",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"= exp fG(h ′, r, t′)∑ exp fG(h∗, r, t∗) (h∗, r, t∗) ∈ Neg(h, r, t) (6) Ideally, Neg(h, r, t) should contain all possible negatives.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"However, knowledge graphs are usually highly incomplete, so the ”hardest” negative triples are very likely to be false negatives (true facts).",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"To address this issue, we instead generate Neg(h, r, t) by uniformly sampling of Ns entities (a small number compared to the number of all possible negatives) from E to replace h or t. Because in real-world knowledge graphs, true negatives are usually far more than false negatives, such set would be unlikely to contain any false negative, and the negative selected by the generator would likely be a true negative.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Using a small Neg(h, r, t) can also significantly reduce computational complexity.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Besides, we adopt the “bern” sampling technique (Wang et al., 2014) which replaces the “1” side in “1-to-N” and “N-to-1” relations with higher probability to further reduce false negatives.
",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
Algorithm 1 summarizes the whole adversarial training process.,3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Both the generator and the dis-
2A proof of such fact can also be found in the supplementary material
criminator require pre-training, which is the same as conventionally training a single KBE model with uniform negative sampling.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Formally speaking, one can pre-train the generator by minimizing the loss function defined in Equation (1), and pre-train the discriminator by minimizing the loss function defined in Equation (2).",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"Line 14 in the algorithm assumes that we are using the vanilla gradient descent as the optimization method, but obviously one can substitute it with any gradientbased optimization algorithm.",3.3 Generative Adversarial Training for Knowledge Graph Embedding Models,[0],[0]
"To evaluate our proposed framework, we test its performance for the link prediction task with different generators and discriminators.",4 Experiments,[0],[0]
"For the generator, we choose two classical probability-based KGE model, DISTMULT and COMPLEX, and for the discriminator, we also choose two classical translation-based KGE model, TRANSE and TRANSD, resulting in four possible combinations of generator and discriminator in total.",4 Experiments,[0],[0]
See Table 1 for a brief summary of these models.,4 Experiments,[0],[0]
"We use three common knowledge base completion datasets for our experiment: FB15k-237, WN18 and WN18RR. FB15k-237 is a subset of FB15k introduced by (Toutanova and Chen, 2015), which removed redundant relations in FB15k and greatly reduced the number of relations.",4.1.1 Datasets,[0],[0]
"Likewise, WN18RR is a subset of WN18 introduced by (Dettmers et al., 2017) which removes reversing relations and dramatically increases the difficulty of reasoning.",4.1.1 Datasets,[0],[0]
"Both FB15k and WN18 are first introduced by (Bordes et al., 2013) and have been commonly used in knowledge graph researches.",4.1.1 Datasets,[0],[0]
Statistics of datasets we used are shown in Table 3.,4.1.1 Datasets,[0],[0]
"Following previous works like (Yang et al., 2015) and (Trouillon et al., 2016), for each run, we report two common metrics, mean reciprocal ranking (MRR) and hits at 10 (H@10).",4.1.2 Evaluation Protocols,[1.0],"['Following previous works like (Yang et al., 2015) and (Trouillon et al., 2016), for each run, we report two common metrics, mean reciprocal ranking (MRR) and hits at 10 (H@10).']"
"We only report scores under the filtered setting (Bordes et al., 2013), which removes all triples appeared in training, validating, and testing sets from candidate triples before obtaining the rank of the ground truth triple.",4.1.2 Evaluation Protocols,[0],[0]
"3 In the pre-training stage, we train every model to convergence for 1000 epochs, and divide every epoch into 100 mini-batches.",4.1.3 Implementation Details,[1.0],"['3 In the pre-training stage, we train every model to convergence for 1000 epochs, and divide every epoch into 100 mini-batches.']"
"To avoid overfitting, we adopt early stopping by evaluating MRR on the validation set every 50 epochs.",4.1.3 Implementation Details,[0],[0]
"We tried γ = 0.5, 1, 2, 3, 4, 5 and L1, L2 distances for TRANSE and TRANSD, and λ = 0.01, 0.1, 1, 10 for DISTMULT and COMPLEX, and determined the best hyperparameters listed on table 2, based on their performances on the validation set after pre-training.",4.1.3 Implementation Details,[0],[0]
"Due to limited computation resources, we deliberately limit the dimensions of embeddings to k = 50, similar to the one used in earlier works, to save time.",4.1.3 Implementation Details,[0],[0]
"We also apply certain constraints or regularizations to these models, which are mostly the same as those described in their original publications, and also listed on table 2.
",4.1.3 Implementation Details,[0],[0]
"In the adversarial training stage, we keep all the hyperparamters determined in the pre-training stage unchanged.",4.1.3 Implementation Details,[0],[0]
"The number of candidate negative triples, Ns, is set to 20 in all cases, which is proven to be optimal among the candidate set of {5, 10, 20, 30, 50}.",4.1.3 Implementation Details,[0],[0]
"We train for 5000 epochs, with 100 mini-batches for each epoch.",4.1.3 Implementation Details,[0],[0]
"We also use early stopping in adversarial training by evaluating MRR on the validation set every 100 epochs.
",4.1.3 Implementation Details,[0],[0]
"We use the self-adaptive optimization method Adam (Kingma and Ba, 2015) for all trainings, and always use the recommended default setting α = 0.001, β1 = 0.9, β2 = 0.999, = 10 −8.",4.1.3 Implementation Details,[0],[0]
Results of our experiments as well as baselines are shown in Table 4.,4.2 Results,[0],[0]
"All settings of adversarial training bring a pronounced improvement to the model, which indicates that our method is consistently effective in various cases.",4.2 Results,[1.0],"['All settings of adversarial training bring a pronounced improvement to the model, which indicates that our method is consistently effective in various cases.']"
"TRANSE performs slightly worse than TRANSD on FB15k-237 and WN18, but better on WN18RR.",4.2 Results,[0],[0]
"Using DISTMULT or COMPLEX as the generator does not affect performance greatly.
",4.2 Results,[0],[0]
"TRANSE and TRANSD enhanced by KBGAN can significantly beat their corresponding baseline implementations, and outperform stronger baselines in some cases.",4.2 Results,[0],[0]
"As a prototypical and proofof-principle experiment, we have never expected state-of-the-art results.",4.2 Results,[0],[0]
"Being simple models pro-
3The KBGAN source code is available at https:// github.com/cai-lw/KBGAN
posed several years ago, TRANSE and TRANSD has their limitations in expressiveness that are unlikely to be fully compensated by better training technique.",4.2 Results,[0],[0]
"In future researches, people may try employing more advanced models into KBGAN, and we believe it has the potential to become stateof-the-art.
",4.2 Results,[0],[0]
"To illustrate our training progress, we plot performances of the discriminator on validation set over epochs, which are displayed in Figure 2.",4.2 Results,[0],[0]
"As all these graphs show, our performances are always in increasing trends, converging to its max-
imum as training proceeds, which indicates that KBGAN is a robust GAN that can converge to good results in various settings, although GANs are wellknown for difficulty in convergence.",4.2 Results,[0.9999999951898249],"['As all these graphs show, our performances are always in increasing trends, converging to its max- imum as training proceeds, which indicates that KBGAN is a robust GAN that can converge to good results in various settings, although GANs are wellknown for difficulty in convergence.']"
"Fluctuations in these graphs may seem more prominent than other KGE models, but is considered normal for an adversially trained model.",4.2 Results,[0],[0]
Note that in some cases the curve still tends to rise after 5000 epochs.,4.2 Results,[1.0],['Note that in some cases the curve still tends to rise after 5000 epochs.']
"We do not have sufficient computation resource to train for more epochs, but we believe that they will also eventually converge.",4.2 Results,[0],[0]
"To demonstrate that our approach does generate better negative samples, we list some examples of them in Table 5, using the KBGAN (TRANSE + DISTMULT) model and the WN18 dataset.",4.3 Case study,[1.0],"['To demonstrate that our approach does generate better negative samples, we list some examples of them in Table 5, using the KBGAN (TRANSE + DISTMULT) model and the WN18 dataset.']"
"All hyperparameters are the same as those described in Section 4.1.3.
",4.3 Case study,[0],[0]
"Compared to uniform random negatives which are almost always totally unrelated, the generator generates more semantically related negative samples, which is different from type relatedness we used as example in Section 3.2, but also helps training.",4.3 Case study,[1.0],"['Compared to uniform random negatives which are almost always totally unrelated, the generator generates more semantically related negative samples, which is different from type relatedness we used as example in Section 3.2, but also helps training.']"
"In the first example, two of the five terms are physically related to the process of distilling liquids.",4.3 Case study,[1.0],"['In the first example, two of the five terms are physically related to the process of distilling liquids.']"
"In the second example, three of the five entities are geographical objects.",4.3 Case study,[0],[0]
"In the third example, two of the five entities express the concept of “gather”.
",4.3 Case study,[0],[0]
"Because we deliberately limited the strength of generated negatives by using a small Ns as described in Section 3.3, the semantic relation is pretty weak, and there are still many unrelated entities.",4.3 Case study,[1.0],"['Because we deliberately limited the strength of generated negatives by using a small Ns as described in Section 3.3, the semantic relation is pretty weak, and there are still many unrelated entities.']"
"However, empirical results (when selecting the optimal Ns) shows that such situation is more beneficial for training the discriminator than generating even stronger negatives.",4.3 Case study,[1.0],"['However, empirical results (when selecting the optimal Ns) shows that such situation is more beneficial for training the discriminator than generating even stronger negatives.']"
We propose a novel adversarial learning method for improving a wide range of knowledge graph embedding models—We designed a generatordiscriminator framework with dual KGE components.,5 Conclusions,[1.0],['We propose a novel adversarial learning method for improving a wide range of knowledge graph embedding models—We designed a generatordiscriminator framework with dual KGE components.']
"Unlike random uniform sampling, the generator model generates higher quality negative examples, which allow the discriminator model to learn better.",5 Conclusions,[1.0],"['Unlike random uniform sampling, the generator model generates higher quality negative examples, which allow the discriminator model to learn better.']"
"To enable backpropagation of error, we introduced a one-step REINFORCE method to seamlessly integrate the two modules.",5 Conclusions,[1.0],"['To enable backpropagation of error, we introduced a one-step REINFORCE method to seamlessly integrate the two modules.']"
"Experimentally, we tested the proposed ideas with four commonly used KGE models on three datasets, and the results showed that the adversarial learning framework brought consistent improvements to various KGE models under different settings.",5 Conclusions,[1.0],"['Experimentally, we tested the proposed ideas with four commonly used KGE models on three datasets, and the results showed that the adversarial learning framework brought consistent improvements to various KGE models under different settings.']"
"We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models.",abstractText,[0],[0]
"Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a nontrivial task.",abstractText,[0],[0]
"Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training.",abstractText,[0],[0]
"Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs.",abstractText,[0],[0]
"This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks.",abstractText,[0],[0]
"In experiments, we adversarially train two translation-based models, TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX.",abstractText,[0],[0]
"We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR.",abstractText,[0],[0]
Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.,abstractText,[0],[0]
KBGAN: Adversarial Learning for Knowledge Graph Embeddings,title,[0],[0]
