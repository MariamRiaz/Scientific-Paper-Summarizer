{"sections": [{"heading": "1 Introduction", "text": "In machine learning and statistics, it is often desirable to represent a large-scale dataset in a more tractable, lowerdimensional form, without losing too much information. One of the most robust ways to achieve this goal is through principal component projection (PCP):\nPCP: project vectors onto the span of the top principal components of the a matrix.\nIt is well-known that PCP decreases noise and increases efficiency in downstream tasks. One of the main applications is principal component regression (PCR):\nPCR: linear regression but restricted to the subspace of top principal components.\nClassical algorithms for PCP or PCR rely on a principal component analysis (PCA) solver to recover the top principal components first; with these components available, the\n*Equal contribution . Future version of this paper shall be found at https://arxiv.org/abs/1608.04773. 1Microsoft Reseaerch 2Princeton University. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly.\nUnfortunately, PCA solvers demand a running time that at least linearly scales with the number of top principal components chosen for the projection. For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylovbased (Musco & Musco, 2015) or Lanczos-based (AllenZhu & Li, 2016a) methods require a running time that is proportional to 1000\u00d740 = 4\u00d7104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations. This is usually computationally intractable."}, {"heading": "1.1 Approximating PCP Without PCA", "text": "In this paper, we propose the following notion of PCP approximation. Given a data matrix A \u2208 Rd\u2032\u00d7d (with singular values no greater than 1) and a threshold \u03bb > 0, we say that an algorithm solves (\u03b3, \u03b5)-approximate PCP if \u2014 informally speaking and up to a multiplicative 1\u00b1\u03b5 error\u2014 it projects (see Def. 3.1 for a formal definition)\n1. eigenvector \u03bd of A>A with value in [ \u03bb(1 +\u03b3), 1 ] to \u03bd,\n2. eigenvector \u03bd of A>A with value in [ 0, \u03bb(1\u2212 \u03b3) ] to ~0,\n3. eigenvector \u03bd of A>A with value in [ \u03bb(1 \u2212 \u03b3), \u03bb(1 +\n\u03b3) ] to \u201canywhere between ~0 and \u03bd.\u201d\nSuch a definition also extends to (\u03b3, \u03b5)-approximate PCR (see Def. 3.2).\nIt was first noticed by Frostig et al. (Frostig et al., 2016) that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold \u03bb. More specifically, they reduced (\u03b3, \u03b5)approximate PCP and PCR to\nO ( \u03b3\u22122 log(1/\u03b5) ) black-box calls of\nany ridge regression subroutine\nwhere each call computes (A>A + \u03bbI)\u22121u for some vector u.1 Our main focus of this paper is to quadratically improve this performance and reduce PCP and PCR to\n1Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG (Johnson & Zhang, 2013), one can usually solve ridge regression to an 10\u22128 accuracy with at most 40 passes of the data.\nO ( \u03b3\u22121 log(1/\u03b3\u03b5) ) black-box calls of\nany ridge regression subroutine\nwhere each call again computes (A>A + \u03bbI)\u22121u. Remark 1.1. Frostig et al. only showed their algorithm satisfies the properties 1 and 2 of (\u03b3, \u03b5)-approximation (but not the property 3), and thus their proof was only for matrix A with no singular value in the range [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)]. This is known as the eigengap assumption, which is rarely satisfied in practice (Musco & Musco, 2015). In this paper, we prove our result both with and without such eigengap assumption. Since our techniques also imply the algorithm of Frostig et al. satisfies property 3, throughout the paper, we say Frostig et al. solve (\u03b3, \u03b5)-approximate PCP and PCR."}, {"heading": "1.2 From PCP to Polynomial Approximation", "text": "The main technique of Frostig et al. is to construct a polynomial to approximate the sign function sgn(x) : [\u22121, 1]\u2192 {\u00b11}:\nsgn(x) := { +1, x \u2265 0; \u22121, x < 0.\nIn particular, given any polynomial g(x) satisfying\u2223\u2223g(x)\u2212 sgn(x)\u2223\u2223 \u2264 \u03b5 \u2200x \u2208 [\u22121,\u2212\u03b3] \u222a [\u03b3, 1] , (1.1)\u2223\u2223g(x)\u2223\u2223 \u2264 1 \u2200x \u2208 [\u2212\u03b3, \u03b3] , (1.2) the problem of (\u03b3, \u03b5)-approximate PCP can be reduced to computing the matrix polynomial g(S) for S := (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI) (cf. Fact 7.1). In other words, \u2022 to project any vector \u03c7 \u2208 Rd to top principal compo-\nnents, we can compute g(S)\u03c7 instead; and\n\u2022 to compute g(S)\u03c7, we can reduce it to ridge regression for each evaluation of Su for some vector u. Remark 1.2. Since the transformation from A>A to S is not linear, the final approximation to the PCP is a rational function (as opposed to a polynomial) over A>A. We restrict to polynomial choices of g(\u00b7) because in this way, the final rational function has all the denominators being A>A + \u03bbI, thus reduces to ridge regressions. Remark 1.3. The transformation from A>A to S ensures that all the eigenvalues of A>A in the range (1 \u00b1 \u03b3)\u03bb roughly map to the eigenvalues of S in the range [\u2212\u03b3, \u03b3].\nMain Challenges. There are two main challenges regarding the design of polynomial g(x).\n\u2022 EFFICIENCY. We wish to minimize the degree n = deg(g(x)) because the computation of g(S)\u03c7 usually requires n calls of ridge regression.\n\u2022 STABILITY. We wish g(x) to be stable; that is, g(S)\u03c7 must be given by a recursive formula where if we make \u03b5\u2032 error in each recursion (due to error incurred from ridge regression), the final error of g(S)\u03c7 must be at most \u03b5\u2032 \u00d7 poly(d).\nRemark 1.4. Efficient routines such as SVRG (Johnson & Zhang, 2013) solve ridge regression and thus compute Su for any u \u2208 Rd, with running times only logarithmically in 1/\u03b5\u2032. Therefore, by setting \u03b5\u2032 = \u03b5/poly(d), one can blow up the running time by a small factor O(log(d)) in order to obtain an \u03b5-accurate solution for g(S)\u03c7.\nThe polynomial g(x) constructed by Frostig et al. comes from truncated Taylor expansion. It has degree O ( \u03b3\u22122 log(1/\u03b5) ) and is stable. This \u03b3\u22122 dependency limits the practical performance of their proposed PCP and PCR algorithms, especially in a high accuracy regime. At the same time,\n\u2022 the optimal degree for a polynomial to satisfy even only (1.1) is \u0398 ( \u03b3\u22121 log(1/\u03b5) ) (Eremenko & Yuditskii,\n2007; 2011).\nFrostig et al. were unable to find a stable polynomial matching this optimal degree and left it as open question.2"}, {"heading": "1.3 Our Results and Main Ideas", "text": "We provide an efficient and stable polynomial approximation to the matrix sign function that has a near-optimal degree O(\u03b3\u22121 log(1/\u03b3\u03b5)). At a high level, we construct a polynomial q(x) that approximately equals ( 1+\u03ba\u2212x\n2 )\u22121/2 for some \u03ba = \u0398(\u03b32); then we set g(x) := x\u00b7q(1+\u03ba\u22122x2) which approximates sgn(x).\nTo construct q(x), we first note that (\n1+\u03ba\u2212x 2\n)\u22121/2 has\nno singular point on [\u22121, 1] so we can apply Chebyshev approximation theory to obtain some q(x) of degree O(\u03b3\u22121 log(1/\u03b3\u03b5)) satisfying\u2223\u2223\u2223q(x)\u2212 (1 + \u03ba\u2212 x\n2 )\u22121/2\u2223\u2223\u2223 \u2264 \u03b5 for every x \u2208 [\u22121, 1] . This can be shown to imply\n\u2223\u2223g(x)\u2212 sgn(x)\u2223\u2223 \u2264 \u03b5 for every x \u2208 [\u22121,\u2212\u03b3] \u222a [\u03b3, 1], so (1.1) is satisfied. In order to prove (1.2) , we prove a separate lemma:3\nq(x) \u2264 (1 + \u03ba\u2212 x\n2\n)\u22121/2 for every x \u2208 [1, 1 + \u03ba] .\nNote that this does not follow from standard Chebyshev theory because Chebyshev approximation guarantees are only with respect to x \u2208 [\u22121, 1]. This proves the \u201cEFFICIENCY\u201d part of the main challenges discussed earlier. As for the \u201cSTABILITY\u201d part, we prove a general theorem regarding any weighted sum of Chebyshev polynomials applied to matrices. We provide a backward recurrence algorithm and show that it is stable under noisy\n2Using degree reduction, Frostig et al. found an explicit polynomial g(x) of degree O ( \u03b3\u22121 log(1/\u03b3\u03b5) ) satisfying (1.1). However, that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coefficients in front of each monomial. Furthermore, it is not clear if their polynomial satisfies the (1.2).\n3We proved a general lemma which holds for any function whose all orders of derivatives are non-negative at x = 0.\ncomputations. This may be of independent interest.\nFor interested readers, we compare our polynomial q(x) with that of Frostig et al. in Figure 1."}, {"heading": "1.4 Related Work", "text": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix AP\u03bb where P\u03bb is the PCP projection matrix (Chan & Hansen, 1990; Boutsidis & Magdon-Ismail, 2014). However, they cost a running time that linearly scales with the number of principal components above \u03bb.\nA significant number of papers have focused on the lowrank case of PCA (Musco & Musco, 2015; Allen-Zhu & Li, 2016a; 2017) and its online variant (Allen-Zhu & Li, 2016b). Unfortunately, all of these methods require a running time that scales at least linearly with respect to the number of top principal components.\nMore related to this paper is work on matrix sign function, which plays an important role in control theory and quantum chromodynamics. Several results have addressed Krylov methods for applying the sign function in the socalled Krylov subspace, without explicitly constructing any approximate polynomial (van den Eshof et al., 2002; Schilders et al., 2008). However, Krylov methods are not (\u03b3, \u03b5)-approximate PCP solvers, and there is no supporting stability theory behind them.4 Other iterative methods have also been proposed, see Section 5 of textbook (Higham, 2008). For instance, Schur\u2019s method is a slow one and also requires the matrix to be explicitly given. The Newton\u2019s iteration and its numerous variants (e.g. (Nakatsukasa & Freund, 2016)) provide rational approximations to the matrix sign function as opposed to polynomial approximations. Our result and Frostig et al. (Frostig et al., 2016) differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.\nUsing matrix Chebyshev polynomials to approximate matrix functions is not new. Perhaps the most celebrated example is to approximate S\u22121 using polynomials on S, used in the analysis of conjugate gradient (Shewchuk, 1994). Independent from this paper,5 Han et al. (Han et al., 2016) used Chebyshev polynomials to approximate the trace of the matrix sign function, i.e., Tr(sgn(S)), which is similar but a different problem.6 Also, they did not study the\n4We anyways have included Krylov method in our empirical evaluation section and shall discuss its performance there, see the full version of this paper.\n5Their paper appeared online two months before us, and we became aware of their work in March 2017.\n6In particular, their degree of the Chebyshev polynomial is O ( \u03b3\u22121(log2(1/\u03b3)+ log(1/\u03b3) log(1/\u03b5)) ) in the language of this\npaper; in contrast, we have degree O ( \u03b3\u22121 log(1/\u03b3\u03b5) ) .\ncase when the matrix-vector multiplication oracle is only approximate (like we do in this paper), or the case when S has eigenvalues in the range [\u2212\u03b3, \u03b3].\nRoadmap. In Section 2, we provide notions for this paper and basics for Chebyshev polynomials. In Section 3, we formally define approximate PCP and PCR, and reduce PCR to PCP. In Section 4, we show a general lemma for Chebyshev approximations. In Section 5, we design our polynomial approximation to sgn(x). In Section 6, we show how to stably compute Chebyshev polynomials. In Section 7, we state our main theorems regarding PCP and PCR. In Section 8, we provide empirical evaluations."}, {"heading": "2 Preliminaries", "text": "We denote by 1[e] \u2208 {0, 1} the indicator function for event e, by \u2016v\u2016 or \u2016v\u20162 the Euclidean norm of a vector v, by M\u2020 the Moore-Penrose pseudo-inverse of a symmetric matrix M, and by \u2016M\u20162 its spectral norm. We sometimes use ~v to emphasize that v is a vector.\nGiven a symmetric d \u00d7 d matrix M and any f : R \u2192 R, f(M) is the matrix function applied to M, which is equal to Udiag{f(D1), . . . , f(Dd)}U> if M = Udiag{D1, . . . , Dd}U> is its eigendecomposition. Throughout the paper, matrix A is of dimension d\u2032 \u00d7 d. We denote by \u03c3max(A) the largest singular value of A. Following the tradition of (Frostig et al., 2016) and keeping the notations light, we assume without loss of generality that \u03c3max(A) \u2264 1. We are interested in PCP and PCR problems with an eigenvalue threshold \u03bb \u2208 (0, 1). Throughout the paper, we denote by \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd \u2265 0 the eigenvalues of A>A, and by \u03bd1, . . . , \u03bdd \u2208 Rd the eigenvectors of A>A corresponding to \u03bb1, . . . , \u03bbd. We denote by P\u03bb the projection matrix P\u03bb := (\u03bd1, . . . , \u03bdj)(\u03bd1, . . . , \u03bdj)\n> where j is the largest index satisfying \u03bbj \u2265 \u03bb. In other words, P\u03bb is a projection matrix to the eigenvectors of A>A with eigenvalues \u2265 \u03bb. Definition 2.1. The principal component projection (PCP) of \u03c7 \u2208 Rd at threshold \u03bb is \u03be\u2217 = P\u03bb\u03c7. Definition 2.2. The principal component regression (PCR) of regressand b \u2208 Rd\u2032 at threshold \u03bb is x\u2217 = arg miny\u2208Rd \u2016AP\u03bby \u2212 b\u20162 or equivalently x\u2217 = (A>A)\u2020P\u03bb(A >b) ."}, {"heading": "2.1 Ridge Regression", "text": "Definition 2.3. A black-box algorithm ApxRidge(A, \u03bb, u) is an \u03b5-approximate ridge regression solver, if for every u \u2208 Rd, it satisfies \u2016ApxRidge(A, \u03bb, u)\u2212(A>A+\u03bbI)\u22121u\u2016 \u2264 \u03b5\u2016u\u2016.\nRidge regression is equivalent to solving well-conditioned linear systems, or minimizing strongly convex and smooth objectives f(y) := 12y >(A>A + \u03bbI)y \u2212 u>y.\nRemark 2.4. There is huge literature on efficient algorithms solving ridge regression. Most notably,\n(1) Conjugate gradient (Shewchuk, 1994) or accelerated gradient descent (Nesterov, 2004) gives fastest fullgradient methods;\n(2) SVRG (Johnson & Zhang, 2013) and its acceleration Katyusha (Allen-Zhu, 2017) give the fastest stochasticgradient method; and\n(3) NUACDM (Allen-Zhu et al., 2016) gives the fastest coordinate-descent method.\nThe running time of (1) is O(nnz(A)\u03bb\u22121/2 log(1/\u03b5)) where nnz(A) is time to multiply A to any vector. The running times of (2) and (3) depend on structural properties of A and are always faster than (1).\nBecause the best complexity of ridge regression depends on the structural properties of A, following Frostig et al., we only compute our running time in terms of the \u201cnumber of black-box calls\u201d to a ridge regression solver."}, {"heading": "2.2 Chebyshev Polynomials", "text": "Definition 2.5. Chebyshev polynomials of 1st and 2nd kind are {Tn(x)}n\u22650 and {Un(x)}n\u22650 where T0(x) := 1, T1(x) := x, Tn+1(x) := 2x \u00b7 Tn(x)\u2212 Tn\u22121(x) U0(x) := 1, U1(x) := 2x, Un+1(x) := 2x \u00b7 Un(x)\u2212 Un\u22121(x)\nFact 2.6 ((Trefethen, 2013)). It satisfies ddxTn(x) = nUn\u22121(x) for n \u2265 1 and\n\u2200n \u2265 0: Tn(x) =  cos(n arccos(x)), if |x| \u2264 1;cosh(n arccosh(x)), if x \u2265 1;(\u22121)n cosh(n arccosh(\u2212x)), if x \u2264 \u22121. In particular, when x \u2265 1,\nTn(x) = 1\n2\n[( x\u2212 \u221a x2 \u2212 1 )n + ( x+ \u221a x2 \u2212 1 )n] Un(x) = 1\n2 \u221a x2 \u2212 1\n[( x+ \u221a x2 \u2212 1 )n+1 \u2212 (x\u2212\u221ax2 \u2212 1)n+1] Definition 2.7. For function f(x) whose domain contains [\u22121, 1], its degree-n Chebyshev truncated series and degree-n Chebyshev interpolation are respectively\npn(x) := n\u2211 k=0 akTk(x) and qn(x) := n\u2211 k=0 ckTk(x) ,\nwhere ak := 2\u2212 1[k = 0]\n\u03c0\n\u222b 1 \u22121 f(x)Tk(x)\u221a 1\u2212 x2 dx\nck := 2\u2212 1[k = 0]\nn+ 1\nn\u2211 j=0 f ( xj ) Tk ( xj ) .\nAbove, xj := cos ( (j+0.5)\u03c0\nn+1\n) \u2208 [\u22121, 1] is the j-th Cheby-\nshev point of order n.\nThe following lemma is known as the aliasing formula for Chebyshev coefficients:\nLemma 2.8 (cf. Theorem 4.2 of (Trefethen, 2013)). Let f be Lipschitz continuous on [\u22121, 1] and {ak}, {ck} be defined in Def. 2.7, then\nc0 = a0+a2n+a4n+... , cn = an+a3n+a5n+... , and\nfor every k \u2208 {1, 2, . . . , n\u2212 1}, ck = ak+(ak+2n+ak+4n+...)+(a\u2212k+2n+a\u2212k+4n+...)\nDefinition 2.9. For every \u03c1 > 0, let E\u03c1 be the ellipse E of foci \u00b11 with major radius 1 + \u03c1. (This is also known as Bernstein ellipse with parameter 1 + \u03c1+ \u221a 2\u03c1+ \u03c12.)\nThe following lemma is the main theory regarding Chebyshev approximation: Lemma 2.10 (cf. Theorem 8.1 and 8.2 of (Trefethen, 2013)). Suppose f(z) is analytic on E\u03c1 and |f(z)| \u2264 M on E\u03c1. Let pn(x) and qn(x) be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x) on [\u22121, 1]. Then, \u2022 max x\u2208[\u22121,1] |f(x)\u2212pn(x)| \u2264 2M \u03c1+ \u221a 2\u03c1+\u03c12 ( 1+\u03c1+ \u221a 2\u03c1+ \u03c12\n)\u2212n; \u2022 max x\u2208[\u22121,1] |f(x)\u2212qn(x)| \u2264 4M \u03c1+ \u221a 2\u03c1+\u03c12 ( 1+\u03c1+ \u221a 2\u03c1+ \u03c12\n)\u2212n. \u2022 |a0| \u2264M and |ak| \u2264 2M ( 1+\u03c1+ \u221a 2\u03c1+ \u03c12\n)\u2212k for k \u2265 1."}, {"heading": "3 Approximate PCP and PCR", "text": "We formalize our notions of approximation for PCP and PCR, and provide a reduction from PCR to PCP."}, {"heading": "3.1 Our Notions of Approximation", "text": "Recall that Frostig et al. (Frostig et al., 2016) work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range\n[ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)]. Their approximation guarantees are very straightforward:\n\u2022 an output \u03be is \u03b5-approximate for PCP on vector \u03c7 if \u2016\u03be \u2212 \u03be\u2217\u2016 \u2264 \u03b5\u2016\u03c7\u2016;\n\u2022 an output x is \u03b5-approximate for PCR with regressand b if \u2016x\u2212 x\u2217\u2016 \u2264 \u03b5\u2016b\u2016.\nUnfortunately, these notions are too strong and impossible to satisfy for matrices that do not have a large eigengap around the projection threshold \u03bb.\nIn this paper we propose the following more general (but yet very meaningful) approximation notions.\nDefinition 3.1. An algorithm B(\u03c7) is (\u03b3, \u03b5)-approximate PCP for threshold \u03bb, if for every \u03c7 \u2208 Rd\n1. \u2225\u2225P(1+\u03b3)\u03bb(B(\u03c7)\u2212 \u03c7)\u2225\u2225 \u2264 \u03b5\u2016\u03c7\u2016.\n2. \u2225\u2225(I\u2212P(1\u2212\u03b3)\u03bb)B(\u03c7)\u2225\u2225 \u2264 \u03b5\u2016\u03c7\u2016.\n3. \u2200i such that \u03bbi \u2208 [ (1 \u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb ] , it satisfies\n|\u3008\u03bdi,B(\u03c7)\u2212 \u03c7\u3009| \u2264 |\u3008\u03bdi, \u03c7\u3009|+ \u03b5\u2016\u03c7\u2016.\nIntuitively, the first property above states that, if projected to the eigenspace with eigenvalues above (1 + \u03b3)\u03bb, then B(\u03c7) and \u03c7 are almost identical; the second property states that, if projected to the eigenspace with eigenvalues below (1 \u2212 \u03b3)\u03bb, then B(\u03c7) is almost zero; and the third property states that, for each eigenvector \u03bdi with eigenvalue in the range [(1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb], the projection \u3008\u03bdi,B(\u03c7)\u3009 must be between 0 and \u3008\u03bdi, \u03c7\u3009 (but up to an error \u03b5\u2016\u03c7\u2016). Naturally, P\u03bb(\u03c7) itself is a (0, 0)-approximate PCP.\nWe propose the following notion for approximate PCR:\nDefinition 3.2. An algorithm C(b) is (\u03b3, \u03b5)-approximate PCR for threshold \u03bb, if for every b \u2208 Rd\u2032\n1. \u2225\u2225(I\u2212P(1\u2212\u03b3)\u03bb)C(b)\u2225\u2225 \u2264 \u03b5\u2016b\u2016.\n2. \u2016AC(b)\u2212 b\u2016 \u2264 \u2016Ax\u2217 \u2212 b\u2016+ \u03b5\u2016b\u2016. where x\u2217 = (A>A)\u2020P(1+\u03b3)\u03bbA>b is the exact PCR solution for threshold (1 + \u03b3)\u03bb.\nThe first notion states that the output x = C(b) has nearly no correlation with eigenvectors below threshold (1\u2212 \u03b3)\u03bb; and the second states that the regression error should be nearly optimal with respect to the exact PCR solution but at a different threshold (1 + \u03b3)\u03bb.\nRelationship to Frostig et al. Under eigengap assumption, our notions are equivalent to Frostig et al.: Fact 3.3. If A has no singular value in [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)], then\n\u2022 Def. 3.1 is equivalent to \u2016B(\u03c7)\u2212P\u03bb(\u03c7)\u2016 \u2264 O(\u03b5)\u2016\u03c7\u2016.\n\u2022 Def. 3.2 implies \u2016C(\u03c7) \u2212 x\u2217\u2016 \u2264 O(\u03b5/\u03bb)\u2016b\u2016 and \u2016C(\u03c7)\u2212 x\u2217\u2016 \u2264 O(\u03b5)\u2016b\u2016 implies Def. 3.2.\nAbove, x\u2217 = (A>A)\u2020P\u03bbA>b is the exact PCR solution."}, {"heading": "3.2 Reductions from PCR to PCP", "text": "If the PCP solution \u03be = P\u03bb(A>b) is computed exactly, then by definition one can compute (A>A)\u2020\u03be which gives a solution to PCR by solving a linear system. However, as pointed by Frostig et al. (Frostig et al., 2016), this computation is problematic if \u03be is only approximate. The following approach has been proposed to improve its accuracy by Frostig et al.\n\u2022 \u201ccompute p((A>A + \u03bbI)\u22121)\u03be where p(x) is a polynomial that approximates function x1\u2212\u03bbx .\u201d This is a good approximation to (A>A)\u2020\u03be because the composition of functions x1\u2212\u03bbx and 1 1+\u03bbx is exactly x\n\u22121. Frostig et al. picked p(x) = pm(x) = \u2211m t=1 \u03bb\nt\u22121xt which is a truncated Taylor series, and used the following procedure to compute sm \u2248 pm((A>A + \u03bbI)\u22121)\u03be:\ns0 = B(A>b), s1 = ApxRidge(A, \u03bb, s0), \u2200k \u2265 1: sk+1 = s1 + \u03bb \u00b7 ApxRidge(A, \u03bb, sk) . (3.1)\nAbove, B is an approximate PCP solver and ApxRidge is an approximate ridge regression solver. Under eigengap assumption, Frostig et al. (Frostig et al., 2016) showed\nLemma 3.4 (PCR-to-PCP). For fixed \u03bb, \u03b3, \u03b5 \u2208 (0, 1), let A be a matrix whose singular values lie in[ 0, \u221a (1\u2212 \u03b3)\u03bb ] \u222a [\u221a (1\u2212 \u03b3)\u03bb, 1 ] . Let ApxRidge be any O( \u03b5m2 )-approximate ridge regression solver, and let B be any (\u03b3,O( \u03b5\u03bbm2 ))-approximate PCP solver\n7. Then, procedure (3.1) satisfies\n\u2016sm\u2212(A>A)\u2020P\u03bbA>b\u2016 \u2264 \u03b5\u2016b\u2016 if m = \u0398(log(1/\u03b5\u03b3)) .\nUnfortunately, the above lemma does not hold without eigengap assumption. In this paper, we fix this issue by proving the following analogous lemma:\nLemma 3.5 (gap free PCR-to-PCP). For fixed \u03bb, \u03b5 \u2208 (0, 1) and \u03b3 \u2208 (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any O( \u03b5m2 )-approximate ridge regression solver, and B be any (\u03b3,O( \u03b5\u03bbm2 ))-approximate PCP solver. Then, procedure (3.1) satisfies,{\n\u2016(I\u2212P(1\u2212\u03b3)\u03bb)sm\u2016 \u2264 \u03b5\u2016b\u2016 , and\n\u2016Asm \u2212 b\u2016 \u2264 \u2016A(A>A)\u2020P(1+\u03b3)\u03bbA>b\u2212 b\u2016+ \u03b5\u2016b\u2016 } if m = \u0398(log(1/\u03b5\u03b3)\nNote that the conclusion of this lemma exactly corresponds to the two properties in our Def. 3.2. The proof of Lemma 3.5 is not hard, but requires a very careful case analysis by decomposing vectors b and each sk into three components, each corresponding to eigenvalues of A>A in the range [0, (1\u2212\u03b3)\u03bb], [(1\u2212\u03b3)\u03bb, (1+\u03b3)\u03bb] and [(1+\u03b3)\u03bb, 1].\n7Recall from Fact 3.3 that this requirement is equivalent to saying that \u2016B(\u03c7)\u2212P\u03bb\u03c7\u2016 \u2264 O( \u03b5 \u221a \u03bb\nm2 )\u2016\u03c7\u2016.\nWe defer the details to the full version."}, {"heading": "4 Chebyshev Approximation Outside [\u22121, 1]", "text": "Classical Chebyshev approximation theory (such as Lemma 2.10) only talks about the behaviors of pn(x) or gn(x) on interval [\u22121, 1]. However, for the purpose of this paper, we must also bound its value for x > 1. We prove the following general lemma in the full version, and believe it could be of independent interest: (we denote by f (k)(x) the k-th derivative of f at x)\nLemma 4.1. Suppose f(z) is analytic on E\u03c1 and for every k \u2265 0, f (k)(0) \u2265 0. Then, for every n \u2208 N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have\n\u2200y \u2208 [0, \u03c1] : 0 \u2264 pn(1 + y), qn(1 + y) \u2264 f(1 + y) .\n5 Our Polynomial Approximation of sgn(x) For fixed \u03ba \u2208 (0, 1], we consider the degree-n Chebyshev interpolation qn(x) = \u2211n k=0 ckTk(x) of the function\nf(x) = (\n1+\u03ba\u2212x 2\n)\u22121/2 on [\u22121, 1]. Def. 2.7 tells us that\nck := 2\u2212 1[k = 0]\nn+ 1\nn\u2211 j=0 (\u221a 2 cos (k(j + 0.5)\u03c0 n+ 1 )) \u00d7 ( 1 + \u03ba\u2212 cos ( (j + 0.5)\u03c0\nn+ 1\n))\u22121/2 .\nOur final polynomial to approximate sgn(x) is therefore\ngn(x) = x\u00b7qn(1+\u03ba\u22122x2) and deg(gn(x)) = 2n+1 .\nWe prove the following theorem in this section:\nTheorem 5.1. For every \u03b1 \u2208 (0, 1], \u03b5 \u2208 (0, 1/2), choosing \u03ba = 2\u03b12, our function gn(x) := x \u00b7 qn(1 + \u03ba\u2212 2x2) satisfies that as long as n \u2265 1\u221a\n2\u03b1 log 3\u03b5\u03b12 , then (see also\nFigure 1)\n\u2022 |gn(x)\u2212 sgn(x)| \u2264 \u03b5 for every x \u2208 [\u22121, \u03b1] \u222a [\u03b1, 1]. \u2022 gn(x) \u2208 [0, 1] for every x \u2208 [0, \u03b1] and gn(x) \u2208\n[\u22121, 0] for every x \u2208 [\u2212\u03b1, 0].\nNote that our degree n = O ( \u03b1\u22121 log(1/\u03b1\u03b5) ) is nearoptimal, because the minimum degree for a polynomial to satisfy even only the first item is \u0398 ( \u03b1\u22121 log(1/\u03b5) ) (Eremenko & Yuditskii, 2007; 2011). However, the results of (Eremenko & Yuditskii, 2007; 2011) are not constructive, and thus may not lead to stable matrix polynomials.\nWe prove Theorem 5.1 by first establishing two simple lemmas. The following lemma is a consequence of Lemma 2.10:\nLemma 5.2. For every \u03b5 \u2208 (0, 1/2) and \u03ba \u2208 (0, 1], if n \u2265 1\u221a\n\u03ba ( log 1\u03ba + log 4 \u03b5 ) then\n\u2200x \u2208 [\u22121, 1], |f(x)\u2212 qn(x)| \u2264 \u03b5 .\nProof of Lemma 5.2. Denoting by f(z) = (\n1+\u03ba\u2212z 2\n)\u22120.5 ,\nwe know that f(z) is analytic on ellipse E\u03c1 with \u03c1 = \u03ba/2, and it satisfies |f(z)| \u2264 \u221a 2/\u03ba in E\u03c1. Applying Lemma 2.10, we know that when n \u2265 1\u221a \u03ba ( log 1\u03ba + log 4 \u03b5\n) it satisfies |f(x)\u2212 qn(x)| \u2264 \u03b5.\nThe next lemma an immediate consequence of our Lemma 4.1 with f(z) = ( 1+\u03ba\u2212z\n2\n)\u22120.5 :\nLemma 5.3. For every \u03b5 \u2208 (0, 1/2), \u03ba \u2208 (0, 1], n \u2208 N, and x \u2208 [0, \u03ba], we have\n0 \u2264 qn(1 + x) \u2264 (\u03ba\u2212 x\n2\n)\u22121/2 .\nProof of Theorem 5.1. We are now ready to prove Theorem 5.1.\n\u2022 When x \u2208 [\u22121, \u03b1] \u222a [\u03b1, 1], it satisfies 1 + \u03ba \u2212 2x2 \u2208 [\u22121, 1]. Therefore, applying Lemma 5.2 we have whenever n \u2265 1\u221a\n\u03ba log 6\u03b5\u03ba = 1\u221a 2\u03b1 log 3\u03b5\u03b12 it satisfies |f(1 + \u03ba\u22122x2)\u2212qn(1+\u03ba\u22122x2)|\u221e \u2264 \u03b5. This further implies\n|gn(x)\u2212sgn(x)| = |xqn(1+\u03ba\u22122x2)\u2212xf(1+\u03ba\u22122x2)| \u2264 |x||f(1 + \u03ba\u2212 2x2)\u2212 qn(1 + \u03ba\u2212 2x2)| \u2264 \u03b5 .\n\u2022 When |x| \u2264 \u03b1, it satisfies 1 + \u03ba \u2212 2x2 \u2208 [1, 1 + \u03ba]. Applying Lemma 5.3 we have for all x \u2208 [0, \u03b1], 0 \u2264 gn(x) = x \u00b7 qn(1 + \u03ba\u2212 2x2) \u2264 x \u00b7 (x2)\u22121/2 = 1 and similarly for x \u2208 [\u2212\u03b1, 0] it satisfies 0 \u2265 gn(x) \u2265 \u22121.\nA Bound on Chebyshev Coefficients. We also give an upper bound to the coefficients of polynomial qn(x). Its proof can be found in the full version, and this upper bound shall be used in our final stability analysis. Lemma 5.4 (coefficients of qn). Let qn(x) =\u2211n k=0 ckTk(x) be the degree-n Chebyshev interpolation\nof f(x) = (\n1+\u03ba\u2212x 2\n)\u22121/2 on [\u22121, 1]. Then, for all i \u2208\n{0, 1, . . . , n}, |ci| \u2264 e \u221a 32(i+ 1)\n\u03ba\n( 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 )\u2212i"}, {"heading": "6 Stable Computation of Matrix Chebyshev Polynomials", "text": "In this section we show that any polynomial that is a weighted summation of Chebyshev polynomials with bounded coefficients, can be stably computed when applied to matrices with approximate computations. We achieve so by first generalizing Clenshaw\u2019s backward method to matrix case in Section 6.1 in order to compute a matrix variant of Chebyshev sum, and then analyze its stability in Section 6.2 with the help from Elloit\u2019s forward-backward transformation (Elliott, 1968).\nRemark 6.1. We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars (Gil et al., 2007), it is not immediately clear why it holds also for matrices. Recall that Chebyshev polynomials satisfy Tn+1(x) = 2xTn(x) \u2212 Tn\u22121(x). In the matrix case, we have Tn+1(M)\u03c7 = 2MTn(M)\u03c7 \u2212 Tn\u22121(M)\u03c7 where \u03c7 \u2208 Rd is a vector. If we analyzed this formula coordinate by coordinate, error could blow up by a factor d per iteration.\nIn addition, we need to ensure that the stability theorem holds for matrices M with eigenvalues that can exceed 1. This is not standard because Chebyshev polynomials are typically analyzed only on domain [\u22121, 1]."}, {"heading": "6.1 Clenshaw\u2019s Method in Matrix Form", "text": "Consider any computation of the form\n~sN := N\u2211 k=0 Tk(M)~ck \u2208 Rd (6.1)\nwhere M \u2208 Rd\u00d7d is symmetric and each ~ck is in Rd. (Note that for PCP and PCR purposes, we it suffices to consider ~ck = c \u2032 k\u03c7 where c \u2032 k \u2208 R is a scalar and \u03c7 \u2208 Rd is a fixed vector for all k. However, we need to work on this more general form for our stability analysis.)\nVector sN can be computed using the following procedure:\nLemma 6.2 (backward recurrence). ~sN = ~b0\u2212M~b1 where ~bN+1 := ~0, ~bN := ~cN , and\n\u2200r \u2208 {N \u2212 1, . . . , 0} : ~br := 2M~br+1 \u2212~br+2 + ~cr \u2208 Rd ."}, {"heading": "6.2 Inexact Clenshaw\u2019s Method in Matrix Form", "text": "We show that, if implemented using the backward recurrence formula, the Chebyshev sum of (6.1) can be stably computed. We define the following model to capture the error with respect to matrix-vector multiplications.\nDefinition 6.3 (inexact backward recurrence). Let M be an approximate algorithm that satisfies \u2016M(u)\u2212Mu\u20162 \u2264 \u03b5\u2016u\u20162 for every u \u2208 Rd. Then, define inexact backward recurrence to be\nb\u0302N+1 := 0, b\u0302N := ~cN , and \u2200r \u2208 {N \u2212 1, . . . , 0} : b\u0302r := 2M ( b\u0302r+1 ) \u2212 b\u0302r+2 + ~cr \u2208 Rd ,\nand define the output as s\u0302N := b\u03020 \u2212M(\u0302b1).\nThe following theorem gives an error analysis to our inexact backward recurrence. We prove it in full version, and the main idea of our proof is to convert each error vector of a recursion of the backward procedure into an error vector corresponding to some original ~ck.\nTheorem 6.4 (stable Chebyshev sum). For every N \u2208 N\u2217, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU \u2265 1, CT \u2265 1, \u03c1 \u2265 1, Cc \u2265\n0 satisfying \u2200k \u2208 {0, 1, . . . , N} :{ \u03c1k\u2016~ck\u2016 \u2264 Cc \u2227 \u2200x \u2208 [a, b] : |Tk(x)|\u2264CT \u03c1 k\n|Uk(x)|\u2264CU\u03c1k\n} .\nThen, if the inexact backward recurrence in Def. 6.3 is applied with \u03b5 \u2264 14NCU , we have\n\u2016s\u0302N \u2212 ~sN\u2016 \u2264 \u03b5 \u00b7 2(1 + 2NCT )NCUCc ."}, {"heading": "7 Algorithms and Main Theorems for PCP and PCR", "text": "We are now ready to state our main theorems for PCP and PCR. We first note a simple fact:\nFact 7.1. (P\u03bb)\u03c7 = I+sgn(S)2 where S := 2(A >A + \u03bbI)\u22121A>A\u2212 I = (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI).\nIn other words, for every vector \u03c7 \u2208 Rd, the exact PCP solution P\u03bb(\u03c7) is the same as computing (P\u03bb)\u03c7 = I+sgn(S)\n2 \u03c7. Thus, we can use our polynomial gn(x) introduced in Section 5 and compute gn(S)\u03c7 \u2248 sgn(S)\u03c7. Finally, in order to compute gn(S), we need to multiply S to deg(gn) vectors; whenever we do so, we call perform ridge regression once.\nSince the high-level structure of our PCP algorithm is very clear, due to space limitation, we present the pseudocodes of our PCP and PCR algorithms in the full version."}, {"heading": "7.1 Our Main Theorems", "text": "We first state our main theorem under the eigengap assumption, in order to provide a direct comparison to that of Frostig et al. (Frostig et al., 2016).\nTheorem 7.2 (eigengap assumption). Given A \u2208 Rd\u2032\u00d7d and \u03bb, \u03b3 \u2208 (0, 1), assume that the singular values of A are in the range [0, \u221a (1\u2212 \u03b3)\u03bb]\u222a [ \u221a (1 + \u03b3)\u03bb, 1]. Given \u03c7 \u2208 Rd and b \u2208 Rd\u2032 , denote by \u03be\u2217 = P\u03bb\u03c7 and x\u2217 = (A>A)\u22121P\u03bbA>b\nthe exact PCP and PCR solutions, and by ApxRidge any \u03b5\u2032-approximate ridge regression solver. Then,\n\u2022 QuickPCP outputs \u03be satisfying \u2016\u03be\u2217 \u2212 \u03be\u2016 \u2264 \u03b5\u2016\u03c7\u2016 with O ( \u03b3\u22121 log 1\u03b3\u03b5 ) oracle calls to ApxRidge as long as\nlog(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) .\n\u2022 QuickPCR outputs x satisfying \u2016x\u2212x\u2217\u2016 \u2264 \u03b5\u2016b\u2016 with O ( \u03b3\u22121 log 1\u03b3\u03bb\u03b5 ) oracle calls to ApxRidge, as long as\nlog(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03bb\u03b5 ) .\nIn contrast, the number of ridge-regression oracle calls was \u0398(\u03b3\u22122 log 1\u03b3\u03b5 ) for PCP and \u0398(\u03b3\n\u22122 log 1\u03b3\u03bb\u03b5 ) for PCR in (Frostig et al., 2016). We include the proof of Theorem 7.2 in the full version.\nWe state our theorem without the eigengap assumption.\nTheorem 7.3 (gap-free). Given A \u2208 Rd\u2032\u00d7d, \u03bb \u2208 (0, 1), and \u03b3 \u2208 (0, 2/3], assume that \u2016A\u20162 \u2264 1. Given \u03c7 \u2208 Rd and b \u2208 Rd\u2032 , and suppose ApxRidge is an \u03b5\u2032approximate ridge regression solver, then\n\u2022 QuickPCP outputs \u03be that is (\u03b3, \u03b5)-approximate PCP withO ( \u03b3\u22121 log 1\u03b3\u03b5 ) oracle calls to ApxRidge as long\nas log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) .\n\u2022 QuickPCR outputs x that is (\u03b3, \u03b5)-approximate PCR with O ( \u03b3\u22121 log 1\u03b3\u03bb\u03b5 ) oracle calls to ApxRidge as\nlong as elog(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03bb\u03b5 ) .\nWe make a final remark here regarding the practical usage of QuickPCP and QuickPCR. Remark 7.4. Since our theory is for (\u03b3, \u03b5)-approximations that have two parameters, the user in principle has to feed in both \u03b3 and n where n is the degree of the polynomial approximation to the sign function. In practice, however, it is usually sufficient to obtain (\u03b5, \u03b5)-approximate PCP and PCR. Therefore, our pseudocodes allow users to set \u03b3 = 0 and thus ignore this parameter \u03b3; in such a case, we shall use \u03b3 = log(n)/n which is equivalent to setting \u03b3 = \u0398(\u03b5) because n = \u0398(\u03b3\u22121 log(1/\u03b3\u03b5))."}, {"heading": "8 Experiments", "text": "We provide empirical evaluations in the full version of this paper."}, {"heading": "9 Conclusion", "text": "We summarize our contributions.\n\u2022 We put forward approximate notions for PCP and PCR that do not rely on any eigengap assumption. Our notions reduce to standard ones under the eigengap assumption. \u2022 We design near-optimal polynomial approximation g(x) to sgn(x) satisfying (1.1) and (1.2). \u2022 We develop general stable recurrence formula for matrix Chebyshev polynomials; as a corollary, our g(x) can be applied to matrices in a stable manner. \u2022 We obtain faster, provable PCA-free algorithms for PCP and PCR than known results."}], "year": 2017, "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "authors": ["Allen-Zhu", "Zeyuan"], "venue": "In STOC,", "year": 2017}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In NIPS,", "year": 2016}, {"title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and Near-Optimal Rate", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "authors": ["Allen-Zhu", "Zeyuan", "Richt\u00e1rik", "Peter", "Qu", "Zheng", "Yuan", "Yang"], "venue": "In ICML,", "year": 2016}, {"title": "Faster SVD-truncated regularized least-squares", "authors": ["Boutsidis", "Christos", "Magdon-Ismail", "Malik"], "venue": "IEEE International Symposium on Information Theory,", "year": 2014}, {"title": "Computing truncated singular value decomposition least squares solutions by rank revealing QR-factorizations", "authors": ["Chan", "Tony F", "Hansen", "Per Christian"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "year": 1990}, {"title": "Error analysis of an algorithm for summing certain finite series", "authors": ["Elliott", "David"], "venue": "Journal of the Australian Mathematical Society,", "year": 1968}, {"title": "Uniform approximation of sgn x by polynomials and entire functions", "authors": ["Eremenko", "Alexandre", "Yuditskii", "Peter"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "year": 2007}, {"title": "Polynomials of the best uniform approximation to sgn (x) on two intervals", "authors": ["Eremenko", "Alexandre", "Yuditskii", "Peter"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "year": 2011}, {"title": "Principal Component Projection Without Principal Component Analysis", "authors": ["Frostig", "Roy", "Musco", "Cameron", "Christopher", "Sidford", "Aaron"], "venue": "In ICML,", "year": 2016}, {"title": "Numerical Methods for Special Functions. Society for Industrial and Applied Mathematics, jan 2007. ISBN 978-0-89871-634-4", "authors": ["Gil", "Amparo", "Segura", "Javier", "Temme", "Nico M"], "year": 2007}, {"title": "Approximating the spectral sums of largescale matrices using chebyshev approximations", "authors": ["Han", "Insu", "Malioutov", "Dmitry", "Avron", "Haim", "Shin", "Jinwoo"], "venue": "arXiv preprint arXiv:1606.00942,", "year": 2016}, {"title": "Functions of Matrices", "authors": ["N. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "authors": ["Musco", "Cameron", "Christopher"], "venue": "In NIPS,", "year": 2015}, {"title": "Computing fundamental matrix decompositions accurately via the matrix sign function in two iterations: The power of zolotarev\u2019s functions", "authors": ["Nakatsukasa", "Yuji", "Freund", "Roland W"], "venue": "SIAM Review,", "year": 2016}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "authors": ["Nesterov", "Yurii"], "year": 2004}, {"title": "Model order reduction: theory, research aspects and applications, volume", "authors": ["Schilders", "Wilhelmus H.A", "Van der Vorst", "Henk A", "Rommes", "Joost"], "year": 2008}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "authors": ["Shewchuk", "Jonathan Richard"], "year": 1994}, {"title": "Numerical methods for the qcdd overlap operator", "authors": ["van den Eshof", "Jasper", "Frommer", "Andreas", "Lippert", "Th", "Schilling", "Klaus", "van der Vorst", "Henk A"], "venue": "i. sign-function and error bounds. Computer Physics Communications,", "year": 2002}], "id": "SP:858c514a3524cdbdc2f5120038ab8f72694c78e2", "authors": [{"name": "Zeyuan Allen-Zhu", "affiliations": []}, {"name": "Yuanzhi Li", "affiliations": []}], "abstractText": "We solve principal component regression (PCR), up to a multiplicative accuracy 1+\u03b3, by reducing the problem to \u00d5(\u03b3\u22121) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires \u00d5(\u03b3\u22122) such black-box calls. We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods.", "title": "Faster Principal Component Regression  and Stable Matrix Chebyshev Approximation"}