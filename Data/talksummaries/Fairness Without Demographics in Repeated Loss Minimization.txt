0	27	Consider a speech recognizer that is deployed to millions of users.
1	26	State-of-the art speech recognizers achieve high overall accuracy, yet it is well known that such systems have systematically high errors on minority accents (Amodei et al., 2016).
3	58	This representation disparity forms our definition of unfairness, and has been observed in face recognition (Grother et al., 2011), language identification (Blodgett et al., 2016; Jurgens et al., 2017), dependency parsing (Blodgett et al., 2016), part-of-speech tagging (Hovy & Sgaard, 2015), academic recommender systems (Sapiezynski et al., 2017), and automatic video captioning (Tatman, 2017).
5	38	As a result, the minority group will shrink and might suffer even higher error rates from a retrained model in a future time step.
6	75	Machine learning driven feedback loops have been observed in predictive policing (Fuster et al., 2017) and credit markets (Fuster et al., 2017), and this problem of disparity amplification is a possibility in any deployed machine learning system that is retrained on user data.
9	32	A model is trained on the resulting user data which is used at the next time step.
10	32	We assume that each user comes from one of K groups, and our goal is to minimize the worst case risk of any group across time.
11	42	However, the group membership and number of groups K are both unknown, as full demographic information is likely missing in real online services.
12	74	We first show that empirical risk minimization (ERM) does not control the worst-case risk over the disparate K groups and show examples where ERM turns initially fair models unfair (Section 3).
22	47	There has been extensive work (Barocas & Selbst, 2016) on guaranteeing fairness for classification over a protected label through constraints such as equalized odds (Woodworth et al., 2017; Hardt et al., 2016), disparate impact (Feldman et al., 2015) and calibration (Kleinberg et al., 2017).
34	23	Representation disparity: Consider the standard lossminimization setting where a user makes a query Z ∼ P , a model θ ∈ Θ makes a prediction, and the user incurs loss `(θ;Z).
45	28	T rounds, where the group proportion α(t)k depends on t and varies according to past losses.
55	35	First, without group membership labels there is no way to directly measure the worst-case risk RTmax, let alone minimize it.
121	27	The dual of the maximization problem (4) provides additional intuition on the behavior of the robust risk.
127	26	Median Estimation: Recall the median estimation problem over two groups mentioned in Section 3.2 where the loss is `(θ;Z) = ‖θ − Z‖1.
133	27	Figure 2a shows the worst-case distribution Q at the minimizer θ∗DRO which completely removes points within distance η∗.
136	22	In this case, thresholding by η∗ corresponds to selecting the single highest risk group which is equivalent to directly minimizingRmax(θ) (1).
139	32	This is closely related to recent observations that the DRO bound can be loose for classification losses such as the zeroone loss due to the worst-case distribution consisting purely of misclassified examples (Hu et al., 2018).
140	141	Even in this case, the estimated loss is still a valid upper bound on the worst case group risk, and as Figure 2 shows, there are examples where the DRO estimate is nearly tight.
146	27	Alternatively, for models where we can compute θ∗(Q) ∈ argminθ∈Θ EQ[`(θ;Z)] efficiently, we can use existing primal solvers that compute the worst-case probability distribution Q∗(θ) ∈ argmaxQ∈B(P,r) EQ[`(θ;Z)] for a given θ based on projected gradient ascent on Q (Namkoong & Duchi, 2016).
149	54	Now, we study how the individual group risk Rk(θ) affects user retention and hence future risk.
150	71	By virtue of providing an upper bound toRmax(θ), optimizingRdro(θ; rmax) at each time step can thus control the future group riskRmax(θ).
169	40	Recall the motivating example in Figure 1 which shows that logistic regression applied to a two-class classification problem is unstable and becomes pathologically unfair.
172	25	At each round we fit a logistic regression classifier using ERM or DRO and gradient descent, constraining the norm of the weight vector to 1.
176	24	Figure 5 shows that ERM is unstable and the minority group rapidly loses accuracy beyond 300 rounds on most runs.
180	25	We now present a real-world, human evaluation of user retention and satisfaction on a text autocomplete task.
186	41	To evaluate the retention and loss for AAE and SAE separately, a turk user is assigned 10 tweets from either the held out AAE tweets or SAE tweets, which they must replicate using a web-based keyboard augmented by the autocomplete system.
204	60	In this work, we do not address the debate on the correctness of Rawlsian justice (Rawls, 2001), and leave finding a suitable philosophical framework for loss minimization to future work.
206	50	First, as fairness is fundamentally a causal question, observational approaches such as DRO can only hope to control limited aspects of fairness.
208	24	In such problems the implied minorities from DRO may differ from well-specified demographic groups who are known to suffer from historical and societal biases.
