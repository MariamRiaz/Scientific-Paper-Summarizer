25	12	Due to space constraints, certain proofs and implementation details are presented as an appendix in a supplementary document.
26	78	We start by summarizing the standard method for spectral clustering as well as the idea behind the more recent accelerated methods.
27	32	Due to space constraints, our exposition is brief; the reader is encouraged to refer to the original works for a more comprehensive discussion.
28	32	To determine the best node-to-cluster assignment, spectral clustering solves a k-means problem with the eigenvectors of the graph Laplacian as features (Shi and Malik, 2000; Ng et al., 2002).
29	32	Let G = (V, E ,W) be a weighted undirected graph with n nodes V = {v1, v2, .
30	37	The graph Laplacian is defined as L = I − D−1/2WD−1/2, where D is a diagonal matrix whose entries are the degree of the nodes in the graph (i.e. the sum of the weighed edges adjacent to each node).
32	22	Spectral clustering consists of computing the first k eigenvectors of L arranged in matrix Uk and subsequently computing a k-means assignment of the n vectors of size k found in the rows of Uk.
39	9	To reduce the computational cost of spectral clustering, (Tremblay et al., 2016) proposed to approximate Uk using a filtering of random vectors (a similar idea was also examined by (Boutsidis et al., 2015)).
45	22	We can project R onto span{Uk} by multiplying each one of its columns by a projector H defined as H = U ( Ik 0 0 0 ) U>.
46	14	(3) It is then a simple consequence of the JohnshonLindenstrauss lemma that the rowsψ>i of matrix Ψ = HR can act as a replacement of the features used in spectral clustering, i.e., the rows φ>i of Φ = Uk.
47	9	Theorem 2.1 (adapted from (Tremblay et al., 2016)).
48	12	For every two nodes vi and vj the restricted isometry relation (1−ε)‖φi−φj‖2 ≤ ‖ψi−ψj‖2 ≤ (1+ε)‖φi−φj‖2 (4) holds with probability larger than 1 − n−β , as long as the dimension is d > 4+2βε2/2−ε3/3 log(n).
49	20	We note that, even though HR is also expensive to compute exactly, it can be easily approximated by applying the graph filter h(L) on each column of R, which entailsO(dc) sparse matrix-vector multiplications (each costing O(m)) using graph Chebychev polynomials (Shuman et al., 2011a; Hammond et al., 2011) or rational graph filters (Isufi et al., 2017; Loukas et al., 2015) (c relates to the quality of the approximation and is usually below 100).
50	14	A more elaborate discussion on the approximation of HR can be found in the appendix.
51	22	The complexity is reduced further by computing the k-means step for only a subset of the nodes.
52	100	The remaining cluster assignments are then inferred by solving a graph Tikhonov regularized interpolation problem involving k additional graph filtering operations, each with a cost linear in cm.
57	15	Before delving to the dynamic setting, we refine the analysis of compressive spectral clustering.
62	10	Note that, as in previous work (Boutsidis et al., 2015), we express the approximation quality in terms of the difference of clustering assignment costs and not of the distance between the assignments themselves.
63	13	We are not aware of any analysis that would allow us to characterize (the perhaps more intuitive goal of) how well XΨ approximates XΦ, which is a combinatorial objective.
64	9	Yet, our approach exhibits the benefit of not penalizing approximation algorithms that choose alternative assignments of the same or similar quality1.
69	25	Importantly, d ∝ k2 is sufficient to guarantee a small error.
70	12	The first step in proving Thm.
72	15	The following lemma relates the two costs by an additive error term that depends on the feature’s differences ‖Ψ−ΦIk×dQ‖F when d ≥ k.Since Φ and Ψ have different sizes we introduced the multiplication by a unitary matrix Q.
73	7	We will first show that any unitary Q can be picked in Lem.
74	10	3.1 and then derive the optimal Q, the one minimizing the additive term, in Thm.
75	21	For any unitary matrix Q ∈ Rd×d, the SC cost CΦ and the CSC cost CΨ are related by CΦ ≤ CΨ ≤ CΦ + 2‖Ψ−ΦIk×dQ‖F , (8) where, the matrix I`×m of size ` ×m above contains only ones on its diagonal and serves to resize matrices.
76	23	Being able to show that the additive term is small encompasses the result of Thm.
80	17	In order to prove this result, we will first express our Frobenius norm exclusively in terms of the singular values of the random matrix R and then in a second step we will study the distribution of these singular values.
82	10	There exists a d× d unitary matrix Q, such that ‖Ψ−ΦIk×dQ‖F = ‖Σ− Ik×d‖F , (9) where Σ is the diagonal matrix holding the singular values of R′ = Ik×nU>R.
83	42	Before presenting the proof, let us observe that R′ is an i.i.d.
