3	34	In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5.
17	54	We make the following main contributions: • A novel factored transition parsing system where the stack elements are sentence spans rather than partial trees (Section 2).
19	41	• The first provably optimal dynamic oracle for 1 constituency parsing which is also extremely efficient (amortized O(1) time) (Section 4).
22	24	We present a new transition-based system for constituency parsing whose fundamental unit of computation is the sentence span.
23	29	It uses a stack in a similar manner to other transition systems, except that the stack contains sentence spans with no requirement that each one correspond to a partial tree structure during a parse.
42	23	In the final step, (4n − 2), nolabel is not allowed 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 since the parser must produce a tree.
43	61	Figure 2 shows a complete example of applying this parsing system to a very short sentence (“I do like eating fish”) that we will use throughout this section and the next.
46	151	The ternary branch in this tree (VP→MD VBP S) is produced by our parser in a straightforward manner: after the phrase “do like” is combined in step 7, no label is assigned in step 8, successfully delaying the creation of a bracket until the verb phrase is fully formed on the stack.
55	72	If the sequential output of the recurrent network for the sentence is f0, ..., fn in the forward direction and bn, ..., b0 in the backward direction then the span (i, j) would be represented as the concatenation of the vector differences (fj − fi) and (bi − bj).
58	19	In initial experiments, we found that there was essentially no difference in performance between using the difference features and concatenating all end- 〈s〉 I do like eating fish 〈/s〉0 f0 b0 1 f1 b1 2 f2 b2 3 f3 b3 4 f4 b4 5 f5 b5 Figure 3: Word spans are modeled by differences in LSTM output.
67	32	For the particular case of our transition constituency parser, we use only four span features to determine a structural action, and three to determine a label action, in each case partitioning the sentence exactly.
75	42	The network structure after the the span features consists of a separate multilayer perceptron for each type of action (structural and label).
78	37	The baseline method of training our parser is what is known as a static oracle: we simply generate the sequence of actions to correctly parse each training sentence, using a short-stack heuristic (i.e., combine first whenever there is a choice of shift and combine).
79	32	This method suffers from a well-documeted problem, however, namely that it only “prepares” the model for the situation where no mistakes have been made during parsing, an inevitably incorrect assumption in practice.
119	126	For an even-step configuration c = 〈z, σ | i | j, t〉, we denote the next reachable gold bracket next(c) to be pXq, and define the dynamic oracle to be: dyna(c) =    {sh} if p = i and q > j {comb} if p < i and q = j {sh, comb} if p < i and q > j (1) As a special case dyna(〈0, [0], ∅〉) = {sh}.
125	22	For an odd-step configuration c = 〈z, σ | i | j, t〉, we simply check if (i, j) is a valid span in the gold tree tG and if so, label it accordingly, otherwise no label.
129	50	For any configuration c = 〈z, σ, t〉, we define the optimal tree t∗(c) to include all reachable gold brackets and nothing else.
130	65	configuration oracle static dynamic 0Some t xt and t e symbol or scaled 3 comb {comb, sh} I do like 1∧52Some text and the symbol or scaled 1 3 1 1 3 undef.
133	23	Trapezoids denote stack spans (top one in red) and the blue triangle denotes the next reachable bracket next(c) which is 1VP5 in all cases.
145	21	(1–2) satisfies the requirement of a dynamic oracle (Def.
160	19	In both cases, all stateaction training pairs for a given sentence are used at the same time, greatly increasing training speed since all examples for the same sentence share the same forward and backward pass through the recurrent part of the network.
163	23	The only regularization which we employ during training is dropout (Hinton et al., 2012), which is applied with probability 0.5 to the recurrent outputs.
188	74	Specifically, we also included morphological features for each word as input to the recurrent network, using a small embedding for each such feature, to demonstrate that our parsing model is able to exploit such additional features.
189	51	We used the predicted morphological features, part-of-speech tags, and lemmas (used in place of word surface forms) supplied with the SPMRL 2014 data set (Seddah et al., 2014).
191	21	Our parsing and evaluation also includes predicting POS tags for multi-word expressions as is the standard practice for the French treebank, though our results are similar whether or not this aspect is included.
194	50	For these experiments, we performed very little hyperparameter tuning, due to time and resource contraints.
195	71	We have every reason to believe that performance could be improved still further with such techniques as random restarts, larger hidden layers, external embeddings, and hyperparameter grid search, as demonstrated by Weiss et al. (2015).
200	33	We also describe a fast dynamic oracle for this parser which can determine the optimal set of actions with respect to a gold training tree in an arbitrary state.
201	36	Using an LSTM model and only a few sentence spans as features, we achieve state-of-the-art accuracy on the Penn Treebank for all parsers without reranking, despite using strictly greedy inference.
202	27	In the future, we hope to achieve still better results using beam search, which is relatively straightforward given that the parsing system already uses a fixed number of actions.
