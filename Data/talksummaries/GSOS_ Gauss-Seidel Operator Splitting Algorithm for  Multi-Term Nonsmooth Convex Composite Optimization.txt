0	119	In this paper, we focus on the multi-term nonsmooth convex composite optimization min x∈X f(x) + n∑ i=1 gi(x), (1) where X is a linear space, gi : X → (−∞,+∞] is a proper, lower semicontinuous convex function for all i = 1, · · · , n, and f : X → (−∞,+∞) is a continuous differentiable convex function with its gradient satisfying the inequality that 1 L ∥∥∇f(x)−∇f(y)∥∥2 ≤ 〈∇f(x)−∇f(y), x− y〉.
1	19	(2) The above multi-term nonsmooth convex composite optimization problem (1) covers a large class of applications in machine learning such as simultaneous low-rank and sparsity (Richard et al., 2012; Zhou et al., 2013), overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010), graph-guided fused Lasso (Chen et al., 2012; Kim & Xing, 2009), graph-guided logistic regression (Chen et al., 2011; Zhong & Kwok, 2014), variational image restoration (Combettes & Pesquet, 2011; Dupé et al., 2009; Pustelnik et al., 2011), and other types of structure regularization paradigms (Teo et al., 2010; 2007).
2	29	By introducing the multi-term nonsmooth regularization term∑n i=1 gi(x) such as structured sparsity (Huang et al., 2011; Bach et al., 2012; Bach, 2010) and nonnegativity (Chen & Plemmons, 2015; Xu & Yin, 2013), more prior information can be included to enhance the accuracy of regularization models.
3	48	However, due to the multi-term nonsmooth regularization term ∑n i=1 gi(x), the optimization problem (1) is too complicated to be solved even for small n. For n ≤ 2, some existing popular first-order optimization methods are accelerated proximal gradient method (Beck & Teboulle, 2009; Nesterov, 2007), smoothing accelerated proximal gradient method (Nesterov, 2005a;b), three operator splitting method (Davis & Yin, 2015), and some primal-dual operator splitting methods such as majorized alternating direction method of multiplier (ADMM) (Cui et al., 2016; Lin et al., 2011), fast proximity method (Li & Zhang, 2016), and so on.
5	41	A directly method for (1) is smoothing accelerated proximal gradient (S-APG) proposed by Nesterov (Nesterov, 2005a;b).
6	11	Then, Yu (Yu, 2013) proposed a new approximation method called PAAPG for handling (1) by combining the proximal average approximation technique and Nesterov’s acceleration technique, which has been enhanced very recently by Shen et al. (Shen et al., 2017).
12	46	This is one of the main differences between existing splitting methods and our proposed method in this paper.
13	10	To split the nonsmooth composite term ∑n i=1 gi(x) more efficiently, we propose a novel operator splitting algorithm to solve problem (1) by harnessing the advantage of GaussSeidel iterations, i.e., the computation of the proximal mapping of the current function gi(x) uses the proximal mappings of gj(x) for all j < i which have already been computed ahead.
21	31	Then, the global convergence of the proposed GSOS algorithm is easily established based on this reformulation.
46	24	Definition 2 Given a maximal monotone operator T : X ⇒ X, the (≥ 0)-enlargement of T is defined as the set T [ ](x) := { v ∈ Y | 〈w − v, z − x〉 ≥ − for all z ∈ X, w ∈ T (z) } .
50	12	The following lemma establishes the property of the enlargement of the composite operatorA∗ ◦∇f ◦A with f satisfying inequalities (6)- (7) or (2), which is an essential ingredient for reformulating the GSOS algorithm as a two-step iterations algorithm.
62	18	Very interesting, it can be shown that its composition with (R∗)−1, i.e., (R∗)−1SR,T1,T2 , is maximal monotone for any invertible linear operatorR.
70	9	Proposition 2 Given a constant ≥ 0 and an invertible linear operatorR, it holds that gph ( SR, ∂g+(A∗◦∇f◦A)[ ],NV ) ⊆ gph ( SR, (∂g+A∗◦∇f◦A)[ ],NV ) ⊆ gph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ][ ] ) .
71	16	In the following, we establish the relationship between the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of prob- lem (1) and [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0), which means that we can recover the solution of problem (1) through [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0).
92	14	Remark 3 Based on Proposition 3, the GSOS algorithm can be regarded as an inexact over-relaxed metric proximal point algorithm for the composite inclusion 0 ∈ (R∗)−1SR,∂g+A∗◦∇f◦A,NV (z).
115	19	• GFB (Raguet et al., 2013): Generalized Forward Backward (GFB) splitting algorithm is a primal firstorder operator splitting algorithm for solving (1) proposed by Raguet et al. (Raguet et al., 2013), which has been shown to outperform other competing algorithms such as (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) for variational image restoration.
117	22	• PA-APG (Yu, 2013): Proximal Average approximated Accelerated Proximal Gradient (PA-APG) algorithm (Yu, 2013) is a primal first-order method, which utilizes the proximal average technique (Bauschke et al., 2008) to separate the multi-term nonsmooth function in (1).
118	13	It has been shown to outperform the smoothing accelerated proximal gradient method (Nesterov, 2005b;a).
119	14	• APA-APG (Shen et al., 2017): An enhanced version of PA-APG, which incorporates the Adaptive Proximal Average approximation technique with the Accelerated Proximal Gradient (APA-APG) method to improve the efficiency of the optimization procedure.
120	25	It is worthwhile to emphasize that PA-APG and APA-APG algorithms can only be applied to a specific class of problems (1), in which the multi-term nonsmooth regularization is Lipschitz continuous.
121	17	Since the nonsmooth regularization terms in overlapping group Lasso and graph-guided fused Lasso are all exactly Lipschitz continuous, the two efficient solvers PG-APG (Yu, 2013) and its enhanced version APA-APG (Shen et al., 2017) are also compared with the GSOS algorithm to illustrate the efficacy of GSOS.
140	12	The primal-dual solver PDM is slightly faster than the primal solver GFB.
142	18	Also, APA-APG is much faster than the other four solvers at the first 50 iterations.
143	12	However, it is slowed down since the stepsize used in AP-APG becomes smaller and smaller as the iterations go on.
144	10	In this subsection, we perform experiments on graphguided fused Lasso which is formulated as min 1 2 ‖Sx− b‖2 + ν ∑ (i,j)∈E αij |xi − xj |, (25) where αij ≥ 0 is the weight for the fused term ‖xi − xj‖ for all (i, j) ∈ E (E is the given graph edge set), and ν is the regularization parameter.
155	13	In this paper, we proposed a novel first-order algorithm called GSOS for addressing multi-term nonsmooth convex composite optimization.
156	42	This algorithm inherits the advantages of the Gauss-Seidel technique and the operator splitting technique, therefore being largely accelerated.
158	22	In addition, we developed a new technique to establish the global convergence and iteration complexity of the GSOS algorithm.
159	32	Last, we applied the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems, and compared it against several state-of-the-art algorithms.
160	44	The experimental results show the great superiority of the GSOS algorithm in terms of both efficiency and effectiveness.
