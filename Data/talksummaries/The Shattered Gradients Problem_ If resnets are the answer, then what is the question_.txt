3	24	The He initialization ensures variance is preserved across rectifier layers, and batch normalization ensures that backpropagation through layers is unaffected by the scale of the weights (Ioffe & Szegedy, 2015).
4	47	It is perhaps surprising then that residual networks (resnets) still perform so much better than standard architectures when networks are sufficiently deep (He et al., 2016a;b).
6	48	We identify the shattered gradient problem: a previously unnoticed difficulty with gradients in deep rectifier networks that is orthogonal to vanishing and exploding gradients.
8	18	Resnets dramatically reduce the tendency of gradients to shatter.
10	10	Shattering should decrease during training.
11	29	Understanding how shattering affects training is an important open problem.
12	17	We refer to networks without skip connections as feedforward nets—in contrast to residual nets (resnets) and highway nets.
14	10	The first step is to simply look at the gradients of neural networks.
19	10	The model is not intended to be applied to real data.
24	47	Weight updates thus depend on @fW@nj —i.e. how the output of the network varies with the output of neurons in one layer (which are just inputs to the next layer).
30	12	Suppose the network has a single hidden layer: fw,b(x) = w >⇢(x · v b).
40	19	Since the inputs lie on a 1-dim grid, it makes sense to compute the autocorrelation function (ACF) of the gradient.
46	28	Training is difficult when gradients behave like white noise.
53	17	Skip-connections significantly change the correlation structure of gradients.
56	9	Although the gradients become progressively less structured, they do not whiten to the extent of the gradients in standard feedforward networks— there are still correlations in the 50-layer resnet whereas in the equivalent feedforward net, the gradients are indistinguishable from white noise.
86	13	We first investigate batch normalization’s effect on neural activations.
97	14	Neurons that are always inactive may as well not exist.
100	23	The raster plot for feedforward networks resembles static television noise: the spatial structure is obliterated.
109	10	The analysis is for fullyconnected networks.
112	10	It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths, see section A3.
121	9	Figure 3 shows the assumption holds, on average, under batch normalization for both activations and coactivations.
151	22	A solution to the exploding variance of resnets is to rescale layers by ↵ = 1p 2 which yields Cres ↵= p 2 (i) = 1 and Rres ↵= p 2 (i, j) = ✓ 3 4 ◆L and so controls the variance but the correlation between gradients still decays exponentially with depth.
174	21	Figures 5a and b compare the covariance of gradients in the first layer of feedforward and resnets ( = 0.1) with a minibatch of 256 random samples from CIFAR-10 for networks of depth 2 and 50.
184	10	We are interested in the effective rank of the covariance matrix of the gradients relative to a “white” matrix Y of the same dimensions with i.i.d.
185	23	The relative effective rank r( )/r(Y) measures the similarity between the second moments of and Y.
195	17	Shattering gradients are not a problem for linear networks, see remark after equation (1).
196	11	Unfortunately, linear networks are not useful since they lack expressivity.
200	19	✓ ⇢(x) ⇢( x) ◆ The key observation is that initializing weights with a mirrored block structure yields linear outputs W W · ✓ ⇢(x) ⇢( x) ◆ = W⇢(x) W⇢( x) = Wx.
203	58	Setting a = 1 at initialization obtains a different kind of LL-init.
205	23	A detailed analysis of learning in linear neural networks by Saxe et al. (2014) showed, theoretically and experimentally, that arbitrarily deep linear networks can be trained when initialized with orthogonal weights.
