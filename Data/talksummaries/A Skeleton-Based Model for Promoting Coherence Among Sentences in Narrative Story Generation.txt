0	91	We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013).
1	59	It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1.
3	43	Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections.
4	22	Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner.
6	51	In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on.
12	13	Therefore, motivated by the way of human writing, we propose a skeleton-based model to improve the coherence of generated text.
13	18	The key idea is to first generate a skeleton and then expand the skeleton to a complete sentence.
14	23	As a simplified sentence representation, the skeleton can help machines learn the dependency of sentences by avoiding the interference of irrelevant information.
15	17	Our model contains two parts: a skeleton- based generative module and a skeleton extraction module.
17	42	The input-to-skeleton component learns to associate inputs and skeletons, and the skeleton-to-sentence component learns to expand a skeleton to a sentence.
22	12	To address this problem, we build a skeleton extraction module to automatically explore sentence skeletons.
48	47	When decoding a sentence, the input-to-skeleton component first generates a skeleton based on the existing text, including the source input and the already generated text, and then the skeleton-to-sentence component expands and reorganizes the skeleton into a complete sentence.
60	14	In the encoding process, we first obtain sentence representations via a word-level Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), and then generate a compressed vector h via a sentence-level LSTM network.
67	18	Given the training pair of skeleton s and target sentence y = {y1, · · · , yi, · · · , yM}, the crossentropy loss is computed as Lθ = − M∑ i=1 PD(yi|s, θ) (2) where θ refers to the parameters of the skeletonto-sentence component.
69	12	Specially, we use the Seq2Seq model with the attention mechanism as the implementation.
70	13	Both the encoder and the decoder are based on LSTM structures.
73	120	We reformulate skeleton extraction as a sentence compression problem and use a sentence compression dataset to train this module.
74	19	In sentence compression, the compressed sentence is required to be grammatical and convey the most important information.
76	22	However, since the style of the sentence compression dataset is very different from that of the narrative story dataset, it is difficult for the pre-trained module to give narrative text accurate compression results.
79	32	We propose a reinforcement learning method to build the connection between the skeleton ex- Algorithm 1 The reinforcement learning method for training the generative module Gφ and the skeleton extraction module Eγ .
102	24	Therefore, to encourage good skeletons and punish bad skeletons, we use the multiplication of the cross-entropy loss in the input-to-skeleton component and that in the skeleton-to-sentence component as the reward: Rc = [K − (R1 ×R2) 1 2 ] (5) where K is the upper bound of the reward, R1 and R2 are cross-entropy losses in the input-toskeleton component and the skeleton-to-sentence component, respectively.
109	12	This dataset contains the pairs of photo sequences and the associated coherent narrative of events through time written by humans.
113	13	In each story, we take the first sentence as the input text, and the following sentences as the target text.
127	14	Generalized-Template Enhanced Seq2Seq Model (GE-Seq2Seq) (Martin et al., 2018).
139	27	Following the previous work (Li et al., 2016; Martin et al., 2018), we use the BLEU score to measure the quality of generated text.
144	17	Although the quantitative evaluation generally indicates the quality of generated stories, it can not accurately evaluate the generated text.
146	14	We randomly choose 100 items for human evaluation.
166	28	It also needs to be noted that the models are all scored below 6 in coherence, meaning that there is still a long way to go before the generated stories satisfy the requirement of humans.
169	88	Compared with the existing models, the sentences generated by our proposed model are connected more logically.
171	16	), it insists on telling getting married when it sees [male] or [female] (1st and 2nd ex.).
