7	54	Scene graphs (Johnson et al., 2015), on the other hand, proposed a type of directed graph to encode information in terms of objects, attributes of objects, and relationships between objects (see Figure 1 for visualization).
8	46	This is a more structured and explainable way of expressing the knowledge from either modality, and is able to serve as an alternative form of common representation.
9	28	In fact, the value of scene graph representation has already been proven in a wide range of visual tasks, including semantic image retrieval (Johnson et al., 2015), caption quality evaluation (Anderson et al., 2016), etc.
10	36	In this paper, we focus on scene graph generation from textual descriptions.
12	70	They first use a dependency parser to obtain the dependency relationship for all words in a sentence, and then use either a rule-based or a learned classifier as post-processing to generate the scene graph.
64	32	The edgecentric view of a scene graph is very similar to a dependency tree: they are both directed acyclic graphs where the edges/arcs have labels.
66	43	We have shown the connection between nodes in scene graphs and words in dependency parsing.
67	19	With alignment between nodes in scene graphs and words in the textual description, scene graph generation and dependency parsing becomes equivalent: we can construct the generated scene graph from the set of labeled edges returned by the dependency parser.
68	32	Unfortunately, such alignment is not provided between the region graphs and region descriptions in the Visual Genome (Krishna et al., 2017) dataset.
72	24	We propose to use two cycles and each cycle further consists of three steps, where we try to align objects, attributes, relations in that order.
75	29	Intuitively, in the first cycle we use a conservative strategy to find “safe” objects, and then scan for their attributes and relations.
76	22	In the second cycle we relax and allow synonyms in aligning object nodes, also followed by the alignment of attribute and relation nodes.
82	67	1 Input: Sentence s; Scene graph G(s) 2 Initialize aligned nodes N as empty set 3 Initialize aligned words W as empty set 4 for o in object nodes of G(s) \N do 5 for w in s \W do 6 if o↔ w according to WBW then 7 Add (o, w); N = N ∪ {o}; W =W ∪ {w} 8 for a in attribute nodes of G(s) \N do 9 for w in s \W do 10 if a↔ w according to WBW or SYN and a’s object node is in N then 11 Add (a,w); N = N ∪ {a}; W =W ∪ {w} 12 for r in relation nodes of G(s) \N do 13 for w in s \W do 14 if r ↔ w according to WBW or SYN and r’s subject and object nodes are both in N then 15 Add (r, w); N = N ∪ {r}; W =W ∪ {w}
85	15	The Arc-Hybrid System In the arc-hybrid system, a configuration consists of a stack σ, a buffer β, and a set T of dependency arcs.
91	14	The SHIFT transition moves the first element of the buffer to the stack.
94	15	The following paragraphs describe how to select the correct transition action (and label l) in each step in order to generate a correct dependency tree.
95	27	BiLSTM Feature Extractor Let the word embeddings of a sentence s be w1, .
100	39	When predicting the transition action, the feature function φ(c) that summarizes the current configuration c = (σ, β, T ) is simply the concatenated BiLSTM vectors of the top three elements in the stack and the first element in the buffer: φ(c) = vs2 ◦ vs1 ◦ vs0 ◦ vb0 (4) MLP Scoring Function The score of transition action y under the current configuration c is determined by a multi-layer perceptron with one hidden layer: f(c, y) =MLP (φ(c))[y] (5) where MLP (x) =W2 · tanh(W1 · x+ b1) + b2 (6) Hinge Loss Function The training objective is to raise the scores of correct transitions above scores of incorrect ones.
104	27	Losses at individual steps are summed throughout the parsing of a sentence, and then parameters are updated using backpropagation.
110	22	We tackle these two challenges by redesigning the edge labels and expanding the set of transition actions.
131	16	Our training set is the intersection of Visual Genome and MS COCO (Lin et al., 2014) train2014 set, which contains a total of 34027 images/ 1070145 regions.
154	46	This shows that improving the parser (about 20% margin) and improving the sentence-graph alignment (about 30% margin) are both promising directions for future research.
158	26	We test if the advantage of our parser can be propagated to computer vision tasks, such as image retrieval.
166	26	Our parser delivers better retrieval performance across all three evaluation metrics: recall@5, recall@10, and median rank.
167	14	We also notice that the numbers in our retrieval setting are higher than those (even with oracle) in Schuster et al. (2015)’s retrieval setting.
171	37	We further show that the gap between edge-centric scene graphs and dependency parses can be filled with a careful redesign of label and action space.
172	57	This motivates us to train a single, customized, end-to-end neural dependency parser for this task, as opposed to prior approaches that used generic dependency parsing followed by heuristics or simple classifier.
175	46	We hope our paper can lead to more thoughts on the creative uses and extensions of existing NLP tools to tasks and datasets in other domains.
176	33	In the future, we plan to tackle more computer vision tasks with this improved scene graph parsing technique in hand, such as image region grounding.
177	15	We also plan to investigate parsing scene graph with cyclic structures, as well as whether/how the image information can help boost parsing quality.
