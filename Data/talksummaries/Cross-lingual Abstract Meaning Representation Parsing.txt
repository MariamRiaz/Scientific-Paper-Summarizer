0	29	Abstract Meaning Representation (AMR) parsing is the process of converting natural language sentences into their corresponding AMR representations (Banarescu et al., 2013).
1	12	An AMR is a graph with nodes representing the concepts of the sentence and edges representing the semantic relations between them.
5	36	Xue et al. (2014) show that structurally aligning English AMRs with Czech and Chinese AMRs is not always possible but that refined annotation guidelines suffice to resolve some of these cases.
9	47	By evaluating the parsers and manually analyzing their output, we show that the parsers are able to recover the AMR structures even when there exist structural differences between the languages, i.e., although AMR is not an interlingua it can act as one.
39	21	We use word alignments, similarly to other annotation projection work, to project the AMR alignments to the target languages.
44	13	The words is, the and of do not generate any AMR nodes, so we ignore their word alignments.
46	66	We invoke an MT system to translate the input sentence into English so that we can use an available English parser to obtain its AMR graph.
51	31	We now turn to the problem of evaluation.
54	46	SILVER We can generate a silver test set by running an automatic (English) AMR parser on the English side of a parallel corpus and use the output AMRs as references.
56	53	FULL-CYCLE In order to perform the evaluation on a gold test set, we propose full-cycle evaluation: after learning the target parser from the English parser, we invert this process to learn a new English parser from the target parser, in the same way that we learned the target parser from the English parser.
57	18	The resulting English parser is then evaluated against the (English) AMR gold standard.
58	14	We hypothesize that the score of the new English parser can be used as a proxy to the score of the target parser.
60	17	In order to do so, we collected professional translations for the English sentences in the AMR test set.1 We were then able to create pairs of human-produced sentences with human-produced AMR graphs.
61	15	A diagram summarizing the different evaluation stages is shown in Figure 2.
77	17	In order to experiment with the approach of Section 2.2, we experimented with translations from Google Translate.3 As Google Translate has access to a much larger training corpus, we also trained baseline MT models using Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017), with the same training data we use for the projection method and default hyper-parameters.
83	54	Due to noisy JAMR alignments and silver training data involved in the annotation projection approach, the MTbased systems give in general better parsing results.
87	21	In the Italian example, the only evident error is that Infine (Lastly) should be ignored.
99	38	In order to investigate the hypothesis that AMR can be shared across these languages, we now look at translational divergence and discuss how it affects parsing, following the classification used in previous work (Dorr et al., 2002; Dorr, 1994), which identifies classes of divergences for several languages.
101	18	Figure 4 shows six sentences displaying these divergences.
104	38	For example, the English sentence I am jealous of you is translated into Spanish as Tengo envidia de ti (I have jealousy of you).
107	16	This divergence happens when verbs expressed in a language with a single word can be expressed with more words in another language.
110	15	For example, We will answer is translated in the Italian sentence Noi daremo una riposta (We will give an answer), where to answer is translated as daremo una risposta (will give an answer).
116	33	Also this parsed graph, in Figure 4c, is structurally correct.
131	14	We computed the Pearson correlation coefficients for the Smatch scores of Table 1 to determine how well silver and full-cycle correlate with gold evaluation.
132	13	Full-cycle correlates better than silver: the Pearson coefficient is 0.95 for full-cycle and 0.47 for silver.
139	16	Full cycle introduces additional noise but it is not as expensive as gold and is more reliable than silver.
154	124	We introduced the problem of parsing AMR structures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR.
155	28	We provided evidence that AMR can be indeed shared across the lan- guages tested and that it is possible to overcome translational divergences.
158	13	The results of the projection-based AMR parsers indicate that there is a vast room for improvements, especially in terms of generating better alignments.
159	28	We encourage further work in this direction by releasing professional translations of the AMR test set into four languages.
