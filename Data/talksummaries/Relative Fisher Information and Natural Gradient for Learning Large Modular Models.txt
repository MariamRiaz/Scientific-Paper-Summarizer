0	64	The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).
2	13	As its empirical counterpart, the observed FIM (Efron & Hinkley, 1978) with respect to (wrt) a sample set Xn = {xk}nk=1 is Î(Θ |Xn) = −∇2l(Θ |Xn), which is often evaluated at the maximum likelihood estimate Θ = Θ̂(Xn).
9	33	It gives a Riemannian metric (Hotelling, 1929; Rao, 1945) of the learning parameter space which is unique (Čencov, 1982; Dowty, 2017).
20	42	For this purpose, a novel concept, the Relative Fisher Information Metric (RFIM), is defined.
22	14	This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic.
31	15	5 gives an algorithmic framework and proof-of-concept experiments on neural network optimization.
41	16	According to the FIM, the infinitesimal square distance 〈δΘ, δΘ〉IΘ(Θ) = 1 n n∑ k=1 Ep(y |xk,Θ) [( δΘᵀ ∂lk ∂Θ )2] (1) measures how much δΘ (with a radius constraint) is statistically along ∂l∂Θ , or equivalently how much δΘ affects intrinsically the conditional distribution p(y |x, Θ).
42	56	Consider the negative log-likelihood function L(Θ) = − ∑n k=1 log p(yk |xk,Θ) wrt the observed pairs {(xk,yk)}nk=1, we try to minimize the loss while maintaining a small learning step size 〈δΘ, δΘ〉IΘ(Θ) on M. At Θt ∈ M, the target is to minimize wrt δΘ the Lagrange function L(Θt + δΘ) + 1 2γ 〈δΘ, δΘ〉IΘ(Θt) ≈ L(Θt) + δΘᵀ 5Θ L(Θt) + 1 2γ δΘᵀIΘ(Θt)δΘ, where γ > 0 is a learning rate.
43	13	The optimal solution of the above quadratic optimization gives a learning step δΘt = −γI−1Θ (Θt)5Θ L(Θt).
44	21	In this update procedure, ∇̃ΘL(Θ) = I−1Θ (Θ)5Θ L(Θ) replaces the role of the usual gradient ∇ΘL(Θ) and is called the natural gradient (Amari, 1997).
49	85	Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux of the error surface, and attaining Fisher efficiency in online learning (see Sec.
50	29	MΘ Θ yx Mθ1 x x+ ∆x θ1x Mθ2h1 h1 + ∆h1 θ2h1 Mθ3 h2 h2 + ∆h2 θ3h2 y Model: Manifold: Computational graph: Metric: Θ Θ I(Θ) θ3 h2 θ3 h2 gy(θ3) θ2 h1 θ2 h1 gh2(θ2) θ1 θ1 gh1(θ1) p(y |Θ,x) = ∑ h1 ∑ h2 p(h1 |θ1,x) p(h2 |θ2,h1) p(y |θ3,h2) Figure 1.
51	17	(left) The traditional global geometry of a MLP; (right) information geometry of subsystems.
53	18	The square under the (sub-)system means the (R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.
65	20	The reference, θf , consists of the majority of the random variables that are considered fixed (therefore allowing us to simplify the analysis).
68	28	The response h is a random variable that reacts to the variations of θ.
70	20	Formally, a subsystem which factorizes the learning machine is characterized by the conditional distribution p(h |θ,θf ), where θ can be estimated based on h and θf .
81	40	Figure 1 shows the traditional global geometry of a learning system, where the curvature is defined by the learner’s parameter sensitivity to the external environment (x and y), as compared to the information geometry of subsystems, where the curvature is defined by the parameter sensitivity wrt hidden interface variables h. The two-colored meshes show that the geometry structure is dynamic and varies with the reference variable θf .
82	14	One should not confuse the RFIM with the diagonal blocks of the FIM (Kurita, 1994).
92	17	Consider a stochastic neuron with input x and weights w. After a nonlinear activation function f , the output y is randomized surrounding the mean f(wᵀx̃) with a variance.
94	69	Using x as the reference, the RFIM of w with respect to y has a common form gy(w |x) = νf (w,x)x̃x̃ᵀ, where νf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron.
103	22	A nonlinear layer increments a linear layer by adding an element-wise activation function applied on W ᵀx̃, and then randomized wrt the choice of the neuron.
157	31	The traditional non-parametric way of applying natural gradient requires re-calculating the FIM and solving a large linear system in each learning step.
158	73	Besides the huge computational cost, it has a large approximation error.
159	30	For example during online learning, a mini-batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations.
160	16	That is, the FIM of a mini-batch is likely to be singular or poorly conditioned.
161	22	A recent series of efforts (Montavon & Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach to applying natural gradient, which memorizes and learns a geometry.
163	35	By dividing the learning system into subsystems, the RFIM potentially gives a systematical implementation of parametric natural gradient descent.
164	13	The memory complexity of storing the Riemannian metric has been reduced from O(D2) to O( ∑ iD 2 i ), where Di = dim(wi) is the size of the i’th neuron.
168	14	The good performance of batch normalization (Ioffe & Szegedy, 2015) provides an empirical support for the RFIM.
