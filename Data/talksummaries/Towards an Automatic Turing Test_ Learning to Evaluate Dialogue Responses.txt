0	36	Building systems that can naturally and meaningfully converse with humans has been a central goal of artificial intelligence since the formulation of the Turing test (Turing, 1950).
2	63	Recently, there has been a surge of interest towards building large-scale non-task-oriented dialogue systems using neural networks (Sordoni et al., 2015b; Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016a; Li et al., 2015).
3	15	These models are trained in an end-to-end manner to optimize a single objective, usually the likelihood of generating the responses from a fixed corpus.
5	58	One of the challenges when developing such systems is to have a good way of measuring progress, in this case the performance of the chatbot.
7	5	The test requires live human interactions, which is expensive and difficult to scale up.
12	77	Despite advances in neural network-based models, evaluating the quality of dialogue responses automatically remains a challenging and understudied problem in the non-task-oriented setting.
13	60	The most widely used metric for evaluating such dialogue systems is BLEU (Papineni et al., 2002), a metric measuring word overlaps originally developed for machine translation.
14	143	However, it has been shown that BLEU and other word-overlap metrics are biased and correlate poorly with human judgements of response quality (Liu et al., 2016).
17	29	While human evaluation should always be used to evaluate dialogue models, it is often too expensive and time-consuming to do this for every model specification (for example, for every combination of model hyperparameters).
18	68	Therefore, having an accurate model that can evaluate dialogue response quality automatically — what could be considered an automatic Turing test — is critical in the quest for building human-like dialogue agents.
19	14	To make progress towards this goal, we make the simplifying assumption that a ‘good’ chatbot is one whose responses are scored highly on appropriateness by human evaluators.
21	96	We also find empirically that asking evaluators for other metrics results in either low inter-annotator agreement, or the scores are highly correlated with appropriateness (see supp.
22	3	Thus, we collect a dataset of appropriateness scores to various dialogue responses, and we use this dataset to train an automatic dialogue evaluation model (ADEM).
23	28	The model is trained in a semi-supervised manner using a hierarchical recur- rent neural network (RNN) to predict human scores.
24	17	We show that ADEM scores correlate significantly with human judgement at both the utterance-level and system-level.
25	110	We also show that ADEM can often generalize to evaluating new models, whose responses were unseen during training, making ADEM a strong first step towards effective automatic dialogue response evaluation.1
26	197	To train a model to predict human scores to dialogue responses, we first collect a dataset of human judgements (scores) of Twitter responses using the crowdsourcing platform Amazon Mechanical Turk (AMT).2 The aim is to have accurate human scores for a variety of conversational responses — conditioned on dialogue contexts — which span the full range of response qualities.
27	12	For example, the responses should include both relevant and irrelevant responses, both coherent and non-coherent responses and so on.
29	44	Following (Liu et al., 2016), we use the following 4 sources of candidate responses: (1) a response selected by a TF-IDF retrieval-based model, (2) a response selected by the Dual Encoder (DE) (Lowe et al., 2015), (3) a response generated using the hierarchical recurrent encoder-decoder (HRED) model (Serban et al., 2016a), and (4) human-generated responses.
31	30	In addition to increasing response variety, this is necessary because we want our evaluation model to learn to compare the reference responses to the candidate responses.
33	52	Note that, in order to maximize the number of responses obtained with a fixed budget, we only obtain one evaluation score per dialogue response in the dataset.
34	8	To train evaluation models on human judgements, it is crucial that we obtain scores of responses that lie near the distribution produced by advanced models.
35	47	This is why we use the Twitter Corpus (Ritter et al., 2011), as such models are pre-trained and readily available.
36	62	Further, the set of topics discussed is quite broad — as opposed to the very specific Ubuntu Dialogue Corpus (Lowe et al., 2015) — and therefore the model may also be suited to other chit-chat domains.
44	46	In particular, the most popular metrics are the BLEU and METEOR scores used for machine translation, and the ROUGE score used for automatic summarization.
45	22	While these metrics tend to correlate with human judgements in their target domains, they have recently been shown to highly biased and correlate very poorly with human judgements for dialogue response evaluation (Liu et al., 2016).
47	7	BLEU BLEU (Papineni et al., 2002) analyzes the co-occurrences of n-grams in the reference and the proposed responses.
51	2	This problem is less critical for machine translation; since the set of reasonable translations of a given sentence or document is rather small, one can reasonably infer the quality of a translated sentence by only measuring the word-overlap between it and one (or a few) reference translations.
52	98	However, in dialogue, the set of appropriate responses given a context is much larger (Artstein et al., 2009); in other words, there is a very high response diversity that is unlikely to be captured by word-overlap comparison to a single response.
54	46	As such, they do not consider the context of the conversation.
