0	38	As machine learning-based models become more widespread and grow in both scale and complexity, methods of interpreting their predictions are increasingly attracting attention from the machine learning community.
2	13	Various problem setups (Palczewska et al., 2013; Tolomei et al., 2017; Fong & Vedaldi, 2017) and interpretation methods, both model-agnostic (Ribeiro et al., 2016; Lundberg & Lee, 2017) and model-specific (Shrikumar et al., 2017; Tolomei et al., 2017; Sundararajan et al., 2017), have recently been proposed in the literature.
3	12	A common trait shared by the majority of these methods is that they do not provide a way of automatically improving the model, since the model is fixed; the main use-case thus becomes manual analytics by the user or the developer, which is both time and resource-consuming.
4	10	It is thus desirable to derive a framework for obtaining actionable insights into the model’s behavior allowing us to automatically improve a model’s performance.
5	33	One such framework has recently been introduced by Koh & Liang (2017); it deals with finding the most influential training objects.
6	16	They formalize the notion of “influence” via an infinitesimal approximation to leave-one-out retraining: the core question that this work aims to answer is “how would the model’s performance on a test object xtest change if the weight of a training object xtrain is perturbed?” Assuming a smooth parametric model family (e.g., linear models or neural networks), the authors employ the Influence Functions framework from classical statistics (Cook & Weisberg (1980); also see Koh & Liang (2017) for a literature review on the topic) to show that this quantity can be estimated much faster than via straightforward model retraining, which makes their method tractable in a real-world scenario.
8	29	Unfortunately, the method suggested by Koh & Liang (2017) heavily relies on the smooth parametric nature of the model family.
10	19	In particular, decision tree ensembles such as Random Forests (Ho, 1995, RF) and Gradient Boosted Decision Trees (Friedman, 2001, GBDT) are probably the most widely used model family in industry, largely due to their state-of-the-art performance on struc- tured and/or multimodal data.
12	10	In this paper, we propose a way of doing so, while focusing specifically on GBDT.
14	23	For the first one, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming that a small training sample perturbation does not change the trees’ structures) to derive LeafRefit and FastLeafRefit, a well-founded family of approximations to leave-one-out retraining that trade off approximation accuracy for computational complexity.
15	12	For the second, analogously to the Influence Functions framework, we consider infinitesimal training sample weight perturbations and derive LeafInfluence and FastLeafInfluence, methods for estimating gradients of the model’s predictions with respect to training objects’ weights.
20	13	Learning consists of two separate stages: model structure selection and picking the optimal leaf values.
29	10	Since the notion of “influence” is not rigorously defined and partly intuitive, we need to introduce a well-defined, measurable quantity that aims to capture the desired intuition; we refer to it as a proxy for influence.
41	27	Moreover, these two operations respond to small training set perturbations differently: the tree structure is piecewise constant (i.e., it either stays the same or changes abruptly), whereas leaf values change more smoothly.
42	8	Thus, a natural assumption to make is: Assumption 1.
60	10	Its definition, however, allowed for an arbitrary choice of the update set U t telling us which training points’ prediction changes to take into account at boosting step t. It is intuitively clear that different strategies of selecting U t allow us to optimize the trade-off between computational complexity and quality of approximating leave-one-out retraining; thus, FastLeafRefit provides a principled way of obtaining approximations of different rigor to LeafRefit.
91	20	First, let us derive the desired expression5 for f tG;l(A t−1(w),w): ∂f tG;l(A t−1(w),w) ∂wi = − ∂ ∂wi [ Gtl(A t−1(w),w) HtG;l(A t−1(w),w) ] = = − ∂Gtl(A t−1(w),w) ∂wi HtG;l(A t−1(w),w) HtG;l(A t−1(w),w)2 + + ∂HtG;l(A t−1(w),w) ∂wi Gtl(A t−1(w),w) HtG;l(A t−1(w),w)2 (7) Let us calculate the derivatives of Gtl(A t−1(w),w) and HtG;l(A t−1(w),w) separately: ∂Gtl(A t−1(w),w) ∂wi = = ∑ j∈Itl [ δijg t j(A t−1 j (w)) + wjh t j(A t−1 j (w))J(A t−1)ij ] = = Itl (i)g t i(A t−1 i (w)) + ∑ j∈Itl wjh t j(A t−1 j (w))J(A t−1)ij ; ∂HtG;l(A t−1(w),w) ∂wi = Itl (i) Plugging this back into Equations 7 and grouping terms with and without Itl (i) separately, we get: ∂f tG;l(A t−1(w),w) ∂wi = −Itl (i) f tG;l + g t i(A t−1 i (w)) HtG;l(A t−1(w),w) − − ∑ j∈Itl wjh t j(A t−1 j (w))J(A t−1)ij HtG;l(A t−1(w),w) , which proves the first part of Proposition 1.
94	9	This concludes the proof of Proposition 1.
95	10	6 that leaf value derivatives at step t depend on the Jacobi matrix J(At−1)ij .
102	8	6 can now be precomputed only once during GBDT training and not for Algorithm 4 LeafInfluence Inputs: training point index i0, sample-to-leaf assignments {Itl } T,L t=1,l=1, {gti(A t−1 i )} T,n t=1,i=1, {hti(A t−1 i )} T,n t=1,i=1, {kti(A t−1 i )} T,n t=1,i=1, leaf formula type formula Outputs: leaf value derivatives {∂f t l (A t−1) ∂wi }T,Lt=1,l=1 J(A0)ij ← 0, i = 1 .
103	20	n end for return {∂f t l (A t−1) ∂wi }T,Lt=1,l=1 each training object i whose influence we want to compute.
106	11	This is equivalent to assuming J(At−1)ij = 0 ∀ j /∈ U t, making J(At−1)ij a sparse matrix with the number of nonzero elements in each row bounded by C := maxt |U t|.
112	8	Does FastLeafInfluence yield a notable runtime speedup over FastLeafRefit?
127	10	Finally, we average the results over the test objects.
140	10	We then conduct two experiments: A.
148	8	3a) in targeting training objects harmful for a particular test object.
157	13	In the following experiment we demonstrate how our methods allow to detect domain mismatch and get a hint on how the distribution of points in Xtrain should be modified in order to better match the distribution of points in Xtest.
161	18	Originally we had 169/1853 readmitted patients in this group and 2140/20000 overall; after we get 17/1601 in the age ∈ [40; 50) group and 1988/19848 overall.
163	21	Training set Xtrain is naturally split into four parts {Xitrain}4i=1 depending on the value of y and whether age ∈ [40; 50).
165	9	Below we confirm this expectation.
