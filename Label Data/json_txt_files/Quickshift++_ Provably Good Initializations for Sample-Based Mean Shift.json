{"sections": [{"heading": "1. Introduction", "text": "Quick Shift (Vedaldi & Soatto, 2008) is a mode-seeking based clustering algorithm that has a growing popularity in computer vision. It proceeds by repeatedly moving each sample to its closest sample point that has higher empirical density if one exists within a \u03c4 -radius ball, otherwise we stop. Thus each path ends at a point which can be viewed as a local mode of the empirical density. Then, points that end up at the same mode are assigned to the same cluster. The most popular choice of empirical density function is the Kernel Density Estimator (KDE) with Gaussian Kernel. The algorithm also appears in Rodriguez & Laio (2014).\nQuick Shift was designed as a faster alternative to the wellknown Mean Shift algorithm (Cheng, 1995; Comaniciu & Meer, 2002). Mean Shift is equivalent to performing a gradient ascent of the KDE starting at each sample until convergence (Arias-Castro et al., 2016). Samples that correspond to the same points of convergence are in the same cluster and the points of convergence are taken to be the estimates of the modes. Thus, both procedures hill-climb to the local modes of the empirical density function and cluster based on these modes. The key differences are that Quick Shift restricts the steps to sample points (and thus is a sample-based version of Mean Shift) and has the extra \u03c4 parameter which allows it to merge close segments together.\n1Google Research, Mountain View, CA 2Uber Inc, San Francisco, CA 3Princeton University, Princeton, NJ. Correspondence to: Heinrich Jiang <heinrich.jiang@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nOne of the drawbacks of these two procedures, as well as many mode-seeking based clustering algorithms, is that the point-modes of the density functions are often poor representations of the clusters. This will happen when the high-density regions within a cluster are of arbitrary shape and have some variations causing the underlying density function to have possibly many apparent, but not so salient modes. In this case, such procedures asymptotically recover all of the modes separately, leading to over-segmentation. To combat this effect, practitioners often increase the kernel bandwidth, which makes the density estimate more smooth. However, this can cause the density estimate to deviate too far from the original density we are intending to cluster based on.1 Thus, practitioners may not wish to identify the clusters based on the point-modes of the density function, but rather identify them based on locally high density regions of the dataset (See Figure 1).2\nWe propose modeling these locally high-density regions as cluster-cores (to be precisely defined later), which can be of arbitrary shape, size, and density level, and are thus better suited at capturing the possibly complex topological properties of clusters that can arise in practice. In other words, these cluster-cores are better at expressing the clusters and are more stable as they are less sensitive to the small fluctuations that can arise in the empirical density function. We parameterize the cluster-core by \u03b2 where 0 < \u03b2 < 1, which determines how much the density is allowed to vary within the cluster-core. We estimate them from a finite sample using a minor modification of the MCores algorithm of Jiang & Kpotufe (2017).\nWe introduce Quickshift++, which first estimates these cluster-cores, and then runs the Quick Shift based hillclimbing procedure on each remaining sample until it reaches a cluster-core. Samples that end up in the same cluster-core are assigned to the same cluster; thus,\n1KDE with Gaussian kernel and bandwidth h approximates the underlying density convolved with a Gaussian with mean 0 and covariance h2I. Thus, the higher h is, the more the KDE deviates from the original density.\n2Over-segmentation is also dealt with in Quick Shift via the \u03c4 parameter, but a threshold for the distance between two modes which should be clustered together is hard to determine in practice. Moreover, there may not even be a good setting of \u03c4 which works everywhere in the input space.\nthe cluster-cores can be seen as representing the highconfidence regions within each cluster. We utilize the k-NN density estimator as our empirical density.\nDespite the simplicity of our approach, we show that Quickshift++ considerably outperforms the popular density-based clustering algorithms, while being efficient. Another desirable property of Quickshift++ is that it is simple to tune its two hyperparameters \u03b2 and k.3 We show that a few settings of \u03b2 turn out to work for a wide range of applications and that the procedure is stable in choices of k.\nWe then give a novel statistical consistency analysis for Quickshift++ which provides guarantees that points within a cluster-core\u2019s attraction regions (to be described later) are correctly assigned. We also show promising results on image segmentation, which further validates the desirability of using cluster-cores on real-data applications."}, {"heading": "2. Related Works and Contributions", "text": "We show that Quickshift++ is a new and powerful addition to the family of clustering procedures known as densitybased clustering, which most notably includes DBSCAN (Ester et al., 1996) and Mean Shift (Cheng, 1995). Such procedures operate on the estimated density function based on a finite sample to recover structures in the density function that ultimately correspond to the clusters. There are several advantages of density-based clustering over classical objective-based procedures such as k-means and spectral clustering. Density-based procedures can automatically detect the number of clusters, while objective-based procedures typically require this as an input. Density-based clustering algorithms also make little assumptions on the shapes of the clusters as well as their relative positions.\nDensity-based clustering procedures can roughly be classified into two categories: hill-climbing based approaches (discussed earlier, which includes both Mean Shift and\n3The \u03c4 parameter from Quick Shift is unnecessary here because we climb until we reach a cluster-core as our stopping condition.\nQuick Shift) and density-level set based approaches. We now discuss the latter approach, which takes the connected components of the density-level set defined by {x : f(x) \u2265 \u03bb} for some density level \u03bb as the clusters. This statistical notion of clustering traces back to Hartigan (1975). Since then, there has been extensive work done, e.g. Tsybakov et al. (1997); Cadre (2006); Rigollet et al. (2009); Singh et al. (2009); Chaudhuri & Dasgupta (2010); Rinaldo & Wasserman (2010); Kpotufe & von Luxburg (2011); Balakrishnan et al. (2013); Chaudhuri et al. (2014); Chen et al. (2017). More recently, Sriperumbudur & Steinwart (2012); Jiang (2017a) show that the popular DBSCAN algorithm turns out to converge to these clusters. However, one of the main drawbacks of this approach is that the density-level \u03bb is fixed and thus such methods perform poorly when the clusters are at different density-levels. Moreover, the question of how to choose \u03bb remains (e.g. Steinwart (2011)).\nJiang & Kpotufe (2017) provide an alternative notion of clusters, called modal-sets, which are regions of flat density which are local maximas of the density. They can be of arbitrary shape, dimension, or density. They provide a procedure, MCores, which estimates these with consistency guarantees. Our notion of cluster-core is similar to modalsets, but the density within a cluster-core is allowed to vary by a substantial amount in order to capture such variations seen in real data as a the flat density of modal-sets may be too restrictive in practice. It turns out that a small modification of MCores allows us to estimate these cluster-cores. Thus Quickshift++ has the advantage over DBSCAN in that clusters can be at any density level and that furthermore, the density levels are chosen adaptively.\nMcores however consists of an over-simplistic final clustering: it simply assigns each point to its closest modal-set, while in practice, clusters tend not to follow the geometry induced by the Euclidean metric. Quickshift++ on the other hand clusters the remaining points by a hill-climbing method which we show is far better in practice.\nThus, Quickshift++ combines the strengths of both densitybased clustering approaches while avoiding many of their weaknesses. In addition to the general advantages of densitybased clustering algorithms shared by both approaches, it is also able to both (1) recover clusters at varying density levels and (2) not suffer from the over-segmentation issue described in Figure 1. To our knowledge, no other procedure has been shown to have this property.\nFor our theoretical analysis, we give guarantees about Quickshift++\u2019s ability to recover the clusters based on attraction regions defined by the gradient flows. Wasserman et al. (2014); Arias-Castro et al. (2016) showed that Mean Shift\u2019s iterates approximate the gradient flows. Some progress has been made in understanding Quick Shift (Jiang, 2017b; Verdinelli & Wasserman, 2018). There are also related lines\nof work in mode clustering e.g. (Li et al., 2007; Chaco\u0301n, 2012; Genovese et al., 2016; Chen et al., 2016). In this paper, we show that Quickshift++ recovers the interior of its attraction region, thus adding to our statistical understanding of hill-climbing based clustering procedures."}, {"heading": "3. Algorithm", "text": ""}, {"heading": "3.1. Basic Definitions", "text": "Let X[n] = {x1, ..., xn} be n i.i.d. samples drawn from an unknown density f , defined over the Lebesgue measure on Rd. Suppose that f has compact support X .\nOur procedure will operate on the k-NN density estimator:\nDefinition 1. Let rk(x) := inf{r > 0 : |B(x, r)\u2229X[n]| \u2265 k}, i.e., the distance from x to its k-th nearest neighbor. Define the k-NN density estimator as\nfk(x) := k\nn \u00b7 vd \u00b7 rk(x)d ,\nwhere vd is the volume of a unit ball in Rd."}, {"heading": "3.2. Cluster-Cores", "text": "We define the cluster core with respect to fixed fluctuation parameter \u03b2 as follows.\nDefinition 2. Let 0 < \u03b2 < 1. Closed and connected set M \u2282 X is a cluster-core if M is a connected component (CC) of {x \u2208 X : f(x) \u2265 (1\u2212 \u03b2) \u00b7maxx\u2032\u2208M f(x\u2032)}.\nNote that when \u03b2 \u2192 0, then the cluster-cores become the modes or local-maximas of f . When \u03b2 \u2192 1, then the cluster-core becomes the entire support X . We next give a very basic fact about cluster-cores, that they do not overlap.\nLemma 1. Suppose that M1, M2 are distinct cluster-cores of f . Then M1 \u2229M2 = \u2205.\nProof. Suppose otherwise. We have that M1 and M2 are CCs of {x \u2208 X : f(x) \u2265 \u03bb1} and {x \u2208 X : f(x) \u2265 \u03bb2}, respectively for some \u03bb1, \u03bb2. Clearly, if \u03bb1 = \u03bb2, then it follows that M1 =M2. Then, without loss of generality, let \u03bb1 < \u03bb2. Then since the CCs of {x \u2208 X : f(x) \u2265 \u03bb2} are nested in the CCs of {x \u2208 X : f(x) \u2265 \u03bb1}, then it follows that M2 \u2286M1. Then, \u03bb2 = (1\u2212 \u03b2) supx\u2208M2 f(x) \u2264 (1\u2212 \u03b2) supx\u2208M1 f(x) = \u03bb1, a contradiction. As desired.\nAlgorithm 1 is a simple modification of MCores by Jiang & Kpotufe (2017). The difference is that we use a multiplicative fluctuation parameter \u03b2, while Jiang & Kpotufe (2017) uses an additive one. The latter requires knowledge of the scale of the density function, which is difficult to determine in practice. Moreover, the multiplicative fluctuation adapts to clusters at varying density levels more reasonably than\na fixed additive fluctuation. It uses the levels of the mutual k-NN graph of the sample points, defined below. Definition 3. Let G(\u03bb) denote the \u03bb-level of the mutual k-NN graph with vertices {x \u2208 X[n] : fk(x) \u2265 \u03bb} and an edge between x and x\u2032 iff ||x\u2212 x\u2032|| \u2264 min{rk(x), rk(x\u2032)}.\nIt is known that G(\u03bb) approximates the CCs of the \u03bb-level sets of the true density, defined as {x \u2208 X : f(x) \u2265 \u03bb} see e.g. (Chaudhuri & Dasgupta, 2010). Moreover, it can be seen that the CCs of G(\u03bb) forms a hierarchical nesting structure as \u03bb decreases.\nAlgorithm 1 proceeds by performing a top-down sweep of the levels of the mutual k-NN graph, G(\u03bb). As \u03bb decreases, it is clear that more nodes appear and that connectivity increases. In other words, as we scan top-down, the CCs of G(\u03bb) become larger, some CCs can merge, or new CCs can appear. When a new CC appears at level \u03bb, then intuitively, it should correspond to a local maxima of f , which appears at a density level approximately \u03bb. This follows from the fact that the CCs of G(\u03bb) approximates the CCs of {x \u2208 X : f(x) \u2265 \u03bb}. Thus, the idea is that when a new CC appears in G(\u03bb), then we can take the corresponding CC in G(\u03bb\u2212 \u03b2\u03bb) (which is the density level (1\u2212 \u03b2) times that of the highest point in the CC) to estimate the cluster-core.\nAlgorithm 1 MCores (estimating cluster-cores) Parameters k, \u03b2 Initialize M\u0302 := \u2205. Sort the xi\u2019s in decreasing order of fk values (i.e. fk(xi) \u2265 fk(xi+1)). for i = 1 to n do\nDefine \u03bb := fk(xi). Let A be the CC of G(\u03bb\u2212 \u03b2\u03bb) containing xi. if A is disjoint from all cluster-cores in M\u0302 then\nAdd A to M\u0302. end if\nend for return M\u0302.\nAlgorithm 2 Quickshift++\nLet M\u0302 be the cluster-cores obtained by running Algorithm 1. Initialize directed graph G with vertices {x1, ..., xn} and no edges. for i = 1 to n do\nIf xi is not in any cluster-core, then add to G an edge from xi to its closest sample x \u2208 X[n] such that fk(x) > fk(xi). end for For each cluster-core M \u2208 M\u0302, let C\u0302M be the points x \u2208 X[n] such that the directed path in G starting at x ends in M . return {C\u0302M :M \u2208 M\u0302}."}, {"heading": "3.3. Quickshift++", "text": "Quickshift++ (Algorithm 2) proceeds by first running Algorithm 1 to obtain the cluster-cores, and then moving each sample point to its nearest neighbor that has higher k-NN density until it reaches some cluster-core. All samples that end up in the same cluster-core after the hill-climbing are assigned to the same cluster. Note that since the highest empirical density sample point is contained in a cluster-core, it follows that each sample point not in a cluster-core will eventually be assigned to a unique cluster-core. Thus, Quickshift++ provides a clustering assignment of every sample point. Remark 1. Although it seems a similar procedure could have been constructed by using Mean Shift in place of Quick Shift, Mean Shift could have convergence outside of the estimated cluster-cores, while Quick Shift guarantees that each sample outside of a cluster-core get assigned to some cluster-core."}, {"heading": "3.4. Implementation", "text": "The implementation details for the MCores modification can be inferred from Jiang & Kpotufe (2017). This step runs in O(nk \u00b7 \u03b1(n)) where \u03b1 is the Inverse Ackermann function (Cormen, 2009), in addition to the time it takes to compute the k-NN sets for the n sample points. To cluster the remaining points, for each sample not in a cluster-core, we must find its nearest sample of higher k-NN density. Although this is worst-case O(n) time for each sample point, fortunately we see that in practice (as long as k is not too small) for the vast majority of cases, the nearest sample with higher density is within the k-nearest neighbor set so it only takes O(k) in most cases. It is an open problem whether there the nearest sample with higher empirical density is often in its k-NN set. Code release is at https://github.com/google/quickshift."}, {"heading": "4. Theoretical Analysis", "text": "For the theoretical analysis, we make first the following regularity assumption, that the density is continuously differentiable and lower bounded on X . Assumption 1. f is continuously differentiable on X and there exists \u03bb0 > 0 such that infx\u2208X f(x) \u2265 \u03bb0.\nLet M1, ...,MC be the cluster-cores of f . Then we can define the following notion of attraction region for each cluster-core based on the gradient ascent curve or flow. This is similar to notions of attraction regions for some previous analyses of mode-based clustering, such as Wasserman et al. (2014); Arias-Castro et al. (2016), where the intuition is that attraction regions are defined based by following the direction of the gradient of the underlying density. In our situation, instead of an attraction region defined as all points\nwhich flow towards a particular point-mode, the attraction region is defined around a cluster-core.\nDefinition 4 (Attraction Regions). Let path \u03c0x : R\u2192 Rd satisfy \u03c0x(0) = x, \u03c0\u2032x(t) = \u2207f(\u03c0x(t)). For cluster-core Mi, its attraction region Ai is the set of points x \u2208 X that satisfy limt\u2192\u221e \u03c0x(t) \u2208Mi.\nIt is clear that these attraction regions are well-defined. The flow path is well-defined since the density is differentiable and since each cluster-core is defined as a CC of a level set, the density must decay around its boundaries. In other words, once an ascent path reaches a cluster-core, it cannot leave the cluster-core.\nHowever, it is in general not the case that the space can be partitioned into attraction regions. For example, if a flow reaches a saddle point, it will get stuck there and thus any point whose flow ends up at a saddle point will not belong to any attraction region. In this paper, we only give guarantees about the clustering of points which are in an attraction region.\nThe next regularity assumption we make is that the clustercores are on the interior of the attraction region (to avoid situations such as when the cluster-cores intersect with the boundary of the input space).\nAssumption 2. There exists R0 > 0 such that Mi + B(0, R0) \u2286 Ai for i = 1, ..., C, where M + B(0, r) denotes {x : infy\u2208M ||x\u2212 y|| \u2264 r}. Definition 5 (Level Set). The \u03bb level set of f is defined as Lf (\u03bb) := {x \u2208 X : f(x) \u2265 \u03bb}.\nThe next assumption says that the level sets are continuous w.r.t. the level in the following sense where we denote the -interior of A as A := {x \u2208 A, infy\u2208\u2202A ||x \u2212 y|| \u2265 } (\u2202A is the boundary of A):\nAssumption 3 (Uniform Continuity of Level Sets). For each > 0, there exists \u03b4 > 0 such that for 0 < \u03bb \u2264 \u03bb\u2032 \u2264 ||f ||\u221e with |\u03bb\u2212 \u03bb\u2032| < \u03b4, then Lf (\u03bb) \u2286 Lf (\u03bb\u2032).\nThis ensures that there are no approximately flat areas in which the procedure may get stuck at. The assumption is borrowed from (Jiang, 2017b). Finally, we need the following regularity condition which ensures that level sets away from cluster-cores do not get arbitrarily thin. This is adapted from standard analyses of level-set estimation (e.g. Assumption B of Singh et al. (2009)).\nAssumption 4. Let \u00b5 denote the Lebesgue measure on Rd. For any r > 0, there exists \u03c3 > 0 such that the following holds for any connected component A of any level-set of f which is not contained inMi for any i: \u00b5(B(x, r)\u2229A) \u2265 \u03c3 for all x \u2208 A.\nFor our consistency results, we prove that Quickshift++ can cluster the sample points in the (R, \u03c1)-interior of an attrac-\ntion region (defined below) for each cluster-core properly where R, \u03c1 > 0 are fixed and can be chosen arbitrarily small.\nDefinition 6 ((R, \u03c1)-interior of Attraction Regions). Define the (R, \u03c1)-interior of Ai, denoted as A(R,\u03c1)i , as the set of points x0 \u2208 Ai such that each path P from x0 to any point y \u2208 \u2202Ai satisfies the following.\nsup x\u2208P inf x\u2032\u2208B(x,R) f(x\u2032) \u2265 sup x\u2032\u2208B(y,R) f(x\u2032) + \u03c1.\nIn other words, points in the interior satisfy the property that any path leaving its attraction region must sufficiently decrease in density at some point. This decrease threshold is parameterized by R and \u03c1.\nWe first give a guarantee on the first step of MCores recovers, that the cluster-cores are reasonably recovered. The proof follows from the analysis of Jiang & Kpotufe (2017) by replacing modal-sets with cluster-cores, and the results match up to constant factors. The proof is omitted here.\nTheorem 1. [Adapted from Theorem 3, 4 of Jiang & Kpotufe (2017)] Suppose that Assumptions 1, 3, and 4 hold. Let 0 < \u03b2 < 1, , \u03b4 > 0 and suppose that k \u2261 k(n) is chosen such that log2 n/k \u2192 0 and n4/(4+d)/k \u2192 0. Let M1, ...,MC be the cluster-cores of f . Then for n sufficiently large depending on f , \u03b4, , and \u03b2, with probability at least 1\u2212 \u03b4, MCores returns C cluster-core estimates M\u03021, ..., M\u0302C such that Mi \u2229X[n] \u2286 M\u0302i \u2286Mi+B(0, ) for i \u2208 1, ..., C. Remark 2. The original result from Jiang & Kpotufe (2017) is about -approximate modal-set which are defined as levelsets whose density has range . Our notion of cluster-core is similar, but the range is a \u03b2-proportion of the highest density level within the level-set. Using a proportion is more interpretable and thus more useful, as the scale of the density function is difficult to determine in practice.\nIn other words, with high probability, MCores estimates each cluster-core bijectively and that for each cluster-core, MCores\u2019 estimate contains all of the sample points and that the estimate does not over-estimate by much.\nWe now state the main result, which says that as long as the cluster-cores are sufficiently well estimated (up to a certain Hausdorff error) by MCores (via previous theorem), then Quickshift++ will correctly cluster the (R, \u03c1)-interiors of the attraction regions with high probability.\nTheorem 2. Suppose that Assumptions 1, 2, 3, and 4 hold. Let 0 < R < R0 and \u03c1, \u03b4 > 0. Suppose that k \u2261 k(n) is chosen such that log2 n/k \u2192 0 and n4/(4+d)/k \u2192 0. Suppose that M\u03021, ..., M\u0302C are the cluster-cores returned by Algorithm 1 and satisfyMi\u2229X[n] \u2286 M\u0302i \u2286Mi+B(0, R/4) for i = 1, ..., C. Then for n sufficiently large depending on f , \u03c1, \u03b4 and R, the following holds with probably at least 1 \u2212 2\u03b4 uniformly in x \u2208 A(R,\u03c1)i \u2229X[n] and i \u2208 [C]: Quickshift++ clusters x to the cluster corresponding to Mi."}, {"heading": "4.1. Proof of Theorem 2", "text": "We require the following uniform bound on k-NN density estimator, which follows from Dasgupta & Kpotufe (2014).\nLemma 2. Let \u03b4 > 0. Suppose that f is Lipschitz continuous with compact support X (e.g. there exists L such that |f(x)\u2212f(x\u2032)| \u2264 L|x\u2212x\u2032| for all x, x\u2032 \u2208 X ) and f satsifies Assumption 1. Then exists constant C depending on f such that the following holds if n \u2265 C2\u03b4,n with probability at least 1\u2212 \u03b4.\nsup x\u2208X |fk(x)\u2212 f(x)| \u2264 C ( C\u03b4,n\u221a k + ( k n )1/d) .\nwhere C\u03b4,n := 16 log(2/\u03b4) \u221a d log n.\nWe next need the following uniform concentration bound on balls intersected with level-sets, which says that if such a set has large enough probability mass, then it will contain a sample point with high probability.\nLemma 3. Let E := {B(x, r) \u2229 Lf (\u03bb) : x \u2208 Rd, r > 0, \u03bb > 0}. Then the following holds with probability at least 1\u2212 \u03b4 uniformly for all E \u2208 E\nF(E) \u2265 C\u03b4,n \u221a d log n\nn \u21d2 E \u2229Xn 6= \u2205.\nProof. The indicator functions 1[B(x, f) \u2229 Lf (\u03bb)] for x \u2208 Rd, \u03bb > 0 have VC-dimension d+ 1. This is because the balls over Rd have VC-dimension d+ 1 and the level-sets Lf (\u03bb) has VC-dimension 1 and thus their intersection has VC-dimension d+ 1 (Van Der Vaart & Wellner, 2009). The result follows by applying Theorem 15 of Chaudhuri & Dasgupta (2010).\nProof of Theorem 2. Suppose that x0 \u2208 A(R,\u03c1)i \u2229X[n] and Quickshift++ gives directed path x0 \u2192 x1 \u2192 \u00b7 \u00b7 \u00b7 \u2192 xL where x1, ..., xL\u22121 are outside of cluster-cores and xL is in a cluster-core but xL 6\u2208 Ai.\nWe first show that ||xi \u2212 xi+1|| \u2264 R/2 for i = 0, ..., L\u2212 1. By Assumption 3 and 4, we have that there exists \u03c4 > 0 and \u03c3 > 0 such that the following holds uniformly for i = 0, ..., L\u2212 1:\n\u00b5 ( B(xi, R/2) \u2229 Lf (f(xi) + \u03c4) ) \u2265 \u03c3.\nHence, since the density is uniformly lower bounded by \u03bb0, we have\nF ( B(xi, R/2) \u2229 Lf (f(xi) + \u03c4) ) \u2265 \u03c3\u03bb0.\nThen by Lemma 3, for n suffiicently large such that \u03c3\u03bb0 > C\u03b4,n \u221a d logn n , then with probability at least 1\u2212 \u03b4 there exists sample point x\u2032i in B(xi, R/2) \u2229 Lf (f(xi) + \u03c4) for i = 0, ..., L\u2212 1.\nNext, choose n sufficiently large such that by Lemma 2, we have with probability at least 1\u2212 \u03b4 that\nsup x\u2208X |fk(x)\u2212 f(x)| \u2264 min{\u03c4, \u03c1}/3.\nThus, we have\nfk(x \u2032 i) \u2265 f(x\u2032i)\u2212 \u03c4/3 \u2265 f(xi) + 2\u03c4/3 \u2265 fk(xi) + \u03c4/3 > fk(xi).\nMoreover ||xi \u2212 x\u2032i|| \u2264 R/2 and x\u2032i \u2208 X[n], it follows that ||xi \u2212 xi+1|| \u2264 R/2 for i = 0, ..., L\u2212 1.\nLet \u03c0 : [0, 1]\u2192 Rd be the piecewise linear path defined by \u03c0(j/L) = xj for j = 0, ..., L. Let t2 = min{t \u2208 [0, 1] : \u03c0(t) \u2208 \u2202Ai}. Then, by definition of A(R,\u03c1)i , there exists 0 \u2264 t1 < t2 such that x := \u03c0(t1) and y := \u03c0(t2) satisfies y \u2208 \u2202Ai and\ninf x\u2032\u2208B(x,R) f(x\u2032) \u2265 sup x\u2032\u2208B(y,R) f(x\u2032) + \u03c1.\nThus, there exists indices p, q \u2208 {0, ..., L \u2212 1} such that p \u2264 q, |xp \u2212 x| \u2264 R, and |xq \u2212 y| \u2264 R. Thus, we have f(xp) \u2265 f(xq) + \u03c1, but fk(xp) \u2264 fk(xq). However, we have\nfk(xp) \u2265 f(xp)\u2212 \u03c1/3 \u2265 f(xq) + 2\u03c1/3 \u2265 fk(xq) + \u03c1/3 > fk(xq),\na contradiction, as desired."}, {"heading": "5. Simulations", "text": "Figure 3 provides simple verification that Quickshift++ provides reasonable clusterings in a wide variety of situations where other density-based procedures are known to fail. For instance, in the two rings dataset (first row), we\nsee that Mean Shift and Quick Shift suffer from the oversegmentation issue coupled with the oversized bandwidth which causes them to recover clusters that have points from both the rings even though the rings are separated. In the three Gaussians dataset (third row), we see that DBSCAN fails because the three clusters are of different density levels and thus no matter which density-level we set, DBSCAN will not be able to recover the three clusters."}, {"heading": "6. Image Segmentation", "text": "In order to apply clustering to image segmentation, we use the following standard approach (see e.g. Felzenszwalb & Huttenlocher (2004)): we transform each pixel into a 5-dimensional vector where two coordinates correspond to the location of the pixel and three correspond to each of the RGB color channels. Then segmentation is done by clustering this 5-dimensional dataset.\nWe observed that for Quickshift++, setting \u03b2 = 0.9 is reasonable across a wide range of images, \u03b2 was fixed to this value for segmentation here. We compare Quickshift++ to Quick Shift, as the latter is often used for segmentation. Quick Shift often over-segments in some areas and undersegments in other areas under any hyperparameter setting and we showed the settings which provided a reasonable trade-off. On the other hand Quickshift++ gives us reason-\nable segmentations in many cases and can capture segments that may be problematic for other procedures.\nAs shown in the figures, it moreover has the interesting property of being able to recover segments of widely varying shapes and sizes in the same image, which suggests that modelling the dense regions of the segments as cluster-cores instead of point-modes may be useful as we compare to Quick Shift. Although this is only qualitative, it further suggests that Quickshift++ is a versatile algorithm and begins to show its potential application in many more areas."}, {"heading": "7. Clustering Experiments", "text": "We ran Quickshift++ against other clustering algorithms on the various real datasets and scored against the groundtruth using the adjusted rand index and the adjusted mutual information scores.\nDatasets Used: Summary of the datasets can be found in Figure 8. Seeds, glass, and iris are standard UCI datasets (Lichman, 2013) used for clustering. Banknote is another UCI dataset which involves identifying whether a banknote is forged or not, based on various statistics of an image of the banknote. Page Blocks is a UCI dataset which involves determining the type of a portion of a page (e.g. text, image, etc) based on various statistics of an image of the portion. Phonemes (Friedman et al., 2001) is a dataset which involves the log periodograms of spoken phonemes. Images is a UCI dataset called Statlog, based on features extracted from various images, and letters is the UCI letter recognition dataset. We also used a small subset of MNIST (LeCun et al., 2010) for our experiments.\nWe evaluate performance under the Adjusted Mutual Information and Rand Index scores (Vinh et al., 2010) which are metrics to compare clusterings. Not only do we show that Quickshift++ considerably outperforms the popular densitybased clustering procedures under optimal tuning (Figure 9), but that it is also robust in its hyperparameter k (Figure 7), all while fixing \u03b2 = 0.3 for all but one of the datasets. Such robustness to its tuning parameters is highly desirable since optimal tuning is usually not available in practice."}, {"heading": "8. Conclusion", "text": "We presented Quickshift++, a new density-based clustering procedure that first estimates the cluster-cores of the density, which are locally high-density regions. Then remaining points are assigned to its appropriate cluster-core using a hill-climbing procedure based on Quick Shift. Such clustercores turn out to be more stable and expressive representations of the possibly complex clusters than point-modes. As a result, Quickshift++ enjoys the advantages of the popular density-based clustering algorithms while avoiding many of their respective weaknesses. We then gave guarantees for cluster recovery. Finally, we showed that the algorithm has strong and robust performance on real datasets and has promising applications to image segmentation."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their helpful feedback."}], "year": 2018, "references": [{"title": "On the estimation of the gradient lines of a density and the consistency of the mean-shift algorithm", "authors": ["E. Arias-Castro", "D. Mason", "B. Pelletier"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Cluster trees on manifolds", "authors": ["S. Balakrishnan", "S. Narayanan", "A. Rinaldo", "A. Singh", "L. Wasserman"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Kernel estimation of density level sets", "authors": ["B. Cadre"], "venue": "Journal of multivariate analysis,", "year": 2006}, {"title": "Clusters and water flows: a novel approach to modal clustering through morse theory", "authors": ["J.E. Chac\u00f3n"], "venue": "arXiv preprint arXiv:1212.1384,", "year": 2012}, {"title": "Rates of convergence for the cluster tree", "authors": ["K. Chaudhuri", "S. Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Consistent procedures for cluster tree estimation and pruning", "authors": ["K. Chaudhuri", "S. Dasgupta", "S. Kpotufe", "U. von Luxburg"], "venue": "IEEE Transactions on Information Theory,", "year": 2014}, {"title": "A comprehensive approach to mode clustering", "authors": ["Chen", "Y.-C", "C.R. Genovese", "L Wasserman"], "venue": "Electronic Journal of Statistics,", "year": 2016}, {"title": "Density level sets: Asymptotics, inference, and visualization", "authors": ["Chen", "Y.-C", "C.R. Genovese", "L. Wasserman"], "venue": "Journal of the American Statistical Association,", "year": 2017}, {"title": "Mean shift, mode seeking, and clustering", "authors": ["Y. Cheng"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "year": 1995}, {"title": "Mean shift: A robust approach toward feature space analysis", "authors": ["D. Comaniciu", "P. Meer"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "year": 2002}, {"title": "Introduction to algorithms", "authors": ["T.H. Cormen"], "venue": "MIT press,", "year": 2009}, {"title": "Optimal rates for k-nn density and mode estimation", "authors": ["S. Dasgupta", "S. Kpotufe"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "A densitybased algorithm for discovering clusters in large spatial databases with noise", "authors": ["M. Ester", "Kriegel", "H.-P", "J. Sander", "X Xu"], "venue": "In Kdd,", "year": 1996}, {"title": "Efficient graphbased image segmentation", "authors": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "International journal of computer vision,", "year": 2004}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "New York,", "year": 2001}, {"title": "Non-parametric inference for density modes", "authors": ["C.R. Genovese", "M. Perone-Pacifico", "I. Verdinelli", "L. Wasserman"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2016}, {"title": "Clustering algorithms, volume 209", "authors": ["J.A. Hartigan"], "venue": "Wiley New York,", "year": 1975}, {"title": "Density level set estimation on manifolds with dbscan", "authors": ["H. Jiang"], "venue": "arXiv preprint arXiv:1703.03503,", "year": 2017}, {"title": "On the consistency of quick shift", "authors": ["H. Jiang"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2017}, {"title": "Modal-set estimation with an application to clustering", "authors": ["H. Jiang", "S. Kpotufe"], "venue": "In Artificial Intelligence and Statistics,", "year": 2017}, {"title": "Pruning nearest neighbor cluster trees", "authors": ["S. Kpotufe", "U. von Luxburg"], "venue": "arXiv preprint arXiv:1105.0540,", "year": 2011}, {"title": "Mnist handwritten digit database", "authors": ["Y. LeCun", "C. Cortes", "C. Burges"], "venue": "AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist,", "year": 2010}, {"title": "A nonparametric statistical approach to clustering via mode identification", "authors": ["J. Li", "S. Ray", "B.G. Lindsay"], "venue": "Journal of Machine Learning Research,", "year": 2007}, {"title": "Optimal rates for plug-in estimators of density level", "authors": ["P. Rigollet", "R Vert"], "venue": "sets. Bernoulli,", "year": 2009}, {"title": "Generalized density clustering", "authors": ["A. Rinaldo", "L. Wasserman"], "venue": "The Annals of Statistics, pp. 2678\u20132722,", "year": 2010}, {"title": "Clustering by fast search and find of density", "authors": ["A. Rodriguez", "A. Laio"], "venue": "peaks. Science,", "year": 2014}, {"title": "Adaptive hausdorff estimation of density level sets", "authors": ["A. Singh", "C. Scott", "R Nowak"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "Consistency and rates for clustering with dbscan", "authors": ["B. Sriperumbudur", "I. Steinwart"], "venue": "In Artificial Intelligence and Statistics,", "year": 2012}, {"title": "Adaptive density level set clustering", "authors": ["I. Steinwart"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory, pp", "year": 2011}, {"title": "On nonparametric estimation of density level sets", "authors": ["Tsybakov", "A. B"], "venue": "The Annals of Statistics,", "year": 1997}, {"title": "A note on bounds for vc dimensions", "authors": ["A. Van Der Vaart", "J.A. Wellner"], "venue": "Institute of Mathematical Statistics collections,", "year": 2009}, {"title": "Quick shift and kernel methods for mode seeking", "authors": ["A. Vedaldi", "S. Soatto"], "venue": "vision\u2013ECCV", "year": 2008}, {"title": "Analysis of a mode clustering diagram", "authors": ["I. Verdinelli", "L. Wasserman"], "venue": "arXiv preprint arXiv:1805.04187,", "year": 2018}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "authors": ["N.X. Vinh", "J. Epps", "J. Bailey"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "Feature selection for high-dimensional clustering", "authors": ["L. Wasserman", "M. Azizyan", "A. Singh"], "venue": "arXiv preprint arXiv:1406.2240,", "year": 2014}], "id": "SP:2dcc8bbaa3d2011252a1f86869222395c1fbefb8", "authors": [{"name": "Heinrich Jiang", "affiliations": []}, {"name": "Jennifer Jang", "affiliations": []}, {"name": "Samory Kpotufe", "affiliations": []}], "abstractText": "We provide initial seedings to the Quick Shift clustering algorithm, which approximate the locally high-density regions of the data. Such seedings act as more stable and expressive cluster-cores than the singleton modes found by Quick Shift. We establish statistical consistency guarantees for this modification. We then show strong clustering performance on real datasets as well as promising applications to image segmentation.", "title": "Quickshift++: Provably Good Initializations for Sample-Based Mean Shift"}