27	22	In this work, we focus on learning an optimal policy for this task using reinforcement learning, in the spirit of other recent attempts to learn policies for different types of active learning (Fang et al., 2017; Woodward and Finn, 2017).
29	21	Our learned policy outperforms a static baseline by improving its success rate on object retrieval while asking fewer questions on average.
30	14	The learned policy also learns to distribute queries more uniformly across concepts than the baseline.
56	70	Opportunistic Active Learning (OAL) is a setting that incorporates active learning queries into interactive tasks.
59	32	For the problem of understanding natural-language object descriptions, O corresponds to the set of objects, M corresponds to the set of possible concepts that can be used to describe the objects, for example their categories (such as ball or bottle) or perceptual properties (such as red or tall).
70	18	It is restricted to the set of objects available in the current interaction, OA.
85	23	Before guessing, the agent is allowed to ask queries of the following two types: • Label queries - A yes/no question about whether a predicate can be used to describe one of the objects in the active training set, e.g. “Is this object yellow?”.
93	23	Bounding boxes of objects present in the image are also annotated, along with attributes of objects.
95	78	Using the annotations, we can associate a predicate in the active training set, but we chose to return a single example to allow the agent to minimize the amount of supervision obtained list of objects and attributes relevant to each image region, and use these to answer queries from the agent.
99	73	The objects and attributes associated with active training regions are used to answer queries.
106	15	The agent learns a separate binary classifier for each predicate, and we represent images with a “deep” feature representation obtained from the penultimate layer of the VGG network (Simonyan and Zisserman, 2014) pretrained on ImageNet (Russakovsky et al., 2015).
111	25	3 Then the best guess, from the objects present, is chosen using the weighted sum of the decisions of the classifiers, using their estimated F1 as a weight: oguess = argmaxo∈OA k∑ i=1 d(pi, o) ∗ C(pi)
112	20	We model the task as a Markov Decision Process (MDP).
124	14	If OA is the set of objects present in the active training set of the current interaction, and P is the set of predicates that have been seen by the agent in all interactions so far, then the set of possible label queries is P × OA.
140	19	The agent asks a fixed number of queries before guessing.
143	17	We use the REINFORCE algorithm (Williams, 1992) to learn a policy for the MDP.
146	14	Since both the number of candidate objects and classifiers varies, and the latter is quite large, it is necessary to identify useful features for the task to obtain a vector representation needed by most learning algorithms.
155	12	• Highest score among regions in the active test set, and the differences between this and the second highest, and average scores respectively – a good guess is expected to have a high score to indicate relevance to the description, and substantial differences would indicate that the guess is discriminative.
158	23	We compared directly using these features to training a regressor that uses them to predict the probability of a successful guess, and then using this as a higher-level policy feature.
164	15	Label queries also have an image region specified, and for these we have additional features that use the VGG feature space in which the region is represented for classification: • Margin of the image region from the hyperplane of the classifier of the predicate – motivated by uncertainty sampling.
185	28	This “supervised” learning phase is used to initialize the RL policy.
190	12	We do this to ensure that performance improvements seen at test time are purely from learning a strategy for opportunistic active learning, not from acquiring useful classifiers in the process of learning the policy.
193	13	Table 1 compares the average success rate (fraction of successful dialogs in which the correct object is identified), and average dialog length (average number of system turns) of the best learned policy, and the baseline static policy on the final batch of testing.
195	41	The learned agent guesses correctly in a significantly higher fraction of dialogs compared to the static agent, using a significantly lower number of questions per dialog.
196	25	When either the group of guess or query features is ablated, the success rate clearly decreases.
197	12	While the mean success rate still remains above the baseline, the difference is no longer statistically significant.
224	14	The simulation could also potentially be improved using positiveunlabeled learning methods (Liu et al., 2002; Li and Liu, 2003) instead of assuming that an object or attribute not labeled in an image region is not present in the image.
225	171	It would also be interesting to compare the effectiveness of the opportunistic active learning framework, as well as the policy learning, across a variety of applications.
226	61	This paper has shown how to formulate an opportunistic active learning problem as a reinforcement learning problem, and learn a policy that can effectively trade-off opportunistic active learning queries against task completion.
227	28	We evaluated this approach on the task of grounded object retrieval from natural language descriptions and learn a policy that retrieves the correct object in a larger fraction of dialogs than a previously proposed static baseline, while also lowering average dialog length.
