0	89	The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics.
1	157	Algorithms solving these problems are often used to extract features to compare large-scale datasets, as well as used for problems in regression (Kakade & Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis & Mineiro, 2014), word embeddings (Dhillon et al., 2011), and many others.
2	81	Given two symmetric matrices A,B ∈ Rd×d whereB is positive definite.
3	172	The GenEV problem is to find generalized eigenvectors v1, .
4	15	, vd where each vi satisfies vi ∈ arg max v∈Rd ∣∣v>Av∣∣ s.t.
5	143	{ v>Bv = 1 v>Bvj = 0 ∀j ∈ [i− 1] The values λi def = v>i Avi are known as the generalized eigenvalues, and it satisfies |λ1| ≥ · · · |λd|.
6	46	Following the  tradition of (Wang et al., 2016; Garber & Hazan, 2015), we assume without loss of generality that λi ∈ [−1, 1].
7	11	Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and denoting by Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY >Y , the CCA problem is to find canonical-correlation vectors {(φi, ψi)}ri=1 where r = min{dx, dy} and each pair (φi, ψi) ∈ arg max φ∈Rdx ,ψ∈Rdy { φ>Sxyψ } such that { φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈ [i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈ [i− 1] The values σi def = φ>i Sxyψi ≥ 0 are known as the canonical-correlation coefficients, and 1 ≥ σ1 ≥ · · · ≥ σr ≥ 0 is always satisfied.
8	49	It is a fact that solving CCA exactly can be reduced to solving GenEV exactly, if one defines B = diag{Sxx, Syy} ∈ Rd×d andA = [[0, Sxy]; [S>xy, 0]] ∈ Rd×d for d def = dx+dy; see Lemma 2.3.
9	42	(This reduction does not always hold if the generalized eigenvectors are computed only approximately.)
10	34	Despite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently.
11	3	In the breakthrough result of Ma, Lu and Foster (Ma et al., 2015), they proposed to study algorithms to find top k generalized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA).
12	27	They designed an alternating minimization algorithm whose running time is only linear in the input matrix sparsity and nearly-linear in k. Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate.
13	88	Unfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with κ and 1/gap, where • κ ≥ 1 is the condition number of matrix B in GenEV, or of matrices X>X,Y >Y in CCA; and • gap ∈ [0, 1) is the eigengap λk−λk+1λk in GenEV, or σk−σk+1 σk in CCA.
14	20	These parameters are usually not constants and scale with the problem size.
