1	10	Gradient descent and its variants (e.g., stochastic gradient) are widely used in machine learning applications due to their favorable computational properties.
2	59	This is notably true in the deep learning setting, where gradients can be computed efficiently via backpropagation (Rumelhart et al., 1988).
3	18	Gradient descent is especially useful in high-dimensional settings because the number of iterations required to reach a point with small gradient is independent of the dimension (“dimension-free”).
6	23	This bound does not depend on the dimension of x.
7	41	In convex optimization, finding an -first-order stationary point is equivalent to finding an approximate global optimum.
8	16	In non-convex settings, however, convergence to first-order stationary points is not satisfactory.
9	118	For non-convex functions, first-order stationary points can be global minima, local minima, saddle points or even local maxima.
10	127	Finding a global minimum can be hard, but fortunately, for many non-convex problems, it is sufficient to find a local minimum.
11	16	Indeed, a line of recent results show that, in many problems of interest, all local minima are global minima (e.g., in tensor decomposition (Ge et al., 2015), dictionary learning (Sun et al., 2016a), phase retrieval (Sun et al., 2016b), matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016), matrix completion (Ge et al., 2016), and certain classes of deep neural networks (Kawaguchi, 2016)).
13	65	On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems (see, e.g., Jain et al., 2015; Sun et al., 2016b).
15	148	Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible.
16	91	Ge et al. (2015) showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2).
17	28	Lee et al. (2016) proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding Algorithm 1 Perturbed Gradient Descent (Meta-algorithm) for t = 0, 1, .
18	29	do if perturbation condition holds then xt ← xt + ξt, ξt uniformly ∼ B0(r) xt+1 ← xt − η∇f(xt) noise.
20	137	Previous work explains why gradient descent avoids saddle points in the nonconvex setting, but not why it is efficient—all of them have runtime guarantees with high polynomial dependency in dimension d. For instance, the number of iterations required in Ge et al. (2015) is at least Ω(d4), which is prohibitive in high dimensional setting such as deep learning (typically with millions of parameters).
21	41	Therefore, we wonder whether gradient descent type of algorithms are fundamentally slow in escaping saddle points, or it is the lack of our theoretical understanding while gradient descent is indeed efficient.
22	73	This motivates the following question: Can gradient descent escape saddle points and converge to local minima in a number of iterations that is (almost) dimension-free?
23	37	In order to answer this question formally, this paper investigates the complexity of finding -second-order stationary points.
24	81	For ρ-Hessian Lipschitz functions (see Definition 5), these points are defined as (Nesterov & Polyak, 2006): ‖∇f(x)‖ ≤ , and λmin(∇2f(x)) ≥ − √ ρ .
25	156	Under the assumption that all saddle points are strict (i.e., for any saddle point xs, λmin(∇2f(xs)) < 0), all second-order stationary points ( = 0) are local minima.
28	34	For `-smooth functions that are also Hessian Lipschitz, we show that perturbed gradient descent will converge to an -second-order stationary point in Õ(`(f(x0)− f?
29	25	)/ 2), where Õ(·) hides polylog factors.
30	19	This guarantee is almost dimension free (up to polylog(d) factors), answering the above highlighted question affirmatively.
31	26	Note that this rate is exactly the same as the well-known convergence rate of gradient descent to first-order stationary points (Nesterov, 1998), up to log factors.
32	32	Furthermore, our analysis admits a maximal step size of up to Ω(1/`), which is the same as that in analyses for first-order stationary points.
33	18	As many real learning problems present strong local geometric properties, similar to strong convexity in the global setting (see, e.g. Bhojanapalli et al., 2016; Sun & Luo, 2016; Zheng & Lafferty, 2016), it is important to note that our analysis naturally takes advantage of such local struc- ture.
36	164	This paper presents the first sharp analysis that shows that (perturbed) gradient descent finds an approximate secondorder stationary point in at most polylog(d) iterations, thus escaping all saddle points efficiently.
37	32	Our main technical contributions are as follows: • For `-gradient Lipschitz, ρ-Hessian Lipschitz functions (possibly non-convex), gradient descent with appropriate perturbations finds an -second-order stationary point in Õ(`(f(x0)−f?
40	66	This means that gradient descent can escape all saddle points with only logarithmic overhead in runtime.
41	41	• When the function has local structure, such as local strong convexity (see Assumption A3.a), the above results can be further improved to linear convergence.
