0,1,label2,summary_sentences
"ar X
iv :1
80 2.
06 09
3v 4
[ cs
.L G",text,[0],[0]
"Residual networks (He et al., 2016) are deep neural networks in which, roughly, subnetworks determine how a feature transformation should differ from the identity, rather than how it should differ from zero.",1 Introduction,[0],[0]
"After enabling the winning entry in the ILSVRC 2015 classification task, they have become established as a central idea in deep networks.
",1 Introduction,[0],[0]
Hardt & Ma (2017) provided a theoretical analysis that shed light on residual networks.,1 Introduction,[0],[0]
"They showed that (a) any linear transformation with a positive determinant and a bounded condition number can be approximated by a “deep linear network” of the form f(x) = ΘLΘL−1...Θ1x, where,
for large L, each layer Θi is close to the identity, and (b) for networks that compose near-identity transformations this way, if the excess loss is large, then the gradient is steep.",1 Introduction,[0],[0]
"Bartlett et al. (2018) extended both results to the nonlinear case, showing that any smooth, bi-Lipschitz map can be represented as a composition of near-identity functions, and that a suboptimal loss in a composition of near-identity functions implies that the functional gradient of the loss with respect to a function in the composition cannot be small.",1 Introduction,[0],[0]
"These results are interesting because they suggest that, in many cases, this non-convex objective may be efficiently optimized through gradient descent if the layers stay close to the identity, possibly with the help of a regularizer.
",1 Introduction,[0],[0]
"This paper describes and analyzes such algorithms for linear regression with d input variables and d response variables with respect to the quadratic loss, the same setting analyzed by Hardt and Ma.",1 Introduction,[0],[0]
We abstract away sampling issues by analyzing an algorithm that performs gradient descent with respect to the population loss.,1 Introduction,[0],[0]
We focus on the case that the distribution on the input patterns is isotropic.,1 Introduction,[0],[0]
(The data may be transformed through a preprocessing step to satisfy this constraint.),1 Introduction,[0],[0]
"
",1 Introduction,[0],[0]
"The traditional analysis of convex optimization algorithms (see Boyd & Vandenberghe, 2004) provides a bound in terms of the quality of the initial solution, together with bounds on the eigenvalues of the Hessian of the loss.",1 Introduction,[0],[0]
"For the non-convex problem of this paper, we show that if gradient descent starts at the identity in each layer, and if the excess loss of that initial solution is bounded by a constant, then the Hessian remains well-conditioned enough throughout training for successful learning.",1 Introduction,[0],[0]
"Specifically, there is a constant c0 such that, if the excess loss of the identity (over the least squares linear map) is at most c0, then back-propagation initialized at the identity in each layer achieves loss within at most ǫ of optimal in time polynomial in log(1/ǫ), d, and L (Section 3).",1 Introduction,[0],[0]
"On the other hand, we show that there is a constant c1 and a least squares matrix Φ such that the identity has excess loss c1 with respect to Φ, but backpropagation with identity initialization fails to learn Φ (Section 6).
",1 Introduction,[0],[0]
"We also show that if the least squares matrix Φ is symmetric positive definite then gradient descent with identity initialization achieves excess loss at most ǫ in a number of steps bounded by a polynomial in log(d/ǫ), L and the condition number of Φ (Section 4).
",1 Introduction,[0],[0]
"In contrast, for any least squares matrix Φ that is symmetric but has a negative eigenvalue, we show that no such guarantee is possible for a wide variety of algorithms of this type: the excess loss is forever bounded below by the square of this negative eigenvalue.",1 Introduction,[0],[0]
"This holds for step-and-project algorithms, and also algorithms that initialize to the identity and regularize by early stopping or penalizing ∑
i ||Θi − I|| 2 F (Section 6).",1 Introduction,[0],[0]
"Both this and the previous impossibility result can be
proved using a least squares matrix Φ with a positive determinant and a good condition number.",1 Introduction,[0],[0]
"Recall that such Φ were proved by Hardt and Ma to have a good approximation as a product of near-identity matrices – we prove that gradient descent cannot learn them, even with the help of regularizers that reward near-identity representations.
",1 Introduction,[0],[0]
"In Section 5 we provide a convergence guarantee for a least squares matrix Φ that may not be symmetric, but satisfies the positivity condition u⊤Φu",1 Introduction,[0],[0]
> γ for some γ > 0,1 Introduction,[0],[0]
that appears in the bounds.,1 Introduction,[0],[0]
We call such matrices γ-positive.,1 Introduction,[0],[0]
Such Φ include rotations by acute angles.,1 Introduction,[0],[0]
"In this case, we consider an algorithm that regularizes in addition to a near-identity initialization.",1 Introduction,[0],[0]
"After the gradient update, the algorithm performs what we call power projection, projecting its hypothesis ΘLΘL−1...Θ1 onto the set of γ-positive matrices.",1 Introduction,[0],[0]
"Second, it “balances” Θ1, ...,ΘL so that, informally, they contribute equally to ΘLΘL−1...Θ1.",1 Introduction,[0],[0]
(See Section 5 for the details.),1 Introduction,[0],[0]
"We view this
regularizer as a theoretically tractable proxy for regularizers that promote positivity and balance between layers by adding penalties.
",1 Introduction,[0],[0]
"While, in practice, deep networks are non-linear, analysis of the linear case can provide a tractable way to gain insight through rigorous theoretical analysis (Saxe et al., 2013; Kawaguchi, 2016; Hardt & Ma, 2017).",1 Introduction,[0],[0]
We might view back-propagation in the non-linear case as an approximation to a procedure that locally modifies the function computed by each layer in a manner that reduces the loss as fast as possible.,1 Introduction,[0],[0]
"If a non-linear network is obtained by composing transformations, each of which is chosen from a Hilbert space of functions (as in Daniely et al. (2016)), then a step in “function space” corresponds to a step in an (infinite-dimensional) linear space of functions.
",1 Introduction,[0],[0]
Related work.,1 Introduction,[0],[0]
The motivation for this work comes from the papers of Hardt & Ma (2017) and Bartlett et al. (2018).,1 Introduction,[0],[0]
Saxe et al. (2013) studied the dynamics of a continuous-time process obtained by taking the step size of backpropagation applied to deep linear neural networks to zero.,1 Introduction,[0],[0]
Kawaguchi (2016) showed that deep linear neural networks have no suboptimal local minima.,1 Introduction,[0],[0]
"In the case that L = 2, the problem studied here has a similar structure as problems arising from low-rank approximation of matrices, especially as regards algorithms that approximate a matrix A by iteratively improving an approximation of the form UV .",1 Introduction,[0],[0]
"For an interesting survey on the rich literature on these algorithms, please see Ge et al. (2017a); successful algorithms have included a regularizer that promotes balance in the sizes of U and V .",1 Introduction,[0],[0]
"Taghvaei et al. (2017) studied the properties of critical points on the loss when learning deep linear neural networks in the presence of a weight decay regularizer; they studied networks that transform the input to the output through a process indexed by a continuous variable, instead of through discrete layers.",1 Introduction,[0],[0]
"Lee et al. (2016) showed that, given regularity conditions, for a random initialization, gradient descent converges to a local minimizer almost surely; while their paper yields useful insights, their regularity condition does not hold for our problem.",1 Introduction,[0],[0]
Many papers have analyzed learning of neural networks with non-linearities.,1 Introduction,[0],[0]
The papers most closely related to this work analyze algorithms based on gradient descent.,1 Introduction,[0],[0]
"Some of these (Andoni et al., 2014; Brutzkus & Globerson, 2017; Ge et al., 2017b; Li & Yuan, 2017; Zhong et al., 2017; Zhang et al., 2018; Brutzkus et al., 2018; Ge et al., 2018) analyze constant-depth networks.",1 Introduction,[0],[0]
Daniely (2017) showed that stochastic gradient descent learns a subclass of functions computed by log-depth networks in polynomial time; this class includes constant-degree polynomials with polynomially bounded coefficients.,1 Introduction,[0],[0]
"Other theoretical treatments of neural network learning algorithms include Lee et al. (1996); Arora et al. (2014); Livni et al. (2014); Janzamin et al. (2015); Safran & Shamir (2016); Zhang et al. (2016); Nguyen & Hein (2017); Zhang et al. (2017); Orhan & Pitkow (2018), although these are less closely related.
",1 Introduction,[0],[0]
Our three upper bound analyses combine a new upper bound on the operator norm of the Hessian of a deep linear network with the result of Hardt and Ma that gradients are lower bounded in terms of the loss for near-identity matrices.,1 Introduction,[0],[0]
They otherwise have different outlines.,1 Introduction,[0],[0]
The bound in terms of the loss of the initial solution proceeds by showing that the distance from each layer to the identity grows slowly enough that the loss is reduced before the layers stray far enough to harm the conditioning of the Hessian.,1 Introduction,[0],[0]
"The bound for symmetric positive definite matrices proceeds by showing that, in this case, all of the layers are the same, and each of their eigenvalues converges to the Lth root of a corresponding eigenvalue of Φ. As mentioned above, the bound for γ-positive matrices Φ is for an algorithm that achieves favorable conditioning through regularization.
",1 Introduction,[0],[0]
"We expect that the theoretical analysis reported here will inform the design of practical algorithms
for learning non-linear deep networks.",1 Introduction,[0],[0]
One potential avenue for this arises from the fact that the leverage provided by regularizing toward the identity appears to already be provided by a weaker policy of promoting the property that the composition of layers is (potentially asymmetric) positive definite.,1 Introduction,[0],[0]
"Also, balancing singular values of the layers of the network aided our analysis; an analogous balancing of Jacobians associated with various layers may improve conditioning in practice in the non-linear case.",1 Introduction,[0],[0]
"For a joint distribution P with support contained in ℜd × ℜd and g : ℜd → ℜd, define ℓP (g) = E(X,Y )∼P (||g(X)",2.1 Setting,[0],[0]
− Y || 2/2).,2.1 Setting,[0],[0]
"We focus on the case that, for (X,Y ) drawn from P , the marginal on X is isotropic, with",2.1 Setting,[0],[0]
EXX⊤ = Id.,2.1 Setting,[0],[0]
"For convenience, we assume that Y = ΦX for Φ ∈ ℜd×d.",2.1 Setting,[0],[0]
This assumption is without loss of generality: if Φ is the least squares matrix (so that f defined by f(X) = ΦX minimizes ℓP,2.1 Setting,[0],[0]
"(f) among linear functions), for any linear g we have
ℓP (g) =",2.1 Setting,[0],[0]
E‖g(X),2.1 Setting,[0],[0]
− f(X)‖ 2/2 + E‖f(X)−,2.1 Setting,[0],[0]
"Y ‖2/2
+ E",2.1 Setting,[0],[0]
((g(X),2.1 Setting,[0],[0]
− f(X))(f(X),2.1 Setting,[0],[0]
"− Y ))
= E‖g(X)",2.1 Setting,[0],[0]
− f(X)‖2/2 +,2.1 Setting,[0],[0]
"E‖f(X)− Y ‖2/2
= E‖g(X)",2.1 Setting,[0],[0]
− ΦX)‖2/2 + E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2,
since f is the projection of Y onto the set of linear functions ofX.",2.1 Setting,[0],[0]
"So assuming Y = ΦX corresponds to setting Φ as the least squares matrix and replacing the loss ℓP (g) by the excess loss
E‖g(X)",2.1 Setting,[0],[0]
− ΦX‖2/2 = E‖g(X),2.1 Setting,[0],[0]
− Y ‖2/2−,2.1 Setting,[0],[0]
E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2.
",2.1 Setting,[0],[0]
We study algorithms that learn linear mappings parameterized by deep networks.,2.1 Setting,[0],[0]
"The network with L layers and parameters Θ = (Θ1, . . .",2.1 Setting,[0],[0]
",ΘL) computes the parameterized function fΘ(x) = ΘLΘL−1 · · ·Θ1x, where",2.1 Setting,[0],[0]
"x ∈ ℜd and Θi ∈ ℜd×d.
",2.1 Setting,[0],[0]
"We use the notation Θi:j = ΘjΘj−1 · · ·Θi for i ≤ j, so that we can write fΘ(x) = Θ1:Lx = Θi+1:LΘiΘ1:i−1x.
",2.1 Setting,[0],[0]
"When there is no possibility of confusion, we will sometimes refer to loss ℓ(fΘ) simply as ℓ(Θ).",2.1 Setting,[0],[0]
"Because the distribution of X is isotropic, ℓ(Θ) = 12 ||Θ1:L − Φ|| 2 F with respect to least squares matrix Φ. When Θ is produced by an iterative algorithm, will we also refer to loss of the tth iterate by ℓ(t).
",2.1 Setting,[0],[0]
Definition 1.,2.1 Setting,[0],[0]
"For γ > 0, a matrix A ∈ ℜd×d is γ-positive if, for all unit length u, we have u⊤Au > γ.",2.1 Setting,[0],[0]
"We use ||A||F for the Frobenius norm of matrix A, ||A||2 for its operator norm, and σmin(A) for its least singular value.",2.2 Tools and background,[0],[0]
"For vector v, we use ||v|| for its Euclidian norm.
",2.2 Tools and background,[0],[0]
"For a matrix A and a matrix-valued function B, define DAB(A) to be the matrix with
(DAB(A))i,j = ∂vec(B(A))i ∂vec(A)j ,
where vec(A) is the column vector constructed by stacking the columns of A. We use Td,d to denote the d2 × d2 permutation matrix mapping vec(A) to vec(A⊤) for A ∈ ℜd×d.",2.2 Tools and background,[0],[0]
"For A ∈ ℜn×m and B ∈ ℜp×q, A⊗B denotes the Kronecker product, that is, the np×mq matrix of n×m blocks, with the i, jth block given by AijB.
We will need the gradient and Hessian of ℓ. (The gradient, which can be computed using backprop, is of course well known.)",2.2 Tools and background,[0],[0]
"The proof is in Appendix A.
Lemma 1.
DΘiℓ (fΘ)=(vec(Id))",2.2 Tools and background,[0],[0]
"⊤ (( Θ⊤1:i−1 ⊗ (Θ1:L−Φ) ⊤Θi+1:L ))
",2.2 Tools and background,[0],[0]
"= vec(G)⊤,
where G is the d× d matrix given by
G def = Θ⊤i+1:",2.2 Tools and background,[0],[0]
"L (Θ1:L − Φ)Θ ⊤ 1:i−1. (1)
",2.2 Tools and background,[0],[0]
"For i < j,
DΘjDΘiℓ (fΘ) =",2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
",2.2 Tools and background,[0],[0]
"((Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1)Td,d + (Θ ⊤ i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L)).
DΘiDΘiℓ (fΘ)",2.2 Tools and background,[0],[0]
=,2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",2.2 Tools and background,[0],[0]
"Td,d.",2.2 Tools and background,[0],[0]
"In this section, we prove an upper bound for gradient descent in terms of the loss of the initial solution.",3 Targets near the identity,[0],[0]
"First, set Θ(0) =",3.1 Procedure and upper bound,[0],[0]
"(I, I, ..., I), and then iteratively update
Θ (t+1)",3.1 Procedure and upper bound,[0],[0]
i = Θ,3.1 Procedure and upper bound,[0],[0]
(t),3.1 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",3.1 Procedure and upper bound,[0],[0]
"L − Φ
)
",3.1 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
Theorem 1.",3.1 Procedure and upper bound,[0],[0]
"There are positive constants c1 and c2 and polynomials p1 and p2 such that, if ℓ(Θ (0) 1:L) ≤ c1, L ≥ c2, and η ≤ 1 p1(L,d,||Φ||2) , then the above gradient descent procedure achieves
ℓ(fΘ(t)) ≤ ǫ within t = p2
(
1 η
)",3.1 Procedure and upper bound,[0],[0]
"ln (
ℓ(0) ǫ
)
iterations.",3.1 Procedure and upper bound,[0],[0]
"The following lemma, which is implicit in the proof of Theorem 2.2 in Hardt & Ma (2017), shows that the gradient is steep if the loss is large and the singular values of the layers are not too small.
",3.2 Proof of Theorem 1,[0],[0]
Lemma 2 (Hardt & Ma 2017).,3.2 Proof of Theorem 1,[0],[0]
Let ∇Θℓ(Θ) be the gradient of ℓ(Θ) with respect to any flattening of Θ.,3.2 Proof of Theorem 1,[0],[0]
"If, for all layers i, σmin(Θi) ≥ 1− a, then ||∇Θℓ(Θ)|| 2 ≥ 4ℓ(Θ)L(1− a)2L.
Next, we show that, if Θ(t) and Θ(t+1) are both close to the identity, then the gradient is not changing very fast between them, so that rapid progress continues to be made.",3.2 Proof of Theorem 1,[0],[0]
"We prove this through an upper bound on the operator norm of the Hessian that holds uniformly over members of a ball around the identity, which in turn can be obtained through a bound on the Frobenius norm.",3.2 Proof of Theorem 1,[0],[0]
"The proof is in Appendix B.
Lemma 3.",3.2 Proof of Theorem 1,[0],[0]
"Choose an arbitrary Θ with ||Θi||2 ≤ 1 + z for all i, and least squares matrix Φ with ||Φ||2 ≤ (1 + z)
L.",3.2 Proof of Theorem 1,[0],[0]
"Let ∇2 be the Hessian of ℓ(fΘ) with respect to an arbitrary flattening of the parameters of Θ. We have
||∇2||F ≤ 3Ld 5(1 + z)2L.
Armed with Lemmas 2 and 3, let us now analyze gradient descent.",3.2 Proof of Theorem 1,[0],[0]
"Very roughly, our strategy will be to show that the distance from the identity to the various layers grows slowly enough for the leverage from Lemmas 2 and 3 to enable successful learning.",3.2 Proof of Theorem 1,[0],[0]
Let R(Θ) = maxi ||Θi − I||2.,3.2 Proof of Theorem 1,[0],[0]
"From the update, we have
||Θ (t+1)",3.2 Proof of Theorem 1,[0],[0]
i − I||2 ≤ ||Θ (t) i,3.2 Proof of Theorem 1,[0],[0]
"− I||2 + η||(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤||2
≤ ||Θ (t) i",3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(Θ (t)))L||Θ (t) 1:L − Φ||2 ≤ ||Θ,3.2 Proof of Theorem 1,[0],[0]
(t),3.2 Proof of Theorem 1,[0],[0]
i,3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
"+R(Θ (t)))L||Θ (t) 1:L − Φ||F .
",3.2 Proof of Theorem 1,[0],[0]
If R(t) = maxs≤tR(Θ(s)),3.2 Proof of Theorem 1,[0],[0]
(so R(0) = 0) and ℓ(t) = 12 ||Θ (t) 1:,3.2 Proof of Theorem 1,[0],[0]
"L − Φ|| 2 F , this implies
R(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(t))L √ 2ℓ(t).,3.2 Proof of Theorem 1,[0],[0]
"(2)
By Lemma 3, for all Θ on the line segment from Θ(t) to Θ(t+1), we have
||∇2Θ||2 ≤ ||∇ 2 Θ||F ≤",3.2 Proof of Theorem 1,[0],[0]
"3Ld 5 max{(1 +R(t+ 1))2L, ||Φ||22},
so that
ℓ(t+ 1) ≤ ℓ(t)− η||∇Θ(t)",3.2 Proof of Theorem 1,[0],[0]
||,3.2 Proof of Theorem 1,[0],[0]
"2 +
3 2",3.2 Proof of Theorem 1,[0],[0]
η2Ld5 max{(1,3.2 Proof of Theorem 1,[0],[0]
"+R(t+ 1))2L, ||Φ||22}||∇Θ(t) || 2.
",3.2 Proof of Theorem 1,[0],[0]
"Thus, if we ensure
η ≤ 1
3Ld5 max{(1 +R(t+ 1))2L, ||Φ||22} , (3)
we have ℓ(t+ 1) ≤ ℓ(t)− (η/2)||∇Θ(t) || 2, which, using Lemma 2, gives
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).",3.2 Proof of Theorem 1,[0],[0]
"(4)
Pick any c ≥ 1.",3.2 Proof of Theorem 1,[0],[0]
"Assume that L ≥ (4/3) ln c = c2, ℓ(Θ (0) 1:L) ≤
ln(c)2
8c10 = c1 and η ≤ 1
3Ld5 max{c4,||Φ||22} .
",3.2 Proof of Theorem 1,[0],[0]
"We claim that, for all t ≥ 0,
1. R(t) ≤ ηc √ 2ℓ(0) ∑ 0≤s<t exp ( − sηLc4 )
2.",3.2 Proof of Theorem 1,[0],[0]
"ℓ(t) ≤ ( exp (
−2tηL c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"The base case holds as R(0) = 0 and ℓ(0) = ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Before starting the inductive step, notice that for any t ≥ 0,
ηc √ 2ℓ(0) ∑
0≤s<t exp
(
− sηL
c4
)
≤ ηc √ 2ℓ(0)× 1
1− exp (
−ηL c4
)
≤ ηc √ 2ℓ(0)× 2c4
ηL (since ηLc4 ≤ 1)
= 2c5 √ 2ℓ(0)
L ≤
ln c
L ≤ 3/4
where the last two inequalities follow from the constraints on ℓ(0) and L.
Using (2),
R(t+ 1) ≤ R(t) + η(1",3.2 Proof of Theorem 1,[0],[0]
"+R(t))L √ 2ℓ(t)
≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
"+ η
(
1 + ln c
L
)L √
2ℓ(t)
≤ R(t) + ηc √ 2ℓ(t)
≤ R(t) + ηc √ 2ℓ(0) exp
(
− tηL
c4
)
≤ ηc √ 2ℓ(0)",3.2 Proof of Theorem 1,[0],[0]
"∑
0≤s<t+1 exp
(
− sηL
c4
)
.
",3.2 Proof of Theorem 1,[0],[0]
"Since R(t+ 1) ≤ ln cL , the choice of η satisfies (3), so
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).
",3.2 Proof of Theorem 1,[0],[0]
"Now consider (1−R(t))2L:
ln ( (1−R(t))2L )",3.2 Proof of Theorem 1,[0],[0]
"= 2L ln(1−R(t))
",3.2 Proof of Theorem 1,[0],[0]
≥ 2L(−2R(t)),3.2 Proof of Theorem 1,[0],[0]
since R(t) ∈,3.2 Proof of Theorem 1,[0],[0]
"[0, 3/4]
≥ 2L
(
−2 ln c
L
)
since R(t) ≤",3.2 Proof of Theorem 1,[0],[0]
ln,3.2 Proof of Theorem 1,[0],[0]
"c
L
(1−R(t))2L ≥ 1/c4.
",3.2 Proof of Theorem 1,[0],[0]
"Using this in the bound on ℓ(t+ 1):
ℓ(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
"( 1− 2ηL(1 −R(t))2L ) ℓ(t)
≤
(
1− 2ηL
c4
)
ℓ(t)
≤
(
exp
(
− 2ηL
c4
))",3.2 Proof of Theorem 1,[0],[0]
"(
exp
(
− 2tηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0)
",3.2 Proof of Theorem 1,[0],[0]
"=
(
exp
(
− 2(t+ 1)ηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Solving ℓ(0) exp (
−2tηL c4
)
≤ ǫ for t and recalling that η < 1/c4 completes the proof of the theorem.",3.2 Proof of Theorem 1,[0],[0]
"In this section, we analyze the procedure of Section 3.1 when the least squares matrix Φ is symmetric and positive definite.
Theorem 2.",4 Symmetric positive definite targets,[0],[0]
"There is an absolute positive constant c3 such that, if Φ is symmetric and γ-positive with 0 <",4 Symmetric positive definite targets,[0],[0]
"γ < 1, and L ≥ c3 ln (||Φ||2/γ), then for all η ≤
1 L(1+||Φ||22) , gradient descent achieves
ℓ(fΘ(t))",4 Symmetric positive definite targets,[0],[0]
"≤ ǫ in poly(L, ||Φ||2/γ, 1/η) log(d/ǫ) iterations.
",4 Symmetric positive definite targets,[0],[0]
Note that a symmetric matrix is γ-positive when its minimum eigenvalue is at least γ.,4 Symmetric positive definite targets,[0],[0]
"Let Φ be a symmetric, real, γ-positive matrix with γ > 0, and let Θ(0),Θ(1), ... be the iterates of gradient descent with a step size 0 < η ≤ 1
L(1+||Φ||22) .
",4.1 Proof of Theorem 2,[0],[0]
Definition 2.,4.1 Proof of Theorem 2,[0],[0]
"Symmetric matrices A ⊆ ℜd×d are commuting normal matrices if there is a single unitary matrix U such that for all A ∈ A, U⊤AU is diagonal.
",4.1 Proof of Theorem 2,[0],[0]
"We will use the following well-known facts about commuting normal matrices.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 4 (Horn & Johnson 2013),4.1 Proof of Theorem 2,[0],[0]
.,4.1 Proof of Theorem 2,[0],[0]
"If A ⊆ ℜd×d is a set of symmetric commuting normal matrices and A,B ∈ A, the following hold:
• AB = BA;
",4.1 Proof of Theorem 2,[0],[0]
"• for all scalars α and β, A ∪ {αA+ βB,AB} are commuting normal;
• there is a unitary matrix U such that U⊤AU and U⊤BU are real and diagonal;
• the multiset of singular values of A is the same as the multiset of magnitudes of its eigenvalues;
• ||A− I||2 is the largest value of |z",4.1 Proof of Theorem 2,[0],[0]
"− 1| for an eigenvalue z of A.
Lemma 5.",4.1 Proof of Theorem 2,[0],[0]
"The matrices {Φ} ∪ {Θ (t) i : i ∈ {1, ..., L}, t ∈ Z +} are commuting normal.",4.1 Proof of Theorem 2,[0],[0]
"For all t, Θ (t) 1 = ... = Θ (t) L .
Proof.",4.1 Proof of Theorem 2,[0],[0]
The proof is by induction.,4.1 Proof of Theorem 2,[0],[0]
"The base case follows from the fact that Φ and I are commuting normal.
",4.1 Proof of Theorem 2,[0],[0]
"For the induction step, the fact that
{Φ} ∪ {
Θ (s)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
} ∪ {
Θ (s+1)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
}
are commuting normal follows from Lemma 4.",4.1 Proof of Theorem 2,[0],[0]
"The update formula now reveals that Θ (t+1) 1 = ... = Θ (t+1) L .
",4.1 Proof of Theorem 2,[0],[0]
Now we are ready to analyze the dynamics of the learning process.,4.1 Proof of Theorem 2,[0],[0]
"Let Φ = U⊤DLU be a diagonalization of Φ. Let Γ = max{1, ||Φ||2}.",4.1 Proof of Theorem 2,[0],[0]
"We next describe a sense in which gradient descent learns each eigenvalue independently.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 6.,4.1 Proof of Theorem 2,[0],[0]
"For each t, there is a real diagonal matrix D̂(t) such that, for all i, Θ (t)",4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U and
D̂(t+1) = D̂(t)",4.1 Proof of Theorem 2,[0],[0]
− η(D̂(t))L−1((D̂(t))L −DL).,4.1 Proof of Theorem 2,[0],[0]
"(5)
Proof.",4.1 Proof of Theorem 2,[0],[0]
Lemma 5 implies that there is a single real U such that Θ (t),4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U for all i. Applying Lemma 1, recalling that Θ (t) 1 = ...",4.1 Proof of Theorem 2,[0],[0]
"= Θ (t) L , and applying the fact that Θ (t) i and Φ commute, we get
Θ (t+1)",4.1 Proof of Theorem 2,[0],[0]
i = Θ,4.1 Proof of Theorem 2,[0],[0]
(t),4.1 Proof of Theorem 2,[0],[0]
"i − η(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L−1 (
(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L − Φ ) .
",4.1 Proof of Theorem 2,[0],[0]
"Replacing each matrix by its diagonalization, we get
U⊤D̂(t+1)U = U⊤D̂(t)U",4.1 Proof of Theorem 2,[0],[0]
"− η(U⊤(D̂(t))L−1U) ( U⊤(D̂(t))LU − U⊤DLU )
",4.1 Proof of Theorem 2,[0],[0]
"= U⊤D̂(t)U − ηU⊤(D̂(t))L−1 ( (D̂(t))L −DL ) U,
and left-multiplying by U and right-multiplying by U⊤ gives (5).
",4.1 Proof of Theorem 2,[0],[0]
We will now analyze the convergence of each D̂ (t) kk to Dkk separately.,4.1 Proof of Theorem 2,[0],[0]
"Let us focus for now on an arbitrary single index k, let λ = Dkk and λ̂",4.1 Proof of Theorem 2,[0],[0]
(t) =,4.1 Proof of Theorem 2,[0],[0]
"D̂ (t) kk .
",4.1 Proof of Theorem 2,[0],[0]
Recalling that ||Φ||2 ≤,4.1 Proof of Theorem 2,[0],[0]
"Γ, we have γ 1/L ≤ λ ≤",4.1 Proof of Theorem 2,[0],[0]
"Γ1/L. Also, Γ1/L = e 1 L ln Γ ≤ e1/a ≤ 1+2/a whenever a ≥ 1 and L ≥ a ln Γ. Similarly, γ1/L ≥ 1 − a whenever L ≥ a ln(1/γ).",4.1 Proof of Theorem 2,[0],[0]
"Thus, there are absolute constants c3 and c4 such that",4.1 Proof of Theorem 2,[0],[0]
|1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ)
L < 1 for all L ≥ c3 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"We claim that, for all t, λ̂(t) lies between 1 and λ inclusive, so that |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
The base case holds because λ̂(t) = 1 and |1,4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
Now let us work on the induction step.,4.1 Proof of Theorem 2,[0],[0]
"Applying (5) together with Lemma 1, we get
λ̂(t+1) = λ̂(t) +",4.1 Proof of Theorem 2,[0],[0]
η(λ̂(t))L−1(λL,4.1 Proof of Theorem 2,[0],[0]
− (λ̂(t))L).,4.1 Proof of Theorem 2,[0],[0]
"(6)
By the induction hypothesis, we just need to show that sign(λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)),4.1 Proof of Theorem 2,[0],[0]
= sign(λ − λ̂(t)) and |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)| (i.e., the step is in the correct direction, and does not “overshoot”).",4.1 Proof of Theorem 2,[0],[0]
"First, to see that the step is in the right direction, note that λL ≥ (λ̂(t))L if and only if λ ≥ (λ̂(t)), and the inductive hypothesis implies that λ̂(t), and therefore (λ̂(t))L−1, is non-negative.",4.1 Proof of Theorem 2,[0],[0]
To show that |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|, it suffices to show that η(λ̂(t))L−1 ∣ ∣ ∣ λL − (λ̂(t))L) ∣ ∣ ∣ ≤ |λ",4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|,
which, in turn would be implied by η ≤
∣ ∣ ∣ ∣ 1 (λ̂(t))L−1( ∑L−1 i=0 (λ̂ (t))iλL−1−i) ∣ ∣ ∣ ∣ (since λL − (λ̂(t))L = (λ −
λ̂(t))",4.1 Proof of Theorem 2,[0],[0]
"∑L−1 i=0 (λ̂ (t))iλL−1−i), which follows from the inductive hypothesis and η ≤ 1 LΓ2 .",4.1 Proof of Theorem 2,[0],[0]
"We have proved that each λ̂(t) lies between λ and 1, so that |1−",4.1 Proof of Theorem 2,[0],[0]
λ̂(t)| ≤ |1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"Now, since the step is in the right direction, and does not overshoot,
|λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ |λ̂(t) − λ| − η(λ̂(t))L−1|λL,4.1 Proof of Theorem 2,[0],[0]
"− (λ̂(t))L|
≤ |λ̂(t) − λ|
( 1− η(λ̂(t))L−1 ( L−1 ∑
i=0
(λ̂(t))iλL−1−i ))
≤ |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
"− λ| ( 1− ηLγ2 ) ,
since the fact that λ̂(t) lies between 1 and λ implies that λ̂(t) ≥ γ1/L. Thus, |λ̂(t) − λ| ≤ (
1− ηLγ2 )t c4",4.1 Proof of Theorem 2,[0],[0]
ln(Γ/γ).,4.1 Proof of Theorem 2,[0],[0]
"This implies that, for any ǫ ∈ (0, 1), for any absolute constant c5, there is a
constant c6 such that, after c6 1 ηLγ2 ln ( dL lnΓ γǫ )
steps, we have |λ̂(t)−λ| ≤ c5γ √ ǫ
LΓ √ d .Writing",4.1 Proof of Theorem 2,[0],[0]
"r = λ̂(t)−λ,
this implies, if c5 is small enough, that
((λ̂(t))L − λL)2 = ((λ+r)L−λL)2
≤ Γ2",4.1 Proof of Theorem 2,[0],[0]
"( ( 1+ r
λ
)L −1
)2
≤ Γ2 ( 2c5rL
λ
)2
≤ Γ2 ( 2c5rL
γ
)2
≤ ǫ
d .
",4.1 Proof of Theorem 2,[0],[0]
"Thus, after O (
1 ηLγ2
ln (
dL lnΓ γǫ
))
steps, (Dkk − D̂ (t) kk ) 2 ≤ ǫ/d for all k, and therefore ℓ(Θ(t))",4.1 Proof of Theorem 2,[0],[0]
"≤ ǫ,
completing the proof.",4.1 Proof of Theorem 2,[0],[0]
"We have seen that if the least squares matrix is symmetric, γ-positivity is sufficient for convergence of gradient descent.",5 Asymmetric positive definite matrices,[0],[0]
We shall see in Section 6 that positivity is also necessary for a broad family of gradient-based algorithms to converge to the optimal solution when the least squares matrix is symmetric.,5 Asymmetric positive definite matrices,[0],[0]
"Thus, in the symmetric case, positivity characterizes the success of gradient methods.
",5 Asymmetric positive definite matrices,[0],[0]
"In this section, we show that positivity suffices for the convergence of a gradient method even without the assumption that the least squares matrix is symmetric.
",5 Asymmetric positive definite matrices,[0],[0]
Note that the set of γ-positive (but not necessarily symmetric) matrices includes both rotations by an acute angle and “partial reflections” of the form ax + b refl(x) where refl(·) is a lengthpreserving reflection and 0 ≤,5 Asymmetric positive definite matrices,[0],[0]
|b| < a.,5 Asymmetric positive definite matrices,[0],[0]
"Since ( u⊤Au )⊤
= u⊤A⊤u, a matrix A is γ-positive if",5 Asymmetric positive definite matrices,[0],[0]
"and only if u⊤(A+A⊤)u ≥ 2γ for all unit length u, i.e. A+A⊤ is positive definite with eigenvalues at least 2γ.",5 Asymmetric positive definite matrices,[0],[0]
"The algorithm analyzed in this section uses a construction that is new, as far as we know, that we call a balanced factorization.",5.1 Balanced factorizations,[0],[0]
"This factorization may be of independent interest.
",5.1 Balanced factorizations,[0],[0]
Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .,5.1 Balanced factorizations,[0],[0]
The principal Lth root of a complex number whose expression in polar coordinates is reθi is r1/Leθi/L.,5.1 Balanced factorizations,[0],[0]
"The principal Lth root of a matrix A is the matrix B such that BL = A, and each eigenvalue of B is the principal Lth root of the corresponding eigenvalue of A.
Definition 3.",5.1 Balanced factorizations,[0],[0]
"If A be a matrix with polar decomposition RP , then A has the balanced factorization A = A1, ..., AL where for each i,
Ai = R 1/LPi, with Pi = R (L−i)/LP 1/LR−(L−i)/L,
and each of the Lth roots is the principal Lth root.
",5.1 Balanced factorizations,[0],[0]
The motivation for balanced factorization is as follows.,5.1 Balanced factorizations,[0],[0]
"We want each factor to do a 1/L fraction of the total amount of rotation, and a 1/L fraction of the total amount of scaling.",5.1 Balanced factorizations,[0],[0]
"However, the scaling done by the ith factor should be done in directions that take account of the partial rotations done by the other factors.",5.1 Balanced factorizations,[0],[0]
"The following is the key property of the balanced factorization; its proof is in Appendix C.
Lemma 7.",5.1 Balanced factorizations,[0],[0]
"If σ1, ..., σd are the singular values of A, and A1, ..., AL is a balanced factorization of A, then the following hold: (a) A = ∏L
i=1Ai; (b) for each i ∈ {1, ..., L}, σ 1/L 1 , ..., σ 1/L d are the singular
values of Ai.",5.1 Balanced factorizations,[0],[0]
The following is the power projection algorithm.,5.2 Procedure and upper bound,[0],[0]
"It has a positivity parameter γ > 0, and uses H = {A : ∀u s.t. ||u||",5.2 Procedure and upper bound,[0],[0]
"= 1, u⊤Au ≥ γ} as its “hypothesis space”.",5.2 Procedure and upper bound,[0],[0]
"First, it initializes Θ(0)i = γ 1/LI for all i ∈ {1, ..., L}.",5.2 Procedure and upper bound,[0],[0]
"Then, for each t, it does the following.
",5.2 Procedure and upper bound,[0],[0]
• Gradient Step.,5.2 Procedure and upper bound,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",5.2 Procedure and upper bound,[0],[0]
i = Θ,5.2 Procedure and upper bound,[0],[0]
(t),5.2 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",5.2 Procedure and upper bound,[0],[0]
"L − Φ
)
",5.2 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
• Power Project.",5.2 Procedure and upper bound,[0],[0]
Compute the projection Ψ(t+1/2) (w.r.t.,5.2 Procedure and upper bound,[0],[0]
"the Frobenius norm) of Θ (t+1/2) 1:L
onto H.
• Factor.",5.2 Procedure and upper bound,[0],[0]
"Let Θ (t+1) 1 , ...,Θ (t+1) L be the balanced factorization of Ψ (t+1/2), so that Ψ(t+1/2) =
Θ (t+1) 1:L .
",5.2 Procedure and upper bound,[0],[0]
Theorem 3.,5.2 Procedure and upper bound,[0],[0]
For any Φ such that u⊤Φu >,5.2 Procedure and upper bound,[0],[0]
"γ for all unit-length u, the power projection algorithm produces Θ(t) with ℓ(Θ(t))",5.2 Procedure and upper bound,[0],[0]
"≤ ǫ in poly(d, ||Φ||F , 1 γ ) log(1/ǫ) iterations.",5.2 Procedure and upper bound,[0],[0]
Lemma 8.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L ∈ H.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"Θ (0) 1:L = γI ∈ H, and, for all t, Ψ (t+1/2) is obtained by projection onto H, and Θ (t+1) 1:L = Ψ(t+1/2).
",5.3 Proof of Theorem 3,[0],[0]
Definition 4.,5.3 Proof of Theorem 3,[0],[0]
The exponential of a matrix A is exp(A),5.3 Proof of Theorem 3,[0],[0]
"def = ∑∞
k=0 1 k!A k, and B is a logarithm of A if A = exp(B).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 9 (Culver 1966).,5.3 Proof of Theorem 3,[0],[0]
"A real matrix has a real logarithm if and only if it is invertible and each Jordan block belonging to a negative eigenvalue occurs an even number of times.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 10.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L has a real Lth root.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Since Θ (t) 1:L ∈ H implies u,5.3 Proof of Theorem 3,[0],[0]
⊤Θ(t)1,5.3 Proof of Theorem 3,[0],[0]
:Lu > 0,5.3 Proof of Theorem 3,[0],[0]
"for all u, Θ (t) 1:L does not have a negative eigenvalue and is invertible.",5.3 Proof of Theorem 3,[0],[0]
"By Lemma 9, Θ (t) 1:L has a real logarithm.",5.3 Proof of Theorem 3,[0],[0]
"Thus, its real Lth root can be constructed via exp(log(Θ (t) 1:L)/L).
",5.3 Proof of Theorem 3,[0],[0]
"The preceding lemma implies that the algorithm is well-defined, since all of the required roots can be calculated.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 11.,5.3 Proof of Theorem 3,[0],[0]
"H is convex.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Suppose A and B are in H and λ ∈ (0, 1).",5.3 Proof of Theorem 3,[0],[0]
"We have
u⊤(λA+ (1− λ)B)u = λu⊤Au+ (1− λ)u⊤Bu ≥ γ.
Lemma 12.",5.3 Proof of Theorem 3,[0],[0]
"For all A ∈ H, σmin(A) ≥ γ.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Let u and v be singular vectors such that u⊤Av = σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
"γ ≤ v⊤Av = σmin(A)v ⊤u ≤ σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 13.,5.3 Proof of Theorem 3,[0],[0]
"For all t, σmin(Θ (t) i )",5.3 Proof of Theorem 3,[0],[0]
"≥ γ 1/L.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"First, σmin(Θ (0) i ) =",5.3 Proof of Theorem 3,[0],[0]
γ 1/L ≥ γ1/L. Now consider t > 0.,5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t−1/2) was projected into H, we have σmin(Ψ(t−1/2))",5.3 Proof of Theorem 3,[0],[0]
≥ γ.,5.3 Proof of Theorem 3,[0],[0]
"Lemma 7 then completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
"Define U(t) = max {
maxs≤tmaxi ||Θ (s) i ||2, ||Φ|| 1/L 2
}
, B(t) = mins≤tmini σmin(Θ (s) i ), and recall that
ℓ(t) = ||Θ (t) 1:L − Φ|| 2 F .
",5.3 Proof of Theorem 3,[0],[0]
"Arguing as in the initial portion of Section 3.2, as long as
η ≤ 1
3Ld5U(t)2L (7)
we have ℓ(t + 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLB(t)2L ) ℓ(t) (see Equation 4).,5.3 Proof of Theorem 3,[0],[0]
"Lemma 13 gives B(t) ≥ γ1/L, so ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
"( 1− ηLγ2 )
ℓ(t).",5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t+1/2) is the projection of Θ (t+1/2) 1:L onto a convex set H that
contains Φ, and Θ (t+1) 1:L = Ψ (t+1/2), (7) implies
ℓ(t+ 1) ≤ ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLγ2 ) ℓ(t).,5.3 Proof of Theorem 3,[0],[0]
"(8)
Next, we prove an upper bound on U .
",5.3 Proof of Theorem 3,[0],[0]
Lemma 14.,5.3 Proof of Theorem 3,[0],[0]
"For all t, U(t) ≤ ( √
ℓ(t) + ||Φ||F
)1/L .
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Recall that ℓ(t) = ||Θ (t) 1:L−Φ|| 2 F .,5.3 Proof of Theorem 3,[0],[0]
"By the triangle inequality, ||Θ (t) 1:L||F ≤",5.3 Proof of Theorem 3,[0],[0]
√ ℓ(t)+ ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
Thus ||Θ (t) 1:L||2 ≤ √ ℓ(t) + ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
"By Lemma 7, for all i, we have ||Θ (t)",5.3 Proof of Theorem 3,[0],[0]
i ||2 ≤ ( √ ℓ(t) + ||Φ||F )1/L .,5.3 Proof of Theorem 3,[0],[0]
"Since ||Φ||2 ≤ ||Φ||F , this completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
Note that the triangle inequality implies that ℓ(0) ≤ ||Θ (0) 1:L|| 2 F + ||Φ|| 2 F ≤,5.3 Proof of Theorem 3,[0],[0]
γ 2d,5.3 Proof of Theorem 3,[0],[0]
+,5.3 Proof of Theorem 3,[0],[0]
||Φ||2F .,5.3 Proof of Theorem 3,[0],[0]
Since σmin(Φ) ≥,5.3 Proof of Theorem 3,[0],[0]
"γ, we have ||Φ|| 2 F ≥ γ 2d, so ℓ(t) ≤ 2||Φ||2F and U(t) ≤ (3||Φ||2) 1/L.",5.3 Proof of Theorem 3,[0],[0]
"Now, if we set η = 1 cLd5||Φ||2
F , for a large enough absolute constant c, then (7) is satisfied, so that (8) gives ℓ(t+1) ≤ (
1− γ 2
cd5||Φ||2 F
)
ℓ(t) and the power projection algorithm achieves ℓ(t+ 1) ≤ ǫ after
O
(
d5||Φ||2F γ2 log
(
ℓ(0)
ǫ
))
",5.3 Proof of Theorem 3,[0],[0]
"=O
(
d5||Φ||2F γ2 log
(
||Φ||2F ǫ
))
updates.",5.3 Proof of Theorem 3,[0],[0]
"In this section, we show that positive definite Φ are necessary for several gradient descent algorithms with different kinds of regularization to minimize the loss.",6 Failure,[0],[0]
"One family of algorithms that we will
analyze is parameterized by a function ψ mapping the number of inputs d and the number of layers L to a radius ψ(d, L), step sizes ηt and initialization parameter γ ≥ 0.",6 Failure,[0],[0]
"In particular, a ψ-step-and-project algorithm is any instantiation of the following algorithmic template.
",6 Failure,[0],[0]
Initialize each Θ (0),6 Failure,[0],[0]
"i = γ 1/LI for some γ ≥ 0 and iterate:
• Gradient Step.",6 Failure,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",6 Failure,[0],[0]
i = Θ,6 Failure,[0],[0]
(t),6 Failure,[0],[0]
"i − ηt(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤.
• Project.",6 Failure,[0],[0]
Set each Θt+1i,6 Failure,[0],[0]
"to the projection of Θ t+1/2 i onto {A : ||A− I||2 ≤ ψ(d, L)}.
",6 Failure,[0],[0]
We will also show that Penalty Regularized Gradient Descent which uses gradient descent with any step sizes ηt on the regularized objective ℓ(Θ) + κ 2 ∑ i ||I,6 Failure,[0],[0]
"−Θ|| 2 F also fails to minimize the loss.
",6 Failure,[0],[0]
"Both results use the simple observation that when Θ1:L and Φ are mutually diagonalizable then
||Θ1:L − Φ|| 2 F = ||U ⊤D̂U",6 Failure,[0],[0]
"− U⊤DU ||2F = d ∑
j=1
(D̂jj −Djj) 2,
where the Dii are the eigenvalues of Φ.
Theorem 4.",6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then Penalty Regularized Gradient Descent produces hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and L is even, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"For all t, Penalty Regularized Gradient Descent produces Θ (t+1) i = (1 − κ)Θ (t) i + κI",6 Failure,[0],[0]
− ηt(Θ (t) i+1:L) ⊤ ( Θ (t) 1:L −Φ ) (Θ (t) 1:i−1) ⊤.,6 Failure,[0],[0]
"Thus, by induction, the Θ(t)i are matrix polynomials of Φ, and therefore they are all commuting normal.",6 Failure,[0],[0]
As in Lemmas 5 and 6 each Θ (t) i is the same U ⊤D̃(t)U and Θ (t) 1,6 Failure,[0],[0]
:L = U ⊤(D̃(t))LU .,6 Failure,[0],[0]
"Since L is even, each (D̃(t))Ljj ≥ 0, so ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L−Φ|| 2 F ≥ λ 2/2.
To analyze step-and-project algorithms, it is helpful to first characterize the project step (see also (Lefkimmiatis et al., 2013)).
",6 Failure,[0],[0]
Lemma 15.,6 Failure,[0],[0]
"Let X be a symmetric matrix and let U⊤DU be its diagonalization.
",6 Failure,[0],[0]
"For a > 0, let Y be the Frobenius norm projection of X onto Ba = {A : A is symmetric psd and ||A−I||2 ≤ a}.",6 Failure,[0],[0]
"Then Y = U
⊤D̃U where D̃ is obtained from D by projecting all of its diagonal elements onto [1− a, 1 + a].
",6 Failure,[0],[0]
"Thus {X,Y } are symmetric commuting normal matrices.
",6 Failure,[0],[0]
Proof.,6 Failure,[0],[0]
"First, if X ∈ Ba, then Y = X and we are done.",6 Failure,[0],[0]
Assume X 6∈ Ba.,6 Failure,[0],[0]
"Clearly U ⊤D̃U ∈ Ba, so we just need to show that any member of Ba is at least as far from X as U⊤D̃U is.",6 Failure,[0],[0]
"Let Λ be the multiset of eigenvalues of X (with repetitions) that are not in [1 − a, 1 + a], and for each λ ∈ Λ, let eλ be the adjustment to λ necessary to bring it to [1− a, 1 + a]; i.e., so that λ+ eλ is the projection of λ onto [1− a, 1 + a].
",6 Failure,[0],[0]
"If uλ is the eigenvector associated with λ, we have U ⊤D̃U −X = ∑ λ∈Λ eλuλu ⊤ λ , so that ||U ⊤D̃U",6 Failure,[0],[0]
− X||2F = ∑,6 Failure,[0],[0]
λ∈Λ,6 Failure,[0],[0]
e 2 λ.,6 Failure,[0],[0]
Let Z be an arbitrary member of Ba.,6 Failure,[0],[0]
We would like to show that ||Z,6 Failure,[0],[0]
− X|| 2 F ≥ ∑ λ∈Λ e 2 λ.,6 Failure,[0],[0]
"Since Z ∈ Ba, we have ||Z − I||2 ≤ a. ||Z",6 Failure,[0],[0]
− I||2 is the largest singular value of Z,6 Failure,[0],[0]
"− I so, for any unit length vector, in particular some uλ for λ ∈ Λ, |u ⊤ λ (Z",6 Failure,[0],[0]
− I)uλ| = |u ⊤ λ,6 Failure,[0],[0]
"Zuλ − 1| ≤ a, which implies u⊤λZuλ ∈",6 Failure,[0],[0]
"[1 − a, 1 + a].",6 Failure,[0],[0]
Since U is unitary U ⊤(X,6 Failure,[0],[0]
"− Z)U has the same eigenvalues as X − Z, and, since the Frobenius norm is a function of the eigenvalues, ||U⊤(X − Z)U ||F = ||X − Z||F .",6 Failure,[0],[0]
But since u⊤λZuλ ∈,6 Failure,[0],[0]
"[1 − a, 1 + a] for all λ ∈ Λ, just summing over the diagonal elements, we get ||U⊤(X − Z)U",6 Failure,[0],[0]
"||2F ≥ ∑ λ∈Λ e 2 λ, completing the proof.
",6 Failure,[0],[0]
Theorem 5.,6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then ψ-step-and-project algorithms produce hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and either L is even or ψ(L, d) ≤ 1, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"As in Lemmas 5 and 6, the Θ (t+1/2)",6 Failure,[0],[0]
i are identical and mutually diagonalizable with Φ. Lemma 15 shows that this is preserved by the projection step.,6 Failure,[0],[0]
Thus there is a real diagonal D̃(t) such that each Θ (t) i = U ⊤D(t)i,6 Failure,[0],[0]
"U , so Θ (t) 1:L = U ⊤(D̃(t))LU .",6 Failure,[0],[0]
"When L is even, each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"When ψ(d, L) ≤ 1",6 Failure,[0],[0]
"then the projection ensures that the elements of D̃(t) are non-negative, and thus each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"In either case, ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L− Φ||2F ≥ λ 2/2.
",6 Failure,[0],[0]
"One choice of Φ that satisfies the requirements of Theorems 4 and 5 is Φ = diag(−λ, 1, 1, ..., 1).",6 Failure,[0],[0]
"For constant λ, the loss of Θ(0) = (I, I, ..., I) is a constant for this target.",6 Failure,[0],[0]
"Another choice is Φ = diag(−λ,−λ, 1, 1, ..., 1), which has a positive determinant.
",6 Failure,[0],[0]
Our proof of failure to minimize the loss exploits the fact that the layers are initialized to multiples of the identity.,6 Failure,[0],[0]
"Since the training process is a continuous function of the initial solution, this implies that any convergence to a good solution will be very slow if the initializations are sufficiently close to the identity.",6 Failure,[0],[0]
"We thank Yair Carmon, Nigel Duffy, Matt Feiszli, Roy Frostig, Vineet Gupta, Moritz Hardt, Tomer Koren, Antoine Saliou, Hanie Sedghi, Yoram Singer and Kunal Talwar for valuable conversations.
",Acknowledgements,[0],[0]
Peter Bartlett gratefully acknowledges the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS).,Acknowledgements,[0],[0]
"We rely on the following facts (Horn, 1986; Harville, 1997).
",A Proof of Lemma 1,[0],[0]
Lemma 16.,A Proof of Lemma 1,[0],[0]
"For compatible matrices (and, where m,n, p, q, r, s are mentioned, A ∈ ℜm×n, B ∈ ℜp×q, X ∈ ℜr×s):
A⊗ (B ⊗ E) =",A Proof of Lemma 1,[0],[0]
"(A⊗B)⊗E,
AC ⊗BD = (A⊗B)(C ⊗D),
(A⊗B)⊤ = A⊤ ⊗B⊤,
vec(AXB) = (B⊤ ⊗A)vec(X),
Tm,nvec(A) def = vec(A⊤),
Tn,mTm,n = Imn,
Tm,n = T ⊤ n,m,
T1,n = Tn,1 =",A Proof of Lemma 1,[0],[0]
"In,
DX(A(B(X)))",A Proof of Lemma 1,[0],[0]
"= DB(A(B(X)))DX (B(X)),
DX(A(X)B(X))",A Proof of Lemma 1,[0],[0]
= (B(X) ⊤ ⊗ Im)DXA(X),A Proof of Lemma 1,[0],[0]
+,A Proof of Lemma 1,[0],[0]
"(Iq ⊗A(X))DXB(X),
DX(A(X) T ) = Tn,mDX(A(X)),
DX(AXB)",A Proof of Lemma 1,[0],[0]
"= B ⊤ ⊗A,
DA(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(Imn ⊗ vec(B))
=",A Proof of Lemma 1,[0],[0]
"(Inq ⊗ Tm,p)(In ⊗ vec(B)⊗",A Proof of Lemma 1,[0],[0]
"Im),
DB(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(vec(A)⊗ Ipq)
= (Tp,q ⊗ Imn)(Iq ⊗ vec(A)⊗",A Proof of Lemma 1,[0],[0]
"Ip).
",A Proof of Lemma 1,[0],[0]
"Armed with Lemma 16, we now prove Lemma 1.",A Proof of Lemma 1,[0],[0]
"We have
DΘifΘ(x) = DΘi (Θi+1:LΘiΘ1:i−1x) =",A Proof of Lemma 1,[0],[0]
"(Θ1:i−1x) ⊤ ⊗Θi+1:L.
Again, from Lemma 16
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= DΘi
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
= DΘ1:j−1x
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
DΘi (Θ1:j−1x)
(by the chain rule, since i < j)
= DΘ1:j−1x
(
(
(Θ1:j−1x)⊗Θ ⊤ j+1:L
)⊤ ) (
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.",A Proof of Lemma 1,[0],[0]
"(9)
Define P = Θ1:j−1x and Q = Θj+1:L, so that P ∈ ℜd×1 and Q ∈ ℜd×d.",A Proof of Lemma 1,[0],[0]
"We have
DP
(
( P ⊗Q⊤ )⊤ )
= Td2,dDP
( P ⊗Q⊤ )
",A Proof of Lemma 1,[0],[0]
"= Td2,d(I1 ⊗ Td,d ⊗ Id)(Id ⊗ vec(Q T ))",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Q ⊤)).
",A Proof of Lemma 1,[0],[0]
"Substituting back into (9), we get
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Θ ⊤ j+1:L))
",A Proof of Lemma 1,[0],[0]
"(
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.
",A Proof of Lemma 1,[0],[0]
"The product rule in Lemma 16 gives, for each i,
DΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= E(DΘi(ℓ(fΘ(X)))
= E(DΘi( 1
2 (fΘ(X)− ΦX)
⊤(fΘ(X)",A Proof of Lemma 1,[0],[0]
"−ΦX)))
",A Proof of Lemma 1,[0],[0]
= E(((Θ1:L − Φ)X) ⊤DΘifΘ(X)),A Proof of Lemma 1,[0],[0]
"= E ( ((Θ1:L − Φ)X) ⊤ ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
( (I1 ⊗,A Proof of Lemma 1,[0],[0]
"((Θ1:L − Φ)X) ⊤) ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
"= E (( (Θ1:i−1X) ⊤ ⊗ ((Θ1:L −Φ)X) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
(( X⊤Θ⊤1:i−1 ) ⊗,A Proof of Lemma 1,[0],[0]
( X⊤(Θ1:L −Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
L )),A Proof of Lemma 1,[0],[0]
"= E ( (X⊤ ⊗X⊤) (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
))
= E ((X ⊗X)vec(1))⊤ ( Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L )
= E ( vec(XX⊤) )",A Proof of Lemma 1,[0],[0]
"⊤ (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
)
= (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Hence,
(DΘiℓ (fΘ))",A Proof of Lemma 1,[0],[0]
"⊤ =
(
Θ1:i−1 ⊗Θ ⊤ i+1:L(Θ1:L − Φ)
)
(vec(Id))
",A Proof of Lemma 1,[0],[0]
"= vec ( Θ⊤i+1:L(Θ1:L − Φ)IdΘ ⊤ 1:i−1 ) .
",A Proof of Lemma 1,[0],[0]
"Also, recalling that i < j, we have
DΘjDΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= DΘj
( (vec(Id))",A Proof of Lemma 1,[0],[0]
T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘj
(
Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L
)
= (Id2 ⊗ (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ) (Id ⊗ Td,d ⊗",A Proof of Lemma 1,[0],[0]
"Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘj",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ) .
",A Proof of Lemma 1,[0],[0]
"Continuing with the subproblem,
DΘj
(
(Θ1:L −Φ) ⊤Θi+1:",A Proof of Lemma 1,[0],[0]
"L
)
",A Proof of Lemma 1,[0],[0]
"= (Θ⊤i+1:L ⊗ Id)DΘj
( (Θ1:L − Φ) ⊤ )
",A Proof of Lemma 1,[0],[0]
+ (Id ⊗ (Θ1:,A Proof of Lemma 1,[0],[0]
L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id)DΘj
( Θ⊤1:L )
+",A Proof of Lemma 1,[0],[0]
(Id ⊗ (Θ1:L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 ) DΘj (Θ ⊤ j )
+",A Proof of Lemma 1,[0],[0]
"(Id ⊗ (Θ1:L − Φ) ⊤) ( Θ⊤i+1:j−1 ⊗Θj+1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 )",A Proof of Lemma 1,[0],[0]
"Td,d
+ (Id ⊗ (Θ1:L − Φ) ⊤)",A Proof of Lemma 1,[0],[0]
"( Θ⊤i+1:j−1 ⊗Θj+1:L )
=",A Proof of Lemma 1,[0],[0]
( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 ),A Proof of Lemma 1,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Finally,
DΘiDΘiℓ (fΘ) = DΘi
( (vec(Id)) T ( Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
"L − Φ) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘi
(
Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
L − Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
"L
)
= (Id2 ⊗ (vec(Id)) T )",A Proof of Lemma 1,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘi",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L )
and
DΘi
(
(Θ1:L − Φ) ⊤Θi+1:L
)
= (Θ⊤i+1:L ⊗ Id)DΘi
( (Θ1:L − Φ) ⊤ )
= (Θ⊤i+1:L ⊗ Id)DΘi
( Θ⊤1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ) DΘi(Θ ⊤ i ) =",A Proof of Lemma 1,[0],[0]
(Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ),A Proof of Lemma 1,[0],[0]
"Td,d = (
Θ⊤i+1:LΘi+1:",A Proof of Lemma 1,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",A Proof of Lemma 1,[0],[0]
"Td,d.",A Proof of Lemma 1,[0],[0]
"We have ||∇2||2F = 2 ∑
i<j
||DΘjDΘiℓ(fΘ)|| 2 F +
∑
i
||DΘiDΘiℓ(fΘ)|| 2 F .",B Proof of Lemma 3,[0],[0]
"(10)
Let’s start with the easier term.",B Proof of Lemma 3,[0],[0]
Choose Θ such that ||Θi − I||2 ≤ z,B Proof of Lemma 3,[0],[0]
"for all i. We have
||DΘiDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(Id)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣
F
≤ ∣ ∣
∣
∣ ∣ ∣ (Id2 ⊗ (vec(Id)) ⊤) (Id ⊗ Td,d ⊗ Id) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗Id2 )( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
∣ ∣ ∣ ∣ ∣ ∣
F
≤ d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 ) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ∣ ∣
∣
∣ ∣
∣ vec(Θ⊤1:i−1)
∣ ∣ ∣ ∣ ∣ ∣
F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ||Θ1:i−1||F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4 ||Θ1:i−1||2
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 ) ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ F × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ F
≤",B Proof of Lemma 3,[0],[0]
"d5(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ 2 × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ 2
≤ d5(1 + z)2(L−1).
",B Proof of Lemma 3,[0],[0]
"Similarly,
||DΘjDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(I)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )
)
∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣ ∣ ∣
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ) ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"(∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
= d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ||Θi+1:j−1||F × ∣ ∣ ∣ ∣ ∣ ∣ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
( d(1 + z)2L−1−i + 2d(1 + z)2L−1−i ),B Proof of Lemma 3,[0],[0]
"= 3d5(1 + z)2L−2.
",B Proof of Lemma 3,[0],[0]
"Putting these together with (10), we get ||∇2||2F ≤",B Proof of Lemma 3,[0],[0]
"L 29d10(1 + z)4L, so that
||∇2||F ≤ 3Ld 5(1 + z)2L.",B Proof of Lemma 3,[0],[0]
"Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .
",C Proof of Lemma 7,[0],[0]
"Lemma 17 ((Horn & Johnson, 2013)).",C Proof of Lemma 7,[0],[0]
"A is a unitary matrix if and only if all of the (complex) eigenvalues z of A have magnitude 1.
",C Proof of Lemma 7,[0],[0]
"Lemma 18 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is unitary then A is normal.
",C Proof of Lemma 7,[0],[0]
"Lemma 19 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is normal with eigenvalues λ1, ..., λd, the singular values of A are |λ1|, ..., |λd|.
Lemma 20.",C Proof of Lemma 7,[0],[0]
"If A is unitary, then A1/L is unitary, and thus Ai/L is unitary for any non-negative integer i.
Lemma 21.",C Proof of Lemma 7,[0],[0]
"If A is invertible and normal with singular values σ1, ..., σd, then, for any positive integer L, the singular values of A1/L are σ 1/L 1 , ..., σ 1/L d .
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"Follows from Lemma 19 together with the fact that raising a non-singular matrix to a power results in raising its eigenvalues to the same power.
",C Proof of Lemma 7,[0],[0]
"Lemma 22 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A = RP is the polar decomposition of A, then the singular values of A are the same as the singular values of P .
",C Proof of Lemma 7,[0],[0]
Lemma 23.,C Proof of Lemma 7,[0],[0]
"If σ1, ..., σd are the principal components of A, and A = ∏L i=1Ai is a balanced factorization of A, then then σ 1/L 1 , ..., σ 1/L d are the principal components of Ai, for each i ∈ {1, ..., L}.
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"The singular values of Ai = RiPi are the same as the singular values of Pi, which is similar to P 1/L, whose singular values are the Lth roots of the singular values of P , which are the same as the singular values of A.
Lemma 24.",C Proof of Lemma 7,[0],[0]
"If A1, ..., AL is a balanced factorization of A, then
A = L ∏
i=1
Ai.
Proof.",C Proof of Lemma 7,[0],[0]
"We have
A = RP
= R1/LR1−1/LP 1/LP 1−1/L = R1/LR1−1/LP 1/LR−(1−1/L)R1−1/LP 1−1/L = R1P1R 1−1/LP 1−1/L = A1R 1−1/LP 1−1/L = A1R 1/LR1−2/LP 1/LP 1−2/L
and so on.",C Proof of Lemma 7,[0],[0]
"We analyze algorithms for approximating a function f(x) = Φxmapping R to R using deep linear neural networks, i.e. that learn a function h parameterized by matrices Θ1, ...,ΘL and defined by h(x)",abstractText,[0],[0]
= ΘLΘL−1...Θ1x.,abstractText,[0],[0]
We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.,abstractText,[0],[0]
"We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix Φ, in the case where the initial hypothesis Θ1 = ...",abstractText,[0],[0]
= ΘL = I has excess loss bounded by a small enough constant.,abstractText,[0],[0]
"On the other hand, we show that gradient descent fails to converge for Φ whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help.",abstractText,[0],[0]
"If Φ is symmetric positive definite, we show that an algorithm that initializes Θi = I learns an ǫ-approximation of f using a number of updates polynomial in L, the condition number of Φ, and log(d/ǫ).",abstractText,[0],[0]
"In contrast, we show that if the least squares matrix Φ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge.",abstractText,[0],[0]
"We analyze an algorithm for the case that Φ satisfies uΦu > 0 for all u, but may not be symmetric.",abstractText,[0],[0]
This algorithm uses two regularizers: one that maintains the invariant u⊤ΘLΘL−1...,abstractText,[0],[0]
Θ1u > 0,abstractText,[0],[0]
"for all u, and another that “balances” Θ1, ...,ΘL so that they have the same singular values.",abstractText,[0],[0]
"Single-task learning in computer vision has enjoyed much success in deep learning, with many single-task models now performing at or beyond human accuracies for a wide array of tasks.",1. Introduction,[0],[0]
"However, an ultimate visual system for full scene understanding must be able to perform many diverse perceptual tasks simultaneously and efficiently, especially within the limited compute environments of embedded systems
1Magic Leap, Inc. Correspondence to: Zhao Chen <zchen@magicleap.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
such as smartphones, wearable devices, and robots/drones.",1. Introduction,[0],[0]
"Such a system can be enabled by multitask learning, where one model shares weights across multiple tasks and makes multiple inferences in one forward pass.",1. Introduction,[0],[0]
"Such networks are not only scalable, but the shared features within these networks can induce more robust regularization and boost performance as a result.",1. Introduction,[0],[0]
"In the ideal limit, we can thus have the best of both worlds with multitask networks: more efficiency and higher performance.
",1. Introduction,[0],[0]
"In general, multitask networks are difficult to train; different tasks need to be properly balanced so network parameters converge to robust shared features that are useful across all tasks.",1. Introduction,[0],[0]
"Methods in multitask learning thus far have largely tried to find this balance by manipulating the forward pass of the network (e.g. through constructing explicit statistical relationships between features (Long & Wang, 2015) or optimizing multitask network architectures (Misra et al., 2016), etc.), but such methods ignore a key insight: task imbalances impede proper training because they manifest as imbalances between backpropagated gradients.",1. Introduction,[0],[0]
"A task that is too dominant during training, for example, will necessarily express that dominance by inducing gradients which have relatively large magnitudes.",1. Introduction,[0],[0]
"We aim to mitigate such issues at their root by directly modifying gradient magnitudes through tuning of the multitask loss function.
",1. Introduction,[0],[0]
"In practice, the multitask loss function is often assumed to be linear in the single task losses Li, L = ∑ i wiLi, where the sum runs over all T tasks.",1. Introduction,[0],[0]
"In our case, we propose an adaptive method, and so wi can vary at each training step t: wi = wi(t).",1. Introduction,[0],[0]
"This linear form of the loss function is convenient for implementing gradient balancing, as wi very directly and linearly couples to the backpropagated gradient magnitudes from each task.",1. Introduction,[0],[0]
The challenge is then to find the best value for each wi at each training step t that balances the contribution of each task for optimal model training.,1. Introduction,[0],[0]
"To optimize the weights wi(t) for gradient balancing, we propose a simple algorithm that penalizes the network when backpropagated gradients from any task are too large or too small.",1. Introduction,[0],[0]
"The correct balance is struck when tasks are training at similar rates; if task i is training relatively quickly, then its weight wi(t) should decrease relative to other task weights wj(t)|j 6=i to allow other tasks more influence on
training.",1. Introduction,[0],[0]
"Our algorithm is similar to batch normalization (Ioffe & Szegedy, 2015) with two main differences: (1) we normalize across tasks instead of across data batches, and (2) we use rate balancing as a desired objective to inform our normalization.",1. Introduction,[0],[0]
"We will show that such gradient normalization (hereafter referred to as GradNorm) boosts network performance while significantly curtailing overfitting.
",1. Introduction,[0],[0]
"Our main contributions to multitask learning are as follows:
1.",1. Introduction,[0],[0]
"An efficient algorithm for multitask loss balancing which directly tunes gradient magnitudes.
2.",1. Introduction,[0],[0]
"A method which matches or surpasses the performance of very expensive exhaustive grid search procedures, but which only requires tuning a single hyperparameter.
3.",1. Introduction,[0],[0]
A demonstration that direct gradient interaction provides a powerful way of controlling multitask learning.,1. Introduction,[0],[0]
"Multitask learning was introduced well before the advent of deep learning (Caruana, 1998; Bakker & Heskes, 2003), but the robust learned features within deep networks and their excellent single-task performance have spurned renewed interest.",2. Related Work,[0],[0]
"Although our primary application area is computer vision, multitask learning has applications in multiple other fields, from natural language processing (Collobert & Weston, 2008; Hashimoto et al., 2016; Søgaard & Goldberg, 2016) to speech synthesis (Seltzer & Droppo, 2013; Wu et al., 2015), from very domain-specific applications such as traffic prediction (Huang et al., 2014) to very general cross-domain work (Bilen & Vedaldi, 2017).",2. Related Work,[0],[0]
"Multitask learning has also been explored in the context of curriculum learning (Graves et al., 2017), where subsets of tasks are subsequently trained based on local rewards; we here explore the opposite approach, where tasks are jointly trained based on global rewards such as total loss decrease.
",2. Related Work,[0],[0]
"Multitask learning is very well-suited to the field of computer vision, where making multiple robust predictions is crucial for complete scene understanding.",2. Related Work,[0],[0]
"Deep networks have been used to solve various subsets of multiple vision tasks, from 3-task networks (Eigen & Fergus, 2015; Teichmann et al., 2016) to much larger subsets as in UberNet (Kokkinos, 2016).",2. Related Work,[0],[0]
"Often, single computer vision problems can even be framed as multitask problems, such as in Mask R-CNN for instance segmentation (He et al., 2017) or YOLO-9000 for object detection (Redmon & Farhadi, 2016).",2. Related Work,[0],[0]
Particularly of note is the rich and significant body of work on finding explicit ways to exploit task relationships within a multitask model.,2. Related Work,[0],[0]
"Clustering methods have shown success beyond deep models (Jacob et al., 2009; Kang et al., 2011), while constructs such as deep relationship networks (Long & Wang, 2015) and cross-stich networks (Misra et al., 2016)
give deep networks the capacity to search for meaningful relationships between tasks and to learn which features to share between them.",2. Related Work,[0],[0]
"Work in (Warde-Farley et al., 2014) and (Lu et al., 2016) use groupings amongst labels to search through possible architectures for learning.",2. Related Work,[0],[0]
"Perhaps the most relevant to the current work, (Kendall et al., 2017) uses a joint likelihood formulation to derive task weights based on the intrinsic uncertainty in each task.",2. Related Work,[0],[0]
"For a multitask loss function L(t) = ∑ wi(t)Li(t), we aim to learn the functions wi(t) with the following goals: (1) to place gradient norms for different tasks on a common scale through which we can reason about their relative magnitudes, and (2) to dynamically adjust gradient norms so different tasks train at similar rates.",3.1. Definitions and Preliminaries,[0],[0]
"To this end, we first define the relevant quantities, first with respect to the gradients we will be manipulating.
",3.1. Definitions and Preliminaries,[0],[0]
•,3.1. Definitions and Preliminaries,[0],[0]
W : The subset of the full network weights W ⊂ W where we actually apply GradNorm.,3.1. Definitions and Preliminaries,[0],[0]
"W is generally chosen as the last shared layer of weights to save on compute costs1.
",3.1. Definitions and Preliminaries,[0],[0]
• G(i)W (t) = ||∇Wwi(t)Li(t)||2: the L2 norm of the gradient of the weighted single-task loss wi(t)Li(t),3.1. Definitions and Preliminaries,[0],[0]
"with respect to the chosen weights W .
• GW (t) = Etask[G(i)W (t)]: the average gradient norm across all tasks at training time t.
We also define various training rates for each task i:
• L̃i(t) = Li(t)/Li(0): the loss ratio for task",3.1. Definitions and Preliminaries,[0],[0]
"i at time t. L̃i(t) is a measure of the inverse training rate of task i (i.e. lower values of L̃i(t) correspond to a faster training rate for task i)2.
• ri(t) = L̃i(t)/Etask[L̃i(t)]",3.1. Definitions and Preliminaries,[0],[0]
": the relative inverse training rate of task i.
With the above definitions in place, we now complete our description of the GradNorm algorithm.",3.1. Definitions and Preliminaries,[0],[0]
"As stated in Section 3.1, GradNorm should establish a common scale for gradient magnitudes, and also should balance
1In our experiments this choice of W causes GradNorm to increase training time by only ∼ 5% on NYUv2.
2Networks in this paper all had stable initializations and Li(0) could be used directly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When Li(0) is sharply dependent on initialization, we can use a theoretical initial loss instead.",3.2. Balancing Gradients with GradNorm,[0],[0]
"E.g. for Li the CE loss across C classes, we can use Li(0) = log(C).
training rates of different tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The common scale for gradients is most naturally the average gradient norm, GW (t), which establishes a baseline at each timestep t by which we can determine relative gradient sizes.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The relative inverse training rate of task i, ri(t), can be used to rate balance our gradients.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Concretely, the higher the value of ri(t), the higher the gradient magnitudes should be for task i in order to encourage the task to train more quickly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Therefore, our desired gradient norm for each task i is simply:
G (i) W (t) 7→ GW (t)× [ri(t)]",3.2. Balancing Gradients with GradNorm,[0],[0]
"α, (1)
where α is an additional hyperparameter.",3.2. Balancing Gradients with GradNorm,[0],[0]
α sets the strength of the restoring force which pulls tasks back to a common training rate.,3.2. Balancing Gradients with GradNorm,[0],[0]
"In cases where tasks are very different in their complexity, leading to dramatically different learning dynamics between tasks, a higher value of α should be used to enforce stronger training rate balancing.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When tasks are more symmetric (e.g. the synthetic examples in Section 4), a lower value of α is appropriate.",3.2. Balancing Gradients with GradNorm,[0],[0]
Note that α = 0 will always try to pin the norms of backpropagated gradients from each task to be equal at W .,3.2. Balancing Gradients with GradNorm,[0],[0]
"See Section 5.4 for more details on the effects of tuning α.
",3.2. Balancing Gradients with GradNorm,[0],[0]
"Equation 1 gives a target for each task i’s gradient norms, and we update our loss weights wi(t) to move gradient
norms towards this target for each task.",3.2. Balancing Gradients with GradNorm,[0],[0]
"GradNorm is then implemented as an L1 loss function Lgrad between the actual and target gradient norms at each timestep for each task, summed over all tasks:
Lgrad(t;wi(t))",3.2. Balancing Gradients with GradNorm,[0],[0]
"= ∑ i ∣∣∣∣G(i)W (t)−GW (t)× [ri(t)]α∣∣∣∣ 1 (2)
where the summation runs through all T tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When differentiating this loss Lgrad, we treat the target gradient norm GW (t)× [ri(t)]α as a fixed constant to prevent loss weights wi(t) from spuriously drifting towards zero.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Lgrad is then differentiated only with respect to the wi, as the wi(t) directly control gradient magnitudes per task.",3.2. Balancing Gradients with GradNorm,[0],[0]
The computed gradients ∇wiLgrad are then applied via standard update rules to update each wi (as shown in Figure 1).,3.2. Balancing Gradients with GradNorm,[0],[0]
The full GradNorm algorithm is summarized in Algorithm 1.,3.2. Balancing Gradients with GradNorm,[0],[0]
"Note that after every update step, we also renormalize the weights wi(t) so that ∑ i wi(t) = T in order to decouple gradient normalization from the global learning rate.",3.2. Balancing Gradients with GradNorm,[0],[0]
"To illustrate GradNorm on a simple, interpretable system, we construct a common scenario for multitask networks: training tasks which have similar loss functions but different loss scales.",4. A Toy Example,[0],[0]
"In such situations, if we naı̈vely pick wi(t) = 1
Algorithm 1 Training with GradNorm Initialize wi(0) = 1 ∀i Initialize network weightsW Pick value for α > 0 and pick the weightsW (usually the
final layer of weights which are shared between tasks) for t = 0",4. A Toy Example,[0],[0]
"to max train steps do
Input batch xi to compute Li(t) ∀i and L(t) = ∑ i wi(t)Li(t)",4. A Toy Example,[0],[0]
"[standard forward pass] Compute G(i)W (t) and ri(t) ∀i Compute GW (t) by averaging the G (i) W (t)
Compute Lgrad = ∑ i|G (i) W (t)−GW (t)× [ri(t)]α|1 Compute GradNorm gradients∇wiLgrad, keeping targets GW (t)× [ri(t)]α constant Compute standard gradients∇WL(t) Update wi(t) 7→ wi(t+ 1) using ∇wiLgrad UpdateW(t) 7→ W(t+ 1) using∇WL(t)",4. A Toy Example,[0],[0]
"[standard
backward pass] Renormalize wi(t+ 1) so that ∑ i wi(t+ 1) = T
end for
for all loss weights wi(t), the network training will be dominated by tasks with larger loss scales that backpropagate larger gradients.",4. A Toy Example,[0],[0]
"We will demonstrate that GradNorm overcomes this issue.
",4. A Toy Example,[0],[0]
"Consider T regression tasks trained using standard squared loss onto the functions
fi(x) =",4. A Toy Example,[0],[0]
"σi tanh((B + i)x), (3)
where tanh(·) acts element-wise.",4. A Toy Example,[0],[0]
"Inputs are dimension 250 and outputs dimension 100, while B and i are constant matrices with their elements generated IID from N (0, 10) and N (0, 3.5), respectively.",4. A Toy Example,[0],[0]
Each task therefore shares information in B but also contains task-specific information i.,4. A Toy Example,[0],[0]
The σi are the key parameters within this problem; they are fixed scalars which set the scales of the outputs fi.,4. A Toy Example,[0],[0]
A higher scale for fi induces a higher expected value of squared loss for that task.,4. A Toy Example,[0],[0]
"Such tasks are harder to learn due to the higher variances in their response values, but they also backpropagate larger gradients.",4. A Toy Example,[0],[0]
"This scenario generally leads to suboptimal training dynamics when the higher σi tasks dominate the training across all tasks.
",4. A Toy Example,[0],[0]
"To train our toy models, we use a 4-layer fully-connected ReLU-activated network with 100 neurons per layer as a common trunk.",4. A Toy Example,[0],[0]
A final affine transformation layer gives T final predictions (corresponding to T different tasks).,4. A Toy Example,[0],[0]
"To ensure valid analysis, we only compare models initialized to the same random values and fed data generated from the same fixed random seed.",4. A Toy Example,[0],[0]
"The asymmetry α is set low to 0.12 for these experiments, as the output functions fi are all of the same functional form and thus we expect the asymmetry between tasks to be minimal.
",4. A Toy Example,[0],[0]
"In these toy problems, we measure the task-normalized testtime loss to judge test-time performance, which is the sum of the test loss ratios for each task, ∑ i Li(t)/Li(0).",4. A Toy Example,[0],[0]
We do this because a simple sum of losses is an inadequate performance metric for multitask networks when different loss scales exist; higher loss scale tasks will factor disproportionately highly in the loss.,4. A Toy Example,[0],[0]
"There unfortunately exists no general single scalar which gives a meaningful measure of multitask performance in all scenarios, but our toy problem was specifically designed with tasks which are statistically identical except for their loss scales σi.",4. A Toy Example,[0],[0]
"There is therefore a clear measure of overall network performance, which is the sum of losses normalized by each task’s variance σ2i - equivalent (up to a scaling factor) to the sum of loss ratios.
",4. A Toy Example,[0],[0]
"For T = 2, we choose the values (σ0, σ1)",4. A Toy Example,[0],[0]
"= (1.0, 100.0) and show the results of training in the top panels of Figure 2.",4. A Toy Example,[0],[0]
"If we train with equal weightswi = 1, task 1 suppresses task 0 from learning due to task 1’s higher loss scale.",4. A Toy Example,[0],[0]
"However, gradient normalization increases w0(t) to counteract the larger gradients coming from T1, and the improved task balance results in better test-time performance.
",4. A Toy Example,[0],[0]
The possible benefits of gradient normalization become even clearer when the number of tasks increases.,4. A Toy Example,[0],[0]
"For T = 10, we sample the σi from a wide normal distribution and plot the results in the bottom panels of Figure 2.",4. A Toy Example,[0],[0]
GradNorm significantly improves test time performance over naı̈vely weighting each task the same.,4. A Toy Example,[0],[0]
"Similarly to the T = 2 case, for T = 10 the wi(t) grow larger for smaller σi tasks.
",4. A Toy Example,[0],[0]
"For both T = 2 and T = 10, GradNorm is more stable and outperforms the uncertainty weighting proposed by (Kendall et al., 2017).",4. A Toy Example,[0],[0]
"Uncertainty weighting, which enforces that wi(t) ∼ 1/Li(t), tends to grow the weights wi(t) too large and too quickly as the loss for each task drops.",4. A Toy Example,[0],[0]
"Although such networks train quickly at the onset, the training soon deteriorates.",4. A Toy Example,[0],[0]
"This issue is largely caused by the fact that uncertainty weighting allows wi(t) to change without constraint (compared to GradNorm which ensures∑ wi(t) = T always), which pushes the global learning rate up rapidly as the network trains.
",4. A Toy Example,[0],[0]
The traces for each wi(t) during a single GradNorm run are observed to be stable and convergent.,4. A Toy Example,[0],[0]
"In Section 5.3 we will see how the time-averaged weightsEt[wi(t)] lie close to the optimal static weights, suggesting GradNorm can greatly simplify the tedious grid search procedure.",4. A Toy Example,[0],[0]
"We use two variants of NYUv2 (Nathan Silberman & Fergus, 2012) as our main datasets.",5. Application to a Large Real-World Dataset,[0],[0]
"Please refer to the Supplementary Materials for additional results on a 9-task facial landmark dataset found in (Zhang et al., 2014).",5. Application to a Large Real-World Dataset,[0],[0]
"The standard NYUv2 dataset carries depth, surface normals, and semantic
segmentation labels (clustered into 13 distinct classes) for a variety of indoor scenes in different room types (bathrooms, living rooms, studies, etc.).",5. Application to a Large Real-World Dataset,[0],[0]
"NYUv2 is relatively small (795 training, 654 test images), but contains both regression and classification labels, making it a good choice to test the robustness of GradNorm across various tasks.
",5. Application to a Large Real-World Dataset,[0],[0]
"We augment the standard NYUv2 depth dataset with flips and additional frames from each video, resulting in 90,000 images complete with pixel-wise depth, surface normals, and room keypoint labels (segmentation labels are, unfortunately, not available for these additional frames).",5. Application to a Large Real-World Dataset,[0],[0]
"Keypoint labels are professionally annotated by humans, while surface normals are generated algorithmically.",5. Application to a Large Real-World Dataset,[0],[0]
The full dataset is then split by scene for a 90/10 train/test split.,5. Application to a Large Real-World Dataset,[0],[0]
See Figure 6 for examples.,5. Application to a Large Real-World Dataset,[0],[0]
"We will generally refer to these two datasets as NYUv2+seg and NYUv2+kpts, respectively.
",5. Application to a Large Real-World Dataset,[0],[0]
All inputs are downsampled to 320 x 320 pixels and outputs to 80 x 80 pixels.,5. Application to a Large Real-World Dataset,[0],[0]
"We use these resolutions following (Lee et al., 2017), which represents the state-of-the-art in room keypoint prediction and from which we also derive our VGG-style model architecture.",5. Application to a Large Real-World Dataset,[0],[0]
These resolutions also allow us to keep models relatively slim while not compromising semantic complexity in the ground truth output maps.,5. Application to a Large Real-World Dataset,[0],[0]
"We try two different models: (1) a SegNet (Badrinarayanan et al., 2015; Lee et al., 2017) network with a symmetric VGG16 (Simonyan & Zisserman, 2014) encoder/decoder,
and (2) an FCN (Long et al., 2015) network with a modified ResNet-50",5.1. Model and General Training Characteristics,[0],[0]
"(He et al., 2016) encoder and shallow ResNet decoder.",5.1. Model and General Training Characteristics,[0],[0]
"The VGG SegNet reuses maxpool indices to perform upsampling, while the ResNet FCN learns all upsampling filters.",5.1. Model and General Training Characteristics,[0],[0]
"The ResNet architecture is further thinned (both in its filters and activations) to contrast with the heavier, more complex VGG SegNet:",5.1. Model and General Training Characteristics,[0],[0]
stride-2 layers are moved earlier and all 2048-filter layers are replaced by 1024-filter layers.,5.1. Model and General Training Characteristics,[0],[0]
"Ultimately, the VGG SegNet has 29M parameters versus 15M for the thin ResNet.",5.1. Model and General Training Characteristics,[0],[0]
All model parameters are shared amongst all tasks until the final layer.,5.1. Model and General Training Characteristics,[0],[0]
"Although we will focus on the VGG SegNet in our more in-depth analysis, by designing and testing on two extremely different network topologies we will further demonstrate GradNorm’s robustness to the choice of base architecture.
",5.1. Model and General Training Characteristics,[0],[0]
"We use standard pixel-wise loss functions for each task: cross entropy for segmentation, squared loss for depth, and cosine similarity for normals.",5.1. Model and General Training Characteristics,[0],[0]
"As in (Lee et al., 2017), for room layout we generate Gaussian heatmaps for each of 48 room keypoint types and predict these heatmaps with a pixel-wise squared loss.",5.1. Model and General Training Characteristics,[0],[0]
"Note that all regression tasks are quadratic losses (our surface normal prediction uses a cosine loss which is quadratic to leading order), allowing us to use ri(t) for each task i as a direct proxy for each task’s relative inverse training rate.
",5.1. Model and General Training Characteristics,[0],[0]
All runs are trained at a batch size of 24 across 4 Titan X GTX 12GB GPUs and run at 30fps on a single GPU at inference.,5.1. Model and General Training Characteristics,[0],[0]
All NYUv2 runs begin with a learning rate of 2e5.,5.1. Model and General Training Characteristics,[0],[0]
"NYUv2+kpts runs last 80000 steps with a learning rate
decay of 0.2 every 25000 steps.",5.1. Model and General Training Characteristics,[0],[0]
NYUv2+seg runs last 20000 steps with a learning rate decay of 0.2 every 6000 steps.,5.1. Model and General Training Characteristics,[0],[0]
"Updating wi(t) is performed at a learning rate of 0.025 for both GradNorm and the uncertainty weighting ((Kendall et al., 2017)) baseline.",5.1. Model and General Training Characteristics,[0],[0]
"All optimizers are Adam, although we find that GradNorm is insensitive to the optimizer chosen.",5.1. Model and General Training Characteristics,[0],[0]
We implement GradNorm using TensorFlow v1.2.1.,5.1. Model and General Training Characteristics,[0],[0]
In Table 1 we display the performance of GradNorm on the NYUv2+seg dataset.,5.2. Main Results on NYUv2,[0],[0]
"We see that GradNorm α = 1.5 improves the performance of all three tasks with respect to the equal-weights baseline (where wi(t) = 1 for all t,i), and either surpasses or matches (within statistical noise) the best performance of single networks for each task.",5.2. Main Results on NYUv2,[0],[0]
"The GradNorm Static network uses static weights derived from a GradNorm network by calculating the time-averaged weights Et[wi(t)] for each task during a GradNorm training run, and retraining a network with weights fixed to those values.",5.2. Main Results on NYUv2,[0],[0]
GradNorm thus can also be used to extract good values for static weights.,5.2. Main Results on NYUv2,[0],[0]
"We pursue this idea further in Section 5.3 and show that these weights lie very close to the optimal weights extracted from exhaustive grid search.
",5.2. Main Results on NYUv2,[0],[0]
"To show how GradNorm can perform in the presence of a larger dataset, we also perform extensive experiments on the NYUv2+kpts dataset, which is augmented to a factor of 50x more data.",5.2. Main Results on NYUv2,[0],[0]
The results are shown in Table 2.,5.2. Main Results on NYUv2,[0],[0]
"As with the NYUv2+seg runs, GradNorm networks outperform other multitask methods, and either matches (within noise) or surpasses the performance of single-task networks.
",5.2. Main Results on NYUv2,[0],[0]
Figure 3 shows test and training loss curves for GradNorm (α = 1.5) and baselines on the larger NYUv2+kpts dataset for our VGG SegNet models.,5.2. Main Results on NYUv2,[0],[0]
"GradNorm improves test-time depth error by ∼ 5%, despite converging to a much higher training loss.",5.2. Main Results on NYUv2,[0],[0]
"GradNorm achieves this by aggressively rate balancing the network (enforced by a high asymmetry α = 1.5), and ultimately suppresses the depth weight wdepth(t) to lower than 0.10 (see Section 5.4 for more details).",5.2. Main Results on NYUv2,[0],[0]
"The same
trend exists for keypoint regression, and is a clear signal of network regularization.",5.2. Main Results on NYUv2,[0],[0]
"In contrast, uncertainty weighting (Kendall et al., 2017) always moves test and training error in the same direction, and thus is not a good regularizer.",5.2. Main Results on NYUv2,[0],[0]
"Only results for the VGG SegNet are shown here, but the Thin ResNet FCN produces consistent results.",5.2. Main Results on NYUv2,[0],[0]
"For our VGG SegNet, we train 100 networks from scratch with random task weights on NYUv2+kpts.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
Weights are sampled from a uniform distribution and renormalized to sum to T = 3.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"For computational efficiency, we only train for 15000 iterations out of the normal 80000, and then compare the performance of that network to our GradNorm
α = 1.5 VGG SegNet network at the same 15000 steps.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"The results are shown in Figure 4.
",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even after 100 networks trained, grid search still falls short of our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even more remarkably, there is a strong, negative correlation between network performance and task weight distance to our time-averaged GradNorm weights Et[wi(t)].",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"At an L2 distance of ∼ 3, grid search networks on average have almost double the errors per task compared to our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
GradNorm has therefore found the optimal grid search weights in one single training run.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
The only hyperparameter in our algorithm is the asymmetry α.,5.4. Effects of tuning the asymmetry α,[0],[0]
"The optimal value of α for NYUv2 lies near α = 1.5, while in the highly symmetric toy example in Section 4 we used α = 0.12.",5.4. Effects of tuning the asymmetry α,[0],[0]
"This observation reinforces our characterization of α as an asymmetry parameter.
",5.4. Effects of tuning the asymmetry α,[0],[0]
"Tuning α leads to performance gains, but we found that for NYUv2, almost any value of 0",5.4. Effects of tuning the asymmetry α,[0],[0]
< α < 3 will improve network performance over an equal weights baseline (see Supplementary for details).,5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 5 shows that higher values of α tend to push the weights wi(t) further apart, which more aggressively reduces the influence of tasks which overfit or learn too quickly (in our case, depth).",5.4. Effects of tuning the asymmetry α,[0],[0]
"Remarkably, at α = 1.75 (not shown) wdepth(t) is suppressed to below 0.02 at no detriment to network performance on the depth task.",5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 6 shows visualizations of the VGG SegNet outputs on test set images along with the ground truth, for both the NYUv2+seg and NYUv2+kpts datasets.",5.5. Qualitative Results,[0],[0]
"Ground truth labels are juxtaposed with outputs from the equal weights network, 3 single networks, and our best GradNorm network.",5.5. Qualitative Results,[0],[0]
"Some
improvements are incremental, but GradNorm produces superior visual results in tasks for which there are significant quantitative improvements in Tables 1 and 2.",5.5. Qualitative Results,[0],[0]
"We introduced GradNorm, an efficient algorithm for tuning loss weights in a multi-task learning setting based on balancing the training rates of different tasks.",6. Conclusions,[0],[0]
"We demonstrated on both synthetic and real datasets that GradNorm improves multitask test-time performance in a variety of scenarios, and can accommodate various levels of asymmetry amongst the different tasks through the hyperparameter α.",6. Conclusions,[0],[0]
"Our empirical results indicate that GradNorm offers su-
perior performance over state-of-the-art multitask adaptive weighting methods and can match or surpass the performance of exhaustive grid search while being significantly less time-intensive.
",6. Conclusions,[0],[0]
"Looking ahead, algorithms such as GradNorm may have applications beyond multitask learning.",6. Conclusions,[0],[0]
"We hope to extend the GradNorm approach to work with class-balancing and sequence-to-sequence models, all situations where problems with conflicting gradient signals can degrade model performance.",6. Conclusions,[0],[0]
"We thus believe that our work not only provides a robust new algorithm for multitask learning, but also reinforces the powerful idea that gradient tuning is fundamental for training large, effective models on complex tasks.",6. Conclusions,[0],[0]
"Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly.",abstractText,[0],[0]
We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes.,abstractText,[0],[0]
"We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques.",abstractText,[0],[0]
"GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter α.",abstractText,[0],[0]
"Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks.",abstractText,[0],[0]
"Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",abstractText,[0],[0]
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,title,[0],[0]
"Deep neural networks have become the state-of-the-art systems for image recognition (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Qiao et al., 2018; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Wang et al., 2017; Zeiler & Fergus, 2013) as well as other vision tasks (Chen et al., 2015; Girshick et al., 2014; Long et al., 2015; Qiao et al., 2017; Ren et al., 2015; Shen et al., 2015; Xie & Tu, 2015).",1. Introduction,[0],[0]
"The architectures keep going deeper, e.g., from five convolutional layers (Krizhevsky et al., 2012) to 1001 layers (He et al., 2016b).",1. Introduction,[0],[0]
"The benefit of deep architectures is their strong learning capacities because each new layer can potentially introduce more non-linearities and typically uses larger receptive fields (Simonyan & Zisserman, 2014).",1. Introduction,[0],[0]
"In addition, adding certain types of layers (e.g. (He et al., 2016b)) will not harm the performance theoretically since they can just learn identity mapping.",1. Introduction,[0],[0]
"This makes stacking up layers more appealing in the network designs.
",1. Introduction,[0],[0]
1Johns Hopkins University 2Shanghai University 3Hikvision Research.,1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Siyuan Qiao <siyuan.qiao@jhu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Although deeper architectures usually lead to stronger learning capacities, cascading convolutional layers (e.g. VGG (Simonyan & Zisserman, 2014)) or blocks (e.g. ResNet (He et al., 2016a)) is not necessarily the only method to achieve this goal.",1. Introduction,[0],[0]
"In this paper, we present a new way to increase the depth of the networks as an alternative to stacking up convolutional layers or blocks.",1. Introduction,[0],[0]
Figure 2 provides an illustration that compares our proposed convolutional network that gradually updates the feature representations against the traditional convolutional network that computes its output simultaneously.,1. Introduction,[0],[0]
"By only adding an ordering to the channels without any additional computation, the later computed channels become deeper than the corresponding ones in the traditional convolutional network.",1. Introduction,[0],[0]
We refer to the neural networks with the proposed computation orderings on the channels as Gradually Updated Neural Networks (GUNN).,1. Introduction,[0],[0]
Figure 1 provides two examples of architecture designs based on cascading building blocks and GUNN.,1. Introduction,[0],[0]
"Without repeating the building blocks, GUNN increases the depths of the networks as well as their learning capacities.
",1. Introduction,[0],[0]
It is clear that converting plain networks to GUNN increases the depths of the networks without any additional computations.,1. Introduction,[0],[0]
"What is less obvious is that GUNN in fact eliminates the overlap singularities inherent in the loss landscapes of the cascading-based convolutional networks, which have been shown to adversely affect the training of deep neural networks as well as their performances (Wei et al., 2008;
Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"Overlap singularity is when internal neurons collapse into each other, i.e. they are unidentifiable by their activations.",1. Introduction,[0],[0]
"It happens in the networks, increases the training difficulties and degrades the performances (Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"However, if a plain network is converted to GUNN, the added computation orderings will break the symmetry between the neurons.",1. Introduction,[0],[0]
We prove that the internal neurons in GUNN are impossible to collapse into each other.,1. Introduction,[0],[0]
"As a result, the effective dimensionality can be kept during training and the model will be free from the degeneracy caused by collapsed neurons.",1. Introduction,[0],[0]
"Reflected in the training dynamics and the performances, this means that converting to GUNN will make the plain networks easier to train and perform better.",1. Introduction,[0],[0]
"Figure 3 compares the training dynamics of a 15-layer plain network on CIFAR-10 dataset (Krizhevsky & Hinton, 2009) before and after converted to GUNN.
",1. Introduction,[0],[0]
"In this paper, we test our proposed GUNN on highly competitive benchmark datasets, i.e. CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015).",1. Introduction,[0],[0]
Experimental results demonstrate that our proposed GUNNbased networks achieve the state-of-the-art performances compared with the previous cascading-based architectures.,1. Introduction,[0],[0]
"The research focuses of image recognition have moved from feature designs (Dalal & Triggs, 2005; Lowe, 2004) to architecture designs (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Sermanet et al., 2014; Simonyan
& Zisserman, 2014; Szegedy et al., 2015; Xie et al., 2017; Zeiler & Fergus, 2013) due to the recent success of the deep neural networks.",2. Related Work,[0],[0]
"Highway Networks (Srivastava et al., 2015) proposed architectures that can be trained end-to-end with more than 100 layers.",2. Related Work,[0],[0]
The main idea of Highway Networks is to use bypassing paths.,2. Related Work,[0],[0]
"This idea was further investigated in ResNet (He et al., 2016a), which simplifies the bypassing paths by using only identity mappings.",2. Related Work,[0],[0]
"As learning ultra-deep networks became possible, the depths of the models have increased tremendously.",2. Related Work,[0],[0]
"ResNet with pre-activation (He et al., 2016b) and ResNet with stochastic depth (Huang et al., 2016) even managed to train neural networks with more than 1000 layers.",2. Related Work,[0],[0]
"FractalNet (Larsson et al., 2016) argued that in addition to summation, concatenation also helps train a deep architecture.",2. Related Work,[0],[0]
"More recently, ResNeXt (Xie et al., 2017) used group convolutions in ResNet and outperformed the original ResNet.",2. Related Work,[0],[0]
"DenseNet (Huang et al., 2017b) proposed an architecture with dense connections by feature concatenation.",2. Related Work,[0],[0]
"Dual Path Net (Chen et al., 2017) finds a middle point between ResNet and DenseNet by concatenating them in two paths.",2. Related Work,[0],[0]
"Unlike the above cascading-based methods, GUNN eliminates the overlap singularities caused by the architecture symmetry.",2. Related Work,[0],[0]
"The detailed analyses can be found in Section 4.3.
",2. Related Work,[0],[0]
"Alternative to increasing the depth of the neural networks, another trend is to increase the widths of the networks.",2. Related Work,[0],[0]
"GoogleNet (Szegedy et al., 2015; 2016) proposed an Inception module to concatenate feature maps produced by different filters.",2. Related Work,[0],[0]
"Following ResNet (He et al., 2016a), the WideResNet (Zagoruyko & Komodakis, 2016) argued that compared with increasing the depth, increasing the width of the networks can be more effective in improving the performances.",2. Related Work,[0],[0]
"Besides varying the width and the depth, there are also other design strategies for deep neural networks (Hariharan et al., 2015; Kontschieder et al., 2015; Pezeshki et al., 2016; Rasmus et al., 2015; Yang & Ramanan, 2015).",2. Related Work,[0],[0]
"Deeply-Supervised Nets (Lee et al., 2014) used auxiliary classifiers to provide direct supervisions for the internal layers.",2. Related Work,[0],[0]
"Network in Network (Lin et al., 2013) adds micro perceptrons to the convolutional layers.",2. Related Work,[0],[0]
"We consider a feature transformation F : Rm×n → Rm×n, where n denotes the channel of the features and m denotes the feature location on the 2-D feature map.",3.1. Feature Update,[0],[0]
"For example, F can be a convolutional layer with n channels for both the input and the output.",3.1. Feature Update,[0],[0]
Let x ∈,3.1. Feature Update,[0],[0]
Rm×n be the input and y ∈,3.1. Feature Update,[0],[0]
"Rm×n be the output, we have
y = F(x) (1)
Suppose that F can be decomposed into channel-wise transformation Fc(·) that are independent with eath other, then for any location k and channel c we have
ykc = Fc(xr(k))",3.1. Feature Update,[0],[0]
"(2)
where xr(k) denotes the receptive field of the location k",3.1. Feature Update,[0],[0]
"and Fc denotes the transformation on channel c.
Let UC denote a feature update on channel set C, i.e.,
UC(x) :",3.1. Feature Update,[0],[0]
"y k c = Fc(xr(k)),∀c ∈ C, k ykc",3.1. Feature Update,[0],[0]
"= x k c ,∀c",3.1. Feature Update,[0],[0]
"∈ C, k
(3)
",3.1. Feature Update,[0],[0]
"Then, UC = F when C = {1, ..., n}.",3.1. Feature Update,[0],[0]
"By defining the feature update UC on channel set C, the commonly used one-layer CNN is a special case of feature updates where every channel is updated simultaneously.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, we can also update the channels gradually.",3.2. Gradually Updated Neural Networks,[0],[0]
"For example, the proposed GUNN can be formulated by
GUNN(x) =",3.2. Gradually Updated Neural Networks,[0],[0]
(Ucl ◦ Uc(l−1),3.2. Gradually Updated Neural Networks,[0],[0]
"◦ ... ◦ Uc2 ◦ Uc1)(x)
",3.2. Gradually Updated Neural Networks,[0],[0]
"where l⋃
i=1
ci = {1, 2, ..., n} and ci ∩ cj = Φ, ∀i 6= j
(4) When l = 1, GUNN is equivalent to F .
",3.2. Gradually Updated Neural Networks,[0],[0]
"Note that the number of parameters and computation of GUNN are the same as those of the corresponding F for any partitions c1, ..., cl of {1, ..., n}.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, by decomposing F into channel-wise transformations and sequentially applying them, the later computed channels are deeper than the previous ones.",3.2. Gradually Updated Neural Networks,[0],[0]
"As a result, the depth of the network can be increased, as well as the network’s learning capacity.",3.2. Gradually Updated Neural Networks,[0],[0]
"We consider the residual learning proposed by ResNet (He et al., 2016a) in our model.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Specifically, we consider the channel-wise transformation Fc : Rm×n → Rm×1 to be
Fc(x) = Gc(x) + xc (5)
",3.3. Channel-wise Update by Residual Learning,[0],[0]
Algorithm 1 Back-propagation for GUNN Input :U(·) =,3.3. Channel-wise Update by Residual Learning,[0],[0]
(Ucl ◦ Uc(l−1),3.3. Channel-wise Update by Residual Learning,[0],[0]
"◦ ... ◦ Uc1)(·), input x,
output y = U(x), gradients ∂L/∂y, and parameters Θ for U .
",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Output :∂L/∂Θ, ∂L/∂x ∂L/∂x← ∂L/∂y for i←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"l to 1 do
yc ←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"xc, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"∈ ci ∂L/∂y, ∂L/∂Θci ← BP(y, ∂L/∂x, Uci ,Θci) (∂L/∂x)c ← (∂L/∂y)c, ∀c ∈ ci (∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"6∈ ci
end
where Gc is a convolutional neural network Gc : Rm×n → Rm×1.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"The motivation of expressing F in a residual learning manner is to reduce overlap singularities (Orhan & Pitkow, 2018), which will be discussed in Section 4.",3.3. Channel-wise Update by Residual Learning,[0],[0]
Here we show the backpropagation algorithm for learning the parameters in GUNN that uses the same amount of computations and memory as in F .,3.4. Learning GUNN by Backpropagation,[0],[0]
"In Eq. 4, let the feature update Uci be parameterized by Θci .",3.4. Learning GUNN by Backpropagation,[0],[0]
"Let BP(x, ∂L/∂y, f,Θ) be the back-propagation algorithm for differentiable function y = f(x; Θ) with the loss L and the parameters Θ. Algorithm 1 presents the back-propagation algorithm for GUNN.",3.4. Learning GUNN by Backpropagation,[0],[0]
"Since Uci has the residual structures (He et al., 2016a), the last two steps can be merged into
(∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c (6)
which further simplifies the implementation.",3.4. Learning GUNN by Backpropagation,[0],[0]
It is easy to see that converting networks to GUNN-based does not increase the memory usage in feed-forwarding.,3.4. Learning GUNN by Backpropagation,[0],[0]
"Given Algorithm 1, converting networks to GUNN will not affect the memory in both the training and the evaluation.",3.4. Learning GUNN by Backpropagation,[0],[0]
Overlap singularities are inherent in the loss landscapes of some network architectures which are caused by the nonidentifiability of subsets of the neurons.,4. GUNN Eliminates Overlap Singularities,[0],[0]
"They are identified and discussed in previous work (Wei et al., 2008; Anandkumar & Ge, 2016; Orhan & Pitkow, 2018), and are shown to be harmful for the performances of deep networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Intuitively, overlap singularities exist in architectures where the internal neurons collapse into each other.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"As a result, the models are degenerate and the effective dimensionality is reduced.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"(Orhan & Pitkow, 2018) demonstrated through experiments that residual learning (see Eq. 5) helps to reduce the overlap singularities in deep networks, which partly explains the exceptional performances of ResNet (He et al., 2016a) compared with plain networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"In the following, we first use linear transformation as an example to demonstrate
how GUNN-based networks break the overlap singularities.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Then, we generalize the results to ReLU DNN.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Finally, we compare GUNN with the previous state-of-the-art network architectures from the perspective of singularity elimination.",4. GUNN Eliminates Overlap Singularities,[0],[0]
Consider a linear function y = f(x) :,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Rn → Rn such that
yi = n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (7)
Suppose that there exists a pair of collapsed neurons yp and yq (p < q).",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, for ∀x, yp = yq, and the equality holds after any number of gradient descents, i.e. ∆yp = ∆yq .
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
Eq. 7 describes a plain network.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"The solution for the existence of yp and yq is that ωp,j = ωq,j ,∀j.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This is the case that is mostly discussed previously, which happens in the networks and degrades the performances.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When we add the residual learning, Eq. 7 becomes
yi = xi + n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (8)
Collapsed neurons require that ωp,p + 1 = ωq,p, ωq,q + 1 = ωp,q.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This will make the collapse of yp and yq very hard when ω is initialized from a normal distribution N (0, √ 2/n) as in ResNet, but still possible.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Next, we convert Eq. 8 to GUNN, i.e.,
yi = xi + i−1∑ j=1 ωi,jyj + n∑ j=i ωi,jxj , ∀i ∈ {1, .., n} (9)
Suppose that yp and yq (p < q) collapse.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Consider ∆y, the value difference at x after one step of gradient descent on ω with input x, ∂L/∂y and learning rate .",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When → 0,
∆yi = ∂L
∂yi ( i−1∑ j=1 y2j + n∑ j=i x2j )",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"+ i−1∑ j=1 ωi,j∆yj (10)
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"As ∆yp = ∆yq,∀x, we have ωq,j = 0, ∀j : p",4.1. Overlap Singularities in Linear Transformations,[0],[0]
< j < q.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"But this condition will be broken in the next update; thus, q = p + 1.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, we derive that yp = yq = 0.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
But these will also be broken in the next step of gradient descent optimization.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Hence, yp and yq cannot collapse into each other.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
The complete proof can be found in the appendix.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"In practice, architectures are usually composed of several linear layers and non-linearity layers.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Analyzing all the possible architectures is beyond our scope.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Here, we discuss the commonly used ReLU DNN, in which only linear transformations and ReLUs are used by simple layer cascading.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Following the notations in §3, we use y = G(x) + x, in which G(x) is a ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Note that G is continuous piecewise linear (PWL) function (Arora et al., 2018), which means that there exists a finite set of polyhedra whose union is Rn, and G is affine linear over each polyhedron.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Suppose that we convert G(x)+x to GUNN and there exists a pair of collapsed neurons yp and yq (p < q).,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, the set of polyhedra for yp is the same as for yq.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Let P be a polyhedron for yp and yq defined above.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, ∀x,P, i,
yi = xi + i−1∑ j=1 ωi,j(P)yj + n∑ j=i ωi,j(P)xj (11)
where ω(P) denotes the parameters for polyhedron P. Note that on each P, y is a function of x in the form of Eq. 9; hence, yp and yq cannot collapse into each other.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Since the union of all polyhedra is Rn, we conclude that GUNN eliminates the overlap singularities in ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"The previous two subsections consider the GUNN conversion where |ci| = 1,∀i (see Eq. 4).",4.3. Discussions and Comparisons,[0],[0]
But this will slow down the computation on GPU due to the data dependency.,4.3. Discussions and Comparisons,[0],[0]
"Without specialized hardware or library support, we decide to increase |ci| to > 10.",4.3. Discussions and Comparisons,[0],[0]
"The resulted models run at the speed between ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b).",4.3. Discussions and Comparisons,[0],[0]
But this change introduces singularities into the channels from the same set ci.,4.3. Discussions and Comparisons,[0],[0]
"Then, the residual learning helps GUNN to reduce the singularities within the same set ci since we initialize the parameters from a normal distributionN (0, √ 2/n).",4.3. Discussions and Comparisons,[0],[0]
"We will compare the results of GUNN with and without residual learning in the experiments.
",4.3. Discussions and Comparisons,[0],[0]
We compare GUNN with the state-of-the-art architectures from the perspective of overlap singularities.,4.3. Discussions and Comparisons,[0],[0]
"ResNet (He et al., 2016a) and its variants use residual learning, which reduces but cannot eliminate the singularities.",4.3. Discussions and Comparisons,[0],[0]
"ResNeXt (Xie et al., 2017) uses group convolutions to break the symmetry between groups, which further helps to avoid neuron collapses.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet (Huang et al., 2017b) concatenates the outputs of layers as the input to the next layer.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet and GUNN both create dense connections, while DenseNet reuses the outputs by concatenating and GUNN by adding them back to the inputs.",4.3. Discussions and Comparisons,[0],[0]
But the channels within the same layer of DenseNet are still possible to collapse into each other since they are symmetric.,4.3. Discussions and Comparisons,[0],[0]
"In contrast, adding back makes residual learning possible in GUNN.",4.3. Discussions and Comparisons,[0],[0]
This makes residual learning indispensable in GUNN-based networks.,4.3. Discussions and Comparisons,[0],[0]
"In this section, we will present the details of our architectures for the CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015) datasets.",5. Network Architectures,[0],[0]
"Since the proposed GUNN is a method for increasing the depths of the convolutional networks, specifying the architectures to be converted is equivalent to specifying the GUNN-based architectures.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The architectures before conversion, the Simultaneously Updated Neural Networks (SUNN), become natural baselines for our proposed GUNN networks.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We first study what baseline architectures can be converted.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There are two assumptions about the feature transformation F (see Eq. 1): (1) the input and the output sizes are the same, and (2) F is channel-wise decomposable.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the first assumption, we will first use a convolutional layer with Batch Normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to transform the feature space to a new space where the number of the channels is wanted.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the second assumption, instead of directly specifying the transform F , we focus on designing Fci , where ci is a subset of the channels (see Eq. 4).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To be consistent with the term update used in GUNN and SUNN, we refer to Fci as the update units for channels ci.
Bottleneck Update Units In the architectures proposed in this paper, we adopt bottleneck neural networks as shown in Figure 4 for the update units for both the SUNN and GUNN.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Suppose that the update unit maps the input features of channel size nin to the output features of size nout.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Each unit contains three convolutional layers.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The first convolutional layer transforms the input features to K × nout using a 1× 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The second convolutional layer is of kernel size 3× 3, stride 1, and padding 1, outputting the features of size K × nout.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The third layer computes the features of size nout using a 1 × 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The output is then added back to the input, following the residual architecture proposed in ResNet (He et al., 2016a).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We add batch normalization layer (Ioffe & Szegedy, 2015) and ReLU layer (Nair & Hinton, 2010) after the first and the second convolutional layers, while only adding batch
normalization layer after the third layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Stacking up M update units also generates a new one.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In total, we have two hyperparameters for designing an update unit: the expansion rate K and the number of the 3-layer update units M .
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"One Resolution, One Representation Our architectures will have only one representation at one resolution besides the pooling layers and the convolutional layers that initialize the needed numbers of channels.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Take the architecture in Table 1 as an example.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
There are two processes for each resolution.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The first one is the transition process, which computes the initial features with the dimensions of the next resolution, then down samples it to 1/4 using a 2×2 average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
A convolutional operation is needed here because F is assumed to have the same input and output sizes.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The next process is using GUNN to update this feature space gradually.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Each channel will only be updated once, and all channels will be updated after this process.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Unlike most of the previous networks, after this two processes, the feature transformations at this resolution are complete.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There will be no more convolutional layers or blocks following this feature representation, i.e., one resolution, one representation.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, the network will compute the initial features for the next resolution, or compute the final vector representation of the entire image by a global average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"By designing networks in this way, SUNN networks usually have about 20 layers before converting to GUNN-based networks.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Channel Partitions With the clearly defined update units, we can easily build SUNN and GUNN layers by using the units to update the representations following Eq. 4.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The hyperparameters for the SUNN/GUNN layer are the number of the channels N and the partition over those channels.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In our proposed architectures, we evenly partition the channels into P segments.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, we can useN and P to represent the configuration of a layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Together with the hyperparameters in the update units, we have four hyperparameters to tune for one SUNN/GUNN layer, i.e. {N,P,K,M}.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We have implemented two neural networks based on GUNN to compete with the previous state-of-the-art methods on CIFAR datasets, i.e., GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
Table 1 shows the big picture of GUNN-15.,5.2. Architectures for CIFAR,[0],[0]
"Here, we present the details of the hyperparameter settings for GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-15, we have three GUNN layers, Conv2, Conv3 and Conv4.",5.2. Architectures for CIFAR,[0],[0]
"The configuration for Conv2 is {N = 240, P = 20,K = 2,M = 1}, the configuration for Conv3 is {N = 300, P = 25,K = 2,M = 1}, and the configuration for Conv4 is {N = 360, P = 30,K = 2,M = 1}.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-24, based on GUNN-15, we change the number of output channels of Conv1 to 720, Trans1 to 900, Trans2 to 1080, and Trans3 to 1080.",5.2. Architectures for CIFAR,[0],[0]
"The hyperparameters are {N = 720, P = 20,K = 3,M = 2} for
Conv2, {N = 900, P = 25,K = 3,M = 2} for Conv3, and {N = 1080, P = 30,K = 3,M = 2} for Conv3.",5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-15 is 1585746 for CIFAR10 and 1618236 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-24 is 29534106 for CIFAR-10 and 29631396 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
"The GUNN-15 is aimed to compete with the methods published in an early stage by using a much smaller model, while GUNN-24 is targeted at comparing with ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b) to get the state-of-the-art performance.",5.2. Architectures for CIFAR,[0],[0]
We implement a neural network GUNN-18 to compete with the state-of-the-art neural networks on ImageNet with a similar number of parameters.,5.3. Architectures for ImageNet,[0],[0]
Table 2 shows the big picture of the neural network architecture of GUNN-18.,5.3. Architectures for ImageNet,[0],[0]
"Here, we present the detailed hyperparameters for the GUNN layers in GUNN-18.",5.3. Architectures for ImageNet,[0],[0]
"The GUNN layers include Conv2, Conv3, Conv4 and Conv5.",5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 400, P = 10,K = 2,M = 1} for Conv2,
{N = 800, P = 20,K = 2,M = 1} for Conv3, {N = 1600, P = 40,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 28909736.,5.3. Architectures for ImageNet,[0],[0]
"The GUNN-18 is targeted at competing with the previous state-of-the-art methods that have similar numbers of parameters, e.g., ResNet50 (Xie et al., 2017), ResNeXt-50 (Xie et al., 2017) and DenseNet-264 (Huang et al., 2017b).
",5.3. Architectures for ImageNet,[0],[0]
We also implement a wider GUNN-based neural networks Wide-GUNN-18 for better capacities.,5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 1200, P = 30,K = 2,M = 1} for Conv2, {N = 1600, P = 40,K = 2,M = 1} for Conv3, {N = 2000, P = 50,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 45624936.,5.3. Architectures for ImageNet,[0],[0]
"The Wide-GUNN-18 is targeted at competing with ResNet-101, ResNext-101, DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",5.3. Architectures for ImageNet,[0],[0]
"In this section, we demonstrate the effectiveness of the proposed GUNN on several benchmark datasets.",6. Experiments,[0],[0]
"CIFAR CIFAR (Krizhevsky & Hinton, 2009) has two color image datasets: CIFAR-10 (C10) and CIFAR-100 (C100).",6.1. Benchmark Datasets,[0],[0]
Both datasets consist of natural images with the size of 32× 32 pixels.,6.1. Benchmark Datasets,[0],[0]
"The CIFAR-10 dataset has 10 categories, while the CIFAR-100 dataset has 100 categories.",6.1. Benchmark Datasets,[0],[0]
"For both of the datasets, the training and test set contain 50, 000 and 10, 000 images, respectively.",6.1. Benchmark Datasets,[0],[0]
"To fairly compare our method with the state-of-the-arts (He et al., 2016a; Huang et al., 2017b; 2016; Larsson et al., 2016; Lee et al., 2014; Lin et al., 2013; Romero et al., 2014; Springenberg et al., 2014; Srivastava et al., 2015; Xie et al., 2017), we use the same training and testing strategies, as well as the data processing methods.",6.1. Benchmark Datasets,[0],[0]
"Specifically, we adopt a commonly used data augmentation scheme, i.e., mirroring and shifting, for these two datasets.",6.1. Benchmark Datasets,[0],[0]
"We use channel means and standard derivations to normalize the images for data pre-processing.
",6.1. Benchmark Datasets,[0],[0]
"ImageNet The ImageNet dataset (Russakovsky et al., 2015) contains about 1.28 million color images for training and 50, 000 for validation.",6.1. Benchmark Datasets,[0],[0]
The dataset has 1000 categories.,6.1. Benchmark Datasets,[0],[0]
"We adopt the same data augmentation methods as in the state-of-the-art architectures (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017) for training.",6.1. Benchmark Datasets,[0],[0]
"For testing, we use single-crop at the size of 224× 224.",6.1. Benchmark Datasets,[0],[0]
"Following the
state-of-the-arts (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017), we report the validation error rates.",6.1. Benchmark Datasets,[0],[0]
We train all of our networks using stochastic gradient descents.,6.2. Training Details,[0],[0]
"On CIFAR-10/100 (Krizhevsky & Hinton, 2009), the initial learning rate is set to 0.1, the weight decay is set to 1e−4, and the momentum is set to 0.9 without dampening.",6.2. Training Details,[0],[0]
We train the models for 300 epochs.,6.2. Training Details,[0],[0]
The learning rate is divided by 10 at 150th epoch and 225th epoch.,6.2. Training Details,[0],[0]
"We set the batch size to 64, following (Huang et al., 2017b).",6.2. Training Details,[0],[0]
"All the results reported for CIFAR, regardless of the detailed configurations, were trained using 4 NVIDIA Titan X GPUs with the data parallelism.",6.2. Training Details,[0],[0]
"On ImageNet (Russakovsky et al., 2015), the learning rate is also set to 0.1 initially, and decreases following the schedule in DenseNet (Huang et al., 2017b).",6.2. Training Details,[0],[0]
The batch size is set to 256.,6.2. Training Details,[0],[0]
"The network parameters are also initialized following (He et al., 2016a).",6.2. Training Details,[0],[0]
We use 8 Tesla V100 GPUs with the data parallelism to get the reported results.,6.2. Training Details,[0],[0]
"Our results are directly comparable with ResNet, WideResNet, ResNeXt and DenseNet.",6.2. Training Details,[0],[0]
We train two models GUNN-15 and GUNN-24 for the CIFAR-10/100 dataset.,6.3. Results on CIFAR,[0],[0]
Table 3 shows the comparisons between our method and the previous state-of-the-art methods.,6.3. Results on CIFAR,[0],[0]
Our method GUNN achieves the best results in the test of both the single model and the ensemble test.,6.3. Results on CIFAR,[0],[0]
"Here, we use Snapshot Ensemble (Huang et al., 2017a).
",6.3. Results on CIFAR,[0],[0]
Baseline Methods,6.3. Results on CIFAR,[0],[0]
Here we present the details of baseline methods in Table 3.,6.3. Results on CIFAR,[0],[0]
"The performances of ResNet (He et al., 2016a) are reported in Stochastic Depth (Huang et al., 2016) for both C10 and C100.",6.3. Results on CIFAR,[0],[0]
"The WideResNet (Zagoruyko & Komodakis, 2016) WRN-40-10 is reported in their official code repository on GitHub.",6.3. Results on CIFAR,[0],[0]
"The ResNeXt in the third group is of configuration 16 × 64d, which has the best result reported in the paper (Xie et al., 2017).",6.3. Results on CIFAR,[0],[0]
"The DenseNet is of configuration DenseNet-BC (k = 40), which achieves the best performances on CIFAR-10/100.",6.3. Results on CIFAR,[0],[0]
"The Snapshot Ensemble (Huang et al., 2017a) uses 6 DenseNet-100 to ensemble during inference.",6.3. Results on CIFAR,[0],[0]
"We do not compare with methods that use more data augmentation (e.g. (Zhang et al., 2017)) or stronger regularizations (e.g. (Gastaldi, 2017)) for the fairness of comparison.
",6.3. Results on CIFAR,[0],[0]
"Ablation Study For ablation study, we compare GUNN with SUNN, i.e., the networks before the conversion.",6.3. Results on CIFAR,[0],[0]
"Table 5 shows the comparison results, which demonstrate the effectiveness of GUNN.",6.3. Results on CIFAR,[0],[0]
We also compare the performances of GUNN with and without residual learning.,6.3. Results on CIFAR,[0],[0]
"We evaluate the GUNN on the ImageNet classification task, and compare our performances with the state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"These methods include VGGNet (Simonyan & Zisserman, 2014), ResNet (He et al., 2016a), ResNeXt (Xie
et al., 2017), DenseNet (Huang et al., 2017b), DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",6.4. Results on ImageNet,[0],[0]
The comparisons are shown in Table 4.,6.4. Results on ImageNet,[0],[0]
"The results of ours, ResNeXt, and DenseNet are directly comparable as these methods use the same framework for training and testing networks.",6.4. Results on ImageNet,[0],[0]
"Table 4 groups the methods by their numbers of parameters, except VGGNet which has 1.38× 108 parameters.
",6.4. Results on ImageNet,[0],[0]
"The results presented in Table 4 demonstrate that with the similar number of parameters, GUNN can achieve comparable performances with the previous state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"For GUNN-18, we also conduct an ablation experiment by comparing the corresponding SUNN with GUNN of the same configuration.",6.4. Results on ImageNet,[0],[0]
"Consistent with the experimental results on the CIFAR-10/100 dataset, the proposed GUNN improves the accuracy on ImageNet dataset.",6.4. Results on ImageNet,[0],[0]
"In this paper, we propose Gradually Updated Neural Network (GUNN), a novel, simple yet effective method to increase the depths of neural networks as an alternative to cascading layers.",7. Conclusions,[0],[0]
"GUNN is based on Convolutional Neural Networks (CNNs), but differs from CNNs in the way of computing outputs.",7. Conclusions,[0],[0]
The outputs of GUNN are computed gradually rather than simultaneously as in CNNs in order to increase the depth.,7. Conclusions,[0],[0]
"Essentially, GUNN assumes the input and the output are of the same size and adds a computation ordering to the channels.",7. Conclusions,[0],[0]
The added ordering increases the receptive fields and non-linearities of the later computed channels.,7. Conclusions,[0],[0]
"Moreover, it eliminates the overlap singularities inherent in the traditional convolutional networks.",7. Conclusions,[0],[0]
We test GUNN on the task of image recognition.,7. Conclusions,[0],[0]
"The evaluations are done in three highly competitive benchmarks, CIFAR10, CIFAR-100 and ImageNet.",7. Conclusions,[0],[0]
The experimental results demonstrate the effectiveness of the proposed GUNN on image recognition.,7. Conclusions,[0],[0]
"In the future, since the proposed GUNN can be used to replace CNNs in other neural networks, we will study the applications of GUNN in other visual tasks, such as object detection and semantic segmentation.",7. Conclusions,[0],[0]
"We thank Wanyu Huang, Huiyu Wang and Chenxi Liu for their insightful comments and suggestions.",Acknowledgments,[0],[0]
We gratefully acknowledge funding supports from NSF award CCF-1317376 and ONR N00014-15-1-2356.,Acknowledgments,[0],[0]
This work was also supported in part by the National Natural Science Foundation of China under Grant 61672336.,Acknowledgments,[0],[0]
Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition.,abstractText,[0],[0]
The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks.,abstractText,[0],[0]
"In this paper, we present an alternative method to increase the depth.",abstractText,[0],[0]
"Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner.",abstractText,[0],[0]
"The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better.",abstractText,[0],[0]
Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.,abstractText,[0],[0]
Gradually Updated Neural Networks for Large-Scale Image Recognition,title,[0],[0]
"In recent years, there has been an explosion of interest in sequence labelling tasks.",1. Introduction,[0],[0]
"Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR) (Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al.,
*Equal contribution 1Baidu Silicon Valley AI Lab, 1195 Bordeaux Dr, Sunnyvale, CA 94089, USA.",1. Introduction,[0],[0]
"Correspondence to: Hairong Liu <liuhairong@baidu.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2016), machine translation (Sébastien et al., 2015), and parsing (Vinyals et al., 2015).",1. Introduction,[0],[0]
"These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al., 2016a), and 2) a fixed and pre-determined decomposition of target sequences into these basic units.",1. Introduction,[0],[0]
"While these two preconditions greatly simplify the problems, especially the training processes, they are also strict and unnecessary constraints, which usually lead to suboptimal solutions.",1. Introduction,[0],[0]
"CTC models are especially harmed by fixed basic units in target space, because they build on the independence assumption between successive outputs in that space - an assumption which is often violated in practice.
",1. Introduction,[0],[0]
"The problem with fixed set of basic units is obvious: it is really hard, if not impossible, to determine the optimal set of basic units beforehand.",1. Introduction,[0],[0]
"For example, in English ASR, if we use words as basic units, we will need to deal with the large vocabulary-sized softmax, as well as rare words and data sparsity problem.",1. Introduction,[0],[0]
"On the other hand, if we use characters as basic units, the model is forced to learn the complex rules of English spelling and pronunciation.",1. Introduction,[0],[0]
"For example, the ""oh"" sound can be spelled in any of following ways, depending on the word it occurs in - { o, oa, oe, ow, ough, eau, oo, ew }.",1. Introduction,[0],[0]
"While CTC can easily model commonly co-occuring grams together, it is impossible to give roughly equal probability to many possible spellings when transcribing unseen words.",1. Introduction,[0],[0]
"Most speech recognition systems model phonemes, sub-phoneme units and senones e.g, (Xiong et al., 2016a) to get around these problems.",1. Introduction,[0],[0]
"Similarly, state-of-the-art neural machine translation systems use pre-segmented word pieces e.g, (Wu et al., 2016a) aiming to find the best of both worlds.
",1. Introduction,[0],[0]
"In reality, groups of characters are typically cohesive units for many tasks.",1. Introduction,[0],[0]
"For the ASR task, words can be decomposed into groups of characters that can be associated with sound (such as ‘tion’ and ‘eaux’).",1. Introduction,[0],[0]
"For the machine translation task, there may be values in decomposing words as root words and extensions (so that meaning may be shared explicitly between ‘paternal’ and ‘paternity’).",1. Introduction,[0],[0]
"Since this information is already available in the training data, it is perhaps, better to let the model figure it out by itself.",1. Introduction,[0],[0]
"At the same time, it raises another import question: how to de-
compose a target sequence into basic units?",1. Introduction,[0],[0]
"This is coupled with the problem of automatic selection of basic units, thus also better to let the model determine.",1. Introduction,[0],[0]
"Recently, there are some interesting attempts in these directions in the seq2seq framework.",1. Introduction,[0],[0]
"For example, Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decomposition to decompose target sequences with variable length units as a function of both input sequence and the output sequence.
",1. Introduction,[0],[0]
"In this work, we propose Gram-CTC - a strictly more general version of CTC - to automatically seek the best set of basic units from the training data, called grams, and automatically decompose target sequences into sequences of grams.",1. Introduction,[0],[0]
"Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences.",1. Introduction,[0],[0]
"Since it is a loss function, it can be applied to many seq2seq tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks.",1. Introduction,[0],[0]
"Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.",1. Introduction,[0],[0]
"The basic text units that previous works utilized for text prediction tasks (e.g,, automatic speech recognition, handwriting recognition, machine translation, and image captioning) can be generally divided into two categories: handcrafted ones and learning-based ones.
",2. Related Work,[0],[0]
Hand-crafted Basic Units.,2. Related Work,[0],[0]
"Fixed sets of characters (graphemes) (Graves et al., 2006; Amodei et al., 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al., 2016; Sébastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.",2. Related Work,[0],[0]
"Using these fixed deterministic decompositions of text sequences defines a prior, which is not necessarily optimal for end-to-end learning.
",2. Related Work,[0],[0]
• Word-segmented models remove the component of learning to spell and thus enable direct optimization towards reducing Word Error Rate (WER).,2. Related Work,[0],[0]
"However, these models suffer from having to handle a large vocabulary (1.7 million in (Soltau et al., 2016)), out-of-vocabulary words (Soltau et al., 2016; Sébastien et al., 2015) and data sparsity problems (Soltau et al., 2016).
",2. Related Work,[0],[0]
"• Using characters results in much smaller vocabularies (e.g, 26 for English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015),
which is very noisy for languages like English.
",2. Related Work,[0],[0]
"• Word-pieces lie at the middle-ground of words and characters, providing a good trade-off between vocabulary size and context size, while the performance of using word pieces is sensitive to the choice of the word-piece set and its decomposition.
",2. Related Work,[0],[0]
"• For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016).",2. Related Work,[0],[0]
"However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.
",2. Related Work,[0],[0]
Learning-based Basic Units.,2. Related Work,[0],[0]
"More recently, attempts have been made to learn basic unit sets automatically.",2. Related Work,[0],[0]
"(Luong and Manning, 2016) proposed a hybrid WordCharacter model which translates mostly at the word level and consults the character components for rare words.",2. Related Work,[0],[0]
"Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decompositions framework to decomposes target sequences with variable length-ed basic units as a function of both input sequence and the output sequence.
",2. Related Work,[0],[0]
"There exist some earlier works on the “unit discovery” task (Cartwright and Brent, 1994; Goldwater et al., 2006).",2. Related Work,[0],[0]
"A standard problem with MLE solutions to this task is that there are degenerate solutions, i.e., predicting the full corpus with probability 1 at the start.",2. Related Work,[0],[0]
Often Bayesian priors or “minimum description length” constraints are used to remedy this.,2. Related Work,[0],[0]
"CTC (Graves et al., 2006) is a very popular method in seq2seq learning since it does not require the alignment information between inputs and outputs, which is usually expensive, if not impossible, to obtain.
",3.1. CTC,[0],[0]
"Since there is no alignment information, CTC marginalizes over all possible alignments.",3.1. CTC,[0],[0]
"That is, it tries to maximize p(l|x) =",3.1. CTC,[0],[0]
"∑ π p(π|x), where x is input, and π represent a valid alignment.",3.1. CTC,[0],[0]
"For example, if the size of input is 3, and the output is ‘hi’, whose length is 2, there are three possible alignments, ‘-hi’, ‘h-i’ and ‘hi-’, where ‘-’ represents blank.",3.1. CTC,[0],[0]
"For the details, please refer to the original paper (Graves et al., 2006).",3.1. CTC,[0],[0]
"In CTC, the basic units are fixed, which is not desirable in some applications.",3.2. From CTC to Gram-CTC,[0],[0]
"Here we generalize CTC by considering a sequence of basic units, called gram, as a whole, which is usually more reasonable in many applications.
",3.2. From CTC to Gram-CTC,[0],[0]
"Let G be a set of n-grams of the set of basic units C of the target sequence, and τ be the length of the longest gram in G. A Gram-CTC network has a softmax output layer with |G|+1 units, that is, the probability over all grams inG and one additional symbol, blank.",3.2. From CTC to Gram-CTC,[0],[0]
"To simplify the problem, we also assume C ⊆ G. 1
For an input sequence x of length T , let y = Nw(x) be the sequence of network outputs, and denote by ytk as the probability of the k-th gram at time t, where k is the index of grams in G′ = G ∪ {blank}, then we have
p(π|x)",3.2. From CTC to Gram-CTC,[0],[0]
"= T∏ t=1 ytπt ,∀π ∈ G ′T (1)
Just as in the case of CTC, here we refer to the elements of G′T as paths, and denote them by π, which represents a possible alignment between input and output.",3.2. From CTC to Gram-CTC,[0],[0]
"The difference is that for each word in the target sequence, it may be decomposed into different sequences of grams.",3.2. From CTC to Gram-CTC,[0],[0]
"For example, the word ‘hello’ can only be decomposed into the sequence [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] for CTC (assume uni-gram CTC here), but it also can be decomposed into the sequence [‘he’, ‘ll’, ‘o’] if ‘he’ and ‘ll’ are in G.
For each π, we map it into a target sequence in the same way as CTC using the collapsing function that 1) removes all repeated labels from the path and then 2) removes all blanks.",3.2. From CTC to Gram-CTC,[0],[0]
"Note that essentially it is these rules which de-
1This is because there may be no valid decompositions for some target sequences if C 6⊆ G. Since Gram-CTC will figure out the ideal decomposition of target sequences into grams during training, this condition guarantees that there is at least one valid decomposition for every target sequence.
termine the transitions between the states of adjacent time steps in Figure 1.",3.2. From CTC to Gram-CTC,[0],[0]
This is a many-to-one mapping and we denote it by B. Note that other rules can be adopted here and the general idea presented in this paper does not depend on these specific rules.,3.2. From CTC to Gram-CTC,[0],[0]
"For a target sequence l, B−1(l) represents all paths mapped to l. Then, we have
p(l|x) = ∑
π∈B−1(l)
p(π|x) (2)
",3.2. From CTC to Gram-CTC,[0],[0]
"This equation allows for training sequence labeling models without any alignment information using CTC loss, because it marginalizes over all possible alignments during training.",3.2. From CTC to Gram-CTC,[0],[0]
"Gram-CTC uses the same effect to enable the model to marginalize over not only alignments, but also decompositions of the target sequence.
",3.2. From CTC to Gram-CTC,[0],[0]
"Note that for each target sequence l, the set B−1(l) has O(τ2) more paths than it does in CTC.",3.2. From CTC to Gram-CTC,[0],[0]
"This is because there are O(τ) times more valid states per time step, and each state may have a valid transition from O(τ) states in the previous time step.",3.2. From CTC to Gram-CTC,[0],[0]
"The original CTC method is thus, a special case of Gram-CTC when G = C and τ = 1.",3.2. From CTC to Gram-CTC,[0],[0]
"While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.",3.2. From CTC to Gram-CTC,[0],[0]
"Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length τ .",3.2. From CTC to Gram-CTC,[0],[0]
"To efficiently compute p(l|x), we also adopt the dynamic programming algorithm.",3.3. The Forward-Backward Algorithm,[0],[0]
"The essence here is identifying
the states of the problem, so that we may solve future states by reusing solutions to earlier states.",3.3. The Forward-Backward Algorithm,[0],[0]
"In our case, the state must contain all the information required to identify all valid extensions of an incomplete path π such that the collapsing function will eventually collapse the complete π back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path π.",3.3. The Forward-Backward Algorithm,[0],[0]
"Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} is the length of the last gram (li−j+1:i) used for making the prefix.",3.3. The Forward-Backward Algorithm,[0],[0]
j = 0 is valid and means that blank was used.,3.3. The Forward-Backward Algorithm,[0],[0]
"We denote the gram (li−j+1:i) by g j i (l), and the state (l1:i, j) as s j i (l).",3.3. The Forward-Backward Algorithm,[0],[0]
"For readability, we will further shorten sji (l) to s j i and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i (l) to g j i .,3.3. The Forward-Backward Algorithm,[0],[0]
"For a state s, its corresponding gram is denoted by sg , and the positions of the first character and last character of sg are denoted by b(s) and e(s), respectively.",3.3. The Forward-Backward Algorithm,[0],[0]
"During dynamic programming, we are dealing with sequence of states, for a state sequence ζ, its corresponding gram sequences is unique, denoted by ζg .
",3.3. The Forward-Backward Algorithm,[0],[0]
Figure 1 illustrates partially the dynamic programming process for the target sequence ‘CAT’.,3.3. The Forward-Backward Algorithm,[0],[0]
Here we suppose G contains all possible uni-grams and bi-grams.,3.3. The Forward-Backward Algorithm,[0],[0]
"Thus, for each character in ‘CAT’, there are three possible states associated with it: 1) the current character, 2) the bi-gram ending in current character, and 3) the blank after current character.",3.3. The Forward-Backward Algorithm,[0],[0]
There is also one blank at beginning.,3.3. The Forward-Backward Algorithm,[0],[0]
"In total we have 10 states.
",3.3. The Forward-Backward Algorithm,[0],[0]
"Supposing the maximum length of grams inG is τ , we first scan l to get the set S of all possible states, such that for all sji ∈ S, its corresponding g j i ∈",3.3. The Forward-Backward Algorithm,[0],[0]
"G′. i ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", |l|}",3.3. The Forward-Backward Algorithm,[0],[0]
"and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ}.",3.3. The Forward-Backward Algorithm,[0],[0]
"For a target sequence l, define the forward variable αt(s) for any s ∈ S to the total probability of all valid paths prefixes that end at state s at time t.
αt(s) def = ∑ ζ|B(ζg)=l1:e(s),ζt=s t∏ t′=1 yt ′ ζt′g (3)
",3.3. The Forward-Backward Algorithm,[0],[0]
"Following this definition, we have the following rules for initialization
α1(s) =  y1b s = s 0 0 y1 gii
s = sii ∀i ∈ {1, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(4)
and recursion
αt(s) =  α̂it−1 ∗ ytb when s = s0i , [α̂i−jt−1 + αt−1(s)] ∗ ytgji when s = sji and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i 6=,3.3. The Forward-Backward Algorithm,[0],[0]
"g j i−j , [α̂i−jt−1 + αt−1(s)− αt−1(s j i−j)] ∗ ytgji
when s = sji and g j i = g j i−j
(5)
where α̂it = ∑τ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
αt(s j i ) and y t b is the probability of blank at time,3.3. The Forward-Backward Algorithm,[0],[0]
"t.
The total probability of the target sequence l is then expressed in the following way:
p(l|x) = τ∑ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
"αT (s j |l|) (6)
similarly, we can define the backward variable βt(s) as:
βt(s) def = ∑ ζ|B(ζg)=lb(s):l,ζt=s T∏ t′=t yt ′ ζt′g (7)
",3.3. The Forward-Backward Algorithm,[0],[0]
"For the initialization and recursion of βt(s), we have
βT",3.3. The Forward-Backward Algorithm,[0],[0]
"(s) =  yTb s = s 0 T yT giT
s = siT ∀i ∈ {1, . .",3.3. The Forward-Backward Algorithm,[0],[0]
.,3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(8)
and
βt(s) =  β̂it+1 ∗ ytb when s = s0i , [β̂i+jt+1 + βt+1(s)] ∗ ytgji when s = sji and g j i 6=",3.3. The Forward-Backward Algorithm,[0],[0]
"g j i+j , [β̂i+jt+1 + βt+1(s)− βt+1(s j i+j)] ∗ ytgji
when s = sji and g j i = g j i+j
(9) where β̂it = ∑τ j=0 βt(s j i+j)",3.3. The Forward-Backward Algorithm,[0],[0]
"Similar to CTC, we have the following expression:
p(l|x) = ∑ s∈S αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg ∀t ∈ {1, . . .",3.4. BackPropagation,[0],[0]
",T} (10)
",3.4. BackPropagation,[0],[0]
"The derivative with regards to ytk is:
∂p(l|x) ∂ytk",3.4. BackPropagation,[0],[0]
"= 1 ytk 2 ∑ s∈lab(l,k) αt(s)βt(s) (11)
where lab(l, k) is the set of states in S whose corresponding gram is",3.4. BackPropagation,[0],[0]
k.,3.4. BackPropagation,[0],[0]
"This is because there may be multiple states corresponding to the same gram.
",3.4. BackPropagation,[0],[0]
"For the backpropagation, the most important formula is the partial derivative of loss with regard to the unnormalized output utk.
∂ ln p(l|x) ∂utk = ytk",3.4. BackPropagation,[0],[0]
"− 1 ytkZt ∑ s∈lab(l,k) αt(s)βt(s) (12)
where Zt def = ∑ s∈S
αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg .
",3.4. BackPropagation,[0],[0]
(a) Training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"(b) Training curves without (blue) and with (orange) joint-training
Gram-CTC C _",3.4. BackPropagation,[0],[0]
"AT
CTC _",3.4. BackPropagation,[0],[0]
C _,3.4. BackPropagation,[0],[0]
"A T _
Gram-CTC C - AT
CTC - C - A T -
(c) Joint-training Architecture
Figure 2.",3.4. BackPropagation,[0],[0]
(Figure 2a) compares the training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.",3.4. BackPropagation,[0],[0]
Figure (2b) Training curve of model with and without joint-training.,3.4. BackPropagation,[0],[0]
"The model corresponding to the orange training curve is jointly trained together with vanilla CTC, such models are often more stable during training.",3.4. BackPropagation,[0],[0]
Figure (2c) Typical joint-training model architecture - vanilla CTC loss is best applied a few levels lower than the Gram-CTC loss.,3.4. BackPropagation,[0],[0]
Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively.,4. Methodology,[0],[0]
"Although Gram-CTC can automatically select useful grams, it is challenging to train with a large G.",4.1. Iterative Gram Selection,[0],[0]
The total number of possible grams is usually huge.,4.1. Iterative Gram Selection,[0],[0]
"For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, . . .",4.1. Iterative Gram Selection,[0],[0]
", which grows exponentially and quickly becomes intractable.",4.1. Iterative Gram Selection,[0],[0]
"However, it is unnecessary to consider many grams, such as ‘aaaa’, which are obviously useless.
",4.1. Iterative Gram Selection,[0],[0]
"In our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies.",4.1. Iterative Gram Selection,[0],[0]
"Then, we train a model with Gram-CTC on all the remaining grams.",4.1. Iterative Gram Selection,[0],[0]
"By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram’s usage.",4.1. Iterative Gram Selection,[0],[0]
"Ultimately, we choose high frequency grams together with all uni-grams as our final gram set G. Table 1 shows the impact of iterative gram selection on WSJ (without LM).",4.1. Iterative Gram Selection,[0],[0]
Figure 2a shows its corresponding training curve.,4.1. Iterative Gram Selection,[0],[0]
"For details, please refer to Section 5.2.",4.1. Iterative Gram Selection,[0],[0]
"Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.",4.2. Joint Training with Vanilla CTC,[0],[0]
"This is often manifested in unstable training curves, forcing us to lower the learning rate which in turn results
in models converging to a worse optima.",4.2. Joint Training with Vanilla CTC,[0],[0]
"To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)).",4.2. Joint Training with Vanilla CTC,[0],[0]
"Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).
",4.2. Joint Training with Vanilla CTC,[0],[0]
"A typical joint-training model looks like Figure 2c, and the corresponding training curve is shown in Figure 2b.",4.2. Joint Training with Vanilla CTC,[0],[0]
The effect of joint-training are shown in Table 4 and Table 5 in the experiments.,4.2. Joint Training with Vanilla CTC,[0],[0]
"We test the Gram-CTC loss on the ASR task, while both CTC and the introduced Gram-CTC are generic techniques for other sequence labelling tasks.",5. Experiments,[0],[0]
"For all of the experiments, the model specification and training procedure are the same as in (Amodei et al., 2015) -",5. Experiments,[0],[0]
"The model is a recurrent neural network (RNN) with 2 two-dimensional convolutional input layers, followed by K forward (Fwd) or bidirectional (Bidi)",5. Experiments,[0],[0]
"Gated Recurrent layers, N cells each, and one fully connected layer before a softmax layer.",5. Experiments,[0],[0]
"In short hand, such a model is written as ‘2x2D Conv - KxN GRU’.",5. Experiments,[0],[0]
"The network is trained end-to-end with the CTC, GramCTC or a weighted combination of both.",5. Experiments,[0],[0]
"This combination is described in the earlier section.
",5. Experiments,[0],[0]
"In all experiments, audio data is is sampled at 16kHz.",5. Experiments,[0],[0]
"Linear FFT features are extracted with a hop size of 10ms and window size of 20ms, and are normalized so that each input feature has zero mean and unit variance.",5. Experiments,[0],[0]
The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame.,5. Experiments,[0],[0]
"At each epoch, 40% of the utterances are randomly selected to add
background noise to.",5. Experiments,[0],[0]
The optimization method we use is stochastic gradient descent with Nesterov momentum.,5. Experiments,[0],[0]
"Learning hyperparameters (batch-size, learning-rate, momentum, and etc.) vary across different datasets and are tuned for each model by optimizing a hold-out set.",5. Experiments,[0],[0]
Typical values are a learning rate of 10−3 and momentum of 0.99.,5. Experiments,[0],[0]
Wall Street Journal (WSJ).,5.1. Data and Setup,[0],[0]
"This corpora consists primarily of read speech with texts drawn from a machinereadable corpus of Wall Street Journal news text, and contains about 80 hours speech data.",5.1. Data and Setup,[0],[0]
"We used the standard configuration of train si284 dataset for training, dev93 for validation and eval92 for testing.",5.1. Data and Setup,[0],[0]
"This is a relatively ‘clean’ task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).
",5.1. Data and Setup,[0],[0]
Fisher-Switchboard.,5.1. Data and Setup,[0],[0]
"This is a commonly used English conversational telephone speech (CTS) corpora, which contains 2300 hours CTS data.",5.1. Data and Setup,[0],[0]
"Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.
10K Speech Dataset.",5.1. Data and Setup,[0],[0]
"We conduct large scale ASR experiments on a noisy internal dataset of 10,000 hours.",5.1. Data and Setup,[0],[0]
"This dataset contains speech collected from various scenarios, such as different background noises, far-field, different accents, and so on.",5.1. Data and Setup,[0],[0]
"Due to its inherent complexities, it is a very challenging task, and can thus validate the effectiveness of the proposed method for real-world application.",5.1. Data and Setup,[0],[0]
"We employ the WSJ dataset for demonstrating different strategies of selecting grams for Gram-CTC, since it is a widely used dataset and also small enough for rapid idea verification.",5.2. Gram Selection,[0],[0]
"However, because it is small, we cannot use large grams here due to data sparsity problem.",5.2. Gram Selection,[0],[0]
"Thus, the auto-refined gram set on WSJ is not optimal for other larger datasets, where larger grams could be effectively used, but the procedure of refinement is the same for them.
",5.2. Gram Selection,[0],[0]
"We first train a model using all uni-grams and bi-grams (29
uni-grams and 262 = 676 bi-grams, in total 705 grams), and then do decoding with the obtained model on another speech dataset to get the statistics of the usage of grams.",5.2. Gram Selection,[0],[0]
Top 100 bi-grams together with all 29 uni-grams (autorefined grams) are used for the second round of training.,5.2. Gram Selection,[0],[0]
"For comparison, we also present the result of the best handpicked grams, as well as the results on uni-grams.",5.2. Gram Selection,[0],[0]
"All the results are shown in Table 1.
",5.2. Gram Selection,[0],[0]
Some interesting observations can be found in Table 1.,5.2. Gram Selection,[0],[0]
"First, the performance of auto-refined grams is only slightly better than the combination of all uni-grams and all bigrams.",5.2. Gram Selection,[0],[0]
This is probably because WSJ is so small that gram learning suffers from the data sparsity problem here (similar to word-segmented models).,5.2. Gram Selection,[0],[0]
"The auto-refined gram set contains only a small subset of bi-grams, thus more robust.",5.2. Gram Selection,[0],[0]
"This is also why we only try bi-grams, not including higher-order grams.",5.2. Gram Selection,[0],[0]
"Second, the performance of best handpicked grams is worse than auto-refined grams.",5.2. Gram Selection,[0],[0]
This is desirable.,5.2. Gram Selection,[0],[0]
"It is time-consuming to handpick grams, especially when you consider high-order grams.",5.2. Gram Selection,[0],[0]
"The method of iterative gram selection is not only fast, but usually better.",5.2. Gram Selection,[0],[0]
"Third, the performance of Gram-CTC on auto-refined grams is only slightly better than CTC on uni-grams.",5.2. Gram Selection,[0],[0]
"This is because Gram-CTC is inherently difficult to train, since it needs to learn both decomposition and alignment.",5.2. Gram Selection,[0],[0]
WSJ is too small to provide enough data to train Gram-CTC.,5.2. Gram Selection,[0],[0]
"Using a large time stride for sequence labelling with RNNs can greatly boost the overall computation efficiency, since it effectively reduces the time steps for recurrent computation, thus speeds up the process of both forward inference and backward propagation.",5.3. Sequence Labelling in Large Stride,[0],[0]
"However, the largest stride that can be used is limited by the gram set we use.",5.3. Sequence Labelling in Large Stride,[0],[0]
The (unigram) CTC has to work in a high time resolution (small stride) in order to have enough number of frames to output every character.,5.3. Sequence Labelling in Large Stride,[0],[0]
"This is very inefficient as we know the same acoustic feature could correspond to several grams of different lengths (e.g., {‘i’, ‘igh’, ‘eye’}) .",5.3. Sequence Labelling in Large Stride,[0],[0]
"The larger the grams are, the larger stride we are potentially able to use.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"DS2 (Amodei et al., 2015) employed non-overlapping bigram outputs to allow for a larger stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
"This imposes an artificial constraint forcing the model to learn, not only the spelling of each word, but also how to split words into bigrams.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For example, part is split as [pa, rt] but the word
apart is forced to be decomposed as [ap, ar, t].",5.3. Sequence Labelling in Large Stride,[0],[0]
GramCTC removes this constraint by allowing the model to decompose words into larger units into the most convenient or sensible decomposition.,5.3. Sequence Labelling in Large Stride,[0],[0]
"Comparison results show this change enables Gram-CTC to work much better than bigram CTC, as in Table 2.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"In Table 2, we compare the performance of trained model and training efficiency on two strides, 2 and 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For GramCTC, we use the auto-refined gram set from previous section.",5.3. Sequence Labelling in Large Stride,[0],[0]
"As expected, using stride 4 almost cuts the training time per epoch into half, compared to stride 2.",5.3. Sequence Labelling in Large Stride,[0],[0]
"From stride 2 to stride 4, the performance of uni-gram CTC drops quickly.",5.3. Sequence Labelling in Large Stride,[0],[0]
This is because small grams inherently need higher time resolutions.,5.3. Sequence Labelling in Large Stride,[0],[0]
"As for Gram-CTC, from stride 2 to stride 4, its performance decreases a little bit, while in experiments on the other datasets, Gram-CTC constantly works better in stride 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
One possible explanation is that WSJ is too small for Gram-CTC to learn large grams well.,5.3. Sequence Labelling in Large Stride,[0],[0]
"In contrast, the performance of bi-gram CTC is not as good as that of Gram-CTC in either stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
Figure 3 illustrates the max-decoding results of both CTC and Gram-CTC on nine utterances.,5.4. Decoding Examples,[0],[0]
"Here the label set for CTC is the set of all characters, and the label set for GramCTC is an auto-refined gram set containing all uni-grams and some high-frequency high-order grams.",5.4. Decoding Examples,[0],[0]
"Here Gram-
CTC uses stride 4 while CTC uses stride 2.
",5.4. Decoding Examples,[0],[0]
"From Figure 3, we can find that: 1) Gram-CTC does automatically find many intuitive and meaningful grams, such as ‘the’, ‘ng’, and ‘are’.",5.4. Decoding Examples,[0],[0]
2) It also decomposes the sentences into segments which are meaningful in term of pronunciation.,5.4. Decoding Examples,[0],[0]
"This decomposition resembles the phonetic decomposition, but in larger granularity and arguably more natural.",5.4. Decoding Examples,[0],[0]
"3) Since Gram-CTC predicts a chunk of characters (a gram) each time, each prediction utilizes larger context and these characters in the same predicted chunk are dependent, thus potentially more robust.",5.4. Decoding Examples,[0],[0]
"One example is the word ‘will’ in the last sentence in Figure 3. 4) Since the output of network is the probability over all grams, the decoding process is almost the same as CTC, still end-toend.",5.4. Decoding Examples,[0],[0]
This makes such decomposition superior to phonetic decomposition.,5.4. Decoding Examples,[0],[0]
"In summary, Gram-CTC combines the advantages of both CTC on characters and CTC on phonemes.",5.4. Decoding Examples,[0],[0]
"The model used here is [2x2D conv, 3x1280 Bidi GRU] with a CTC or Gram-CTC loss.",5.5.1. WSJ DATASET,[0],[0]
The results are shown in Table 3.,5.5.1. WSJ DATASET,[0],[0]
"For all models we trained, language model can greatly improve their performances, in term of WER.",5.5.1. WSJ DATASET,[0],[0]
"Though this dataset contains very limited amount of text data for learning gram selection and decomposition, Gram-
CTC can still improve the vanilla CTC notably.",5.5.1. WSJ DATASET,[0],[0]
The acoustic model trained here is composed of two 2D convolutions and six bi-directional GRU layer in 2048 dimension.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"The corresponding labels are used for training N-gram language models.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
• Switchboard,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"English speech 97S62 • Fisher English speech Part 1 - 2004S13, 2004T19 • Fisher English speech Part 2 - 2005S13, 2005T19
We use a sample of the Switchboard-1 portion of the NIST 2002 dataset (2004S11 RT-02) for tuning language model hyper-parameters.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
The evaluation is done on the NIST 2000 set.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
This configuration forms a standard benchmark for evaluating ASR models.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Results are in Table 4.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
We compare our model against best published results on in-domain data.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"These results can often be improved using out-of-domain data for training the language model, and sometimes the acoustic model as well.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Together these techniques allow (Xiong et al., 2016b) to reach a WER of 5.9 on the SWBD set.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Finally, we experiment on a large noisy dataset collected by ourself for building large-vocabulary Continuous Speech Recognition (LVCSR) systems.",5.5.3. 10K SPEECH DATASET,[0],[0]
"This dataset contains about 10000 hours speech in a diversity of scenarios, such as farfield, background noises, accents.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In all cases, the model is [2x2D Conv, 3x2560 Fwd GRU, LA Conv] with only a change in the loss function.",5.5.3. 10K SPEECH DATASET,[0],[0]
"‘LA Conv’ refers to a look ahead convolution layer as seen in (Amodei et al., 2015) which works together with forward-only RNNs for deployment purpose.
",5.5.3. 10K SPEECH DATASET,[0],[0]
"As with the Fisher-Switchboard dataset, the optimal stride is 4 for Gram-CTC and 2 for vanilla CTC on this dataset.",5.5.3. 10K SPEECH DATASET,[0],[0]
"Thus, in both experiments, both Gram-CTC and vanilla
CTC + Gram-CTC are trained mush faster than vanilla CTC itself.",5.5.3. 10K SPEECH DATASET,[0],[0]
The result is shown in Table 5.,5.5.3. 10K SPEECH DATASET,[0],[0]
Gram-CTC performs better than CTC.,5.5.3. 10K SPEECH DATASET,[0],[0]
"After joint-training with vanilla CTC and alignment information through a CE loss, its performance is further boosted, which verifies joint-training helps training.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In fact, with only a small additional cost of time, it effectively reduces the WER from 27.56% to 25.59% (without language model).",5.5.3. 10K SPEECH DATASET,[0],[0]
"In this paper, we have proposed the Gram-CTC loss to enable automatic decomposition of target sequences into learned grams.",6. Conclusions and Future Work,[0],[0]
We also present techniques to train the Gram-CTC in a clean and stable way.,6. Conclusions and Future Work,[0],[0]
"Our extensive experiments demonstrate the proposed Gram-CTC enables the models to run more efficiently than the vanilla CTC, by using larger stride, while obtaining better performance of sequence labelling.",6. Conclusions and Future Work,[0],[0]
"Comparison experiments on multiplescale datasets show the proposed Gram-CTC obtains stateof-the-art results on various ASR tasks.
",6. Conclusions and Future Work,[0],[0]
"An interesting observation is that the learning of GramCTC implicitly avoids the “degenerated solution” that occurring in the traditional “unit discovery” task, without involving any Bayesian priors or the “minimum description length” constraint.",6. Conclusions and Future Work,[0],[0]
"Using a small gram set that contains only short (up to 5 in our experiments) as well as highfrequency grams may explain the success here.
",6. Conclusions and Future Work,[0],[0]
"We will continue investigating techniques of improving the optimization of Gram-CTC loss, as well as the applications of Gram-CTC for other sequence labelling tasks.",6. Conclusions and Future Work,[0],[0]
Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units.,abstractText,[0],[0]
"These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed.",abstractText,[0],[0]
These drawbacks usually result in sub-optimal performance of modeling sequences.,abstractText,[0],[0]
"In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC.",abstractText,[0],[0]
"While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences.",abstractText,[0],[0]
"Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency.",abstractText,[0],[0]
"We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.",abstractText,[0],[0]
Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,title,[0],[0]
"Generative machine learning models have been used recently to produce extraordinary results, from realistic musical improvisation (Jaques et al., 2016), to changing facial expressions in images (Radford et al., 2015; Upchurch et al., 2016), to creating realistic looking artwork (Gatys et al., 2015).",1. Introduction,[0],[0]
"In large part, these generative models have been successful at representing data in continuous domains.",1. Introduction,[0],[0]
"Recently there is increased interest in training generative models to construct more complex, discrete data types such as arithmetic expressions (Kusner & Hernández-Lobato, 2016), source code (Gaunt et al., 2016; Riedel et al., 2016)
",1. Introduction,[0],[0]
*Equal contribution 1Alan Turing Institute 2University of Warwick 3University of Cambridge.,1. Introduction,[0],[0]
"Correspondence to: <mkusner@turing.ac.uk>, <bpaige@turing.ac.uk>, <jmh233@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
and molecules (Gómez-Bombarelli et al., 2016b).
",1. Introduction,[0],[0]
"To train generative models for these tasks, these objects are often first represented as strings.",1. Introduction,[0],[0]
"This is in large part due to the fact that there exist powerful models for text sequence modeling such as Long Short Term Memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Units (GRUs) (Cho et al., 2014), and Dynamic Convolutional Neural Networks (DCNNs) (Kalchbrenner et al., 2014).",1. Introduction,[0],[0]
"For instance, molecules can be represented by so-called SMILES strings (Weininger, 1988) and GómezBombarelli et al. (2016b) has recently developed a generative model for molecules based on SMILES strings that uses GRUs and DCNNs.",1. Introduction,[0],[0]
"This model is able to encode and decode molecules to and from a continuous latent space, allowing one to search this space for new molecules with desirable properties (Gómez-Bombarelli et al., 2016b).
",1. Introduction,[0],[0]
"However, one immediate difficulty in using strings to represent discrete objects is that the representation is very brittle: small changes in the string can lead to completely different objects, or often do not correspond to valid objects at all.",1. Introduction,[0],[0]
"Specifically, Gómez-Bombarelli et al. (2016b) described that while searching for new molecules, the probabilistic decoder — the distribution which maps from the continuous latent space into the space of molecular structures — would sometimes accidentally put high probability on strings which are not valid SMILES strings or do not encode plausible molecules.
",1. Introduction,[0],[0]
"To address this issue, we propose to directly incorporate knowledge about the structure of discrete data using a grammar.",1. Introduction,[0],[0]
"Grammars exist for a wide variety of discrete domains such as symbolic expressions (Allamanis et al., 2016), standard programming languages such as C (Kernighan et al., 1988), and chemical structures (James et al., 2015).",1. Introduction,[0],[0]
"For instance the set of syntactically valid SMILES strings is described using a context free grammar, which can be used for parsing and validation1.
",1. Introduction,[0],[0]
"Given a grammar, every valid discrete object can be described as a parse tree from the grammar.",1. Introduction,[0],[0]
"Thus, we propose the grammar variational autoencoder (GVAE) which encodes and decodes directly from and to these parse trees.",1. Introduction,[0],[0]
"Generating parse trees as opposed to strings ensures that
1http://opensmiles.org/spec/open-smiles-2-grammar.html
all outputs are valid based on the grammar.",1. Introduction,[0],[0]
"This frees the GVAE from learning syntactic rules and allows it to wholly focus on learning other ‘semantic’ properties.
",1. Introduction,[0],[0]
We demonstrate the GVAE on two tasks for generating discrete data: 1) generating simple arithmetic expressions and 2) generating valid molecules.,1. Introduction,[0],[0]
"We show not only does our model produce a higher proportion of valid outputs than a character based autoencoder, it also produces smoother latent representations.",1. Introduction,[0],[0]
"We also show that this learned latent space is effective for searching for arithmetic expressions that fit data, for finding better drug-like molecules, and for making accurate predictions about target properties.",1. Introduction,[0],[0]
We wish to learn both an encoder and a decoder for mapping data x to and from values z in a continuous space.,2.1. Variational autoencoder,[0],[0]
"The variational autoencoder (Kingma & Welling, 2014; Rezende et al., 2014) provides a formulation in which the encoding z is interpreted as a latent variable in a probabilistic generative model; a probabilistic decoder is defined by a likelihood function p
✓ (x|z) and parameterized by ✓.",2.1. Variational autoencoder,[0],[0]
"Alongside a prior distribution p(z) over the latent variables, the posterior distribution p
✓ (z|x) / p(z)p ✓ (x|z) can then be interpreted as a probabilistic encoder.
",2.1. Variational autoencoder,[0],[0]
"To admit efficient inference, the variational Bayes approach simultaneously learns both the parameters of p
✓ (x|z) as well as those of a posterior approximation q
(z|x).",2.1. Variational autoencoder,[0],[0]
"This is achieved by maximizing the evidence lower bound (ELBO)
",2.1. Variational autoencoder,[0],[0]
"L( , ✓;x) = E q (z|x)",2.1. Variational autoencoder,[0],[0]
"[log p✓(x, z) log q (z|x)] , (1)
with L( , ✓;x)  log p ✓ (x).",2.1. Variational autoencoder,[0],[0]
"So long as p ✓ (x|z) and q
(z|x) can be computed pointwise, and are differentiable with respect to their parameters, the ELBO can be maximized via gradient descent; this allows wide flexibility in choice of encoder and decoder models.",2.1. Variational autoencoder,[0],[0]
Typically these will take the form of exponential family distributions whose parameters are the weights of a multi-layer neural network.,2.1. Variational autoencoder,[0],[0]
"A context-free grammar (CFG) is traditionally defined as a 4-tuple G = (V,⌃, R, S): V is a finite set of non-terminal symbols; the alphabet ⌃ is a finite set of terminal symbols, disjoint from V ; R is a finite set of production rules; and S is a distinct non-terminal known as the start symbol.",2.2. Context-free grammars,[0],[0]
The rules R are formally described as ↵ !,2.2. Context-free grammars,[0],[0]
"for ↵ 2 V and 2 (V [ ⌃)⇤, with ⇤ denoting the Kleene closure.",2.2. Context-free grammars,[0],[0]
"In practice, these rules are defined as a set of mappings from a single left-hand side non-terminal in V to a sequence of terminal and/or non-terminal symbols, and can be interpreted as ‘replacement’ instructions.
",2.2. Context-free grammars,[0],[0]
"Repeatedly applying production rules beginning with a non-terminal symbol defines a tree, with symbols on the right-hand side of the production rule becoming child nodes for the left-hand side parent.",2.2. Context-free grammars,[0],[0]
"The grammar G thus defines a set of possible trees extending from each nonterminal symbol in V , produced by recursively applying rules in R to leaf nodes until all leaf nodes are terminal symbols in ⌃.",2.2. Context-free grammars,[0],[0]
The language of G is the set of all terminal symbol sequences that can be generated as leaf nodes in a tree.,2.2. Context-free grammars,[0],[0]
"Given a string in the language (i.e., a sequence of terminals), a parse tree is a tree rooted at S which has this sequence of terminal symbols as its leaf nodes.",2.2. Context-free grammars,[0],[0]
The ubiquity of context-free languages in computer science is due in part to the presence of efficient parsing algorithms to generate parse trees.,2.2. Context-free grammars,[0],[0]
"For more background on CFGs and automata theory, see e.g. Hopcroft et al. (2006).
",2.2. Context-free grammars,[0],[0]
Our work builds off the work of probabilistic context-free grammars (PCFGs).,2.2. Context-free grammars,[0],[0]
"A PCFG assigns probabilities to each production rule in the grammar, and thus defines a probability distribution over parse trees (Baker, 1979; Booth & Thompson, 1973).",2.2. Context-free grammars,[0],[0]
"A string can be generated by repeatedly sampling and applying production rules, beginning at the start symbol, until no non-terminals remain.",2.2. Context-free grammars,[0],[0]
"Modern approaches allow the probabilities used at each stage to depend on the state of the parse tree (Johnson et al., 2007).",2.2. Context-free grammars,[0],[0]
In this section we describe how a grammar can improve variational autoencoders (VAE) for discrete data.,3. Methods,[0],[0]
It will do so by drastically reducing the number of invalid outputs generated from the VAE.,3. Methods,[0],[0]
"We illustrate our approach on molecular data, however it will extend to any descrete data that can be described by a grammar.
",3. Methods,[0],[0]
"One glaring issue with a character-based VAE is that it may frequently map latent points to sequences that are not valid, hoping the VAE will infer from training data what constitutes a valid sequence.",3. Methods,[0],[0]
"Instead of implicitly encouraging the VAE to produce valid sequences, we propose to give the VAE explicit knowledge about how to produce valid sequences.",3. Methods,[0],[0]
We do this by using a grammar for the sequences: given a grammar we can take any valid sequence and parse it into a parse tree.,3. Methods,[0],[0]
A pre-order traversal on this parse tree yields a sequence of production rules.,3. Methods,[0],[0]
Applying these rules in order will yield the original sequence.,3. Methods,[0],[0]
Our approach then will be to learn a VAE that produces sequences of grammar production rules.,3. Methods,[0],[0]
"The benefit is that it is trivial to generate valid sequences of production rules, as the grammar describes the valid set of rules that can be selected at any point during the generation process.",3. Methods,[0],[0]
"Thus, our model is able to focus on learning semantic properties of sequence data without also having to learn syntactic constraints.",3. Methods,[0],[0]
We propose a grammar variational autoencoder (GVAE) that encodes/decodes in the space of grammar production rules.,3.1. An illustrative example,[0],[0]
"We describe how it works with a simple example.
Encoding.",3.1. An illustrative example,[0],[0]
"Consider a subset of the SMILES grammar as shown in Figure 1, box 1 .",3.1. An illustrative example,[0],[0]
These are the possible production rules that can be used for constructing a molecule.,3.1. An illustrative example,[0],[0]
Imagine we are given as input the SMILES string for benzene: ‘c1ccccc1’.,3.1. An illustrative example,[0],[0]
"Figure 1, box 3 shows this molecule.",3.1. An illustrative example,[0],[0]
To encode this molecule into a continuous latent representation we begin by using the SMILES grammar to parse this string into a parse tree (partially shown in box 2 ).,3.1. An illustrative example,[0],[0]
This tree describes how ‘c1ccccc1’ is generated by the grammar.,3.1. An illustrative example,[0],[0]
"We decompose this tree into a sequence of production rules by performing a pre-order traversal on the branches of the parse tree from left-to-right, shown in box 4 .",3.1. An illustrative example,[0],[0]
"We convert these rules into 1-hot indicator vectors, where each dimension corresponds to a rule in the SMILES grammar, box 5 .",3.1. An illustrative example,[0],[0]
"These 1-hot vectors are concatenated into the rows of a matrix X of dimension T (X)⇥K, where K is the number of production rules in the SMILES grammar, and T (X) is the number of production rules used to generate X.
We use a deep convolutional neural network to map the collection of 1-hot vectors X to a continuous latent vector z.",3.1. An illustrative example,[0],[0]
"The architecture of the encoding network is described in the supplementary material.
",3.1. An illustrative example,[0],[0]
Decoding.,3.1. An illustrative example,[0],[0]
We now describe how we map continuous vectors back to a sequence of production rules (and thus SMILES strings).,3.1. An illustrative example,[0],[0]
"Crucially we construct the decoder so that, at any time while we are decoding a sequence, the decoder will only be allowed to select a subset of production rules that are ‘valid’.",3.1. An illustrative example,[0],[0]
"This will cause the decoder to only produce valid parse sequences from the grammar.
",3.1. An illustrative example,[0],[0]
"We begin by passing the continuous vector z through a recurrent neural network which produces a set of unnormalized log probability vectors (or ‘logits’), shown in Figure 2, box 1 and 2 .",3.1. An illustrative example,[0],[0]
"Exactly like the 1-hot vectors produced by the encoder, each dimension of the logit vectors cor-
responds to a production rule in the grammar.",3.1. An illustrative example,[0],[0]
"We can again write these collection of logit vectors as a matrix F 2 RTmax⇥K , where T
max is the maximum number of timesteps (production rules) allowed by the decoder.",3.1. An illustrative example,[0],[0]
"During the rest of the decoding operations, we will use the rows of F to select a sequence of valid production rules.
",3.1. An illustrative example,[0],[0]
"To ensure that any sequence of production rules generated from the decoder is valid, we keep track of the state of the parsing using a last-in first-out (LIFO) stack.",3.1. An illustrative example,[0],[0]
"This is shown in Figure 2, box 3 .",3.1. An illustrative example,[0],[0]
"At the beginning, every valid parse from the grammar must start with the start symbol: smiles, which is placed on the stack.",3.1. An illustrative example,[0],[0]
"Next we pop off whatever non-terminal symbol that was placed last on the stack (in this case smiles), and we use it to mask out the invalid dimensions of the current logit vector.",3.1. An illustrative example,[0],[0]
"Formally, for every non-terminal ↵ we define a fixed binary mask vector m
↵ 2 [0, 1]K .",3.1. An illustrative example,[0],[0]
"This takes the value ‘1’ for all indices in 1, . . .",3.1. An illustrative example,[0],[0]
",K corresponding to production rules that have ↵ on their left-hand-side.
",3.1. An illustrative example,[0],[0]
"In the previous example, the only production rule in the grammar beginning with smiles is the first so we maskout every dimension except the first, shown in Figure 2, box 4 .",3.1. An illustrative example,[0],[0]
"We then sample from the remaining unmasked rules, using their values in the logit vector.",3.1. An illustrative example,[0],[0]
"To sample from this masked logit at any timestep t we form the following masked distribution:
p(x t = k|↵, z) = m↵,k exp(ftk)P K
j=1 m↵,k exp(ftj) , (2)
where f tk is the (t, k)-element of the logit matrix F. As only the first rule is unmasked we will select this rule smiles !",3.1. An illustrative example,[0],[0]
"chain as the first rule in our sequence, box 5 .",3.1. An illustrative example,[0],[0]
"Now the next rule must begin with chain, so we push it onto the stack (Figure 2, box 3 ).",3.1. An illustrative example,[0],[0]
"We sample this nonterminal and, as before, use it to mask out all of the rules that cannot be applied in the current logit vector.",3.1. An illustrative example,[0],[0]
We then sample a valid rule from this logit vector: chain!,3.1. An illustrative example,[0],[0]
"chain, branched atom.",3.1. An illustrative example,[0],[0]
"Just as before we push the non-terminals on the right-hand side of this rule onto the stack, adding the individual non-terminals in from right to left, such that the leftmost non-terminal is on the top of the stack.",3.1. An illustrative example,[0],[0]
"For the
Algorithm 1 Sampling from the decoder Input: Deterministic decoder output F 2 RTmax⇥K ,
masks m ↵ for each production rule ↵ Output: Sampled productions X from p(X|z)
1: Initialize empty stack S , and push the start symbol S onto the top; set t = 0 2: while S is nonempty do 3:",3.1. An illustrative example,[0],[0]
"Pop the last-pushed non-terminal ↵ from the stack S 4: Use Eq. (2) to sample a production rule R 5: Let x
t be the 1-hot vector corresponding to R 6: Let RHS(R) denote all non-terminals on the righthand side of rule R, ordered from right to left 7: for non-terminal in RHS(R) do 8: Push on to the stack S 9: end for
10: Set X [X>,x t ]",3.1. An illustrative example,[0],[0]
"> 11: Set t t+ 1 12: end while
next state we again pop the last rule placed on the stack and mask the current logit, etc.",3.1. An illustrative example,[0],[0]
"This process continues until the stack is empty or we reach the maximum number of logit vectors T
max .",3.1. An illustrative example,[0],[0]
We describe this decoding procedure formally in Algorithm 1.,3.1. An illustrative example,[0],[0]
"In practice, because sampling from the decoder often finishes before t reaches T
max , we introduce an additional ‘no-op’ rule to the grammar that we use to pad X until the number of rows equals T
max
.
",3.1. An illustrative example,[0],[0]
We note the explicit connection between the process in Algorithm 1 and parsing algorithms for pushdown automata.,3.1. An illustrative example,[0],[0]
"A pushdown automaton is a finite state machine which has access to a single stack for long-term storage, and are equivalent to context-free grammars in the sense that every CFG can be converted into a pushdown automaton, and vice-versa (Hopcroft et al., 2006).",3.1. An illustrative example,[0],[0]
"The decoding algorithm performs the sequence of actions taken by a nondeterministic pushdown automaton at each stage of a parsing algorithm; the nondeterminism is resolved by sampling according to the probabilities in the emitted logit vector.
Contrasting the character VAE.",3.1. An illustrative example,[0],[0]
"Notice that the key difference between this grammar VAE decoder and a
character-based VAE decoder is that at every point in the generated sequence, the character VAE can sample any possible character.",3.1. An illustrative example,[0],[0]
There is no stack or masking operation.,3.1. An illustrative example,[0],[0]
"The grammar VAE however is constrained to select syntactically-valid sequences.
",3.1. An illustrative example,[0],[0]
Syntactic vs. semantic validity.,3.1. An illustrative example,[0],[0]
It is important to note that the grammar encodes syntactically valid molecules but not necessarily semantically valid molecules.,3.1. An illustrative example,[0],[0]
This is mainly because of three reasons.,3.1. An illustrative example,[0],[0]
"First, certain molecules produced by the grammar may be very unstable molecules or not chemically-valid (for instance an oxygen atom cannot bond to 3 other atoms as it only has 2 free electrons for bonding, although it would be possible to generate this in a molecule from the grammar).",3.1. An illustrative example,[0],[0]
"Second, the SMILES language has non-context free aspects, e.g. a ringbond must be opened and closed by the same digit, starting with ‘1’ (as is the case for benzene ‘c1ccccc1’).",3.1. An illustrative example,[0],[0]
"The particular challenge for matching digits, in contrast to matching grouping symbols such as parentheses, is that they do not compose in a nested manner; for example, ‘C12(CCCCC1)CCCCC2’ is a valid molecule.",3.1. An illustrative example,[0],[0]
Keeping track of which digit to use for each ringbond is not context-free.,3.1. An illustrative example,[0],[0]
"Third, we note that the GVAE can output an undetermined sequence if there are still non-terminal symbols on the stack after processing all T max
logit vectors.",3.1. An illustrative example,[0],[0]
"While this could be fixed by a procedure that converts these non-terminals to terminals, for simplicity we mark these sequences as invalid.",3.1. An illustrative example,[0],[0]
"During training, each input SMILES encoded as a sequence of 1-hot vectors X 2 {0, 1}Tmax⇥K , also defines a sequence of T
max mask vectors.",3.2. Training,[0],[0]
"Each mask at timestep t = 1, . . .",3.2. Training,[0],[0]
", T
max is selected by the left-hand side of the production rule indicated in the 1-hot vector x
t .",3.2. Training,[0],[0]
"Given these masks we can compute the decoder’s mapping
p(X|z)",3.2. Training,[0],[0]
"= T (X)Y
t=1
p(x t |z,x1:(t 1)), (3)
with the individual probabilities at each timestep defined as in Eq. (2).",3.2. Training,[0],[0]
"We pad any remaining timesteps after T (X) up
Algorithm 2 Training the Grammar VAE Input: Dataset {X(i)}N
i=1
Output: Trained VAE model p ✓ (X|z), q (z|X) 1: while VAE not converged do 2: Select element: X 2 {X(i)}N
i=1 (or minibatch) 3:",3.2. Training,[0],[0]
"Encode: z ⇠ q
(z|X) 4: Decode: given z, compute logits F 2 RTmax⇥K 5: for t in [1, . . .",3.2. Training,[0],[0]
", T
max ] do 6: Compute p
✓
(x
t |z) via Eq.",3.2. Training,[0],[0]
"(2), with mask m x
t
and logits f t
7: end for 8: Update ✓, using estimates p
✓
(X|z), q (z|X), via gradient descent on the ELBO in Eq.",3.2. Training,[0],[0]
"(4)
9: end while
to T max with a ‘no-op’ rule, a one-hot vector indicating the parse tree is complete and no actions are to be taken.
",3.2. Training,[0],[0]
"In all our experiments, q(z|X) is a Gaussian distribution whose mean and variance parameters are the output of the encoder network, with an isotropic Gaussian prior p(z)",3.2. Training,[0],[0]
"= N (0, I)",3.2. Training,[0],[0]
.,3.2. Training,[0],[0]
"At training time, we sample a value of z from q(z|X) to compute the ELBO
L( , ✓;X) = E q (z|X)",3.2. Training,[0],[0]
"[log p✓(X, z) log q (z|X)] .",3.2. Training,[0],[0]
"(4)
Following Kingma & Welling (2014), we apply a noncentered parameterization on the encoding Gaussian distribution and optimize Eq.",3.2. Training,[0],[0]
"(4) using gradient descent, learning encoder and decoder neural network parameters and ✓.",3.2. Training,[0],[0]
Algorithm 2 summarizes the training procedure.,3.2. Training,[0],[0]
We show the usefulness of our proposed grammar variational autoencoder (GVAE)2 on two sequence optimization problems: 1) searching for an arithmetic expression that best fits a dataset and 2) finding new drug molecules.,4. Experiments,[0],[0]
"We begin by showing the latent space of the GVAE and a character variational autoencoder (CVAE), similar to that of Gómez-Bombarelli et al. (2016b)3, on each of the problems.",4. Experiments,[0],[0]
"We demonstrate that the GVAE learns a smooth, meaningful latent space for arithmetic equations and molecules.",4. Experiments,[0],[0]
"Given this we perform optimization in this latent space using Bayesian optimization, inspired by the technique of Gómez-Bombarelli et al. (2016b).",4. Experiments,[0],[0]
"We demonstrate that the GVAE improves upon a previous character variational autoencoder, by selecting an arithmetic expression that matches the data nearly perfectly, and by finding novel molecules with better drug properties.
2Code available at: https://github.com/mkusner/grammarVAE 3https://github.com/maxhodak/keras-molecules",4. Experiments,[0],[0]
We describe in detail the two sequence optimization problems we seek to solve.,4.1. Problems,[0],[0]
The first consists in optimizing the fit of an arithmetic expression.,4.1. Problems,[0],[0]
"We are given a set of 100,000 randomly generated univariate arithmetic expressions from the following grammar:
S !",4.1. Problems,[0],[0]
S ‘+ ’ T | S ‘⇤ ’ T | S ‘ / ’ T | T T !,4.1. Problems,[0],[0]
‘ ( ’ S ‘ ) ’,4.1. Problems,[0],[0]
| ‘ s i n ( ’ S ‘ ) ’,4.1. Problems,[0],[0]
| ‘ exp ( ’ S ‘ ) ’ T !,4.1. Problems,[0],[0]
"‘x ’ | ‘1 ’ | ‘2 ’ | ‘3 ’
where S and T are non-terminals and the symbol | separates the possible production rules generated from each non-terminal.",4.1. Problems,[0],[0]
"By parsing this grammar we can randomly generate strings of univariate arithmetic equations (functions of x) such as the following: sin(2), x/(3+ 1), 2+ x+ sin(1/2), and x/2 ⇤ exp(x)/exp(2 ⇤ x).",4.1. Problems,[0],[0]
We limit the length of every selected string to have at most 15 production rules.,4.1. Problems,[0],[0]
Given this dataset we train both the CVAE and GVAE to learn a latent space of arithmetic expressions.,4.1. Problems,[0],[0]
We propose to perform optimization in this latent space of expressions to find an expression that best fits a fixed dataset.,4.1. Problems,[0],[0]
A common measure of best fit is the test MSE between the predictions made by a selected expression and the true data.,4.1. Problems,[0],[0]
"In the generated expressions, the presence of exponential functions can result in very large MSE values.",4.1. Problems,[0],[0]
"For this reason, we use as target variable log(1 + MSE) instead of MSE.
",4.1. Problems,[0],[0]
"For the second optimization problem, we follow (GómezBombarelli et al., 2016b) and optimize the drug properties of molecules.",4.1. Problems,[0],[0]
"Our goal is to maximize the water-octanol partition coefficient (logP), an important metric in drug design that characterizes the drug-likeness of a molecule.",4.1. Problems,[0],[0]
"As in Gómez-Bombarelli et al. (2016b) we consider a penalized logP score that takes into account other molecular properties such as ring size and synthetic accessibility (Ertl & Schuffenhauer, 2009).",4.1. Problems,[0],[0]
"The training data for the CVAE and GVAE models are 250,000 SMILES strings (Weininger, 1988) extracted at random from the ZINC database by Gómez-Bombarelli et al. (2016b).",4.1. Problems,[0],[0]
We describe the context-free grammar for SMILES strings that we use to train our GVAE in the supplementary material.,4.1. Problems,[0],[0]
Arithmetic expressions.,4.2. Visualizing the latent space,[0],[0]
"To qualitatively evaluate the smoothness of the VAE embeddings for arithmetic expressions, we attempt interpolating between two arithmetic expressions, as in Bowman et al. (2016).",4.2. Visualizing the latent space,[0],[0]
This is done by encoding two equations and then performing linear interpolation in the latent space.,4.2. Visualizing the latent space,[0],[0]
Results comparing the character and grammar VAEs are shown in Table 1.,4.2. Visualizing the latent space,[0],[0]
"Although the character VAE smoothly interpolates between the text representation of equations, it passes through intermediate points which do not decode to valid equations.",4.2. Visualizing the latent space,[0],[0]
"In contrast, the grammar VAE also provides smooth interpolation and produces valid equations for any location in the latent space.",4.2. Visualizing the latent space,[0],[0]
"A further exploration of a 2-dimensional latent space is shown in the appendix.
Molecules.",4.2. Visualizing the latent space,[0],[0]
We are interested if the GVAE produces a coherent latent space of molecules.,4.2. Visualizing the latent space,[0],[0]
To assess this we begin by encoding a molecule.,4.2. Visualizing the latent space,[0],[0]
We then generate 2 random orthogonal unit vectors in latent space (scaled down to only search the neighborhood of the molecules).,4.2. Visualizing the latent space,[0],[0]
Moving in combinations of these directions defines a grid and at each point in the grid we decode the latent vector 1000 times.,4.2. Visualizing the latent space,[0],[0]
We select the molecule that appears most often as the representative molecule.,4.2. Visualizing the latent space,[0],[0]
Figure 3 shows this latent space search surrounding two different molecules.,4.2. Visualizing the latent space,[0],[0]
Compare this to Figures 13-15 in Gómez-Bombarelli et al. (2016b).,4.2. Visualizing the latent space,[0],[0]
"We note that in each plot of the GVAE the latent space is very smooth, in many cases moving from one grid point to another will only change a single atom in a molecule.",4.2. Visualizing the latent space,[0],[0]
"In the CVAE (Gómez-Bombarelli et al., 2016b) we do not observe such fine-grained smoothness.",4.2. Visualizing the latent space,[0],[0]
We now perform a series of experiments using the autoencoders to produce novel sequences with improved properties.,4.3. Bayesian optimization,[0],[0]
"For this, we follow the approach proposed by GómezBombarelli et al. (2016b) and after training the GVAE, we
train an additional model to predict properties of sequences from their latent representation.",4.3. Bayesian optimization,[0],[0]
"To propose promising new sequences, we can start from the latent vector of an encoded sequence and then use the output of this predictor (including its gradient) to move in the latent space direction most likely to improve the property.",4.3. Bayesian optimization,[0],[0]
"The resulting new latent points can then be decoded into corresponding sequences.
",4.3. Bayesian optimization,[0],[0]
"In practice, measuring the property of each new sequence could be an expensive process.",4.3. Bayesian optimization,[0],[0]
"For example, the sequence could represent an organic photovoltaic molecule and the property could be the result of an expensive quantum mechanical simulation used to estimate the molecule’s powerconversion efficiency (Hachmann et al., 2011).",4.3. Bayesian optimization,[0],[0]
The sequence could also represent a program or expression which may be computationally expensive to evaluate.,4.3. Bayesian optimization,[0],[0]
"Therefore, ideally, we would like the optimization process to perform only a reduced number of property evaluations.",4.3. Bayesian optimization,[0],[0]
"For this, we use Bayesian optimization methods, which choose the next point to evaluate by maximizing an acquisition function that quantifies the benefit of evaluating the property at a particular location (Shahriari et al., 2016).
",4.3. Bayesian optimization,[0],[0]
"After training the GVAE, we obtain a latent feature vector for each sequence in the training data, given by the mean of the variational encoding distributions.",4.3. Bayesian optimization,[0],[0]
"We use these vectors and their corresponding property estimates to train a sparse Gaussian process (SGP) model with 500 inducing points (Snelson & Ghahramani, 2005), which is used to make predictions for the properties of new points in latent space.",4.3. Bayesian optimization,[0],[0]
"After training the SGP, we then perform 5 iterations of batch Bayesian optimization using the expected improvement (EI) heuristic (Jones et al., 1998).",4.3. Bayesian optimization,[0],[0]
"On each iteration, we select a batch of 50 latent vectors by sequentially maximizing the EI acquisition function.",4.3. Bayesian optimization,[0],[0]
"We use the Kriging Believer Algorithm to account for pending evaluations in the batch selection process (Cressie, 1990).",4.3. Bayesian optimization,[0],[0]
"That is, after selecting each new data point in the batch, we add that data point as a new inducing point in the sparse GP model with associated target variable equal to the mean of the GP predictive distribution at that point.",4.3. Bayesian optimization,[0],[0]
"Once a new batch of 50 latent vectors is selected, each point in the batch is transformed into its corresponding sequence using the decoder network in the GVAE.",4.3. Bayesian optimization,[0],[0]
The properties of the newly generated sequences are then computed and the resulting data is added to the training set before retraining the SGP and starting the next BO iteration.,4.3. Bayesian optimization,[0],[0]
"Note that some of the new sequences will be invalid and consequently, it will not be possible to obtain their corresponding property estimate.",4.3. Bayesian optimization,[0],[0]
"In this case we fix the property to be equal to the worst value observed in the original training data.
",4.3. Bayesian optimization,[0],[0]
Arithmetic expressions.,4.3. Bayesian optimization,[0],[0]
Our goal is to see if we can find an arithmetic expression that best fits a fixed dataset.,4.3. Bayesian optimization,[0],[0]
"Specifically, we generate this dataset by selecting 1000
input values, x, that are linearly-spaced between 10 and 10.",4.3. Bayesian optimization,[0],[0]
We then pass these through our true function 1/3+ x+,4.3. Bayesian optimization,[0],[0]
sin(x ⇤ x),4.3. Bayesian optimization,[0],[0]
to generate the true target observations.,4.3. Bayesian optimization,[0],[0]
We use Bayesian optimization (BO) as described above search for this equation.,4.3. Bayesian optimization,[0],[0]
We run BO for 5 iterations and average across 10 repetitions of the process.,4.3. Bayesian optimization,[0],[0]
Table 2 (rows 1 & 2) shows the results obtained.,4.3. Bayesian optimization,[0],[0]
The third column in the table reports the fraction of arithmetic sequences found by BO that are valid.,4.3. Bayesian optimization,[0],[0]
The GVAE nearly always finds valid sequences.,4.3. Bayesian optimization,[0],[0]
"The only cases in which it does not is when there are still non-terminals on the stack of
the decoder upon reaching the maximum number of timesteps T
max , however this is rare.",4.3. Bayesian optimization,[0],[0]
"Additionally, the GVAE finds squences with better scores on average when compared with the CVAE.
",4.3. Bayesian optimization,[0],[0]
"Table 3 shows the top 3 expressions found by GVAE and CVAE during the BO search, together with their associated score values.",4.3. Bayesian optimization,[0],[0]
Figure 4 shows how the best expression found by GVAE and CVAE compare to the true function.,4.3. Bayesian optimization,[0],[0]
"We note that the CVAE has failed to find the sinusoidal portion of the true expression, while the difference between the GVAE expression and the true function is negligible.
Molecules.",4.3. Bayesian optimization,[0],[0]
We now consider the problem of finding new drug-like molecules.,4.3. Bayesian optimization,[0],[0]
"We perform 5 iterations of BO, and average results across 10 trials.",4.3. Bayesian optimization,[0],[0]
Table 2 (rows 3 & 4) shows the overall BO results.,4.3. Bayesian optimization,[0],[0]
"In this problem, the GVAE produces about twice more valid sequences than the CVAE.",4.3. Bayesian optimization,[0],[0]
The valid sequences produced by the GVAE also result in higher scores on average.,4.3. Bayesian optimization,[0],[0]
The best found SMILES strings by each method and their scores are shown in Table 4; the molecules themselves are plotted in Figure 5.,4.3. Bayesian optimization,[0],[0]
We now perform a series of experiments to evaluate the predictive performance of the latent representations found by each autoencoder.,4.4. Predictive performance of latent representation,[0],[0]
"For this, we use the sparse GP model used in the previous Bayesian optimization experiments and look at its predictive performance on a left-out test set with 10% of the data, where the data is formed by the latent representation of the available sequences (these are the inputs to the sparse GP model) and the associated properties of those sequences (these are the outputs in the sparse GP model).",4.4. Predictive performance of latent representation,[0],[0]
Table 5 show the average test RMSE and test loglikelihood for the GVAE and the CVAE across 10 different splits of the data for the expressions and for the molecules.,4.4. Predictive performance of latent representation,[0],[0]
This table shows that the GVAE produces latent features that yield much better predictive performance than those produced by the CVAE.,4.4. Predictive performance of latent representation,[0],[0]
"Parse trees have been used to learn continuous representations of text in recursive neural network models (Socher et al., 2013; Irsoy & Cardie, 2014; Paulus et al., 2014).",5. Related Work,[0],[0]
These models learn a vector at every non-terminal in the parse tree by recursively combining the vectors of child nodes.,5. Related Work,[0],[0]
"Recursive autoencoders learn these representations by minimizing the reconstruction error between true child vectors and those predicted by the parent (Socher et al., 2011a;b).",5. Related Work,[0],[0]
"Recently, Allamanis et al. (2016) learn representations for symbolic expressions from their parse trees.",5. Related Work,[0],[0]
"Importantly, all of these methods are discriminative and do not learn a generative latent space.",5. Related Work,[0],[0]
"Like our decoder, re-
current neural network grammars (Dyer et al., 2016) produce sequences through a linear traversal of the parse tree, but focus on the case where the underlying grammar is unknown and not context-free.",5. Related Work,[0],[0]
Maddison & Tarlow (2014) describe generative models of natural source code based on probabilistic context free grammars and neuro-probabilistic language models.,5. Related Work,[0],[0]
"However, these works are not geared towards learning a latent representation of the data.
",5. Related Work,[0],[0]
"Learning arithmetic expressions to fit data, often called symbolic regression, are generally based on genetic programming (Willis et al., 1997) or other computationally demanding evolutionary algorithms to propose candidate expressions (Schmidt & Lipson, 2009).",5. Related Work,[0],[0]
"Alternatives include running particle MCMC inference to estimate a Bayesian posterior over parse trees (Perov & Wood, 2016).
",5. Related Work,[0],[0]
"In molecular design, searching for new molecules is traditionally done by sifting through large databases of potential molecules and then subjecting them to a virtual screening process (Pyzer-Knapp et al., 2015; Gómez-Bombarelli et al., 2016a).",5. Related Work,[0],[0]
"These databases are too large to search via exhaustive enumeration, and require novel stochastic search algorithms tailored to the domain (Virshup et al., 2013; Rupakheti et al., 2015).",5. Related Work,[0],[0]
"Segler et al. (2017) fit a recurrent neural network to chemicals represented by SMILES strings, however their goal is more akin to density estimation; they learn a simulator which can sample proposals for novel molecules, but it is not otherwise used as part of an optimization or inference process itself.",5. Related Work,[0],[0]
"Our work most closely resembles Gómez-Bombarelli et al. (2016b) for novel molecule synthesis, in that we also learn a latent variable model which admits a continuous representation of the domain.",5. Related Work,[0],[0]
"However, both Segler et al. (2017) and Gómez-Bombarelli et al. (2016b) use character-level models for molecules.",5. Related Work,[0],[0]
"Empirically, it is clear that representing molecules and equations by way of their parse tree generated from a grammar outperforms text-based representations.",6. Discussion,[0],[0]
"We believe this approach will be broadly useful for representation learning, inference, and optimization in any domain which can be represented as text in a context-free language.",6. Discussion,[0],[0]
This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.,Acknowledgements,[0],[0]
"Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio.",abstractText,[0],[0]
"However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges.",abstractText,[0],[0]
"Crucially, state-of-the-art methods often produce outputs that are not valid.",abstractText,[0],[0]
"We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar.",abstractText,[0],[0]
"We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid.",abstractText,[0],[0]
"Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs.",abstractText,[0],[0]
We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.,abstractText,[0],[0]
Grammar Variational Autoencoder,title,[0],[0]
"Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a).",1 Introduction,[0],[0]
"State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language.",1 Introduction,[0],[0]
"One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders,
including RNNs.",1 Introduction,[0],[0]
"Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)).",1 Introduction,[0],[0]
"Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task.",1 Introduction,[0],[0]
"This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.
",1 Introduction,[0],[0]
"Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation.",1 Introduction,[0],[0]
"Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output.",1 Introduction,[0],[0]
"Since vectors correspond to words, it is natural for us to use dependency syntax.",1 Introduction,[0],[0]
"Dependency trees (see Figure 1) represent syntactic relations between words: for example, monkey is a subject of the predicate eats, and banana is its object.
",1 Introduction,[0],[0]
"In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).",1 Introduction,[0],[0]
"GCNs can be regarded as computing a latent-feature representation of a node (i.e. a real-valued vector) based on its k-
1957 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1957–1967
Copenhagen, Denmark, September 7–11, 2017.",1 Introduction,[0],[0]
"c©2017 Association for Computational Linguistics
th order neighborhood (i.e. nodes at most k hops aways from the node) (Gilmer et al., 2017).",1 Introduction,[0],[0]
They are generally simple and computationally inexpensive.,1 Introduction,[0],[0]
"We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017).
",1 Introduction,[0],[0]
"Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework.",1 Introduction,[0],[0]
"As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task.",1 Introduction,[0],[0]
"Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information.",1 Introduction,[0],[0]
"A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016).",1 Introduction,[0],[0]
"Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively.
",1 Introduction,[0],[0]
"In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains).",1 Introduction,[0],[0]
"For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees.",1 Introduction,[0],[0]
"However, in this work we solely focus on dependency syntax and leave more general investigation for future work.
",1 Introduction,[0],[0]
"Our main contributions can be summarized as follows:
• we introduce a method for incorporating structure into NMT using syntactic GCNs;
• we show that GCNs can be used along with RNN and CNN encoders;
• we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German.",1 Introduction,[0],[0]
Notation.,2 Background,[0],[0]
"We use x for vectors, x1:t for a sequence of t vectors, and X for matrices.",2 Background,[0],[0]
The i-th value of vector x is denoted by xi.,2 Background,[0],[0]
We use ◦ for vector concatenation.,2 Background,[0],[0]
"In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx) of translating a source sentence",2.1 Neural Machine Translation,[0],[0]
x1:,2.1 Neural Machine Translation,[0],[0]
Tx (a sequence of Tx words) into a target sentence y1:Ty .,2.1 Neural Machine Translation,[0],[0]
"NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism.",2.1 Neural Machine Translation,[0],[0]
We will now briefly describe the components that we use in this paper.,2.1 Neural Machine Translation,[0],[0]
An encoder is a function that takes as input the source sentence and produces a representation encoding its semantic content.,2.1.1 Encoders,[0],[0]
"We describe recurrent, convolutional and bag-of-words encoders.
",2.1.1 Encoders,[0],[0]
Recurrent.,2.1.1 Encoders,[0],[0]
"Recurrent neural networks (RNNs) (Elman, 1990) model sequential data.",2.1.1 Encoders,[0],[0]
They receive one input vector at each time step and update their hidden state to summarize all inputs up to that point.,2.1.1 Encoders,[0],[0]
"Given an input sequence x1:Tx = x1,x2, . . .",2.1.1 Encoders,[0],[0]
",xTx of word embeddings an RNN is defined recursively as follows:
",2.1.1 Encoders,[0],[0]
"RNN(x1:t) = f(xt,RNN(x1:t−1))
where f is a nonlinear function such as an LSTM (Hochreiter and Schmidhuber, 1997) or a GRU (Cho et al., 2014b).",2.1.1 Encoders,[0],[0]
"We will use the function RNN as an abstract mapping from an input sequence x1:T to final hidden state RNN(x1:Tx), regardless of the used nonlinearity.",2.1.1 Encoders,[0],[0]
"To not only summarize the past of a word, but also its future, a bidirectional RNN (Schuster and Paliwal, 1997; Irsoy and
Cardie, 2014) is often used.",2.1.1 Encoders,[0],[0]
"A bidirectional RNN reads the input sentence in two directions and then concatenates the states for each time step:
BIRNN(x1:Tx , t) = RNNF (x1:t)◦RNNB(xTx:t)
where RNNF and RNNB are the forward and backward RNNs, respectively.",2.1.1 Encoders,[0],[0]
"For further details we refer to the encoder of Bahdanau et al. (2015).
Convolutional.",2.1.1 Encoders,[0],[0]
"Convolutional Neural Networks (CNNs) apply a fixed-size window over the input sequence to capture the local context of each word (Gehring et al., 2016).",2.1.1 Encoders,[0],[0]
"One advantage of this approach over RNNs is that it allows for fast parallel computation, while sacrificing non-local context.",2.1.1 Encoders,[0],[0]
"To remedy the loss of context, multiple CNN layers can be stacked.",2.1.1 Encoders,[0],[0]
"Formally, given an input sequence x1:Tx , we define a CNN as follows:
",2.1.1 Encoders,[0],[0]
"CNN(x1:Tx , t) = f(xt−bw/2c, ..,xt, ..,xt+bw/2c)
where f is a nonlinear function, typically a linear transformation followed by ReLU, andw is the size of the window.
",2.1.1 Encoders,[0],[0]
Bag-of-Words.,2.1.1 Encoders,[0],[0]
In a bag-of-words (BoW) encoder every word is simply represented by its word embedding.,2.1.1 Encoders,[0],[0]
"To give the decoder some sense of word position, position embeddings (PE) may be added.",2.1.1 Encoders,[0],[0]
"There are different strategies for defining position embeddings, and in this paper we choose to learn a vector for each absolute word position up to a certain maximum length.",2.1.1 Encoders,[0],[0]
"We then represent the t-th word in a sequence as follows:
BOW(x1:Tx , t) = xt + pt
where xt is the word embedding and pt is the t-th position embedding.",2.1.1 Encoders,[0],[0]
A decoder produces the target sentence conditioned on the representation of the source sentence induced by the encoder.,2.1.2 Decoder,[0],[0]
"In Bahdanau et al. (2015) the decoder is implemented as an RNN conditioned on an additional input ci, the context vector, which is dynamically computed at each time step using an attention mechanism.
",2.1.2 Decoder,[0],[0]
"The probability of a target word yi is now a function of the decoder RNN state, the previous target word embedding, and the context vector.",2.1.2 Decoder,[0],[0]
The model is trained end-to-end for maximum log likelihood of the next target word given its context.,2.1.2 Decoder,[0],[0]
We will now describe the Graph Convolutional Networks (GCNs) of Kipf and Welling (2016).,2.2 Graph Convolutional Networks,[0],[0]
"For a comprehensive overview of alternative GCN architectures see Gilmer et al. (2017).
",2.2 Graph Convolutional Networks,[0],[0]
"A GCN is a multilayer neural network that operates directly on a graph, encoding information about the neighborhood of a node as a realvalued vector.",2.2 Graph Convolutional Networks,[0],[0]
"In each GCN layer, information flows along edges of the graph; in other words, each node receives messages from all its immediate neighbors.",2.2 Graph Convolutional Networks,[0],[0]
"When multiple GCN layers are stacked, information about larger neighborhoods gets integrated.",2.2 Graph Convolutional Networks,[0],[0]
"For example, in the second layer, a node will receive information from its immediate neighbors, but this information already includes information from their respective neighbors.",2.2 Graph Convolutional Networks,[0],[0]
"By choosing the number of GCN layers, we regulate the distance the information travels: with k layers a node receives information from neighbors at most k hops away.
",2.2 Graph Convolutional Networks,[0],[0]
"Formally, consider an undirected graph G = (V, E), where V is a set of n nodes, and E is a set of edges.",2.2 Graph Convolutional Networks,[0],[0]
"Every node is assumed to be connected to itself, i.e. ∀v ∈ V : (v, v) ∈ E .",2.2 Graph Convolutional Networks,[0],[0]
"Now, let X ∈ Rd×n be a matrix containing all n nodes with their features, where d is the dimensionality of the feature vectors.",2.2 Graph Convolutional Networks,[0],[0]
"In our case, X will contain word embeddings, but in general it can contain any kind of features.",2.2 Graph Convolutional Networks,[0],[0]
"For a 1-layer GCN, the new node representations are computed as follows:
hv = ρ ( ∑ u∈N (v) Wxu + b )
where W ∈ Rd×d is a weight matrix and b ∈ Rd a bias vector.1 ρ is an activation function, e.g. a ReLU.N (v) is the set of neighbors of v, which we assume here to always include v itself.",2.2 Graph Convolutional Networks,[0],[0]
"As stated before, to allow information to flow over multiple hops, we need to stack GCN layers.",2.2 Graph Convolutional Networks,[0],[0]
"The recursive computation is as follows:
h(j+1)v = ρ",2.2 Graph Convolutional Networks,[0],[0]
"( ∑ u∈N (v) W (j)h(j)u + b (j) )
where j indexes the layer, and h(0)v = xv.
1We dropped the normalization factor used by Kipf and Welling (2016), as it is not used in syntactic GCNs of Marcheggiani and Titov (2017).",2.2 Graph Convolutional Networks,[0],[0]
Marcheggiani and Titov (2017) generalize GCNs to operate on directed and labeled graphs.2,2.3 Syntactic GCNs,[0],[0]
"This makes it possible to use linguistic structures such as dependency trees, where directionality and edge labels play an important role.",2.3 Syntactic GCNs,[0],[0]
They also integrate edge-wise gates which let the model regulate contributions of individual dependency edges.,2.3 Syntactic GCNs,[0],[0]
"We will briefly describe these modifications.
Directionality.",2.3 Syntactic GCNs,[0],[0]
"In order to deal with directionality of edges, separate weight matrices are used for incoming and outgoing edges.",2.3 Syntactic GCNs,[0],[0]
"We follow the convention that in dependency trees heads point to their dependents, and thus outgoing edges are used for head-to-dependent connections, and incoming edges are used for dependent-to-head connections.",2.3 Syntactic GCNs,[0],[0]
"Modifying the recursive computation for directionality, we arrive at:
h(j+1)v = ρ",2.3 Syntactic GCNs,[0],[0]
"( ∑ u∈N (v) W (j) dir(u,v) h (j) u + b",2.3 Syntactic GCNs,[0],[0]
"(j) dir(u,v) )
",2.3 Syntactic GCNs,[0],[0]
"where dir(u, v) selects the weight matrix associated with the directionality of the edge connecting u and v (i.e. WIN for u-to-v, WOUT for v-to-u, and WLOOP for v-to-v).",2.3 Syntactic GCNs,[0],[0]
"Note that self loops are modeled separately,
so there are now three times as many parameters as in a non-directional GCN.
2For",2.3 Syntactic GCNs,[0],[0]
"an alternative approach to integrating labels and directions, see applications of GCNs to statistical relation learning (Schlichtkrull et al., 2017).
",2.3 Syntactic GCNs,[0],[0]
Labels.,2.3 Syntactic GCNs,[0],[0]
Making the GCN sensitive to labels is straightforward given the above modifications for directionality.,2.3 Syntactic GCNs,[0],[0]
"Instead of using separate matrices for each direction, separate matrices are now defined for each direction and label combination:
h(j+1)v = ρ ( ∑ u∈N (v) W (j) lab(u,v) h (j) u + b (j) lab(u,v) )
where we incorporate the directionality of an edge directly in its label.
",2.3 Syntactic GCNs,[0],[0]
"Importantly, to prevent over-parametrization, only bias terms are made label-specific, in other words: Wlab(u,v) = Wdir(u,v).",2.3 Syntactic GCNs,[0],[0]
"The resulting syntactic GCN is illustrated in Figure 2 (shown on top of a CNN, as we will explain in the subsequent section).
",2.3 Syntactic GCNs,[0],[0]
Edge-wise gating.,2.3 Syntactic GCNs,[0],[0]
"Syntactic GCNs also include gates, which can down-weight the contribution of individual edges.",2.3 Syntactic GCNs,[0],[0]
"They also allow the model to deal with noisy predicted structure, i.e. to ignore potentially erroneous syntactic edges.",2.3 Syntactic GCNs,[0],[0]
"For each edge, a scalar gate is calculated as follows:
g(j)u,v = σ",2.3 Syntactic GCNs,[0],[0]
"( h(j)u · ŵ (j) dir(u,v) + b̂ (j) lab(u,v) )",2.3 Syntactic GCNs,[0],[0]
"where σ is the logistic sigmoid function, and ŵ
(j) dir(u,v) ∈ R d and b̂(j)lab(u,v) ∈",2.3 Syntactic GCNs,[0],[0]
R are learned parameters for the gate.,2.3 Syntactic GCNs,[0],[0]
"The computation becomes:
h(j+1)v =ρ (∑ u∈N (v) g(j)u,v ( W (j) dir(u,v) h (j) u + b",2.3 Syntactic GCNs,[0],[0]
"(j) lab(u,v) ))",2.3 Syntactic GCNs,[0],[0]
"In this work we focus on exploiting structural information on the source side, i.e. in the encoder.",3 Graph Convolutional Encoders,[0],[0]
"We hypothesize that using an encoder that incorporates syntax will lead to more informative representations of words, and that these representations, when used as context vectors by the decoder, will lead to an improvement in translation quality.",3 Graph Convolutional Encoders,[0],[0]
"Consequently, in all our models, we use the decoder of Bahdanau et al. (2015) and keep this part of the model constant.",3 Graph Convolutional Encoders,[0],[0]
"As is now common practice, we do not use a maxout layer in the decoder, but apart from this we do not deviate from the original definition.",3 Graph Convolutional Encoders,[0],[0]
"In all models we make use of GRUs (Cho et al., 2014b) as our RNN units.
",3 Graph Convolutional Encoders,[0],[0]
"Our models vary in the encoder part, where we exploit the power of GCNs to induce syntacticallyaware representations.",3 Graph Convolutional Encoders,[0],[0]
"We now define a series of encoders of increasing complexity.
BoW + GCN.",3 Graph Convolutional Encoders,[0],[0]
"In our first and simplest model, we propose a bag-of-words encoder (with position embeddings, see §2.1.1), with a GCN on top.",3 Graph Convolutional Encoders,[0],[0]
"In other words, inputs h(0) are a sum of embeddings of a word and its position in a sentence.",3 Graph Convolutional Encoders,[0],[0]
"Since the original BoW encoder captures the linear ordering information only in a very crude way (through the position embeddings), the structural information provided by GCN should be highly beneficial.
",3 Graph Convolutional Encoders,[0],[0]
Convolutional + GCN.,3 Graph Convolutional Encoders,[0],[0]
"In our second model, we use convolutional neural networks to learn word representations.",3 Graph Convolutional Encoders,[0],[0]
"CNNs are fast, but by definition only use a limited window of context.",3 Graph Convolutional Encoders,[0],[0]
"Instead of the approach used by Gehring et al. (2016) (i.e. stacking multiple CNN layers on top of each other), we use a GCN to enrich the one-layer CNN representations.",3 Graph Convolutional Encoders,[0],[0]
Figure 2 shows this model.,3 Graph Convolutional Encoders,[0],[0]
"Note that, while the figure shows a CNN with a window size of 3, we will use a larger window size of 5 in our experiments.",3 Graph Convolutional Encoders,[0],[0]
"We expect this model to perform better than BoW + GCN, because of the additional local context captured by the CNN.
BiRNN + GCN.",3 Graph Convolutional Encoders,[0],[0]
"In our third and most powerful model, we employ bidirectional recurrent neural networks.",3 Graph Convolutional Encoders,[0],[0]
"In this model, we start by encoding the source sentence using a BiRNN (i.e. BiGRU), and use the resulting hidden states as input to a GCN.",3 Graph Convolutional Encoders,[0],[0]
"Instead of relying on linear order only, the GCN will allow the encoder to ‘teleport’ over parts of the input sentence, along dependency edges, con-
necting words that otherwise might be far apart.",3 Graph Convolutional Encoders,[0],[0]
"The model might not only benefit from this teleporting capability however; also the nature of the relations between words (i.e. dependency relation types) may be useful, and the GCN exploits this information (see §2.3 for details).
",3 Graph Convolutional Encoders,[0],[0]
"This is the most challenging setup for GCNs, as RNNs have been shown capable of capturing at least some degree of syntactic information without explicit supervision (Linzen et al., 2016), and hence they should be hard to improve on by incorporating treebank syntax.
",3 Graph Convolutional Encoders,[0],[0]
Marcheggiani and Titov (2017) did not observe improvements from using multiple GCN layers in semantic role labeling.,3 Graph Convolutional Encoders,[0],[0]
"However, we do expect that propagating information from further in the tree should be beneficial in principle.",3 Graph Convolutional Encoders,[0],[0]
"We hypothesize that the first layer is the most influential one, capturing most of the syntactic context, and that additional layers only modestly modify the representations.",3 Graph Convolutional Encoders,[0],[0]
"To ease optimization, we add a residual connection (He et al., 2016) between the GCN layers, when using more than one layer.",3 Graph Convolutional Encoders,[0],[0]
"Experiments are performed using the Neural Monkey toolkit3 (Helcl and Libovický, 2017), which implements the model of Bahdanau et al. (2015) in TensorFlow.",4 Experiments,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 (0.0002 for CNN models).4",4 Experiments,[0],[0]
The batch size is set to 80.,4 Experiments,[0],[0]
"Between layers we apply dropout with a probability of 0.2, and in experiments with GCNs5 we use the same value for edge dropout.",4 Experiments,[0],[0]
"We train for 45 epochs, evaluating the BLEU performance of the model every epoch on the validation set.",4 Experiments,[0],[0]
"For testing, we select the model with the highest validation BLEU.",4 Experiments,[0],[0]
L2 regularization is used with a value of 10−8.,4 Experiments,[0],[0]
All the model selection (incl. hyperparameter selections) was performed on the validation set.,4 Experiments,[0],[0]
"In all experiments we obtain translations using a greedy decoder, i.e. we select the output token with the highest probability at each time step.
",4 Experiments,[0],[0]
"We will describe an artificial experiment in §4.1 and MT experiments in §4.2.
",4 Experiments,[0],[0]
"3https://github.com/ufal/neuralmonkey 4Like Gehring et al. (2016) we note that Adam is too aggressive for CNN models, hence we use a lower learning rate.",4 Experiments,[0],[0]
5GCN code at https://github.com/bastings/neuralmonkey,4 Experiments,[0],[0]
Our goal here is to provide an intuition for the capabilities of GCNs.,4.1 Reordering artificial sequences,[0],[0]
We define a reordering task where randomly permuted sequences need to be put back into the original order.,4.1 Reordering artificial sequences,[0],[0]
"We encode the original order using edges, and test if GCNs can successfully exploit them.",4.1 Reordering artificial sequences,[0],[0]
Note that this task is not meant to provide a fair comparison to RNNs.,4.1 Reordering artificial sequences,[0],[0]
"The input (besides the edges) simply does not carry any information about the original ordering, so RNNs cannot possibly solve this task.
Data.",4.1 Reordering artificial sequences,[0],[0]
"From a vocabulary of 26 types, we generate random sequences of 3-10 tokens.",4.1 Reordering artificial sequences,[0],[0]
"We then randomly permute them, pointing every token to its original predecessor with a label sampled from a set of 5 labels.",4.1 Reordering artificial sequences,[0],[0]
"Additionally, we point every token to an arbitrary position in the sequence with a label from a distinct set of 5 ‘fake’ labels.",4.1 Reordering artificial sequences,[0],[0]
"We sample 25000 training and 1000 validation sequences.
",4.1 Reordering artificial sequences,[0],[0]
Model.,4.1 Reordering artificial sequences,[0],[0]
"We use the BiRNN + GCN model, i.e. a bidirectional GRU with a 1-layer GCN on top.",4.1 Reordering artificial sequences,[0],[0]
"We use 32, 64 and 128 units for embeddings, GRU units and GCN layers, respectively.
Results.",4.1 Reordering artificial sequences,[0],[0]
"After 6 epochs of training, the model learns to put permuted sequences back into order, reaching a validation BLEU of 99.2.",4.1 Reordering artificial sequences,[0],[0]
"Figure 3 shows that the mean values of the bias terms of gates (i.e. b̂) for real and fake edges are far apart, suggesting that the GCN learns to distinguish them.",4.1 Reordering artificial sequences,[0],[0]
"Interestingly, this illustrates why edge-wise gating is beneficial.",4.1 Reordering artificial sequences,[0],[0]
"A gate-less model would not understand which of the two outgoing arcs is fake and which is genuine, because only biases b would then be label-dependent.",4.1 Reordering artificial sequences,[0],[0]
"Consequently, it would only do a mediocre job in reordering.",4.1 Reordering artificial sequences,[0],[0]
"Although using label-specific matrices W would also help, this would not scale to the real scenario (see §2.3).",4.1 Reordering artificial sequences,[0],[0]
Data.,4.2 Machine Translation,[0],[0]
For our experiments we use the En-De and En-Cs News Commentary v11 data from the WMT16 translation task.6,4.2 Machine Translation,[0],[0]
For En-De we also train on the full WMT16 data set.,4.2 Machine Translation,[0],[0]
"As our validation set and test set we use newstest2015 and newstest2016, respectively.
",4.2 Machine Translation,[0],[0]
Pre-processing.,4.2 Machine Translation,[0],[0]
"The English sides of the corpora are tokenized and parsed into dependency
6http://www.statmt.org/wmt16/translation-task.html
trees by SyntaxNet,7 using the pre-trained Parsey McParseface model.8 The Czech and German sides are tokenized using the Moses tokenizer.9 Sentence pairs where either side is longer than 50 words are filtered out after tokenization.
",4.2 Machine Translation,[0],[0]
Vocabularies.,4.2 Machine Translation,[0],[0]
"For the English sides, we construct vocabularies from all words except those with a training set frequency smaller than three.",4.2 Machine Translation,[0],[0]
"For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b).",4.2 Machine Translation,[0],[0]
"Given the size of our data set, and following Wu et al. (2016), we use 8000 BPE merges to obtain robust frequencies for our subword units (16000 merges for full data experiment).",4.2 Machine Translation,[0],[0]
"Data set statistics are summarized in Table 1 and vocabulary sizes in Table 2.
Hyperparameters.",4.2 Machine Translation,[0],[0]
"We use 256 units for word embeddings, 512 units for GRUs (800 for En-De full data set experiment), and 512 units for convolutional layers (or equivalently, 512 ‘channels’).",4.2 Machine Translation,[0],[0]
"The dimensionality of the GCN layers is equiva-
7https://github.com/tensorflow/models/tree/master/syntaxnet 8The used dependency parses can be reproduced by using
the syntaxnet/demo.sh shell script.",4.2 Machine Translation,[0],[0]
"9https://github.com/moses-smt/mosesdecoder
lent to the dimensionality of their input.",4.2 Machine Translation,[0],[0]
"We report results for 2-layer GCNs, as we find them most effective (see ablation studies below).
",4.2 Machine Translation,[0],[0]
Baselines.,4.2 Machine Translation,[0],[0]
"We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN.",4.2 Machine Translation,[0],[0]
"See §2.1.1 for details.
",4.2 Machine Translation,[0],[0]
Evaluation.,4.2 Machine Translation,[0],[0]
"We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10",4.2 Machine Translation,[0],[0]
English-German.,4.2.1 Results,[0],[0]
Table 3 shows test results on English-German.,4.2.1 Results,[0],[0]
"Unsurprisingly, the bag-ofwords baseline performs the worst.",4.2.1 Results,[0],[0]
"We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens.",4.2.1 Results,[0],[0]
"The CNN baseline reaches a higher BLEU4 score than the BoW models, but interestingly its BLEU1 score is lower than the BoW+GCN model.",4.2.1 Results,[0],[0]
"The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for BLEU1 and BLEU4, respectively.",4.2.1 Results,[0],[0]
"The BiRNN, the strongest baseline, reaches a BLEU4 of 14.9.",4.2.1 Results,[0],[0]
"Interestingly, GCNs still manage to improve the result by +2.3 BLEU1 and +1.2 BLEU4 points.",4.2.1 Results,[0],[0]
"Finally, we observe a big jump in BLEU4 by using the full data set and beam search (beam 12).",4.2.1 Results,[0],[0]
"The BiRNN now reaches 23.3, while adding a GCN achieves a score of 23.9.
",4.2.1 Results,[0],[0]
English-Czech.,4.2.1 Results,[0],[0]
Table 4 shows test results on English-Czech.,4.2.1 Results,[0],[0]
"While it is difficult to obtain high absolute BLEU scores on this dataset, we can still see similar relative improvements.",4.2.1 Results,[0],[0]
"Again the BoW baseline scores worst, with the BoW+GCN easily beating that result.",4.2.1 Results,[0],[0]
"The CNN baseline scores BLEU4 of 8.1, but the CNN+GCN improves on that, this time by +1.0 and +0.6 for BLEU1 and BLEU4, respectively.",4.2.1 Results,[0],[0]
"Interestingly, BLEU1 scores for the BoW+GCN and CNN+GCN models are
10See Stanojević and Simaan (2015).",4.2.1 Results,[0],[0]
"TER (Snover et al., 2006) and BEER (Stanojević and Sima’an, 2014) metrics, even though omitted due to space considerations, are consistent with the reported results.
higher than both baselines so far.",4.2.1 Results,[0],[0]
"Finally, the BiRNN baseline scores a BLEU4 of 8.9, but it is again beaten by the BiRNN+GCN model with +1.9 BLEU1 and +0.7 BLEU4.
",4.2.1 Results,[0],[0]
Effect of GCN layers.,4.2.1 Results,[0],[0]
How many GCN layers do we need?,4.2.1 Results,[0],[0]
Every layer gives us an extra hop in the graph and expands the syntactic neighborhood of a word.,4.2.1 Results,[0],[0]
Table 5 shows validation BLEU performance as a function of the number of GCN layers.,4.2.1 Results,[0],[0]
"For English-German, using a 1-layer GCN improves BLEU-1, but surprisingly has little effect on BLEU4.",4.2.1 Results,[0],[0]
"Adding an additional layer gives improvements on both BLEU1 and BLEU4 of +1.3 and +0.73, respectively.",4.2.1 Results,[0],[0]
"For English-Czech, performance increases with each added GCN layer.
",4.2.1 Results,[0],[0]
Effect of sentence length.,4.2.1 Results,[0],[0]
We hypothesize that GCNs should be more beneficial for longer sentences: these are likely to contain long-distance syntactic dependencies which may not be adequately captured by RNNs but directly encoded in GCNs.,4.2.1 Results,[0],[0]
"To test this, we partition the validation data into five buckets and calculate BLEU for each of them.",4.2.1 Results,[0],[0]
Figure 4 shows that GCN-based models outperform their respective baselines rather uniformly across all buckets.,4.2.1 Results,[0],[0]
This is a surprising result.,4.2.1 Results,[0],[0]
"One explanation may be that syntactic parses are noisier for longer sentences, and this prevents us from obtaining extra improvements with GCNs.
",4.2.1 Results,[0],[0]
Discussion.,4.2.1 Results,[0],[0]
Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by BLEU4 (as well as TER and BEER).,4.2.1 Results,[0],[0]
"Consistent gains in terms of Kendall tau and BLEU1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax.",4.2.1 Results,[0],[0]
"We review various accounts to syntax in NMT as well as other convolutional encoders.
",5 Related Work,[0],[0]
Syntactic features and/or constraints.,5 Related Work,[0],[0]
"Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings.",5 Related Work,[0],[0]
Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree.,5 Related Work,[0],[0]
"In the decoder, word and node representations compete under the same attention mechanism.",5 Related Work,[0],[0]
"Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT.
Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments.",5 Related Work,[0],[0]
"Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees.
",5 Related Work,[0],[0]
Multi-task Learning.,5 Related Work,[0],[0]
Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations.,5 Related Work,[0],[0]
Luong et al. (2015a) predict linearized constituency parses as an additional task.,5 Related Work,[0],[0]
"Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side.",5 Related Work,[0],[0]
Nadejde et al. (2017) multi,5 Related Work,[0],[0]
"-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags.
",5 Related Work,[0],[0]
Latent structure.,5 Related Work,[0],[0]
Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer.,5 Related Work,[0],[0]
They encode source words as a learned average of potential parents emulating a relaxed dependency tree.,5 Related Work,[0],[0]
"While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improvements on English-Japanese.",5 Related Work,[0],[0]
"Yogatama et al. (2016) introduce a model for language understanding and generation that composes words into sentences by inducing unlabeled binary bracketing trees.
",5 Related Work,[0],[0]
Convolutional encoders.,5 Related Work,[0],[0]
Gehring et al. (2016) show that CNNs can be competitive to BiRNNs when used as encoders.,5 Related Work,[0],[0]
To increase the receptive field of a word’s context they stack multiple CNN layers.,5 Related Work,[0],[0]
Kalchbrenner et al. (2016) use convolution in both the encoder and the decoder; they make use of dilation to increase the receptive field.,5 Related Work,[0],[0]
"In contrast to both approaches, we use a GCN informed by dependency structure to increase it.",5 Related Work,[0],[0]
"Finally, Cho et al. (2014a) propose a recursive convolutional neural network which builds a tree out of the word leaf nodes, but which ends up compressing the source sentence in a single vector.",5 Related Work,[0],[0]
We have presented a simple and effective approach to integrating syntax into neural machine translation models and have shown consistent BLEU4 improvements for two challenging language pairs: English-German and English-Czech.,6 Conclusions,[0],[0]
"Since GCNs are capable of encoding any kind of graph-based structure, in future work we would like to go be-
yond syntax, by using semantic annotations such as SRL and AMR, and co-reference chains.",6 Conclusions,[0],[0]
We would like to thank Michael Schlichtkrull and Thomas Kipf for their suggestions and comments.,Acknowledgments,[0],[0]
"This work was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518, NWO VICI 277-89-002).",Acknowledgments,[0],[0]
We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation.,abstractText,[0],[0]
"We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data.",abstractText,[0],[0]
Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods.,abstractText,[0],[0]
"GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks).",abstractText,[0],[0]
We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.,abstractText,[0],[0]
Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 772–776, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
The goal of relation extraction is to extract tuples of a particular relation from a corpus of natural language text.,1 Introduction,[0],[0]
"A widely employed approach to relation extraction is based on iterative bootstrapping (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006; Pantel and Pennacchiotti, 2006), which can be applied with only small amounts of supervision and which scales well to very large datasets.
",1 Introduction,[0],[0]
"A well-known problem with iterative bootstrapping is a phenomenon known as semantic drift (Curran et al., 2007): as bootstrapping proceeds it is likely that unreliable patterns will lead to false extractions.",1 Introduction,[0],[0]
"These extraction errors are amplified in the following iterations and the extracted relation will drift away
from the intended target.",1 Introduction,[0],[0]
Semantic drift often results in low precision extractions and therefore poses a major limitation of iterative bootstrapping algorithms.,1 Introduction,[0],[0]
"Previous work on iterative bootstrapping has addressed the issue of reducing semantic drift for example by bagging the results of various runs employing differing seed tuples, constructing filters which identify false tuples or patterns and adding further constraints to the bootstrapping process (T. McIntosh, 2010; McIntosh and Curran, 2009; Curran et al., 2007).
",1 Introduction,[0],[0]
"However, the analysis of Komachi et al. (2008) has shown that semantic drift is an inherent property of iterative bootstrapping algorithms and therefore poses a fundamental problem.",1 Introduction,[0],[0]
"They have shown that iterative bootstrapping without pruning corresponds to an eigenvector computation and thus as the number of iterations increases the resulting ranking will always converge towards the same static ranking of tuples, regardless of the particular choice of seed instances.
",1 Introduction,[0],[0]
"In this paper, we describe an alternative method, that is not susceptible to semantic drift.",1 Introduction,[0],[0]
"We represent our data as a bipartite graph, whose vertices correspond to patterns and tuples respectively and whose edges capture cooccurrences and then measure the distance of a tuple to the seed set in terms of random walk hitting times.",1 Introduction,[0],[0]
"Experimental results confirm that semantic drift is avoided by our method and show that substantial improvements over iterative forms of bootstrapping are possible.
772",1 Introduction,[0],[0]
"From a given corpus, we extract a dataset consisting of tuples and patterns.",2 Scoring with Hitting Times,[0],[0]
"Tuples are pairs of co-occurring strings in the corpus, such as (Bill Gates, Microsoft), which potentially belong to a particular relation of interest.",2 Scoring with Hitting Times,[0],[0]
"In our case, patterns are simply the sequence of tokens occurring between tuple elements, e.g. “is the founder of”.",2 Scoring with Hitting Times,[0],[0]
"We represent all the tuple types1 X and all the extraction pattern types Y contained in a given corpus through an undirected, weighted, bipartite graph G = (V,E) with vertices V = X ∪ Y and edges E ⊂",2 Scoring with Hitting Times,[0],[0]
X,2 Scoring with Hitting Times,[0],[0]
"× Y , where an edge (x, y) ∈ E indicates that tuple x occurrs with pattern y somewhere in the corpus.",2 Scoring with Hitting Times,[0],[0]
"Edge weights are defined through a weight matrix W which holds the weight Wi,j = w(vi, vj) for edges (vi, vj) ∈ E. Specifically, we use the count of how many times a tuple occurs with a pattern in the corpus and weights for unconnected vertices are zero.
",2 Scoring with Hitting Times,[0],[0]
"Our goal is to compute a score vector σ holding a score σi = σ(xi) for each tuple xi ∈ X, which quantifies how well the tuple matches the seed tuples.",2 Scoring with Hitting Times,[0],[0]
"Higher scores indicate that the tuple is more likely to belong to the relation defined through the seeds and thus the score vector effectively provides a ranking of the tuples.
",2 Scoring with Hitting Times,[0],[0]
We define scores of tuples based on their distance2 to the seed tuples in the graph.,2 Scoring with Hitting Times,[0],[0]
"The distance of some tuple x to the seed set S can be naturally formalized in terms of the average time it takes until a random walk starting in S reaches x, the hitting time.",2 Scoring with Hitting Times,[0],[0]
The random walk is defined through the probability distribution over start vertices and through a matrix of transition probabilities.,2 Scoring with Hitting Times,[0],[0]
"Edge weights are constrained to be non-negative, which allows us to define the transition matrix P with Pi,j = p(vj |vi) = 1dviw(vi, vj), where dv =∑
vk∈V w(v, vk) is the degree of a vertex v ∈ V .",2 Scoring with Hitting Times,[0],[0]
The distance of two vertices is measured in terms of the average time of a random walk be1Note that we are using tuple and pattern types rather than particular mentions in the corpus. 2The term is used informally.,2 Scoring with Hitting Times,[0],[0]
"In particular, hitting times are not a distance metric, since they can be asymmetric.
",2 Scoring with Hitting Times,[0],[0]
tween the two.,2 Scoring with Hitting Times,[0],[0]
"Specifically, we adopt the notion of T-truncated hitting time (Sarkar and Moore, 2007) defined as the expected number of steps it takes until a random walk of at most T steps starting at vi reaches vj for the first time:
hT",2 Scoring with Hitting Times,[0],[0]
(vj |vi) = { 0 iff.,2 Scoring with Hitting Times,[0],[0]
"vj = vi or T=0 1 + ∑ vk∈V p(vk|vi)h T−1(vj |vk)
",2 Scoring with Hitting Times,[0],[0]
The truncated hitting time hT,2 Scoring with Hitting Times,[0],[0]
"(vj |vi) can be approximately computed by sampling M independent random walks starting at vi of length T and computing
ĥT",2 Scoring with Hitting Times,[0],[0]
(vj |vi),2 Scoring with Hitting Times,[0],[0]
"= 1
M m∑ k=1 tk",2 Scoring with Hitting Times,[0],[0]
"+ (1− m M )T (1)
",2 Scoring with Hitting Times,[0],[0]
where {t1 . . .,2 Scoring with Hitting Times,[0],[0]
"tm} are the sampled first-hit times of random walks which reach vj within T steps (Sarkar et al., 2008).
",2 Scoring with Hitting Times,[0],[0]
The score σHT (v) of a vertex v /∈,2 Scoring with Hitting Times,[0],[0]
"S to the seed set S is then defined as the inverse of the average T -truncated hitting time of random walks starting at a randomly chosen vertex s ∈ S:
1 σHT (v) = hT",2 Scoring with Hitting Times,[0],[0]
(v|S) = 1 |S| ∑ s∈S hT (v|s) (2),2 Scoring with Hitting Times,[0],[0]
"We extracted tuples and patterns from the fifth edition of the Gigaword corpus (Parker et al., 2011), by running a named entity tagger and extracting all pairs of named entities and extracting occurring within the same sentence which do not have another named entity standing between them.",3 Experiments,[0],[0]
"Gold standard seed and test tuples for a set of relations were obtained from YAGO (Suchanek et al., 2007).",3 Experiments,[0],[0]
"Specifically, we took all relations for which there are at least 300 tuples, each of which occurs at least once in the corpus.",3 Experiments,[0],[0]
"This resulted in the set of relations shown in Table 1, plus the development relation hasWonPrize.
",3 Experiments,[0],[0]
"For evaluation, we use the percentile rank of the median test set element (PRM, see Francois et al. 2007), which reflects the quality of the
full produced ranking, not just the top N elements and is furthermore computable with only a small set of labeled test tuples 3.
",3 Experiments,[0],[0]
We compare our proposed method based on hitting times (HT) with two variants of iterative bootstrapping.,3 Experiments,[0],[0]
The first one (IB1) does not employ pruning and corresponds to the algorithm described in Komachi et al. (2008).,3 Experiments,[0],[0]
The second one (IB2) corresponds to a standard bootstrapping algorithm which employs pruning after each step in order to reduce semantic drift.,3 Experiments,[0],[0]
"Specifically, scores are pruned after projecting from X onto Y and from Y onto X, retaining only the top N (t) =",3 Experiments,[0],[0]
N0t scores at iteration t and setting all other scores to zero.,3 Experiments,[0],[0]
The experiments in this section were conducted on the held out development relation hasWonPrize.,3.1 Parametrizations,[0],[0]
"The ranking produced by both forms of iterative bootstrapping IB1 and IB2 depend on the number of iterations, as shown in Figure 1.",3.1 Parametrizations,[0],[0]
IB1 achieves an optimal ranking after just one iteration and thereafter scores get worse due to semantic drift.,3.1 Parametrizations,[0],[0]
"In contrast, pruning helps avoid semantic drift for IB2, which attains an optimal score after 2 iterations and achieves relatively constant scores for several iterations.",3.1 Parametrizations,[0],[0]
"However, during iteration 9 an incorrect pattern is kept and this at once leads to a drastic loss in accuracy, showing that semantic drift is only deferred and not completely eliminated.
",3.1 Parametrizations,[0],[0]
"Our method HT has parameter T , corresponding to the truncation time, i.e., maximal number of steps of a random walk.",3.1 Parametrizations,[0],[0]
Figure 2 shows the PRM of our method for different values of T .,3.1 Parametrizations,[0],[0]
"Performance gets better as T increases and is optimal for T = 12, whereas for larger values, the performance gets slightly worse again.",3.1 Parametrizations,[0],[0]
"The figure shows that, if T is large enough (> 5), the PRM is relatively constant and there is no phenomenon comparable to semantic drift, which causes instability in the produced rankings.
",3.1 Parametrizations,[0],[0]
3other common metrics do not satisfy these conditions.,3.1 Parametrizations,[0],[0]
"To evaluate the methods, firstly the parameters for each method were set to the optimal values as determined in the previous section.",3.2 Method Comparison,[0],[0]
"For the experiments here, we again use 200 randomly chosen tuples as the seeds for each relation.",3.2 Method Comparison,[0],[0]
"All the remaining gold standard tuples are used for testing.
",3.2 Method Comparison,[0],[0]
Table 1 shows the PRM for the three methods.,3.2 Method Comparison,[0],[0]
"For a majority of the relations (12/16) HT attains the best, i.e. lowest, PRM, which confirms that hitting times constitute an accurate way of measuring the distance of tuples to the seed set.",3.2 Method Comparison,[0],[0]
IB1 and IB2 each perform best on 2/16 of the relations.,3.2 Method Comparison,[0],[0]
"A sign test on these results yields that
HT is better than both IB1 and IB2 at significance level α < 0.01.
",3.2 Method Comparison,[0],[0]
"Moreover, the ranking produced by HT is stable and not affected by semantic drift, given that even where results are worse than for IB1 or IB2, they are still close to the best performing method.",3.2 Method Comparison,[0],[0]
"In contrast, when semantic drift occurs, the performance of IB1 and IB2 can deteriorate drastically, e.g. for the worksAt relation, where both IB1 and IB2 produce rankings that are a lot worse than the one produced by HT.",3.2 Method Comparison,[0],[0]
Figure 3 shows the PRM for each of the three methods as a function of the size of the seed set for the relation created.,3.3 Sensitivity to Seed Set Size,[0],[0]
"For small seed sets, the performance of the iterative methods can be increased by adding more seeds.",3.3 Sensitivity to Seed Set Size,[0],[0]
"However, from a seed set size of 50 onwards, performance remains relatively constant.",3.3 Sensitivity to Seed Set Size,[0],[0]
"In other words, iterative bootstrapping is not benefitting from the information provided by the additional labeled data, and thus has a poor learning performance.",3.3 Sensitivity to Seed Set Size,[0],[0]
"In contrast, for our method based on hitting times, the performance continually improves as the seed set size is increased.",3.3 Sensitivity to Seed Set Size,[0],[0]
"Thus, also in terms of learning performance, our method is more sound than iterative bootstrapping.",3.3 Sensitivity to Seed Set Size,[0],[0]
The paper has presented a graph-based method for seed set expansion which is not susceptible to semantic drift and on most relations outperforms iterative bootstrapping.,4 Conclusions,[0],[0]
The method measures distance between vertices through random walk hitting times.,4 Conclusions,[0],[0]
"One property which makes hitting times an appropriate distance measure is their ability to reflect the overall connectivity structure of the graph, in contrast to measures such as the shortest path between two vertices.",4 Conclusions,[0],[0]
"The hitting time will decrease when the number of paths from the start vertex to the target vertex increases, when the length of paths decreases or when the likelihood (weights) of paths increases.",4 Conclusions,[0],[0]
"These properties are particularly important when the observed graph edges must be assumed to be merely a sample of all plausible edges, possibly perturbated by noise.",4 Conclusions,[0],[0]
"This has also been asserted by previous work, which has shown that hitting times successfully capture the notion of similarity for other natural language processing problems such as learning paraphrases (Kok and Brockett, 2010) and related problems such as query suggestion (Mei et al., 2008).",4 Conclusions,[0],[0]
Future work will be aimed towards employing our hitting time based method in combination with a richer feature set.,4 Conclusions,[0],[0]
"Iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision.",abstractText,[0],[0]
"Unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions.",abstractText,[0],[0]
"This paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph.",abstractText,[0],[0]
"In contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, and it empirically results in better extractions than iterative methods.",abstractText,[0],[0]
Graph-Based Seed Set Expansion for Relation Extraction Using Random Walk Hitting Times,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1537–1546 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Metaphor is pervasive in our everyday communication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system (Lakoff and Johnson, 1980).",1 Introduction,[0],[0]
"In the most influential account of metaphor to date, Lakoff and Johnson explain the phenomenon through the presence of systematic metaphorical associations between two distinct concepts or domains.",1 Introduction,[0],[0]
"For instance, when we talk about “curing juvenile delinquency” or “corruption transmitting through the government ranks”, we view the general concept of crime (the target concept) in terms of the properties of a disease (the source concept).",1 Introduction,[0],[0]
"Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process.
",1 Introduction,[0],[0]
"Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).",1 Introduction,[0],[0]
"A number of approaches to metaphor processing have thus been proposed, focusing pre-
dominantly on classifying linguistic expressions as literal or metaphorical.",1 Introduction,[0],[0]
"They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014).",1 Introduction,[0],[0]
"While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them.",1 Introduction,[0],[0]
"In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016).",1 Introduction,[0],[0]
"Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data.
",1 Introduction,[0],[0]
We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition.,1 Introduction,[0],[0]
"Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.",1 Introduction,[0],[0]
"In this paper, we present a novel architecture which (1) models the interaction between the source and target domains in the metaphor via a gating function; (2) specialises word representations for the metaphor identification task via supervised training; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity.",1 Introduction,[0],[0]
"We experimented with two types of word representations
1537
as inputs to the network: the standard skip-gram word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof.
",1 Introduction,[0],[0]
"We evaluate our method in the metaphor identification task, focusing on adjective–noun, verb– subject and verb–direct object constructions where the verbs and adjectives can be used metaphorically.",1 Introduction,[0],[0]
Our results show that our architecture outperforms both a metaphor agnostic deep learning baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification.,1 Introduction,[0],[0]
"We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existing systems based on hand-coded lexical knowledge.",1 Introduction,[0],[0]
The majority of approaches to metaphor processing cast the problem as classification of linguistic expressions as metaphorical or literal.,2 Related Work,[0],[0]
Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial discourse.,2 Related Work,[0],[0]
"They used the maximum entropy classifier and the verbs’ nominal arguments and their FrameNet roles (Fillmore et al., 2003) as features, reporting encouraging results.",2 Related Work,[0],[0]
"Dunn (2013) used a logistic regression classifier and high-level properties of concepts extracted from SUMO ontology, including domain types (ABSTRACT, PHYSICAL, SOCIAL, MENTAL) and event status (PROCESS, STATE, OBJECT).",2 Related Work,[0],[0]
"Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses.",2 Related Work,[0],[0]
They have shown that the model learned with such coarse semantic features is portable across languages.,2 Related Work,[0],[0]
The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features.,2 Related Work,[0],[0]
"They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of sentence trees.",2 Related Work,[0],[0]
Mohler et al. (2013) aimed at modelling conceptual information.,2 Related Work,[0],[0]
They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets.,2 Related Work,[0],[0]
"The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures
of the known ones.
",2 Related Work,[0],[0]
"With the aim of reducing the dependence on manually-annotated lexical resources, other research focused on modelling metaphor using corpus-driven information alone.",2 Related Work,[0],[0]
Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora.,2 Related Work,[0],[0]
"For example, the feature vector for politics would contain GAME or MECHANISM terms among the frequent features.",2 Related Work,[0],[0]
"As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain.",2 Related Work,[0],[0]
Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples.,2 Related Work,[0],[0]
Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way.,2 Related Work,[0],[0]
"Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context.",2 Related Work,[0],[0]
Gutiérrez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework.,2 Related Work,[0],[0]
"Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional ”single-sense” compositional distributional model.",2 Related Work,[0],[0]
"They then used these representations in the metaphor identification task, achieving promising results.
",2 Related Work,[0],[0]
"The more recent approaches of Shutova et al. (2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features.",2 Related Work,[0],[0]
Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features.,2 Related Work,[0],[0]
"They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively.",2 Related Work,[0],[0]
"They then measured the difference between the phrase representation and those of its component words in terms of their cosine similarity, which served as a predictor of metaphoricity.",2 Related Work,[0],[0]
"They found basic cosine similarity between the component words in the phrase to be a powerful measure – the neural embeddings of the words were compared with cosine similar-
ity and a threshold was tuned on the development set to distinguish between literal and metaphorical phrases.",2 Related Work,[0],[0]
"This approach was their best performing linguistic model, outperformed only by a multimodal system which included both linguistic and visual features.
",2 Related Work,[0],[0]
"Bulat et al. (2017) presented a metaphor identification method that uses representations constructed from human property norms (McRae et al., 2005).",2 Related Work,[0],[0]
"They first learn a mapping from the skip-gram embedding vector space to the property norm space using linear regression, which allows them to generate property norm representations for unseen words.",2 Related Work,[0],[0]
The authors then train an SVM classifier to detect metaphors using these representations as input.,2 Related Work,[0],[0]
Bulat et al. (2017) have shown that the cognitively-driven property norms outperform standard skip-gram representations in this task.,2 Related Work,[0],[0]
"Our method is inspired by the findings of Shutova et al. (2016), who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity.",3 Supervised Similarity Network,[0],[0]
"For example, the phrase ‘colourful personality’ receives a score:
s = cos(xc, xp) (1)
where xc is the embedding for colourful and xp is the embedding for personality.",3 Supervised Similarity Network,[0],[0]
"The combined phrase is classified as being metaphorical based on a threshold, which is optimised on a development dataset.",3 Supervised Similarity Network,[0],[0]
"In this paper, we propose several extensions to this general idea, creating a supervised version of the cosine similarity metric which can be optimised on training data to be more suitable for metaphor detection.",3 Supervised Similarity Network,[0],[0]
Directly comparing the vector representations of both words treats each of the embeddings as an independent unit.,3.1 Word Representation Gating,[0],[0]
"In reality, however, word meanings vary and adapt based on the context.",3.1 Word Representation Gating,[0],[0]
"In case of metaphorical language (e.g. “cure crime”), the source domain properties of the verb (e.g. cure) are projected onto the target domain noun (e.g. crime), resulting in the interaction of the two domains in the interpretation of the metaphor.
",3.1 Word Representation Gating,[0],[0]
"In order to integrate this idea into the metaphor detection method, we can construct a gating function that modulates the representation of one word based on the other.",3.1 Word Representation Gating,[0],[0]
"Given embeddings x1 and x2, the gating values are predicted as a non-linear transformation of x1 and applied to x2 through element-wise multiplication:
g = σ(Wgx1) (2)
x̃2",3.1 Word Representation Gating,[0],[0]
"= x2 g (3)
",3.1 Word Representation Gating,[0],[0]
"whereWg is a weight matrix that is optimised during training, σ is the sigmoid activation function, and represents element-wise multiplication.",3.1 Word Representation Gating,[0],[0]
"In an adjective-noun phrase, this architecture allows the network to first look at the adjective, then use its meaning to change the representation of the noun.",3.1 Word Representation Gating,[0],[0]
"The sigmoid activation function makes it act as a filter, choosing which information from the original embedding gets through to the rest of the network.",3.1 Word Representation Gating,[0],[0]
"While learning a more complex gating function could be beneficial for very large training resources, the filtering approach is more suitable for the annotated metaphor datasets which are relatively small in size.",3.1 Word Representation Gating,[0],[0]
"As the next step, we implement position-specific mappings for the word embeddings.",3.2 Vector Space Mapping,[0],[0]
"The original method uses word embeddings that have been pretrained using the distributional skip-gram objective (Mikolov et al., 2013a).",3.2 Vector Space Mapping,[0],[0]
"While this tunes the vectors for predicting context words, there is no reason to believe that the same space is also optimal for the task of metaphor detection.",3.2 Vector Space Mapping,[0],[0]
"In order to address this shortcoming, we allow the model to learn a mapping from the skip-gram vector space to a new metaphor-specific vector space:
z1 = tanh(Wz1x1) (4)
z2 = tanh(Wz2 x̃2) (5)
where Wz1 and Wz2 are weight matrices, z1 and z2 are the new position-specific word representations.",3.2 Vector Space Mapping,[0],[0]
"While the original embeddings x1 and x2 are pre-trained on a large unannotated corpus, the transformation process is optimised using annotated metaphor examples, resulting in word representations that are more suitable for this task.",3.2 Vector Space Mapping,[0],[0]
"Furthermore, the adjectives and nouns use separate mapping weights, which allows the model to better distinguish between the different functionalities of these words.",3.2 Vector Space Mapping,[0],[0]
"In contrast, the original cosine similarity is not position-specific and would give the same result regardless of the word order.",3.2 Vector Space Mapping,[0],[0]
"If the vectors x1 and x2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multiplication followed by a sum over all elements:
cos(x1, x2) ∝",3.3 Weighted Cosine,[0],[0]
"∑
i
x1,ix2,i (6)
",3.3 Weighted Cosine,[0],[0]
This calculation of cosine similarity can be formulated as a small neural network where the two unit-normalised input vectors are directly multiplied together.,3.3 Weighted Cosine,[0],[0]
"This is followed by a single output neuron, with all the intermediate weights set to value 1.",3.3 Weighted Cosine,[0],[0]
"Such a network would calculate the same sum over the element-wise multiplication, outputting the value of cosine similarity.
",3.3 Weighted Cosine,[0],[0]
"Since there is no reason to assume that all the embedding dimensions are equally important when detecting metaphors, we can explore other strategies for weighting the similarity calculation.
",3.3 Weighted Cosine,[0],[0]
Rei and Briscoe (2014) used a fixed formula to calculate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations.,3.3 Weighted Cosine,[0],[0]
We extend this even further and allow the network to use multiple different weighting strategies which are all optimised during training.,3.3 Weighted Cosine,[0],[0]
"This is done by first creating a vector m, which is an element-wise multiplication of the two word representations:
mi = z1,iz2,i (7)
where mi is the i-th element of vector m and z1,i is the i-th element of vector z1.",3.3 Weighted Cosine,[0],[0]
"After that, the resulting vector is used as input for a hidden neural layer:
d = γ(Wdm) (8)
whereWd is a weight matrix and γ is an activation function.",3.3 Weighted Cosine,[0],[0]
"If the length of d is 1, all the weights in Wd have value 1, and γ is a linear activation, then this formula is equivalent to a regular cosine similarity.",3.3 Weighted Cosine,[0],[0]
"However, we use a larger length for d to capture more features, use tanh as the activation function, and optimise the weights of Wd during training, giving the framework more flexibility to customise the model for the task of metaphor detection.",3.3 Weighted Cosine,[0],[0]
"Based on vector d we can output a prediction for the word pair, showing whether it is literal or metaphorical:
y = σ(Wyd) (9)
where Wy is a weight matrix, σ is the logistic activation function, and y is a real-valued prediction with values between 0 and 1.
",3.4 Prediction and Optimisation,[0],[0]
"We optimise the model based on an annotated training dataset, while minimising the following hinge loss function:
E = ∑
k
qk (10)
qk = { (ỹ − y)2 if |ỹ − y| > 0.4 0, otherwise
(11)
where y is the predicted value, ỹ is the true label, and k iterates over all training examples.",3.4 Prediction and Optimisation,[0],[0]
Equation 11 optimises the model to minimise the squared error between the predicted and true labels.,3.4 Prediction and Optimisation,[0],[0]
"However, this is only done for training examples where the predicted error is not already close enough to the desired result.",3.4 Prediction and Optimisation,[0],[0]
The condition |ỹ,3.4 Prediction and Optimisation,[0],[0]
− y| > 0.4 only updates training examples where the difference from the true label is greater than 0.4.,3.4 Prediction and Optimisation,[0],[0]
The true labels ỹ can only take values 0,3.4 Prediction and Optimisation,[0],[0]
"(literal) or 1 (metaphorical), and the threshold 0.4 is chosen so that datapoints that are on the correct side of the decision boundary by more than 0.1 would be ignored, which helps reduce overfitting and allows the model to focus on the misclassified examples.
",3.4 Prediction and Optimisation,[0],[0]
The diagram of the complete network can be seen in Figure 1.,3.4 Prediction and Optimisation,[0],[0]
"Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations.
",4 Word Representations,[0],[0]
"The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.
",4 Word Representations,[0],[0]
"We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015).",4 Word Representations,[0],[0]
These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space.,4 Word Representations,[0],[0]
"We evaluate our method using two datasets of phrases manually annotated for metaphoricity.
",5 Datasets,[0],[0]
"Since these datasets include examples for different senses (both metaphorical and literal) of the same verbs or adjectives, they allow us to test the extent to which our model is able to discriminate between different word senses, as opposed to merely selecting the most frequent class for a given word.
",5 Datasets,[0],[0]
"Mohammad et al. dataset (MOH) Mohammad et al. (2016) used WordNet to find verbs that had between three and ten senses and extracted the sentences exemplifying them in the corresponding glosses, yielding a total of 1639 verb uses in sentences.",5 Datasets,[0],[0]
Each of these was annotated for metaphoricity by 10 annotators via the crowdsourcing platform CrowdFlower1.,5 Datasets,[0],[0]
Mohammad et al. selected the verbs that were tagged by at least 70% of the annotators as metaphorical or literal to create their dataset.,5 Datasets,[0],[0]
"We extracted verb–direct object and verb–subject relations of the annotated verbs from this dataset, discarding the instances with pronominal or clausal subject or object.",5 Datasets,[0],[0]
This resulted in a dataset of 647 verb–noun pairs (316 metaphorical and 331 literal).,5 Datasets,[0],[0]
"Some examples of annotated verb phrases from MOH are presented in Table 1.
",5 Datasets,[0],[0]
Tsvetkov et al. dataset (TSV) Tsvetkov et al. (2014) construct a dataset of adjective–noun pairs annotated for metaphoricity.,5 Datasets,[0],[0]
This is divided into a training set consisting of 884 literal and 884 metaphorical pairs (TSV-TRAIN) and a test set containing 100 literal and 100 metaphorical pairs (TSV-TEST).,5 Datasets,[0],[0]
Table 2 shows a portion of annotated adjective-noun phrases from TSV-TEST.,5 Datasets,[0],[0]
"TSV-TRAIN was collected from publicly available metaphor collections on the web and manually
1www.crowdflower.com
curated by removing duplicates and metaphorical phrases that depend on wider context for their interpretation (e.g. drowning students).",5 Datasets,[0],[0]
TSVTEST was constructed by extracting nouns that co-occur with a list of 1000 frequent adjectives in the TenTen Web Corpus2 using SketchEngine.,5 Datasets,[0],[0]
The selected adjective-noun pairs were annotated for metaphoricity by 5 annotators with an interannotator agreement of κ = 0.76.,5 Datasets,[0],[0]
"Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.",5 Datasets,[0],[0]
We randomly separated 200 (out of the 1536) examples from the training set to use for development experiments.,5 Datasets,[0],[0]
"The word representations in our model were initialised with either the 100-dimensional skip-gram embeddings or the 2,526-dimensional attribute vectors (Section 4).",6 Experiments and Results,[0],[0]
"These were kept fixed and not updated, which reduces overfitting on the available training examples.",6 Experiments and Results,[0],[0]
"For both word representations we use the same embeddings as Bulat et al. (2017), which makes the results directly comparable and shows that the improvements are coming from the novel architecture and are not due to a different embedding initialisation.
",6 Experiments and Results,[0],[0]
"The network was optimised using AdaDelta (Zeiler, 2012) for controlling adaptive learning rates.",6 Experiments and Results,[0],[0]
The models were evaluated after each full pass over the training data and training was stopped if the F-score on the development set had not improved for 5 epochs.,6 Experiments and Results,[0],[0]
"The transformed embeddings z1 and z2 were set to size 300, layer d was set to size 50.",6 Experiments and Results,[0],[0]
The values for these hyperparameters were chosen experimentally using the development dataset.,6 Experiments and Results,[0],[0]
"In order to avoid drawing conclusions based on outlier results due to random initialisations, we ran each experiment 25 times with random seeds and present the averaged results in this paper.",6 Experiments and Results,[0],[0]
"We implemented the framework using Theano (Al-Rfou et al., 2016) and are making the source code publicly available.3
Table 3 contains results of different system configurations on the TSV dataset.",6 Experiments and Results,[0],[0]
"The original Fscore by Tsvetkov et al. (2014) is still the highest, as they used a range of highly-engineered features that require manual annotation, such as
2https://www.sketchengine.co.uk/ententen-corpus/ 3http://www.marekrei.com/projects/ssn
the lexical abstractness, imageability scores and the relative number of supersenses for each word in the dataset.",6 Experiments and Results,[0],[0]
"Our setup is more similar to the linguistic experiments by Shutova et al. (2016), where metaphor detection is performed using pretrained word embeddings.",6 Experiments and Results,[0],[0]
They also proposed combining the linguistic model with a system using visual word representations and achieved performance improvements.,6 Experiments and Results,[0],[0]
"Recently, Bulat et al. (2017) compared different types of embeddings and showed that attribute-based representations can outperform regular skip-gram embeddings.
",6 Experiments and Results,[0],[0]
"As an additional baseline, we report the performance on metaphor detection using a basic feedforward network (FFN).",6 Experiments and Results,[0],[0]
"In this configuration, the word embeddings x1 and x2 are directly connected to the hidden layer d, skipping all the intermediate network structure.",6 Experiments and Results,[0],[0]
"The FFN achieves 74.4% F-score on TSV-TEST, showing that even such a simple model can perform relatively well in a supervised setting.",6 Experiments and Results,[0],[0]
"Using attribute vectors instead of skip-gram embeddings gives a slight improvement, especially on the recall metric, which is consistent with the findings by Bulat et al. (2017).
",6 Experiments and Results,[0],[0]
"The architecture described in Section 3, which we refer to as a supervised similarity network (SSN), outperforms the baseline and achieves 80.1% F-score using skip-gram embeddings and 80.6% with attribute-based representations.",6 Experiments and Results,[0],[0]
We also created a fusion of these two models where the predictions from both are combined as a weighted average.,6 Experiments and Results,[0],[0]
"In this setting, the two networks are trained in tandem and a real-valued weight, which is also optimised during training, is
used to combine them together.",6 Experiments and Results,[0],[0]
"This configuration achieves 81.1% F-score, indicating that the the skip-gram embeddings and attribute vectors capture somewhat complementary information.",6 Experiments and Results,[0],[0]
"Excluding the system by Tsvetkov et al. (2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al. (2016) without requiring any visual information.",6 Experiments and Results,[0],[0]
"The attribute-based SSN also improves over Bulat et al. (2017) by 5.6% absolute, using the same word representations as input.
",6 Experiments and Results,[0],[0]
Table 4 contains results of different system architectures on the MOH dataset.,6 Experiments and Results,[0],[0]
"Shutova et al. (2016) reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing.",6 Experiments and Results,[0],[0]
"Since this corpus contains only 647 annotated examples, we instead evaluated the systems using 10-fold cross-validation.",6 Experiments and Results,[0],[0]
"The feedforward baseline with skip-gram embeddings returns an F-score that is close to the linguistic configuration of Shutova et al, whereas the best results are achieved by the similarity network with skip-gram embeddings.",6 Experiments and Results,[0],[0]
"In this setting, the attribute-based representations did not improve performance – this is expected, as the attribute norms by McRae et al. (2005) are designed for nouns, whereas the MOH dataset is centered on verbs.
",6 Experiments and Results,[0],[0]
"Table 5 contains examples from the TSV development set, together with gold annotations and predicted scores.",6 Experiments and Results,[0],[0]
"The system confidently detects literal phrases such as sunny country and meaningless discussion, along with metaphorical phrases such as unforgiving heights and blind hope.",6 Experiments and Results,[0],[0]
"The predicted output disagrees with the annotation on
cases such as humane treatment and rich programmer – some of these examples could also be argued as being metaphorical, depending on the specific sense of the words.",6 Experiments and Results,[0],[0]
"While the system was relatively unsure about the false positives (the scores were close to 0.5), it tended to assign more decisive scores to the false negatives.",6 Experiments and Results,[0],[0]
"Results in Section 6 show that performance on the TSV dataset is higher than the MOH dataset, likely due to the former having more examples available for training.",7 The Effects of Training Data,[0],[0]
"Therefore, we ran an additional experiment to investigate the effect of dataset size on the performance of metaphor detection.",7 The Effects of Training Data,[0],[0]
"Gutiérrez et al. (2016) annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training resource.",7 The Effects of Training Data,[0],[0]
"While it contains only 23 unique adjectives, the total number of phrases reaches 8,592.",7 The Effects of Training Data,[0],[0]
"We remove any phrases that occur in the development or test data of TSV, then incrementally add the remaining examples to the TSV training data and evaluate on the TSV-TEST.
",7 The Effects of Training Data,[0],[0]
"Figure 2 shows a graph of the system performance, when increasing the training data at intervals of 500.",7 The Effects of Training Data,[0],[0]
"There is a very rapid increase in performance until around 2,000 training points, whereas the existing TSV-TRAIN is limited to 1,336 examples.",7 The Effects of Training Data,[0],[0]
Providing even more data to the system gives an additional increase that is more gradual.,7 The Effects of Training Data,[0],[0]
"The final performance of the system us-
ing both datasets is 88.3 F-score, which is the highest result reported on the TSV dataset and translates to 36% relative error reduction with respect to the same system trained only on the original dataset.
",7 The Effects of Training Data,[0],[0]
We report the exact values in Table 6 for the different training sets.,7 The Effects of Training Data,[0],[0]
"The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset.",7 The Effects of Training Data,[0],[0]
"In order to include the data from Gutiérrez et al. (2016), we recreated the attribute vectors for a larger vocabulary, which results in a slightly different baseline performance.",7 The Effects of Training Data,[0],[0]
"The architecture in Section 3 also acts as a semantic composition model, extracting the meaning of the phrase by combining the meanings of its component words.",8 Qualitative analysis,[0],[0]
"Therefore, we performed a qualitative experiment to investigate: (1) how well do traditional compositional methods capture metaphors, without any fine-tuning; and (2) whether the supervised representations still retain their domain-specific semantic information.",8 Qualitative analysis,[0],[0]
"For this purpose, we construct three vector spaces and visualise some examples from the TSV training set,
using t-SNE (Van Der Maaten and Hinton, 2008).",8 Qualitative analysis,[0],[0]
"Figure 3 contains examples for three different composition methods: the additive method simply sums the skip-gram embeddings for both words (top); the multiplicative method multiplies the skip-gram embeddings (middle); the final system uses layer m from the SSN model to represent the
phrases (bottom).",8 Qualitative analysis,[0],[0]
"The visualisation shows that the additive and multiplicative models are both comparable when it comes to semantic clustering of the phrases, but metaphorical examples are mixed together with literal clusters.",8 Qualitative analysis,[0],[0]
The SSN is optimised for metaphor classification and therefore it produces representations with a very clear boundary for metaphoricity.,8 Qualitative analysis,[0],[0]
"Interestingly, the graph also reveals a misannotated example in the dataset, since ‘fiery temper’ should be labeled as a metaphor.",8 Qualitative analysis,[0],[0]
"At the same time, this space also retains the general semantic information, as similar phrases with the same label are still positioned close together.",8 Qualitative analysis,[0],[0]
"Future work could investigate models of multi-task training where metaphor detection is trained together with an unsupervised objective, allowing the system to take better advantage of unlabeled data while still learning to separate metaphors.",8 Qualitative analysis,[0],[0]
"In this paper, we introduced the first deep learning architecture designed to capture metaphorical composition and evaluated it on a metaphor identification task.
",9 Conclusion,[0],[0]
"Firstly, we demonstrated that the proposed framework outperforms both a metaphor-agnostic baseline (a feed-forward neural network) as well as previous corpus-driven approaches to metaphor identification.",9 Conclusion,[0],[0]
"The results showed that it is beneficial to construct a specialised network architecture for metaphor detection, which includes a gating function for capturing the interaction between the source and target domains, word embeddings mapped to a metaphor-specific space, and optimisation using a hinge loss function.
",9 Conclusion,[0],[0]
"Secondly, our qualitative analysis indicates that our supervised similarity network learns phrase representations with a very clear boundary for metaphoricity, in contrast to traditional compositional methods.
",9 Conclusion,[0],[0]
"Finally, we show that with a sufficiently large training set our model can also outperform the state-of-the art metaphor identification systems based on hand-coded lexical knowledge.",9 Conclusion,[0],[0]
Ekaterina Shutova’s research is supported by the Leverhulme Trust Early Career Fellowship.,Acknowledgments,[0],[0]
The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding.,abstractText,[0],[0]
"Yet, the majority of metaphor processing systems to date rely on handengineered features and there is still no consensus in the field as to which features are optimal for this task.",abstractText,[0],[0]
"In this paper, we present the first deep learning architecture designed to capture metaphorical composition.",abstractText,[0],[0]
Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.,abstractText,[0],[0]
Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4778–4784 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4778",text,[0],[0]
"Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) has now achieved impressive performance (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention.",1 Introduction,[0],[0]
"NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word.
",1 Introduction,[0],[0]
"Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser,
*Corresponding Author
1989), which forces the model to generate translation strictly matching the ground-truth at the word level.",1 Introduction,[0.9976905277143675],"['Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, 1989), which forces the model to generate translation strictly matching the ground-truth at the word level.']"
"However, in practice it is impossible to generate translation totally the same as ground truth.",1 Introduction,[1.0],"['However, in practice it is impossible to generate translation totally the same as ground truth.']"
"Once different target words are generated, the word-level loss cannot evaluate the translation properly, usually under-estimating the translation.",1 Introduction,[0],[0]
"In addition, the teacher forcing algorithm suffers from the exposure bias (Ranzato et al., 2015) as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference.",1 Introduction,[1.0],"['In addition, the teacher forcing algorithm suffers from the exposure bias (Ranzato et al., 2015) as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference.']"
"Kim and Rush (2016) proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words.",1 Introduction,[1.0],"['Kim and Rush (2016) proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words.']"
"Scheduled sampling(SS) (Bengio et al., 2015; Venkatraman et al., 2015) attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training.",1 Introduction,[1.0],"['Scheduled sampling(SS) (Bengio et al., 2015; Venkatraman et al., 2015) attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training.']"
"However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss.
",1 Introduction,[1.0000000476326814],"['However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss.']"
"In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss.",1 Introduction,[1.0],"['In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss.']"
"However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update.",1 Introduction,[1.0],"['However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update.']"
"Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable
training and limited improvements.",1 Introduction,[0.9999999440280026],"['Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable training and limited improvements.']"
"Lamb et al. (2016); Gu et al. (2017); Ma et al. (2018) respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update.",1 Introduction,[1.0],"['Lamb et al. (2016); Gu et al. (2017); Ma et al. (2018) respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update.']"
"However, these methods do not allow for direct optimization with respect to evaluation metrics.
",1 Introduction,[0],[0]
"In this paper, we propose a method to combine the strengths of the word-level and sequencelevel training, that is the direct gradient update without gradient estimation from word-level training and the greater flexibility from sequence-level training.",1 Introduction,[0],[0]
"Our method introduces probabilistic ngram matching which makes sequence-level objectives (e.g., BLEU, GLEU) differentiable.",1 Introduction,[1.0],"['Our method introduces probabilistic ngram matching which makes sequence-level objectives (e.g., BLEU, GLEU) differentiable.']"
"During training, it abandons teacher forcing and performs greedy search instead to take into consideration the predicted words.",1 Introduction,[1.0],"['During training, it abandons teacher forcing and performs greedy search instead to take into consideration the predicted words.']"
Experiment results show that our method significantly outperforms word-level training with the cross-entropy loss and sequence-level training under the reinforcement framework.,1 Introduction,[1.0],['Experiment results show that our method significantly outperforms word-level training with the cross-entropy loss and sequence-level training under the reinforcement framework.']
The experiments also indicate that greedy search strategy indeed has superiority over teacher forcing.,1 Introduction,[0],[0]
"NMT is based on an end-to-end framework which directly models the translation probability from the source sentence x to the target sentence ŷ:
",2 Background,[0],[0]
"P (ŷ|x) = T∏ j=1 p(ŷj |ŷ<j ,x, θ), (1)
where T is the target length and θ is the model parameters.",2 Background,[0],[0]
"Given the training set D = {XM,YM} withM sentences pairs, the training objective is to maximize the log-likelihood of the training data as
θ = argmax θ {L(θ)}
L(θ) = M∑ m=1 lm∑ j=1 log(p(ŷmj |ŷm<j ,xm, θ)), (2)
where the superior m indicates the m-th sentence in the dataset and lm is the length of m-th target sentence.
",2 Background,[0],[0]
"In the above model, the probability of each target word p(ŷmj |ŷm<j ,xm, θ) is conditioned on the previous target words.",2 Background,[1.0],"['In the above model, the probability of each target word p(ŷmj |ŷm<j ,xm, θ) is conditioned on the previous target words.']"
"The scenario is that in the
training time, the teacher forcing algorithm is employed and the ground truth words from the target sentence are fed as context, while during inference, the ground truth words are not available and the previous predicted words are instead fed as context.",2 Background,[0],[0]
This discrepancy is called exposure bias.,2 Background,[1.0],['This discrepancy is called exposure bias.']
"Many automatic evaluation metrics of machine translation, such as BLEU, GLEU and NIST, are based on the n-gram matching.",3.1 Sequence-Level Objectives,[1.0],"['Many automatic evaluation metrics of machine translation, such as BLEU, GLEU and NIST, are based on the n-gram matching.']"
Assuming that y and ŷ are the output sentence and the ground truth sentence with length T and T ′,3.1 Sequence-Level Objectives,[0],[0]
"respectively, the count of an n-gram g = (g1, . . .",3.1 Sequence-Level Objectives,[0],[0]
", gn) in sentence y is calculated as
Cy(g) =",3.1 Sequence-Level Objectives,[0],[0]
T−n∑ t=0 n∏ i=1,3.1 Sequence-Level Objectives,[0],[0]
"1{gi = yt+i}, (3)
where 1{·} is the indicator function.",3.1 Sequence-Level Objectives,[0],[0]
"The matching count of the n-gram g between ŷ and y is given by
Cŷy(g) = min (Cy(g),Cŷ(g)).",3.1 Sequence-Level Objectives,[0.9999999135792855],"['The matching count of the n-gram g between ŷ and y is given by Cŷy(g) = min (Cy(g),Cŷ(g)).']"
"(4)
Then the precision pn and the recall rn of the predicted n-grams are calculated as follows
pn =
∑ g∈y C
ŷ",3.1 Sequence-Level Objectives,[0],[0]
"y(g)∑
g∈y Cy(g) , (5)
",3.1 Sequence-Level Objectives,[0],[0]
"rn =
∑ g∈y C
ŷ y(g)∑
g∈ŷ Cŷ(g) .",3.1 Sequence-Level Objectives,[0],[0]
"(6)
BLEU, the most widely used metric for machine translation evaluation, is defined based on the n-gram precision as follows
BLEU = BP · exp( N∑ n=1",3.1 Sequence-Level Objectives,[0],[0]
"wn log pn), (7)
where BP stands for the brevity penalty",3.1 Sequence-Level Objectives,[0],[0]
and wn is the weight for the n-gram.,3.1 Sequence-Level Objectives,[0],[0]
"In contrast, GLEU is the minimum of recall and precision of 1-4 grams where 1-4 grams are counted together:
GLEU =",3.1 Sequence-Level Objectives,[0],[0]
"min(p1-4, r1-4).",3.1 Sequence-Level Objectives,[0],[0]
(8),3.1 Sequence-Level Objectives,[0],[0]
"In the output sentence y, the prediction probability varies among words.",3.2 probabilistic Sequence-Level Objectives,[1.0],"['In the output sentence y, the prediction probability varies among words.']"
"Some words are translated by the model with high confidence while some words are translated with high uncertainty.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"However, when calculating the count of n-grams in Eq.(3), all the words in the output sentence are treated equally, regardless of their respective prediction probabilities.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"To give a more precise description of n-gram counts which considers the variety of prediction probabilities, we use the prediction probability p(yj |y<j ,x, θ) as the count of word yj , and correspondingly the count of an n-gram is the product of these probabilistic counts of all the words in the n-gram, not one anymore.",3.2 probabilistic Sequence-Level Objectives,[1.0],"['To give a more precise description of n-gram counts which considers the variety of prediction probabilities, we use the prediction probability p(yj |y<j ,x, θ) as the count of word yj , and correspondingly the count of an n-gram is the product of these probabilistic counts of all the words in the n-gram, not one anymore.']"
"Then the probabilistic count of g = (g1, . . .",3.2 probabilistic Sequence-Level Objectives,[0],[0]
", gn) is calculated by summing over the output sentence y as
C̃y(g) =",3.2 probabilistic Sequence-Level Objectives,[0],[0]
T−n∑ t=0 n∏ i=1,3.2 probabilistic Sequence-Level Objectives,[0],[0]
"1{gi = yt+i} · p(yt+i|y<t+i,x, θ).",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(9)
Now the probabilistic sequence-level objective can be got by replacing Cy(g) with C̃y(g) (the tilde over the head indicates the probabilistic version) and keeping the rest unchanged.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Here, we take BLEU as an example and show how the probabilistic BLEU (denoted as P-BLEU) is defined.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"From this purpose, the matching count of n-gram g in Eq.(4) is modified as follows
C̃ ŷ
y(g) = min(C̃y(g),Cŷ(g)).",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(10)
and the predict precision of n-grams changes into
p̃n =
∑ g∈y C̃ ŷ
y(g)∑ g∈y C̃y(g) .",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(11)
Finally, the probabilistic BLEU (P-BLEU) is defined as
P-BLEU = BP · exp( N∑ n=1 wn log p̃n), (12)
Probabilistic GLEU (P-GLEU) can be defined in a similar way.",3.2 probabilistic Sequence-Level Objectives,[0.9999999868668356],"['(11) Finally, the probabilistic BLEU (P-BLEU) is defined as P-BLEU = BP · exp( N∑ n=1 wn log p̃n), (12) Probabilistic GLEU (P-GLEU) can be defined in a similar way.']"
"Specifically, we denote the probabilistic precision of n-grams as P-Pn.",3.2 probabilistic Sequence-Level Objectives,[1.0],"['Specifically, we denote the probabilistic precision of n-grams as P-Pn.']"
"The probabilistic precision is more reasonable than recall since the denominator in Eq.(11) plays a normalization role, so we modify the definition in Eq.(8) and define P-GLEU as simply the probabilistic precision of 1-4 grams.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"The general probabilistic loss function is:
L(θ) =",3.2 probabilistic Sequence-Level Objectives,[0],[0]
− M∑ m=1,3.2 probabilistic Sequence-Level Objectives,[0],[0]
"P(ym, ŷm), (13)
where P represents the probabilistic sequencelevel objectives, and ym and ŷm are the predicted translation and the ground truth for the m-th sentence respectively.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
The calculation of the probabilistic objective is illustrated in Figure 1.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
This probabilistic loss can work with decoding strategies such as greedy search and teacher forcing.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
In this paper we employ greedy search rather than teacher forcing so as to use the previously predicted words as context and alleviate the exposure bias problem.,3.2 probabilistic Sequence-Level Objectives,[1.0],['In this paper we employ greedy search rather than teacher forcing so as to use the previously predicted words as context and alleviate the exposure bias problem.']
We carry out experiments on Chinese-to-English translation.1 The training data consists of 1.25M pairs of sentences extracted from LDC corpora2.,4.1 Settings,[0],[0]
Sentence pairs with either side longer than 50 were dropped.,4.1 Settings,[0],[0]
We use NIST 2002 (MT 02) as the validation set and NIST 2003-2006 (MT 03-08) as the test sets.,4.1 Settings,[0],[0]
"We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.
",4.1 Settings,[0],[0]
"We apply our method to an attention-based NMT system (Bahdanau et al., 2014) implemented by Pytorch.",4.1 Settings,[0],[0]
"Both source and target vocabularies are limited to 30K. All word embedding sizes are set to 512, and the sizes of hidden units in both encoder and decoder RNNs are also set to 512.",4.1 Settings,[0],[0]
"All parameters are initialized by uniform distribution over [−0.1, 0.1].",4.1 Settings,[0],[0]
The mini-batch stochastic gradient descent (SGD) algorithm is employed to train the model with batch size of 40.,4.1 Settings,[0],[0]
"In addition, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ρ = 0.95 and = 1e-6.",4.1 Settings,[0],[0]
Dropout is applied on the output layer with dropout rate of 0.5.,4.1 Settings,[0],[0]
The beam size is set to 10.,4.1 Settings,[0],[0]
"Systems We first pretrain the baseline model by maximum likelihood estimation (MLE) and then refine the model using probabilistic sequencelevel objectives, including P-BLEU, P-GLEU and P-P2 (probabilistic 2-gram precision).",4.2 Performance,[0],[0]
"In addition, we reproduce previous works which train the NMT model through minimum risk training (MRT) (Shen et al., 2016) and REINFORCE algo-
1Experiment code: https://github.com/ictnlp/GS4NMT 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
",4.2 Performance,[0],[0]
"rithm (RF) (Ranzato et al., 2015).",4.2 Performance,[0],[0]
"When reproducing their works, we set BLEU, GLEU and 2-gram precision as training objectives respectively and find out that GLEU yields the best performance.",4.2 Performance,[0],[0]
"In the following, we only report the results with training objective GLEU.",4.2 Performance,[0],[0]
Performance Table 1 shows the translation performance on test sets measured in BLEU score.,4.2 Performance,[0],[0]
"Simply training NMT model by the probabilistic 2-gram precision achieves an improvement of 1.5 BLEU points, which significantly outperforms the reinforcement-based algorithms.",4.2 Performance,[0],[0]
"We also test the precision of other n-grams and their combinations, but do not notice significant improvements over P-P2.",4.2 Performance,[0],[0]
"Notice that our method only changes the loss function, without any modification on model structure and training data.",4.2 Performance,[0],[0]
We use the probabilistic loss to finetune the baseline model rather than training from scratch.,4.3 Why Pretraining,[0],[0]
This is in line with our motivation: to alleviate the exposure bias and make the model exposed to its own output during training.,4.3 Why Pretraining,[0],[0]
"In the very beginning of the training, the model’s translation capability is nearly zero and the generated sentences are often meaningless and do not contain useful information for the training, so it is unreasonable to directly apply the greedy search strategy.",4.3 Why Pretraining,[0],[0]
"Therefore, we first apply the teacher forcing algorithm to pretrain the model, and then we let the model generate the sentences itself and learn from its own outputs.
",4.3 Why Pretraining,[0],[0]
Another reason favoring pretraining is that pretraining can lower the training cost.,4.3 Why Pretraining,[0],[0]
The training cost of the introduced probabilistic loss is about three times higher than the cost of cross entropy.,4.3 Why Pretraining,[0],[0]
"Without pretraining, the training time will be much higher than usual.",4.3 Why Pretraining,[0],[0]
"Otherwise, the training cost is acceptable if the probabilistic loss is only for finetuning.",4.3 Why Pretraining,[0],[0]
"The probabilistic loss, defined in Eq.(13), is computed from the model output y and reference ŷ.",4.4 Effect of Decoding Strategy,[0],[0]
"In this section, we apply two different decoding strategies to generate y: 1.",4.4 Effect of Decoding Strategy,[0],[0]
"teacher forcing, which uses the ground truth as decoder input.",4.4 Effect of Decoding Strategy,[0],[0]
"2. greedy search, which feeds the word with maximum probability.",4.4 Effect of Decoding Strategy,[0],[0]
"By conducting this experiment, we attempt to figure out where the improvements come from: the modification of loss or the mitigation of exposure bias?
",4.4 Effect of Decoding Strategy,[0],[0]
Figure 2 shows the learning curves of the two decoding strategies with training objective P-P2.,4.4 Effect of Decoding Strategy,[0],[0]
Teacher forcing raises about 0.5 BLEU improvements and greedy search outperform the teacher forcing algorithm by nearly 1 BLEU point.,4.4 Effect of Decoding Strategy,[0],[0]
"We conclude that the probabilistic loss has its own advantage even when trained by the teacher forcing algorithm, and greedy search is effective in alleviating the exposure bias.
",4.4 Effect of Decoding Strategy,[0],[0]
Notice that the greedy search strategy highly relys on the probabilistic loss and can not be conducted independently.,4.4 Effect of Decoding Strategy,[0],[0]
Greedy search together with the word-level loss is very similar with the scheduled sampling(SS).,4.4 Effect of Decoding Strategy,[0],[0]
"However, SS is inconsistent with the word-level loss since the word-level loss requires strict alignment between hypothesis and reference, which can only be accomplished by the teacher forcing algorithm.",4.4 Effect of Decoding Strategy,[0],[0]
"In this section, we explore how the probabilistic objective correlates with the real evaluation metric.",4.5 Correlation with Evaluation Metrics,[0],[0]
"We randomly sample 100 pairs of sentences
from the training set and compute their P-GLEU and GLEU scores (Wu et al. (2016) indicates that GLEU have better performance in the sentencelevel evaluation than BLEU).
",4.5 Correlation with Evaluation Metrics,[0],[0]
"Directly computing the correlation between GLEU and P-GLEU gives the correlation coefficient 0.86, which indicates strong correlation.",4.5 Correlation with Evaluation Metrics,[0],[0]
"In addition, we draw the scatter diagram of the 100 pairs of sentences in Figure 3 with GLEU as x-axis and P-GLEU as y-axix.",4.5 Correlation with Evaluation Metrics,[0],[0]
"Figure 3 shows that PGLEU correlates well with GLEU, suggesting that it is reasonable to directly train the NMT model with P-GLEU.",4.5 Correlation with Evaluation Metrics,[0],[0]
"Word-level loss cannot evaluate the translation properly and suffers from the exposure bias, and sequence-level objectives are usually indifferentiable and require gradient estimation.",5 Conclusion,[0],[0]
"We propose probabilistic sequence-level objectives based on ngram matching, which relieve the dependence on gradient estimation and can directly train the NMT model.",5 Conclusion,[0],[0]
Experiment results show that our method significantly outperforms previous sequence-level training works and successfully alleviates the exposure bias through performing greedy search.,5 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments.,6 Acknowledgments,[0],[0]
This work was supported by the National Natural Science Foundation of China (NSFC) under the project NO.61472428 and the project NO. 61662077.,6 Acknowledgments,[0],[0]
"Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias.",abstractText,[0],[0]
"Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation.",abstractText,[0],[0]
"On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework.",abstractText,[0],[0]
"In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias.",abstractText,[0],[0]
Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.,abstractText,[0],[0]
Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,title,[0],[0]
