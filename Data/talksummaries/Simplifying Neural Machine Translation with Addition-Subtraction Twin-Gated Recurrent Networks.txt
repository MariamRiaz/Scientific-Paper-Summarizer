1	12	Recurrent neural networks (RNN), e.g., LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014), are widely used as the encoder and decoder for NMT.
5	124	These make NMT which is based on these gated RNNs suffer from not only inefficiency in training and inference due to recurrency and heavy computation in recurrent units (Vaswani et al., 2017) but also difficulty in producing interpretable models (Lee et al., 2017).
7	7	In this paper, our key interest is to simplify recurrent units in RNN-based NMT.
8	52	In doing so, we want to investigate how further we can advance RNN-based NMT in terms of the number of parameters (i.e., memory consumption), running speed and interpretability.
9	7	This simplification shall preserve the capability of modeling longdistance dependencies in LSTMs/GRUs and the expressive power of recurrent non-linearities in SRNN.
10	52	The simplification shall also reduce computation load and physical memory consumption in recurrent units on the one hand and allow us to take a good look into the inner workings of RNNs on the other hand.
12	72	In the recurrent units of ATR, we only keep the very essential weight matrices: one over the input and the other over the history (similar to SRNN).
13	21	Comparing with previous RNN variants (e.g., LSTM or GRU), we have the smallest number of weight matrices.
16	85	Specifically, we use the addition and subtraction operations between the weighted history and input to estimate an input and forget gate respectively.
17	31	These add-sub operations not only distinguish the two gates so that we do not need to have different weight matrices for them, but also make the two gates dynamically correlate to each other.
20	35	This property not only allows us to trace each state back to those inputs which contribute more but also establishes unnormalized forward self-attention between the current state and all its previous inputs.
21	30	The self-attention mechanism has already proved very useful in non-recurrent NMT (Vaswani et al., 2017).
22	17	We build our NMT systems on the proposed ATR with a single-layer encoder and decoder.
23	104	Experiments on WMT14 English-German and English-French translation tasks show that our model yields competitive results compared with GRU/LSTM-based NMT.
24	29	When we integrate an orthogonal context-aware encoder (still single layer) into ATR-based NMT, our model (yielding 24.97 and 39.06 BLEU on English-German and English-French translation respectively) is even comparable to deep RNN and non-RNN NMT models which are all with multiple encoder/decoder layers.
26	62	We adapt our model to other language translation and natural language processing tasks, including NIST Chinese-English translation, natural language inference and Chinese word segmentation.
54	7	The function φ(·) is a non-linear recurrent function, abstracting away from details in recurrent units.
55	24	GRU can be considered as a simplified version of LSTM.
59	23	Despite the success of these two gates in handling gradient flow, they consume extensive matrix transformations and weight parameters.
61	21	We therefore propose an addition-subtraction twin-gated recurrent unit (ATR), formulated as follows (see Figure 1c): pt = Whht−1, qt = Wxxt (6) it = σ(pt + qt) (7) ft = σ(pt − qt) (8) ht = it qt + ft ht−1 (9) The hidden state ht in ATR is a weighted mixture of both the current input qt and the history ht−1 controlled by an input gate it and a forget gate ft respectively.
62	7	Notice that we use the transformed representation qt for the current input rather than the raw vector xt due to the potential mismatch in dimensions between ht and xt.
63	51	Similar to GRU, we use gates, especially the forget gate, to control the back-propagated gradient flow to make sure gradients will neither vanish nor explode.
67	13	First, we squeeze the number of weight matrices in gate calculation from four to two (see Equation (2&3) and (7&8)).
77	51	We further empirically demonstrate that it is neither essential in machine translation.
78	3	Third, in GRU the gates for h̃t and ht−1 are coupled and normalized to 1 while we do not explicitly associate the two gates in ATR.
84	95	A practical question for the twin-gated mechanism is whether twin gates are really capable of dynamically weighting the input and history information.
