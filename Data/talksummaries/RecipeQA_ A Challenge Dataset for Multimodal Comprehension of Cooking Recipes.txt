32	9	We believe that recipe comprehension is an elusive challenge and might be seen as important milestone in the long-standing goal of artificial intelligence and machine reasoning (Norvig, 1987; Bottou, 2014).
33	24	Among previous efforts towards multimodal machine comprehension (Tapaswi et al., 2016; Kembhavi et al., 2016; Iyyer et al., 2017; Kembhavi et al., 2017; Kahou et al., 2018), our study is closer to what Kembhavi et al. (2017) envisioned in TQA.
34	15	Our task primarily differs in utilizing substantially larger number of images – the average number of images per recipe in RecipeQA is 12 whereas TQA has only 3 images per question on average.
40	18	• Answers require understanding procedural language, in particular keeping track of entities and/or actions and their state changes.
42	35	• Answers inherently involve multimodal understanding of image(s) and text.
43	61	To sum up, we believe RecipeQA is a challenging benchmark dataset which will serve as a test bed for evaluating multimodal comprehension systems.
44	10	In this paper, we present several statistical analyses on RecipeQA and also obtain baseline performances for a number of multimodal comprehension tasks that we introduce for cooking recipes.
45	114	The Recipe Question Answering (RecipeQA) dataset is a challenging multimodal dataset that evaluates reasoning over real-life cooking recipes.
47	77	2 shows an illustrative cooking recipe from our dataset.
48	84	Each recipe includes an arbitrary number of steps containing both textual and visual elements.
49	77	In particular, each step of a recipe is accompanied by a ‘title’, a ‘description’ and a set of illustrative ‘images’ that are aligned with the title and the description.
50	17	Each of these elements can be considered as a different modality of the data.
54	22	These recipes were collected from Instructables1, which is a how-to web site where users share all kinds of instructions including but not limited to recipes.
57	18	Our assumption is that the mostly viewed recipes contain less noise and include easy-to-understand instructions with high-quality illustrative images.
58	27	In total, we collected about 20K unique recipes from the food category of Instructables.
59	11	We filtered out non-English recipes using a language identification (Lui and Baldwin, 2012), and automatically removed the ones with unreadable contents such as the ones that only contain recipe videos.
62	7	Prior studies employed natural language questions either collected via crowdsourcing platforms such as SQuAD (Rajpurkar et al., 2016) or generated synthetically as in CNN/Daily Mail (Hermann et al., 2015).
63	45	Using natural language questions is a good approach in terms of capturing human understanding, but crowdsourcing is often too costly and does not scale well as the size of the dataset grows.
83	14	RecipeQA dataset contains approximately 20K cooking recipes and over 36K question-answer pairs divided into four major question types reflecting each of the task at hand.
89	12	RecipeQA includes four different types of tasks: (1) Textual cloze, (2) Visual cloze, (3) Visual coherence, and (4) Visual ordering.
96	30	Textual cloze style questions test the ability to infer missing text either in the title or in the step description by taking into account the question’s context which includes a set of illustrative images besides text.
101	7	Visual cloze style questions test a skill similar to that of textual cloze task with the difference that the missing information in this task reside in the visual domain.
136	21	For the textual close task, each candidate answer is compared against the titles or descriptions of the steps by using WMD (Kusner et al., 2015) distance, where such distances are averaged.
139	11	For the visual coherence task, since the aim is to find the incoherent image among other images, the final answer is chosen as the most dissimilar one to the remaining images on average.
142	13	In all these simple baseline models, we use the cosine distance to rank the candidates.
143	22	We report the performance of the baseline models in Table 2 which indicates the ratio of correct answers against the total questions in the test.
147	12	Some qualitative examples are provided in the supplementary material.
169	32	To our knowledge, RecipeQA is the first machine comprehension dataset that deals with understanding procedural knowledge in a multimodal setting.
171	7	Results of our baseline models demonstrate that RecipeQA is a challenging dataset and we plan make it publicly available for other researchers to promote the development of new methods for multimodal machine comprehension.
173	24	We also hope that RecipeQA will serve other purposes for related research problems on cooking recipes as well.
