0	28	Tremendous power of convolutional neural networks (CNNs) have been well demonstrated in a wide variety of computer vision applications, from image classification (Simonyan & Zisserman, 2015) and object detection (Ren et al., 2015) to image segmentation (Long et al., 2015).
2	36	For instance, the ResNet-50 (He et al., 2015) with some 50 convolutional layers needs over 95MB memory for storage and over 3.8 × 109 times of floating number multiplications for calculating each image, and after discarding more than 75% of its weights, the network still works as usual (Wang et al., 2016).
3	27	Admittedly, a heavy neural network is extremely difficult to train and to use in mobile terminal apps due to the limited memory and computational resource.
4	16	Lots of methods have been developed to reduce the amount of parameters in CNNs (Liu et al., 2015) to obtain a considerable compression ratio.
9	17	Although these methods obtained promising performance to reduce the storage of convolution filters, the memory usage introduced by filters is still huge in the stage of online inference.
10	23	This is because except convolution filters, we have to store feature maps (output data) of different layers for the subsequent processes, e.g., over 97MB memory is required for storing feature maps of one single image when running a ResNet-50 (He et al., 2015) without batch normalization layers, and a batch consisting of 32 instances consumes some 3.2GB GPU memory.
11	31	However, existing compression methods tend to directly compress the filters in one step and rarely consider the significant demand of feature maps on the storage and computational cost.
16	14	Circulant matrix is employed to formulate the feature map transformation considering its low space complexity and high mapping speed.
17	27	Based on the obtained compact feature maps, we re-formulate the convolution filters to establish its connections with the input data.
18	22	In summary, the proposed approach makes the following contributions: • We propose to excavate the intrinsic information and decrease the redundancy in feature maps derived from a large number of filters in each layer, and then the network architecture is upgraded to produce a new compact network with fewer filters but the similar discriminativeness.
45	29	An optimal projection should thus not only remove redundancy between feature maps, but also preserve the discriminability of these features.
47	15	To maintain the accuracy of the original network and its representation capability, we propose to preserve the distances between feature maps and form the following objective function for seeking the compact feature maps: min P,c ||PYTYPT − C||2F + λ||D(Ỹ)−D(Y)|| s.t.
66	18	We therefore propose using a circulant matrix (Gray, 2006; Henriques et al., 2015; 2014) to formulate P , which then only has d variables in the Fourier frequency domain.
67	18	We propose the following model to learn the projection for generating the compact low-dimensional feature maps: min p,c ||PYTYPT − C||2F + α||PPT − I||2F + β||c||1, s.t.
86	16	Section 3 proposes an effective approach for learning compact feature maps of a given convolutional layer.
92	30	Thus, we propose minimizing the following function for reconstructing convolution filters of this layer: min F̃ ||Ỹ − X̃T F̃||2F + γ||F̃||2F , (17) where Ỹ is the compact feature maps of L(i+1) after applying Fcn.
96	15	However, when the scale of the dataset is enormous, we cannot construct the two huge matrices X̃ and Ỹ through all instances in the dataset.
123	37	The feature map and convolution filters in the corresponding layer L̃i in the learned compact network are Yi ∈ RH ′ i×W ′ i×d̃i and F ∈ Rs2i×c̃i×d̃i , respectively.
137	15	We first tested the performance of the proposed method and analyzed impacts of parameters on the MNIST dataset using LeNet (LeCun et al., 1998), then compared the proposed method with two benchmark CNNs (VGGNet-16 (Simonyan & Zisserman, 2015) and ResNet-50 (He et al., 2015)) on the ILSVRC 2012 dataset (Russakovsky et al., 2015) which has more than 1 million nature images.
147	16	Although a larger β will produce a smaller network, it also leads to a larger distortion on the Euclidean distances between samples.
172	28	3 details the compression results of the proposed method and several state-of-the-art methods on three benchmark deep CNN models.
175	12	For a convolutional neural network with layers, its compression ratios is calculated as rc1 = ∑p i=1 s 2 i di−1di∑p i=1 s 2 i d̃i−1d̃i , rc2 = ∑p i=1 diH ′ iW ′ i∑p i=1 d̃iH ′ iW ′ i .
177	13	It is obvious that feature maps accounting a considerable proportion of memory usage of the whole network, and the proposed RedCNN can provide significant compression ratios rc2 on every network.
189	14	We found that runtimes of these models after compression were significantly reduced.
190	77	The results are extremely encouraging, e.g., the compressed ResNet can recognize over 500 images per second.
193	141	Note that, the runtime reported here is a bit higher than that in (Vedaldi & Lenc, 2015), due to different configurations and hardware environments.
195	18	Besides convolution filters, the storage of feature maps also accounts for a larger proportion of the online memory usage, we thus no longer search useless connections or weights of filters.
196	36	In this paper, we present a feature map dimensionality reduction method by excavating and removing redundancy in feature maps generated by different filters.
198	16	Experiments conducted on benchmark datasets and models show that the proposed method can achieve considerable compression ratio and speed-up ratios simultaneously without affecting the classification accuracy of the original CNN.
199	45	In addition, the compressed network generated by exploiting the proposed method is still a regular CNN with 32-bit float values which does not have any decoding or other procedures for online inference.
