10	32	A typical heuristic employed by multi-label learning algorithms is to simply treat all such the zeros in the label vector as are true negatives (Yu et al., 2014).
11	20	Another heuristic is to assign different weights to the zeros and ones in the binary label matrix (Yu et al., 2017), which is inspired by matrix factorization based collaborative filtering models that learn from implicit (binary) feedback data (Hu et al., 2008).
13	42	Another important desideratum is scalability, especially in the case of extreme multi-label learning problems (Prabhu & Varma, 2014; Jain et al., 2016; Babbar & Schölkopf, 2017), which are characterized by a massive number of labels, features, and examples.
15	20	Moreover, most of the scalable multi-label learning algorithms only operate in batch setting and are usually not designed to work (Prabhu & Varma, 2014; Bhatia et al., 2015; Jain et al., 2016) in online settings with continuous stream of training examples.
16	29	In this paper, we present a scalable, generative framework for multi-label learning, that not only bring to bear the modeling flexibility of probabilistic, generative models for the multi-label learning problem (Kapoor et al., 2012; Rai et al., 2015), but is also designed to handle the abovementioned challenges in a principled way.
17	135	Our framework is based on a latent factor model for the binary label matrix, and has the following distinguishing aspects: (1) It naturally handles the issue of missing vs negative labels via a principled generative model with a exposure model (Liang et al., 2016) for the label matrix; (2) It is accompanied by a simple and scalable inference procedure (both via Gibbs sampling and via fast point estimation); and (3) Inference can also be easily performed in an online fashion, enabling us to apply it on large-scale problems, even when using moderate computational resources.
18	34	In the multi-label learning problem, we assume that we are givenN training examples {(x1,y1), .
20	75	,yN} ∈ {0, 1}N×L to be the label matrix.
21	72	Given training data {X,Y}, the goal in multi-label learning is to learn a model that can predict the label vector y∗ ∈ {0, 1}L for a new test input x∗ ∈ RD.
22	22	Note that an entry yn` = 0 in the label matrix Y may not necessarily mean a negative label but could simply mean that this label is missing (and its true value could be 0 or 1).
23	33	As we shall show, our generative model can infer the missingness of a label yn` = 0 by associating another binary latent variable ξn` (called exposure variable).
24	24	These exposure variables will be incorporated in a latent factor model (Sec.
35	73	Note that we have associated a binary exposure latent variable ξn` with each label yn` such that ξn` = 0 implies that yn` is 0 because it is missing (not exposed), and ξn` = 1 implies that yn` is exposed (and could be 0 or 1 depending on the outcome of the Bernoulli draw).
37	21	Otherwise, we draw the observed label yn` from a Bernoulli distribution as yn` ∼ Bernoulli(yn`|σ(u>n v`)).
53	22	This enables us to develop a simple Gibbs sampling algorithm for doing inference in our model.
54	24	The conjugacy also allows us to design an online expectation maximization (EM) algorithm (Cappé & Moulines, 2009), which enables us to apply our model on large-scale problems.
61	37	When doing EM, this also leads to subproblems that are like least square regression problems.
78	28	We first show the batch EM updates for our model parameters and then describe the online EM algorithm which can process the training data in small mini-batches of examples, and results in faster convergence in practice.
81	36	The E step update equations are given below: • Expectations of Pólya-gamma variables {ωn`}, ∀n, ` are known to be available in closed form (Scott & Sun, 2013), and are given by ηn` = E[ωn`|ψn`] = 1 2ψn` tanh ( ψn` 2 ) (10) where ψn` = u>n v` is computed using the estimates of un and vm from the previous M step.
82	46	• Expectations of each of the binary exposure variables ξn`, ∀n, `, are given by pn` = E[ξn`|ψn`] = µn`σ(−ψn`) µn`σ(−ψn`) + (1− µn`) (11) The M Step: Given the expectations of the latent variables computed in the E step, the M step maximizes the following expected complete data log-likelihood plus logprior terms, which we denote as Q(U,V,W,µ), where U = {un}Nn=1, V = {v`}L`=1, W, and µ = {µn`}, ∀n, ` Q(U,V,W,µ) = − 1 2 ∑ n,` pn` (κn` − ηn`u>n v`) 2 ηn` + ∑ n,` log Bernoulli(pn`|µn`)− λu N∑ n=1 ||un −Wxn||2 − λv L∑ `=1 ||v`||2 − λw||W||2 + ∑ n,` log Beta(µn`|α1, α2) (12) Note that the first term in the objective function given in Eq.
89	120	• Estimating each of the label latent factors {v`}L`=1 is a weighted ridge-regression problem with solution v` = Σv` ( N∑ n=1 pn`κn`un ) (14) where Σv` = (∑N n=1 pn`ηn`unu > n + λvIK )−1 .
102	23	As we show in our experiments, this enables us to apply our model to be run efficiently on massive data sets (e.g., one of the data sets we experiment with has more than 600k examples with about 50k features per example) even on a standard laptop with very moderate hardware.
189	53	We investigate whether the global frequency of a label necessarily correlates to its exposure probability.
190	29	While it may be the case for some data sets where high label frequency implies a high inferred label exposure probability (e.g., see Fig.
194	52	4 shows the plot of inferred exposure probabilities µnl for two users (one female, one male) plotted against the label frequencies (movie popularities).
195	32	4 shows, our model infers that, a popular movie (shown in red dot in Fig 4) has a high exposure probability for the left user (Female, 25, Healthcare/Doctor) while it has a low exposure probability for the right user (Male, 35, artist).
199	60	We use a set of label exposure latent variables to model this, and infer these exposure probabilities from data.
200	25	Incorporating these latent variables leads to improve multi-label classification accuracies, and also enables doing interesting qualitative analyses.
202	29	We further develop a highly scalable online EM algorithm for performing inference in our model, which allows our model to be applied on large-scale data sets, even on standard machines with moderate hardware.
204	57	For example, it can be extended to a mixture of latent factor models, which will allow handling the cases where a single low-rank model does not adequately capture the structure of the label matrix.
