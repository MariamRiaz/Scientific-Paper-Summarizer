0	13	Phonology has traditionally focused on alternations revealed by paradigms such as the German final devoicing examples [ba:t]/[bE:d@] (‘bath-SG/PL’) and [tsu:k]/[tsy:g@] (‘train-SG/PL’).
1	17	These alternations are usually modeled through phonological grammars which map from underlying representations (URs) to surface representations (SRs) (Chomsky and Halle, 1968).
4	22	This extension of the empirical coverage has required a corresponding extension of the theoretical framework.
5	23	A phonological grammar cannot be construed anymore as a categorical function from URs to SRs.
11	21	In the case of a categorical framework such as OT or HG, the predicted typological structure can be investigated directly by exhaustively listing all the grammars predicted for certain constraint and candidate sets.
15	16	A natural indirect strategy that gets around the problem raised by an infinite typology is to enumerate, not the individual languages in the typology, but the set of implicational universals predicted by the typology.
16	35	An implicational universal is an implication P T−→ P̂ which holds of a given typology T whenever every language in the typology that satisfies the antecedent property P also satisfies the consequent property P̂ (Greenberg, 1963).
21	14	into account every language in the typology, they chart the boundaries and measure the richness of the typological structure predicted by T. Which antecedent and consequent properties P and P̂ should we focus on?
23	42	Within this categorical framework, the simplest, most basic, most atomic antecedent property P is the property of mapping a certain specific UR x to a certain specific SR y. Analogously, the simplest consequent property P̂ is the property of mapping a certain specific UR x̂ to a certain specific SR ŷ.
24	37	We thus focus on the following class of implications: Definition 1 The implicational universal (x, y) T→ (x̂, ŷ) holds relative to a categorical typology T provided each grammar in T which succeeds at the antecedent mapping (i.e., it maps the UR x to the SR y), also succeeds at the consequent mapping (i.e., it maps the UR x̂ to the SR ŷ).
32	54	We propose to extend the notion of T-orders from the categorical to the probabilistic setting as follows: Definition 2 The implicational universal (x, y) T→ (x̂, ŷ) holds relative to a probabilistic typology T provided each grammar in T assigns a probability to the consequent mapping (x̂, ŷ) which is at least as large as the probability it assigns to the antecedent mapping (x, y).
54	38	A stochastic phonological grammar G instead takes a UR x and returns a probability distribution G(·| x) over Gen(x) which assigns a probability G(y| x) to each candidate SR y in Gen(x).
59	23	It says that the probabilityGp(y | x) that the UR x is mapped to the SR y is he probability mass allocated by p to the region {G ∈ T |G(x) = y} of the typology T consisting of those categorical grammars which succeed on the mapping (x, y).
71	33	It is called the stochastic typology corresponding to the categorical typology T and the probability family P , and it is denoted by TP .
97	24	First, it is weaker because the requirement Gp(G(x)| x) = 1 is replaced with the weaker requirement Gp(G(x)| x) > 1/2: the probability assigned to the mappings enforced by G needs not be 1, as long as it is large enough, namely larger than 1/2.
109	59	We assume a set of n constraints C1, .
115	33	To illustrate, consider the following three constraints (from Kiparsky, 1993) for the process of t/d deletion mentioned in section 1: C1 = SYLLABLEWELLFORMEDNESS (SWF) penalizes codas and tautosyllabic consonant clusters; C2 = ALIGN penalizes resyllabification across word boundaries; and C3 = MAX penalizes segment deletion.
121	19	, θn) ∈ Rn assigns a numerical ranking value θk to each constraint Ck.
124	43	, n sampled independently from each other according to some distribution D on R. If the distribution D is continuous, the probability that two stochastic ranking values θh+ h and θk+ k coincide is equal to zero.
125	22	The stochastic ranking vector θ + thus describes the unique ranking θ+ which respects the relative size of the stochastic ranking values: a constraint Ch is ranked above a constraint Ck according to θ+ (namely, Ch θ+ Ck) if and only if the stochastic ranking value of the former is larger than that of the latter (namely, θh + h > θk + k).
126	20	A ranking vector θ thus induces the probability mass function pDθ defined in (8) over the categorical OT typology T. Obviously, this definition yields a probability mass, namely the sum of the masses pDθ (G) over all the categorical OT grammars G in T is indeed 1. pDθ (G) = the probability of sampling 1, .
132	22	To illustrate, figure 1 plots the SOT probability of the mappings (/cost.us/, [cos.us]) and (/cost.me/, [cos.me]) relative to the three constraints C1, C2, C3 listed above as a function of the ranking value θ1 of constraint C1 (horizontal axis) and the ranking value θ2 of constraint C2 (vertical axis) for three choices of the rank- ing value θ3 of constraint C3.4 These plots suggest that the implication (/cost.us/, [cos.us]) SOT→ (/cost.me/, [cos.me]) holds in SOT: the probability of the consequent (/cost.me/, [cos.me]) (plotted in the bottom row) seems to be always larger than the probability of the antecedent (/cost.us/, [cos.us]) (plotted in the top row).
133	34	But how can this conjecture be checked, given that SOT probabilities seem not to admit a closed-form expression?
135	45	Suppose that there exists a positive constant ∆ large enough that the distribution D concentrates most of the probability mass on the interval [−∆,+∆], as stated in (9).
136	36	This assumption holds in particular when D has a bounded support or it is defined through a density (such as a gaussian, as assumed in Boersma, 1997, 1998).
137	14	(D([−∆,+∆]))n > 1/2 (9) For any constraint ranking , consider a ranking vector θ such that the top -ranked constraint has the largest ranking value; the second top - ranked constraint has the second largest ranking value; and so on.
164	21	These plots suggest that the implication (/cost.us/, [cos.us]) SHG→ (/cost.me/, [cos.me]) holds in SHG as well: the probability of the consequent (/cost.me/, [cos.me]) (plotted in the bottom row) seems to be always larger than the probability of the antecedent (/cost.us/, [cos.us]) (plotted in the top row).
175	76	This means that any weight vector w = (w1, .
177	27	min i=1,...,m ∑ k wk(Ck(x, zi)− Ck(x, y))︸ ︷︷ ︸ ξ > 0 min j=1,...,m̂ ∑ k wk(Ck(x̂, ẑj)− Ck(x̂, ŷ))︸ ︷︷ ︸ ξ̂ > 0 (11) Let B be an upper bound on the constraint violation differences, so that |C(x, zi)−C(x, y)| ≤ B and |C(x̂, ẑj) − C(x̂, ŷ)| ≤ B for every i = 1, .
178	67	Suppose again that there exists a positive constant ∆ large enough that the distribution D concentrates most of the proba- bility mass on [−∆,+∆], in the sense that it satisfies the inequality (9).
