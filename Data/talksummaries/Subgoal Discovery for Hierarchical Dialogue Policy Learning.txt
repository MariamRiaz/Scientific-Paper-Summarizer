3	38	For example, to book a hotel involves identifying the location, specifying the check-in date and time, and negotiating the price etc.
8	20	Recently, Peng et al. (2017b) showed that the use of subgoals mitigates the reward sparsity and leads to more effective exploration for dialogue policy learning.
10	25	In this paper, we propose a simple yet effective Subgoal Discovery Network (SDN) that discovers useful subgoals automatically for an RL-based dialogue agent.
12	30	Intuitively, a hub state is a region in the agent’s state space that the agent tends to visit frequently on successful paths to a goal but not on unsuccessful paths.
14	22	We present the first study of learning dialogue agents with automatically discovered subgoals.
15	73	We demonstrate the effectiveness of our approach by building a composite task-completion dialogue agent for travel planning.
19	47	}, the agent observes the current state st of the conversation (Henderson, 2015; Mrkšić et al., 2017; Li et al., 2017), and chooses action at according to a policy π.
21	24	Then, the agent receives a numerical reward rt and switches to next state st+1.
23	30	The agent is to learn to choose optimal actions {at}t=1,2,... so as to maximize the total discounted reward r0 + γr1 + γ2r2 + · · · , where γ ∈ [0, 1] is a discount factor.
29	29	A major open challenge is the automatic discovery of subgoals from data, the main innovation of this work is covered in the next section.
32	29	Then at the end of those segments (subgoals), we equip an intrinsic or extrinsic reward for the HRL algorithm to learn a hierarchical dialogue policy.
35	30	Assume that we have collected a set of successful state trajectories of a task, as shown in Figure 2.
36	57	We want to find subgoal states, such as the three red states s4, s9 and s13, which form the “hubs” of these trajectories.
40	24	SDN repeats a twostage process of generating a state trajectory segment, until a trajectory termination symbol is generated: first it uses an initial segment hidden state to start a new segment, or a trajectory termination symbol to terminate the trajectory, given all previous states; if the trajectory is not terminated, then keep generating the next state in this trajectory segment given previous states until a segment termination symbol is generated.
45	22	Therefore, we use another RNN (RNN2) to encode all previous states to provide relevant information and we transform these information to low dimensional representations as the initial inputs for the RNN1 instances.
58	72	, sT ), we model its likelihood as follows3: LS(s) = ∑ σ⊆S(s),length(σ)≤S length(σ)∏ i=1 p(σi|τ(σ1:i)), (1) where S(s) is the set of all possible segmentations for the trajectory s, σi denotes the ith segment in the segmentation σ, and τ is the concatenation operator.
137	29	In this paper, we obtain dialogue state trajectories from a rulebased agent which is handcrafted by a domain expert, the performance of this rule-based agent can achieve success rate of 32.2% as shown in Figure 4 and Table 1.
172	27	Figure 5 summarizes the performances of these agents against real users in terms of success rate.
179	52	Table 2 shows the subgoals discovered by SDN in a sample dialogue by a rule-based agent interacting with the simulated user.
181	31	At turn 10, the user starts to talk about hotel while the rule-based agent is still working on the pre-defined, unfinished flight subtask until subtask flight is finished at turn 15.
182	28	At turn 16, the user switches to hotel, and so does the rule-based agent until the end of the dialogue.
184	22	Meanwhile, our SDN model detected two subgoals (except for the final goal): one terminating at turn 9 (Subgoal 1), and another terminating at turn 15 (Subgoal 2).
187	29	In Appendix B, Table 3 shows a sample dialogue session by m-HRL agent interacting with a real user.
214	34	Through visualization, we find that SDN discovers reasonable, comprehensible subgoals given only a small amount of suboptimal but successful dialogue state trajectories.
216	40	First, we want to integrate subgoal discovery into dialogue policy learning rather than treat them as two separate processes.
218	32	Third, we would like to generalize SDN to a wide range of complex goal-oriented tasks beyond dialogue, such as the particularly challenging Atari game of Montezuma’s Revenge (Kulkarni et al., 2016).
223	22	Suppose an HRL agent segments the trajectory into a sequence of subgoals as g0, g1, .
224	35	∈ G, and the corresponding subgoal termination time steps as tg0 , tg1 , .
227	24	Here γ ∈ [0, 1] is a discount factor, and the expectations are taken over the randomness of the reward and the state transition, We use deep neural networks to approximate the two Q-value functions as Q∗(s, a, g) ≈ Q(s, a, g; θi) and Q∗(s, g) ≈ Q(s, g; θe).
229	28	(4) Here, De, Di are the replay buffers storing dialogue experience for training top-level and lowlevel policies.
