14	16	The afore-discussed language models are generative in the sense that they merely model the joint distribution of a symbol sequence (Eq.
15	30	While the research community is mostly focused on pushing the limit of PPL (e.g., Jozefowicz et al., 2016), very limited attention has been paid to the discrimination power of language models when they are applied to real tasks, such as ASR and MT (Li and Khudanpur, 2008).
22	15	The goal is to enlarge the difference between the log-likelihoods of “good” and “bad” sentences.
29	14	The recurrent cell takes in the embedding and produces a hidden state ~ht by ~ht = σ(U~xt + V~ht−1), where σ(z) = 1 1+e−z is sigmoid activation function.
38	29	The RNNLM is trained by maximizing the average LM-score over all the x’s in a corpus, or equivalently, minimizing the PPL on the corpus.
43	20	The reference is taken from the text labels of dev93’ set of Wall Street Journal (WSJ) dataset.
44	24	The hypothesis is generated by a CTC-based (Graves et al., 2006) ASR system trained on WSJ training set.
50	24	The LM-score of the erroneous hypothesis is higher than that of the reference.
61	14	(2) Similar formulation is also seen in (Tachioka and Watanabe, 2015), where they only utilize one beam candidate, i.e., K = 1.
72	17	The unbound decreasing is due to the second term in Eq.
78	49	To effectively utilize all the imperfect beam candidates, we propose the following objective, min θ N∑ i=1 B∑ j=1 max { 0, τ−(log pθ(xi)−log pθ(xi,j)) } , (3) where log pθ(xi) − logθ(xi,j) is the margin between the scores of a ground-truth xi and a can- didate xi,j .
79	20	The hinge loss on the margin encourages the log-likelihood of the ground-truth to be at least τ larger than that of the imperfect hypothesis.
80	64	We call an LM trained by the above formulation as Large Margin Language Model (LMLM).
82	26	1b shows the training loss, which steadily decreases and approaches zero rapidly.
86	51	Compared with the histogram by the conventional RNNLM, LMLM significantly moves the distribution to the positive side, indicating more discrimination.
110	18	We use the standard configuration of train si284 dataset for training, dev93 for development and eval92 for testing.
111	14	Our ASR model has one convolution layer, followed by 5 bidirectional RNNs and one fully connected layer, with a CTC loss on top.
112	36	The text labels of the training set are used to train a 4-gram language model, which is employed in the ASR decoder.
113	19	The beam search decoder has a beam width of 2000.
121	13	3 reports the WERs on dev93 and eval92 sets.
125	21	To better understand the proposed methods, we calculate the correlation coefficients between the hypotheses’ WERs and their scores (by different language models).
126	14	In specific, for every utterance in the test set, we have a set of beam candidates, their word level accuracies (100-WER) and scores given by an LM, from which a Pearson correlation coefficient can be calculated.
130	14	4 posts some examples from the test set.
132	38	Words in red are mistakes made by the candidate sentences.
137	15	This is reasonable (though not necessarily desirable) as LM-score is a summation of log-probabilities, each of which is negative.
139	25	In the other two examples, LMLM and rLMLM seem to favor more sensible sentences, though they are not more grammatical than those picked by RNNLM and RNNLMadapted.
141	14	We further validate our methods on a larger noisy dataset collected by Liu et al. (2017).
142	76	The dataset has about 10K hours of spontaneous speech.
143	14	The utterances are corrupted by background noise, and a large portion of them are accented.
172	116	The calculation is done on dev06 set in the same way as Section 4.1.2, but now we change the WERs to BLEUs.
