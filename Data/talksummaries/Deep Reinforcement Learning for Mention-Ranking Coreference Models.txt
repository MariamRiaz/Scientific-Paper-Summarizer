17	19	We use the neural mention-ranking model described in Clark and Manning (2016), which we briefly go over in this section.
26	34	If c = NA, features defined over pairs of mentions are not included.
31	172	At test time, the mention-ranking model links each mention with its highest scoring candidate antecedent.
32	33	Mention-ranking models are typically trained with heuristic loss functions that are tuned via hyperparameters.
35	17	We then propose new training procedures based on reinforcement learning that instead directly optimize for coreference evaluation metrics.
36	36	The heuristic loss from Wiseman et al. is governed by the following error types, which were first proposed by Durrett et al. (2013).
39	101	Then we define the following costs for linking mi to a candidate antecedent c ∈ C(mi): ∆h(c,mi) =    αFN if c = NA ∧ T (mi) 6= {NA} αFA if c 6= NA ∧ T (mi) = {NA} αWL if c 6= NA ∧ a /∈ T (mi) 0 if a ∈ T (mi) for “false new,” “false anaphor,” “wrong link”, and correct coreference decisions.
42	12	We fix αWL = 1.0 and search for αFA and αFN out of {0.1, 0.2, ..., 1.5}with a variant of grid search.
46	18	Finding the best hyperparameter settings for the heuristic loss requires training many variants of the model, and at best results in an objective that is correlated with coreference evaluation metrics.
47	21	To address this, we pose mention ranking in the reinforcement learning framework (Sutton and Barto, 1998) and propose methods that directly optimize the model for coreference metrics.
49	15	Each action ai links the ith mention in the document mi to a candidate antecedent.
52	14	We use the B3 coreference metric for this reward (Bagga and Baldwin, 1998).
59	14	We also explore using the REINFORCE policy gradient algorithm (Williams, 1992).
60	25	We can define a probability distribution over actions using the mention-ranking model’s scoring function as follows: pθ(a) ∝ es(c,m) for any action a = (c,m).
61	21	The REINFORCE algorithm seeks to maximize the expected reward J(θ) = E[a1:T∼pθ]R(a1:T ) It does this through gradient ascent.
66	14	We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data (Pradhan et al., 2012) and evaluate with the MUC, B3, and CEAFφ4 metrics.
67	42	Our experiments were run using predicted mentions from Stanford’s rule-based coreference system (Raghunathan et al., 2010).
70	12	We compare the heuristic loss, REINFORCE, and reward rescaling approaches on both datasets.
71	17	We find that REINFORCE does slightly better than the heuristic loss, but reward rescaling performs significantly better than both across both languages.
72	14	We attribute the modest improvement of REINFORCE to it being poorly suited for a ranking task.
73	43	During training it optimizes the model’s performance in expectation, but at test-time it takes the most probable sequence of actions.
74	41	This mismatch occurs even at the level of an individual decision: the model only links the current mention to a single antecedent, but is trained to assign high probability to all correct antecedents.
76	25	The reward-rescaled max-margin loss combines the best of both worlds, resulting in superior performance.
77	50	In this section we examine the reward-based cost function ∆r and perform error analysis to determine how reward rescaling improves the mention-ranking model’s accuracy.
88	13	The reward-rescaling model makes slightly more errors, meaning its improvement in performance must come from its errors being less severe.
90	58	Proper nouns have a higher average cost for “false new” errors (0.90) than other mentions types (0.77).
95	20	This is reflected in its high average cost of 1.21.
96	43	After prior- itizing these examples during training, the rewardrescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links.
102	190	We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating the need for hyperparameters that must be carefully selected for each particular language, dataset, and evaluation metric.
103	56	Our reward-rescaling approach also increases the model’s accuracy, resulting in significant gains over the current state-of-the-art.
