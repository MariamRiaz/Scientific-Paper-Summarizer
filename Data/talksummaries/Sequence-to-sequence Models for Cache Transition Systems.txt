0	19	Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.
3	55	want-01 person go-01 ARG0 ARG0 ARG1 name “John” name op1 Figure 1: An example of AMR graph representing the meaning of: “John wants to go” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs.
5	13	On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing.
9	13	However, the final performance still falls behind the best-performing models.
16	29	Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs.
22	12	More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories.
23	23	We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing.
36	19	A configuration of our parser has the form: C = (σ, η, β,Gp) where σ, η and β are as described above, and Gp is the partial graph that has been built so far.
37	19	The initial configuration of the parser is ([], [$, .
47	14	Pop pops a pair (i, v) from the stack, where the integer i records the position in the cache that it originally came from.
48	41	We place concept v in position i in the cache, shifting the remainder of the cache one position to the right, and discarding the last element in the cache.
62	63	PushIndex phase: in this phase, the oracle first chooses a position i (as explained below) in the cache to place the candidate concept and removes the vertex at this position and places its index, vertex pair onto the stack.
72	19	For each training example (x1:n, g), the transition system generates the output AMR graph g from the input sequence x1:n through an oracle sequence a1:q ∈ Σ∗a, where Σa is the union of all possible actions.
78	12	Given an input word sequence w1:n′ , we use a bidirectional LSTM to encode it.
80	17	Then the hidden states of both directions are concatenated as the final hidden state for word wj : hwj = [ ←− h wj ; −→ h wj ] Similarly, for the concept sequence, the final hidden state for concept cj is: hcj = [ ←− h cj ; −→ h cj ]
84	12	While generating the t-th output action, the decoder considers three factors: (1) the previous hidden state of the LSTM model st−1; (2) the embedding of the previous generated action et−1; and (3) the previous context vectors for words µwt−1 and concepts µ c t−1, which are calculated using Hw and Hc, respectively.
86	12	The hidden state s0 is initialized as: s0 = Wd[ ←− h w1 ; −→ h wn ; ←− h c1; −→ h cn] + bd, where Wd and bd are model parameters.
96	14	As we have generated a concept sequence from the input word sequence, we maintain two hard attention pointers, lw and lc, to model monotonic attention to word and concept sequences respectively.
109	52	Number of dependencies to words on the right, and the top three dependency labels for them.
114	12	PushIndex features: token features for the leftmost buffer concept and all the concepts in the cache.
124	30	Both training and decoding use a Tesla K20X GPU.
134	17	We use the semi-Markov model from Flanigan et al. (2016) as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph.
143	13	For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template.
145	109	After categorization, we use Stanford CoreNLP (Manning et al., 2014) to get the POS tags and dependencies of the categorized dataset.
146	50	We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases.
147	50	We use a cache size of 5 in our experiments.
148	43	Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction history.
150	76	Table 1 shows the phase-wise accuracy of our sequence-to-sequence model.
151	28	Peng et al. (2018) use a separate feedforward network to predict each phase independently.
153	47	Soft+feats shows the performance of our sequence-to-sequence model with soft attention and transition state features, while Hard+feats is using hard attention.
