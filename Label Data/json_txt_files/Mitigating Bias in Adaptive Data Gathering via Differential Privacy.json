{"sections": [{"heading": "1. Introduction", "text": "Many modern data sets consist of data that is gathered adaptively: the choice of whether to collect more data points of a given type depends on the data already collected. For example, it is common in industry to conduct \u201cA/B\u201d tests to make decisions about many things, including ad targeting, user interface design, and algorithmic modifications, and this A/B testing is often conducted using \u201cbandit learning algorithms\u201d (Bubeck et al., 2012), which adaptively select treatments to show to users in an effort to find the best treatment as quickly as possible. Similarly, sequen-\n1Department of Statistics, The Wharton School, University of Pennsylvania 2Department of Computer Science, University of Pennsylvania. Correspondence to: Seth Neel <sethneel93@gmail.com>, Aaron Roth <aaroth@cis.upenn.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1This extended abstract is missing many details, proofs, and results that can be found in the full version (Neel & Roth, 2018).\ntial clinical trials may halt or re-allocate certain treatment groups due to preliminary results, and empirical scientists may initially try and test multiple hypotheses and multiple treatments, but then decide to gather more data in support of certain hypotheses and not others, based on the results of preliminary statistical tests.\nUnfortunately, as demonstrated by (Nie et al., 2017), the data that results from adaptive data gathering procedures will often exhibit substantial bias. As a result, subsequent analyses that are conducted on the data gathered by adaptive procedures will be prone to error, unless the bias is explicitly taken into account. This can be difficult. (Nie et al., 2017) give a selective inference approach: in simple stochastic bandit settings, if the data was gathered by a specific stochastic algorithm that they design, they give an MCMC based procedure to perform maximum likelihood estimation to recover de-biased estimates of the underlying distribution means. In this paper, we give a related, but orthogonal approach whose simplicity allows for a substantial generalization beyond the simple stochastic bandits setting. We show that in very general settings, if the data is gathered by a differentially private procedure, then we can place strong bounds on the bias of the data gathered, without needing any additional de-biasing procedure. Via elementary techniques, this connection implies the existence of simple stochastic bandit algorithms with nearly optimal worst-case regret bounds, with very strong bias guarantees. By leveraging existing connections between differential privacy and adaptive data analysis (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016), we can extend the generality of our approach to bound not just bias, but to correct for effects of adaptivity on arbitrary statistics of the gathered data. Since the data being gathered will generally be useful for some as yet unspecified scientific analysis, rather than just for the narrow problem of mean estimation, our technique allows for substantially broader possibilities compared to past approaches."}, {"heading": "1.1. Our Results", "text": "This paper has three main contributions:\n1. Using elementary techniques, we provide explicit bounds on the bias of empirical arm means maintained by bandit algorithms in the simple stochastic\nsetting that make their selection decisions as a differentially private function of their observations. Together with existing differentially private algorithms for stochastic bandit problems, this yields an algorithm that obtains an essentially optimal worst-case regret bound, and guarantees minimal bias (on the order of O(1/ \u221a K \u00b7 T )) for the empirical mean maintained for every arm. In the full version (Neel & Roth, 2018), we also extend our results to the linear contextual bandit problem, proving new bounds for a private linear UCB algorithm along the way.\n2. We then make a general observation, relating adaptive data gathering to an adaptive analysis of a fixed dataset (in which the choice of which query to pose to the dataset is adaptive). This lets us apply the large existing literature connecting differential privacy to adaptive data analysis. In particular, it lets us apply the max-information bounds of (Dwork et al., 2015b; Rogers et al., 2016) to our adaptive data gathering setting. This allows us to give much more general guarantees about the data collected by differentially private collection procedures, that extend well beyond bias. For example, it lets us correct the p-values for arbitrary hypothesis tests run on the gathered data.\n3. Finally, we run a set of experiments that measure the bias incurred by the standard UCB algorithm in the stochastic bandit setting, contrast it with the low bias obtained by a private UCB algorithm, and show that there are settings of the privacy parameter that simultaneously can make bias statistically insignificant, while having competitive empirical regret with the non-private UCB algorithm. We also demonstrate in the linear contextual bandit setting how failing to correct for adaptivity can lead to false discovery when applying t-tests for non-zero regression coefficients on an adaptively gathered dataset."}, {"heading": "1.2. Related Work", "text": "This paper bridges two recent lines of work. Our starting point is two recent papers: (Villar et al., 2015) empirically demonstrate in the context of clinical trials that a variety of simple stochastic bandit algorithms produce biased sample mean estimates (Similar results have been empirically observed in the context of contextual bandits (Dimakopoulou et al., 2017)). (Nie et al., 2017) prove that simple stochastic bandit algorithms that exhibit two natural properties (satisfied by most commonly used algorithms, including UCB and Thompson Sampling) result in empirical means that exhibit negative bias. They then propose a heuristic algorithm which computes a maximum likelihood estimator for the sample means from the empirical means gathered by a modified UCB algorithm which adds Gumbel noise to\nthe decision statistics. (Deshpande et al., 2017) propose a debiasing procedure for ordinary least-squares estimates computed from adaptively gathered data that trades off bias for variance, and prove a central limit theorem for their method. In contrast, the methods we propose in this paper are quite different. Rather than giving an ex-post debiasing procedure, we show that if the data were gathered in a differentially private manner, no debiasing is necessary. The strength of our method is both in its simplicity and generality: rather than proving theorems specific to particular estimators, we give methods to correct the p-values for arbitrary hypothesis tests that might be run on the adaptively gathered data.\nThe second line of work is the recent literature on adaptive data analysis (Dwork et al., 2015c;b; Hardt & Ullman, 2014; Steinke & Ullman, 2015; Russo & Zou, 2016; Wang et al., 2016; Bassily et al., 2016; Hardt & Blum, 2015; Cummings et al., 2016; Feldman & Steinke, 2017a;b) which draws a connection between differential privacy (Dwork et al., 2006) and generalization guarantees for adaptively chosen statistics. The adaptivity in this setting is dual to the setting we study in the present paper: In the adaptive data analysis literature, the dataset itself is fixed, and the goal is to find techniques that can mitigate bias due to the adaptive selection of analyses. In contrast, here, we study a setting in which the data gathering procedure is itself adaptive, and can lead to bias even for a fixed set of statistics of interest. However, we show that adaptive data gathering can be re-cast as an adaptive data analysis procedure, and so the results from the adaptive data analysis literature can be ported over."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Simple Stochastic Bandit Problems", "text": "In a simple stochastic bandit problem, there are K unknown distributions Pi over the unit interval [0,1], each with (unknown) mean \u00b5i. Over a series of rounds t \u2208 {1, . . . , T}, an algorithm A chooses an arm it \u2208 [K], and observes a reward yit,t \u223c Pit . Given a sequence of choices i1, . . . , iT , the pseudo-regret of an algorithm is defined to be:\nRegret((P1, . . . , PK), i1, . . . , iT ) = T \u00b7max i \u00b5i \u2212 T\u2211 t=1 \u00b5it\nWe say that regret is bounded if we can put a bound on the quantity Regret((P1, . . . , PK), i1, . . . , iT ) in the worst case over the choice of distributions P1, . . . , PK , and with high probability or in expectation over the randomness of the algorithm and of the reward sampling.\nAs an algorithm A interacts with a bandit problem, it generates a history \u039b , which records the sequence of actions\ntaken and rewards observed thus far: \u039bt = {(i`, yi`,`)} t\u22121 `=1. We denote the space of histories of length T by HT = ([K]\u00d7 R)T .\nThe definition of an algorithm A induces a sequence of T (possibly randomized) selection functions ft : Ht\u22121 \u2192 [K], which map histories onto decisions of which arm to pull at each round."}, {"heading": "2.2. Contextual Bandit Problems", "text": "In the contextual bandit problem, decisions are endowed with observable features. Our algorithmic results in this paper focus on the linear contextual bandit problem, but our general connection between adaptive data gathering and differential privacy extends beyond the linear case. For simplicity of exposition, we specialize to the linear case here.\nThere are K arms i, each of which is associated with an unknown d-dimensional linear function represented by a vector of coefficients \u03b8i \u2208 Rd with ||\u03b8i||2 \u2264 1. In rounds t \u2208 {1, . . . , T}, the algorithm is presented with a context xi,t \u2208 Rd for each arm i with ||xi,t||2 \u2264 1, which may be selected by an adaptive adversary as a function of the past history of play. We write xt to denote the set of all K contexts present at round t. As a function of these contexts, the algorithm then selects an arm it, and observes a reward yit,t. The rewards satisfy E [yi,t] = \u03b8i\u00b7xi,t and are bounded to lie in [0, 1].\nIn the contextual setting, histories incorporate observed context information as well: \u039bt = {(i`, x`, yi`,`)} t\u22121 `=1.\nAgain, the definition of an algorithmA induces a sequence of T (possibly randomized) selection functions ft : Ht\u22121\u00d7 Rd\u00d7K \u2192 [K], which now maps both a history and a set of contexts at round t to a choice of arm at round t."}, {"heading": "2.3. Data Gathering in the Query Model", "text": "Above we\u2019ve characterized a bandit algorithmA as gathering data adaptively using a sequence of selection functions ft, which map the observed history \u039bt \u2208 Ht\u22121 to the index of the next arm pulled. In this model only after the arm is chosen is a reward drawn from the appropriate distribution. Then the history is updated, and the process repeats.\nIn this section, we observe that whether the reward is drawn after the arm is \u201cpulled,\u201d or in advance, is a distinction without a difference. We cast this same interaction into the setting where an analyst asks an adaptively chosen sequence of queries to a fixed dataset, representing the arm rewards. The process of running a bandit algorithm A up to time T can be formalized as the adaptive selection of T queries against a single database of size T - fixed in advance. The formalization consists of observing two things.\nFirst, by the principle of deferred randomness, we can view any (simple or contextual) bandit algorithm as operating in a setting in the rewards available for every arm at every time step have been sampled before the start of the algorithm, rather than online as the algorithm makes its selections. Second, the choice of arm pulled at time t by the bandit algorithm can be viewed as the answer to an adaptively selected query against this fixed dataset of rewards.\nAdaptive data analysis is formalized as an interaction in which a data analystA performs computations on a dataset D, observes the results, and then may choose the identity of the next computation to run as a function of previously computed results (Dwork et al., 2015c;a). A sequence of recent results shows that if the queries are differentially private in the dataset D, then they will not in general overfit D, in the sense that the distribution over results induced by computing q(D) will be \u201csimilar\u201d to the distribution over results induced if q were run on a new dataset, freshly sampled from the same underlying distribution (Dwork et al., 2015c;a; Bassily et al., 2016; Dwork et al., 2015b; Rogers et al., 2016). We will be more precise about what these results say in Section 4.\nRecall that histories \u039b record the choices of the algorithm, in addition to its observations. It will be helpful to introduce notation that separates out the choices of the algorithm from its observations. In the simple stochastic setting and the contextual setting, given a history \u039bt, an action history \u039bAt = (i1, . . . , it\u22121) \u2208 [K]t\u22121 denotes the portion of the history recording the actions of the algorithm.\nIn the simple stochastic setting, a bandit tableau is a T \u00d7K matrix D \u2208 ( [0, 1]K )T . Each row Dt of D is a vector of K real numbers, intuitively representing the rewards that would be available to a bandit algorithm at round t for each of the K arms. In the contextual setting, a bandit tableau is represented by a pair of T \u00d7K matrices: D \u2208 ( [0, 1]K\n)T and C \u2208 ( (Rd)K )T . Intuitively, C represents the contexts presented to a bandit algorithm A at each round: each row Ct corresponds to a set of K contexts, one for each arm. D again represents the rewards that would be available to the bandit algorithm at round t for each of the K arms.\nWe write Tab to denote a bandit tableau when the setting has not been specified: implicitly, in the simple stochastic case, Tab = D, and in the contextual case, Tab = (D,C).\nGiven a bandit tableau and a bandit algorithm A, we have the following interaction:\nWe denote the subset of the reward tableau D corresponding to rewards that would have been revealed to a bandit algorithmA given action history \u039bAt , by \u039bAt (D). Concretely if \u039bAt = (i1, . . . , it\u22121) then \u039b A t (D) = {(i`, yi`,`)} t\u22121 `=1. Given a selection function ft and an action history \u039bAt , de-\nInteract Inputs: Time horizon T , bandit algorithm A, and bandit tableau Tab (D in the simple stochastic case, (D,C) in the contextual case)\n1: for t = 1 to T do 2: (contextual case) ShowA contexts Ct,1, . . . , Ct,K 3: Let A play action it 4: Show A reward Dt,it 5: end for 6: Return: (i1, . . . , iT )\nfine the query q\u039bAt as q\u039bAt (D) = ft(\u039b A t (D)).\nWe now define Algorithms Bandit and InteractQuery. Bandit is a standard contextual bandit algorithm defined by selection functions ft, and InteractQuery is the Interact routine that draws the rewards in advance, and at time t selects action it as the result of query q\u039bAt . With the above definitions in hand, it is straightforward to show that the two Algorithms are equivalent, in that they induce the same joint distribution on their outputs. In both algorithms for convenience we assume we are in the linear contextual setting, and we write \u03b7it to denote the i.i.d. error distributions of the rewards, conditional on the contexts.\nBandit Inputs: T, k, {xit}, {\u03b8i}, ft,\u039b0 = \u2205\n1: for t = 1, . . . , T : do 2: Let it = ft(\u039bt\u22121) 3: Draw yit,t \u223c xit,t \u00b7 \u03b8it + \u03b7it 4: Update \u039bt = \u039bt\u22121 \u222a (it, yit,t) 5: end for 6: Return: \u039bT\nInteractQuery Inputs: T, k,D : Dit = \u03b8i \u00b7 xit + \u03b7it, ft\n1: for t = 1, . . . , T : do 2: Let qt = q\u039bAt\u22121 3: Let it = qt(D) 4: Update \u039bAt = \u039b A t\u22121 \u222a it 5: end for 6: Return: \u039bAT\nClaim 1. Let P1,t be the joint distribution induced by Algorithm Bandit on \u039bt at time t, and let P2,t be the joint distribution induced by Algorithm InteractQuery on \u039bt = \u039b A t (D). Then \u2200t P1,t = P2,t.\nThe upshot of this equivalence is that we can import existing results that hold in the setting in which the dataset\nis fixed, and queries are adaptively chosen. There are a large collection of results of this form that apply when the queries are differentially private (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016) which apply directly to our setting. In the next section we formally define differential privacy in the simple stochastic and contextual bandit setting, and leave the description of the more general transfer theorems to Section 4."}, {"heading": "2.4. Differential Privacy", "text": "We will be interested in algorithms that are differentially private. In the simple stochastic bandit setting, we will require differential privacy with respect to the rewards. In the contextual bandit setting, we will also require differential privacy with respect to the rewards, but not necessarily with respect to the contexts.\nWe now define the neighboring relation we need to define bandit differential privacy:\nDefinition 1. In the simple stochastic setting, two bandit tableau\u2019s D,D\u2032 are reward neighbors if they differ in at most a single row: i.e. if there exists an index ` such that for all t 6= `, Dt = D\u2032t.\nIn the contextual setting, two bandit tableau\u2019s (D,C), (D\u2032, C \u2032) are reward neighbors if C = C \u2032 and D and D\u2032 differ in at most a single row: i.e. if there exists an index ` such that for all t 6= `, Dt = D\u2032t.\nNote that changing a context does not result in a neighboring tableau: this neighboring relation will correspond to privacy for the rewards, but not for the contexts. Remark 1. Note that we could have equivalently defined reward neighbors to be tableaus that differ in only a single entry, rather than in an entire row. The distinction is unimportant in a bandit setting, because a bandit algorithm will be able to observe only a single entry in any particular row.\nDefinition 2. A bandit algorithm A is ( , \u03b4) reward differentially private if for every time horizon T and every pair of bandit tableau Tab,Tab\u2032 that are reward neighbors, and every subset S \u2286 [K]T :\nP [Interact(T,A,Tab) \u2208 S] \u2264\ne P [ Interact(T,A,Tab\u2032) \u2208 S ] + \u03b4\nIf \u03b4 = 0, we say that A is -differentially private."}, {"heading": "3. Privacy Reduces Bias in Stochastic Bandit Problems", "text": "We begin by showing that differentially private algorithms that operate in the stochastic bandit setting compute empirical means for their arms that are nearly unbiased. Together\nwith known differentially private algorithms for stochastic bandit problems, the result is an algorithm that obtains a nearly optimal (worst-case) regret guarantee while also guaranteeing that the collected data is nearly unbiased. We could (and do) obtain these results by combining the reduction to answering adaptively selected queries given by Theorem 1 with the standard generalization theorems in adaptive data analysis (e.g. Corollary 2 in its most general form), but we first prove these de-biasing results from first principles to build intuition.\nTheorem 1. Let A be an ( , \u03b4)-differentially private algorithm in the stochastic bandit setting. Then, for all i \u2208 [K], and all t, we have:\u2223\u2223\u2223E [Y\u0302 ti \u2212 \u00b5i]\u2223\u2223\u2223 \u2264 (e \u2212 1 + T\u03b4)\u00b5i Remark 2. Note that since \u00b5i \u2208 [0, 1], and for 1, e \u2248 1+ , this theorem bounds the bias by roughly +T\u03b4. Often, we will have \u03b4 = 0 and so the bias will be bounded by roughly .\nProof. First we fix some notation. Fix any time horizon T , and let (ft)t\u2208[T ] be the sequence of selection functions induced by algorithm A. Let 1{ft(\u039bt)=i} be the indicator for the event that arm i is pulled at time t. We can write the random variable representing the sample mean of arm i at time T as\nY\u0302 Ti = T\u2211 t=1 1{ft(\u039bt)=i}\u2211T t\u2032=1 1{ft\u2032 (\u039bt\u2032 )=i} yit\nwhere we recall that yi,t is the random variable representing the reward for arm i at time t. Note that the numerator (ft(\u039bt) = i) is by definition independent of yi,t, but the denominator ( \u2211T t\u2032=1 1{ft\u2032 (\u039bt\u2032 )=i}) is not, because for t\n\u2032 > t\u039bt\u2032 depends on yi,t. It is this dependence that leads to bias in adaptive data gathering procedures, and that we must argue is mitigated by differential privacy.\nWe recall that the random variable NTi represents the number of times arm i is pulled through round T : NTi =\u2211T t\u2032=1 1{ft\u2032 (\u039bt\u2032 )=i}. Using this notation, we write the sample mean of arm i at time T , as:\nY\u0302 Ti = T\u2211 t=1 1{ft(\u039bt)=i} NTi \u00b7 yit\nWe can then calculate:\nE[Y\u0302 ti ] = T\u2211 t=1 E[ 1{ft(\u039bt)=i} NTi yit]\n= T\u2211 t=1 E yit\u223cPi [yit \u00b7 E A [ 1{ft(\u039bt)=i} NTi |yit]]\nwhere the first equality follows by the linearity of expectation, and the second follows by the law of iterated expectation.\nOur goal is to show that the conditioning in the inner expectation does not substantially change the value of the expectation. Specifically, we want to show that all t, and any value yit, we have\nE[ 1{ft(\u039bt)=i}\nNi |yit] \u2265 e\u2212 E[\n1{ft(\u039bt)=i}\nNTi ]\u2212 \u03b4\nIf we can show this, then we will have\nE[Y\u0302 Ti ] \u2265 (e\u2212 T\u2211 t=1 E[ 1{ft(\u039bt)=i} NTi ]\u2212 T\u03b4) \u00b7 \u00b5i\n= (e\u2212 E[ NTi NTi ]\u2212 T\u03b4) \u00b7 \u00b5i = (e\u2212 \u2212 T\u03b4) \u00b7 \u00b5i\nwhich is what we want (The reverse inequality is symmetric).\nThis is what we now show to complete the proof. Observe that for all t, i, the quantity 1{ft(\u039bt)=i}Ni can be derived as a post-processing of the sequence of choices (f1(\u039b1), . . . , fT (\u039bT )), and is therefore differentially private in the observed reward sequence. Observe also that the quantity 1{ft(\u039bt)=i}\nNTi is bounded in [0, 1]. Hence (by a\nlemma in the full version) for any pair of values yit, y\u2032it, we have E[\n1{ft(\u039bt)=i} NTi |yit] \u2265 e\u2212 E[ 1{ft(\u039bt)=i} NTi |y\u2032it]\u2212 \u03b4. All\nthat remains is to observe that there must exist some value y\u2032it such that E[ 1{ft(\u039bt)=i} Ni |y\u2032it] \u2265 E[ 1{ft(\u039bt)=i} Ni ]. (Otherwise, this would contradict Ey\u2032it\u223cPi [E[ 1{ft(\u039bt)=i} Ni |y\u2032it]] = E[ 1{ft(\u039bt)=i}\nNTi ]). Fixing any such y\u2032it implies that for all yit\nE[ 1{ft(\u039bt)=i}\nNi |yit] \u2265 e\u2212 E[\n1{ft(\u039bt)=i}\nNTi |y\u2032i,t]\u2212 \u03b4\n\u2265 e\u2212 E[ 1{ft(\u039bt)=i}\nNTi ]\u2212 \u03b4\nas desired. The upper bound on the bias follows symmetrically."}, {"heading": "3.1. A Private UCB Algorithm", "text": "There are existing differentially private variants of the classic UCB algorithm ((Auer et al., 2002; Agrawal, 1995; Lai & Robbins, 1985)), which give a nearly optimal tradeoff between privacy and regret (Mishra & Thakurta, 2014; Tossou & Dimitrakakis, 2017; 2016). For completeness, we give a simple version of a private UCB algorithm in the full version which we use in our experiments. Here, we simply quote the relevant theorem, which is a consequence of a theorem in (Tossou & Dimitrakakis, 2016):\nTheorem 2. (Tossou & Dimitrakakis, 2016) There is an - differentially private algorithm that obtains expected regret bounded by:\nO ( max ( lnT \u00b7 (ln ln(T ) + ln(1/ )) , \u221a kT log T ))\nThus, we can take to be as small as = O( ln 1.5 T\u221a kT )\nwhile still having a regret bound of O( \u221a kT log T ), which is nearly optimal in the worst case (over instances) (Audibert & Bubeck, 2009).\nCombining the above bound with Theorem 1, and letting = O( ln\n1.5 T\u221a kT ), we have:\nCorollary 1. There exists a simple stochastic bandit algorithm that simultaneously guarantees that the bias of the empirical average for each arm i is bounded by O(\u00b5i \u00b7 ln\n1.5 T\u221a kT\n) and guarantees expected regret bounded by O( \u221a kT log T ).\nOf course, other tradeoffs are possible using different values of . For example, the algorithm of (Tossou & Dimitrakakis, 2016) obtains sub-linear regret so long as = \u03c9( ln\n2 T T ). Thus, it is possible to obtain non-trivial regret while guaranteeing that the bias of the empirical means remains as low as polylog(T )/T ."}, {"heading": "4. Max Information & Arbitrary Hypothesis Tests", "text": "Up through this point, we have focused our attention on showing how the private collection of data mitigates the effect that adaptivity has on bias, in both the stochastic and (in the full version) contextual bandit problems. In this section, we draw upon more powerful results from the adaptive data analysis literature to go substantially beyond bias: to correct the p-values of hypothesis tests applied to adaptively gathered data. These p-value corrections follow from the connection between differential privacy and a quantity called max information, which controls the extent to which the dependence of selected test on the dataset can distort the statistical validity of the test (Dwork et al., 2015b; Rogers et al., 2016). We briefly define max information, state the connection to differential privacy, and illustrate how max information bounds can be used to perform adaptive analyses in the private data gathering framework.\nDefinition 3 (Max-Information (Dwork et al., 2015b).). Let X,Z be jointly distributed random variables over domain (X ,Z). Let X \u2297 Z denote the random variable that draws independent copies of X,Z according to their marginal distributions. The \u03b2-approximate maxinformation between X,Z, denoted I\u03b2(X,Z), is defined\nas:\nI\u03b2(X,Z) = log sup O\u2282(X\u00d7Z), P[(X,Z)\u2208O]>\u03b2 P [(X,Z) \u2208 O]\u2212 \u03b2 P [X \u2297 Z \u2208 O]\nFollowing (Rogers et al., 2016), define a test statistic t : D \u2192 R, where D is the space of all datasets. For D \u2208 D, given an output a = t(D), the p-value associated with the test t on dataset D is p(a) = PD\u223cP0 [t(D) \u2265 a], where P0 is the null hypothesis distribution. Consider an algorithm A, mapping a dataset to a test statistic. Definition 4 (Valid p-value Correction Function (Rogers et al., 2016).). A function \u03b3 : [0, 1] \u2192 [0, 1] is a valid p-value correction function for A if the procedure:\n1. Select a test statistic t = A(D)\n2. Reject the null hypothesis if p(t(D)) \u2264 \u03b3(\u03b1)\nhas probability at most \u03b1 of rejection, when D \u223c P0.\nThen the following theorem gives a valid p-value correction function when (D,A(D)) have bounded \u03b2-approximate max information.\nTheorem 3 ((Rogers et al., 2016).). Let A be a datadependent algorithm for selecting a test statistics such that I\u03b2(X,A(X)) \u2264 k. Then the following function \u03b3 is a valid p-value correction function for A: \u03b3(\u03b1) = max(\u03b1\u2212\u03b2\n2k , 0)\nFinally, we can connect max information to differential privacy, which allows us to leverage private algorithms to perform arbitrary valid statistical tests.\nTheorem 4 (Theorem 20 from (Dwork et al., 2015b).). Let A be an -differentially private algorithm, let P be an arbitrary product distribution over datasets of size n, and let D \u223c P . Then for every \u03b2 > 0:\nI\u03b2(D,A(D)) \u2264 log(e)( 2n/2 + \u221a n log(2/\u03b2)/2)\nRemark 3. We note that a hypothesis of this theorem is that the data is drawn from a product distribution. In the contextual bandit setting, this corresponds to rows in the bandit tableau being drawn from a product distribution. This will be the case if contexts are drawn from a distribution at each round, and then rewards are generated as some fixed stochastic function of the contexts. Note that contexts (and even rewards) can be correlated with one another within a round, so long as they are selected independently across rounds.\nWe now formalize the process of running a hypothesis test against an adaptively collected dataset. A bandit algorithm A generates a history \u039bT \u2208 HT . Let the reward portion of the gathered dataset be denoted by DA. We define an adaptive test statistic selector as follows.\nDefinition 5. Fix the reward portion of a bandit tableau D and bandit algorithmA. An adaptive test statistic selector is a function s from action histories to test statistics such that s(\u039bAT ) is a real-valued function of the adaptively gathered dataset DA.\nImportantly, the selection of the test statistic s(\u039bAT ) can depend on the sequence of arms pulled by A (and in the contextual setting, on all contexts observed), but not otherwise on the reward portion of the tableau D. For example, tA = s(\u039b A T ) could be the t-statistic corresponding to the null hypothesis that the arm i\u2217 which was pulled the great-\nest number of times has mean \u00b5: tA(DA) = \u2211NT i\u2217 t=1 yi\u2217t\u2212\u00b5\u221a\nNT i\u2217\nBy virtue of Theorems 3 and 4, and our view of adaptive data gathering as adaptively selected queries, we get the following corollary:\nCorollary 2. Let A be an reward differentially private bandit algorithm, and let s be an adaptive test statistic selector. Fix \u03b2 > 0, and let \u03b3(\u03b1) =\n\u03b1\n2log(e)( 2T/2+\n\u221a T log(2/\u03b2)/2) , for \u03b1 \u2208 [0, 1]. Then for any adaptively selected statistic tA = s(\u039bAT ), and any product distribution P corresponding to the null hypothesis for tA\nPD\u223cP,A [p(tA(D)) \u2264 \u03b3(\u03b1)] \u2264 \u03b1\nIf we set = O(1/ \u221a T ) in Corollary 2, then \u03b3(\u03b1) = O(\u03b1)\u2013 i.e. a valid p-value correction that only scales \u03b1 by a constant."}, {"heading": "5. Experiments", "text": "We first validate our theoretical bounds on bias in the simple stochastic bandit setting. As expected the standard UCB algorithm underestimates the mean at each arm, while the private UCB algorithm of (?) obtains very low bias. While using the suggested by the theory effectively reduces bias and achieves near optimal asymptotic regret, the resulting private algorithm only achieves non-trivial regret for large T due to large constants and logarithmic factors in our bounds. This motivates a heuristic choice of that provides no theoretical guarantees on bias reduction, but leads to regret that is comparable to the non-private UCB algorithm. We find empirically that even with this large choice of we achieve an 8 fold reduction in bias relative to UCB. This is consistent with the observation that our guarantees hold in the worst-case, and suggests that there is room for improvement in our theoretical bounds \u2014 both improving constants in the worst-case bounds on bias and on regret, and for proving instance specific bounds. Finally, we show that in the linear contextual bandit setting collecting data adaptively with a linear UCB algorithm and then conducting t-tests for regression coefficients yields incorrect inference (absent a p-value correction). These findings\nconfirm the necessity of our methods when drawing conclusions from adaptively gathered data."}, {"heading": "5.1. Stochastic Multi-Armed Bandit", "text": "In our first stochastic bandit experiment we set K = 20 and T = 500. The K arm means are equally spaced between 0 and 1 with gap \u2206 = .05, with \u00b50 = 1. We run UCB and -private UCB for T rounds with = .05, and after each run compute the difference between the sample mean at each arm and the true mean. We repeat this process 10, 000 times, averaging to obtain high confidence estimates of the bias at each arm. The average absolute bias over all arms for private UCB was .00176, with the bias for every arm being statistically indistinguishable from 0 at 95% confidence (see Figure 1 for confidence intervals) while the average absolute bias (over arms) for UCB was .0698, or over 40 times higher. The most biased arm had a measured bias of roughly 0.14, and except for the top 4 arms, the bias of each arm was statistically significant. It is worth noting that private UCB achieves bias significantly lower than the = .05 guaranteed by the theory, indicating that the theoretical bounds on bias obtained from differential privacy are conservative. Figures 1, 2 show the bias at each arm for private UCB vs. UCB, with 95% confidence intervals around the bias at each arm. Not only is the bias for private UCB an order of magnitude smaller on average, it does not exhibit the systemic negative bias evident in Figure 2.\nNoting that the observed reduction in bias for = .05 exceeded that guaranteed by the theory, we run a second experiment withK = 5, T = 100000,\u2206 = .05, and = 400, averaging results over 1000 iterations. Figure 5 shows that private UCB achieves sub-linear regret comparable with UCB. While = 400 provides no meaningful theoretical guarantee, the average absolute bias at each arm mean obtained by the private algorithm was .0015 (statistically indistinguishable from 0 at 95% confidence for each arm), while the non-private UCB algorithm obtained average bias .011, 7.5 times larger. The bias reduction for the arm with the smallest mean (for which the bias is the worst with the non private algorithm) was by more than a factor of 10.\nFigures 3,4 show the bias at each arm for the private and non-private UCB algorithms together with 95% confidence intervals; again we observe a negative skew in the bias for UCB, consistent with the theory in (Nie et al., 2017)."}, {"heading": "5.2. Linear Contextual Bandits", "text": "Our second experiment confirms that adaptivity leads to bias in the linear contextual bandit setting in the context of hypothesis testing \u2013 and in particular can lead to false discovery in testing for non-zero regression coefficients. The set up is as follows: for K = 5 arms, we observe rewards\nyi,t \u223c N (\u03b8\u2032ixit, 1), where \u03b8i, xit \u2208 R5, ||\u03b8i|| = ||xit|| = 1. For each arm i, we set \u03b8i1 = 0. Subject to these constraints, we pick the \u03b8 parameters uniformly at random (once per run), and select the contexts x uniformly at random (at each round). We run a linear UCB algorithm (OFUL (?)) for T = 500 rounds, and identify the arm i\u2217 that has been selected most frequently. We then conduct a z-test for whether the first coordinate of \u03b8i\u2217 is equal to 0. By construction the null hypothesis H0 : \u03b8i\u22171 = 0 of the experiment is true, and absent adaptivity, the p-value should be distributed uniformly at random. In particular, for any value of \u03b1 the probability that the corresponding p-value is less than \u03b1 is exactly \u03b1. We record the observed p-value, and repeat the experiment 1000 times, displaying the histogram of observed p-values in Figure 6. As expected, the adaptivity of the data gathering process leads the p-values to exhibit a strong downward skew. The dotted blue line demarcates \u03b1 = .05. Rather than probability .05 of falsely rejecting the null hypothesis at 95% confidence, we observe that 76% of the observed p-values fall below the .05 threshold. This shows that a careful p-value correction in the style of Section 2.3 is essential even for simple testing of regression coefficients, lest bias lead to false discovery."}], "year": 2018, "references": [{"title": "Sample mean based index policies with o(log n) regret for the multi-armed bandit problem", "authors": ["R. Agrawal"], "venue": "Advances in Applied Probability,", "year": 1995}, {"title": "Minimax policies for adversarial and stochastic bandits", "authors": ["Audibert", "J.-Y", "S. Bubeck"], "venue": "In COLT, pp", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "authors": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn.,", "year": 2002}, {"title": "Algorithmic stability for adaptive data analysis", "authors": ["R. Bassily", "K. Nissim", "A. Smith", "T. Steinke", "U. Stemmer", "J. Ullman"], "venue": "In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing,", "year": 2016}, {"title": "Adaptive learning with robust generalization guarantees", "authors": ["R. Cummings", "K. Ligett", "K. Nissim", "A. Roth", "Z.S. Wu"], "venue": "In Conference on Learning Theory,", "year": 2016}, {"title": "Accurate inference for adaptive linear models", "authors": ["Y. Deshpande", "L. Mackey", "V. Syrgkanis", "M. Taddy"], "venue": "arXiv preprint arXiv:1712.06695,", "year": 2017}, {"title": "Estimation considerations in contextual bandits", "authors": ["M. Dimakopoulou", "S. Athey", "G. Imbens"], "venue": "arXiv preprint arXiv:1711.07077,", "year": 2017}, {"title": "Calibrating noise to sensitivity in private data analysis", "authors": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In Proceedings of the Third Conference on Theory of Cryptography,", "year": 2006}, {"title": "The reusable holdout: Preserving validity in adaptive data analysis", "authors": ["C. Dwork", "V. Feldman", "M. Hardt", "T. Pitassi", "O. Reingold", "A. Roth"], "year": 2015}, {"title": "Generalization for adaptivelychosen estimators via stable median", "authors": ["V. Feldman", "T. Steinke"], "venue": "In Conference on Learning Theory, pp", "year": 2017}, {"title": "Calibrating noise to variance in adaptive data analysis", "authors": ["V. Feldman", "T. Steinke"], "venue": "arXiv preprint arXiv:1712.07196,", "year": 2017}, {"title": "The ladder: a reliable leaderboard for machine learning competitions", "authors": ["M. Hardt", "A. Blum"], "venue": "In Proceedings of the 32nd International Conference on International Conference on Machine Learning-Volume", "year": 2015}, {"title": "Preventing false discovery in interactive data analysis is hard", "authors": ["M. Hardt", "J. Ullman"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2014}, {"title": "Asymptotically efficient adaptive allocation rules", "authors": ["T. Lai", "H. Robbins"], "venue": "Adv. Appl. Math.,", "year": 1985}, {"title": "Private stochastic multi-arm bandits: From theory to practice", "authors": ["N. Mishra", "A. Thakurta"], "venue": "In ICML Workshop on Learning, Security, and Privacy,", "year": 2014}, {"title": "Mitigating bias in adaptive data gathering via differential privacy", "authors": ["S. Neel", "A. Roth"], "venue": "arXiv preprint arXiv:1806.02329,", "year": 2018}, {"title": "Why adaptively collected data have negative bias and how to correct for it", "authors": ["X. Nie", "X. Tian", "J. Taylor", "J. Zou"], "year": 2017}, {"title": "Max-information, differential privacy, and post-selection hypothesis testing", "authors": ["R.M. Rogers", "A. Roth", "A.D. Smith", "O. Thakkar"], "venue": "In IEEE 57th Annual Symposium on Foundations of Computer Science,", "year": 2016}, {"title": "Controlling bias in adaptive data analysis using information theory", "authors": ["D. Russo", "J. Zou"], "venue": "In Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "authors": ["T. Steinke", "J. Ullman"], "venue": "In Conference on Learning Theory, pp", "year": 2015}, {"title": "Algorithms for differentially private multi-armed bandits", "authors": ["A.C.Y. Tossou", "C. Dimitrakakis"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "year": 2016}, {"title": "Achieving privacy in the adversarial multi-armed bandit", "authors": ["A.C.Y. Tossou", "C. Dimitrakakis"], "venue": "CoRR, abs/1701.04222,", "year": 2017}, {"title": "Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges", "authors": ["S.S. Villar", "J. Bowden", "J. Wason"], "venue": "Statistical science: a review journal of the Institute of Mathematical Statistics,", "year": 2015}, {"title": "A minimax theory for adaptive data analysis", "authors": ["Wang", "Y.-X", "J. Lei", "S.E. Fienberg"], "venue": "arXiv preprint arXiv:1602.04287,", "year": 2016}], "id": "SP:27d5185b5eb6be3ba84d0947aeefa704372c9499", "authors": [{"name": "Seth Neel", "affiliations": []}, {"name": "Aaron Roth", "affiliations": []}], "abstractText": "Data that is gathered adaptively \u2014 via bandit algorithms, for example \u2014 exhibits bias. This is true both when gathering simple numeric valued data \u2014 the empirical means kept track of by stochastic bandit algorithms are biased downwards \u2014 and when gathering more complicated data \u2014 running hypothesis tests on complex data gathered via contextual bandit algorithms leads to false discovery. In this paper, we show that this problem is mitigated if the data collection procedure is differentially private. This lets us both bound the bias of simple numeric valued quantities (like the empirical means of stochastic bandit algorithms), and correct the p-values of hypothesis tests run on the adaptively gathered data. Moreover, there exist differentially private bandit algorithms with near optimal regret bounds: we apply existing theorems in the simple stochastic case, and give a new analysis for linear contextual bandits. We complement our theoretical results with experiments validating our theory1.", "title": "Mitigating Bias in Adaptive Data Gathering via Differential Privacy"}