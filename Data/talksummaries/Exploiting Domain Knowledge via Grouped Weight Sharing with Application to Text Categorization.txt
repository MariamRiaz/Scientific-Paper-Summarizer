1	12	However, a downside of learning from scratch is failing to capitalize on prior linguistic or semantic knowledge, often encoded in existing resources such as ontologies.
2	15	Such prior knowledge can be particularly valuable when estimating highly flexible models.
3	57	In this work, we address how to exploit known relationships between words when training neural models for NLP tasks.
4	19	We propose exploiting the feature-hashing trick, originally proposed as a means of neural network compression (Chen et al., 2015).
9	41	We then use feature hashing to share a subset of weights between the embeddings of words that belong to the same semantic group(s).
20	5	We denote this input by Ep ∈ RV×d (V is the vocabulary size and d the dimension of the word embeddings).
24	12	We then apply independent sets of linear convolution filters on these two matrices.
31	59	To initialize Es, for each dimension j of each word embedding ei, we use a hash function hi to map (hash) the index j to one of the K group IDs: hi : N → {g(i)1 , g (i) 2 ...g (i) K }.
38	4	, d} do 4: ei,j := ghi(j),j · b(i, j) 5: end for 6: end for For illustration, consider Figure 1.
40	10	The group embeddings gg1 , gg2 are initialized as averages over the pre-trained embeddings of the words they comprise.
42	9	Similarly, e1,3 and e2,3 will share value at gg1,3.
45	12	We update Es and group embeddings g in a manner similar to Chen et al. (2015).
46	100	In the forward propagation before each training step (mini-batch), we derive the value of ei,j from g: ei,j := ghi(j),j ∗ b(i, j) (1) We use this newly updated ei,j to perform forward propagation in the CNN.
47	41	During backward propagation, we first compute the gradient of Es, and then we use this to derive the gradient w.r.t gs.
48	25	To do this, for each dimension j in ggk , we aggregate the gradients w.r.t E s whose elements are mapped to this dimension: ∇ggk,j := ∑ (i,j) ∇Esi,j · δhi(j)=gk · b(i, j) (2) where δhi(j)=gk = 1 when h i(j) = gk, and 0 otherwise.
52	12	The number of parameters in our approach scales linearly with the number of channels.
55	15	We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)3; a customer review (CR) dataset (Hu and Liu, 2004)4; and an opinion dataset (MPQA) (Wiebe et al., 2005)5.
57	23	The task here is to classify published articles describing clinical trials as relevant or not to a well-specified clinical question.
59	5	We use data from reviews that concerned: clopidogrel (CL) for cardiovascular conditions (Dahabreh et al., 2013); biomarkers for assessing iron deficiency in anemia (AN) experienced by patients with kidney disease (Chung et al., 2012); statins (ST) (Cohen et al., 2006); and proton beam (PB) therapy (Terasawa et al., 2009).
64	43	We also use the Brown clustering algorithm7 on the three sentiment datasets.
66	19	For the biomedical datasets, we use the Medical Subject Headings (MeSH) terms8 attached to each abstract to classify them.
67	17	Each MeSH term has a tree number indicating the path from the root in the UMLS.
73	42	Then we run beliefpropagation on the graph to encourage linked words to have similar vectors.
80	7	Because the biomedical datasets are imbalanced, we use downsampling (Zhang et al., 2016a; Zhang and Wallace, 2015) to effectively train on balanced subsets of the data.
81	46	We developed our approach using the MR sentiment dataset, tuning our approach to constructing groups from the available resources – experiments on other sentiment datasets were run after we finalized the model and hyperparameters.
82	17	Similarly, we used the anemia (AN) review as a development set for the biomedical tasks, especially w.r.t.
83	12	constructing groups from MeSH terms using UMLS.
85	4	Results on the sentiment and biomedical corpora in are presented in Tables 2 and 3, respectively.12 These exploit different external resources to induce the word groupings that in turn inform weight sharing.
86	39	We report AUC for the biomedical datasets because these are highly imbalanced (see Table 1).
87	44	Our method improves performance compared to all relevant baselines (including an approach that prior work due to different preprocessing steps.
