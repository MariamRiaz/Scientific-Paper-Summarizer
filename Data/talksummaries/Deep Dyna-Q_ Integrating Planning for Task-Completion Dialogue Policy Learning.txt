9	39	Most of recent studies in this area have adopted this strategy (Su et al., 2016a; Lipton et al., 2016; Zhao and Eskenazi, 2016; Williams et al., 2017; Dhingra et al., 2017; Li et al., 2017; Liu and Lane, 2017; Peng et al., 2017b; Budzianowski et al., 2017; Peng et al., 2017a).
10	1	However, user simulators usually lack the conversational complexity of human interlocutors, and the trained agent is inevitably affected by biases in the design of the simulator.
15	4	Compared to previous works (Singh et al., 2002; Li et al., 2016a; Su et al., 2016b; Papangelis, 2012), our dialogue agent learns in a much more efficient way, using only a small number of real user interactions, which amounts to an affordable cost in many nontrivial dialogue tasks.
17	2	Specifically, we incorporate a model of the environment, referred to as the world model, into the dialogue agent, which simulates the environment and generates simulated user experience.
26	39	Inspired by Mnih et al. (2015), we propose Deep Dyna-Q (DDQ) by combining Dyna-Q with deep learning approaches to representing the state-action space by neural networks (NN).
27	9	By employing the world model for planning, the DDQ method can be viewed as a model-based RL approach, which has drawn growing interest in the research community.
28	34	However, most model-based RL methods (Tamar et al., 2016; Silver et al., 2016b; Gu et al., 2016; Racanière et al., 2017) are developed for simulation-based, synthetic problems (e.g., games), but not for human-in-the-loop, real-world problems.
29	29	To these ends, our main contributions in this work are two-fold: • We present Deep Dyna-Q, which to the best of our knowledge is the first deep RL framework that incorporates planning for taskcompletion dialogue policy learning.
30	107	• We demonstrate that a task-completion dia- logue agent can efficiently adapt its policy on the fly, by interacting with real users via RL.
31	66	This results in a significant improvement in success rate on a nontrivial task.
32	223	Our DDQ dialogue agent is illustrated in Figure 2, consisting of five modules: (1) an LSTMbased natural language understanding (NLU) module (Hakkani-Tür et al., 2016) for identifying user intents and extracting associated slots; (2) a state tracker (Mrkšić et al., 2016) for tracking the dialogue states; (3) a dialogue policy which selects the next action2 based on the current state; (4) a model-based natural language generation (NLG) module for converting dialogue actions to natural language response (Wen et al.); and (5) a world model for generating simulated user actions and simulated rewards.
33	360	As illustrated in Figure 1(c), starting with an initial dialogue policy and an initial world model (both trained with pre-collected human conversational data), the training of the DDQ agent consists of three processes: (1) direct reinforcement learning, where the agent interacts with a real user, collects real experience and improves the dialogue policy; (2) world model learning, where the world model is learned and refined using real experience; and (3) planning, where the agent improves the dialogue policy using simulated experience.
34	16	Although these three processes conceptually can occur simultaneously in the DDQ agent, we implement an iterative training procedure, as shown in Algorithm 1, where we specify the order in which they occur within each iteration.
41	10	Finally, we store the experience (s, a, r, au, s′) in the replay buffer Du.
47	86	In the planning process (lines 23-41 in Algorithm 1), the world model is employed to generate simulated experience that can be used to improve dialogue policy.
50	13	In DDQ, we use two replay buffers, Du for storing real experience and Ds for simulated experience.
53	18	Similar to Schatzmann et al. (2007), at the beginning of each dialogue, we uniformly draw a user goal G = (C,R), where C is a set of con- straints and R is a set of requests (line 26 in Algorithm 1).
60	89	The user action can also be converted to natural language via NLG, e.g., "which theater will show batman?"
61	82	In each dialogue turn, the world model takes as input the current dialogue state s and the last agent action a (represented as an one-hot vector), and generates user response au, reward r, and a binary variable t, which indicates whether the dialogue terminates (line 33).
62	105	The generation is accomplished using the world model M(s, a; θM ), a MLP shown in Figure 3, as follows: h = tanh(Wh(s, a) + bh) r = Wrh+ br au = softmax(Wah+ ba) t = sigmoid(Wth+ bt) where (s, a) is the concatenation of s and a, and W and b are parameter matrices and vectors, respectively.
63	9	In this process (lines 19-22 in Algorithm 1), M(s, a; θM ) is refined via minibatch SGD using real experience in the replay buffer Du.
64	26	As shown in Figure 3,M(s, a; θM ) is a multi-task neural network (Liu et al., 2015) that combines two classification tasks of simulating au and t, respectively, and one regression task of simulating r. The lower layers are shared across all tasks, while the top layers are task-specific.
65	10	We evaluate the DDQ method on a movie-ticket booking task in both simulation and human-in-theloop settings.
