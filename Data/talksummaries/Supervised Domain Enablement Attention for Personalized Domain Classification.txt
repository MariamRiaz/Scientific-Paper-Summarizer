1	24	In natural language understanding, domain classification is a task that finds the most relevant domain given an input utterance (Tur and de Mori, 2011).
2	50	For example, “make a lion sound” and “find me an apple pie recipe” should be classified as ZooKeeper and AllRecipe, respectively.
3	117	Recent IPDAs cover more than several thousands of diverse domains by including third-party developed domains such as Alexa Skills (Kumar et al., 2017; Kim et al., 2018a; Kim and Kim, 2018), Google Actions, and Cortana Skills, which makes domain classification to be a more challenging task.
6	9	The enabled domain vectors and the attention weights are automatically trained in an end-to-end fashion to be helpful for the domain classification.
8	2	First, we use logistic sigmoid instead of softmax as the attention activation function to relax the constraint that the weight sum over all the enabled domains is 1 to the constraint that each attention weight is between 0 and 1 regardless of the other weights (Martins and Astudillo, 2016; Kim et al., 2017).
14	13	Given an input utterance, each word of the utterance is represented as a dense vector through word embedding followed by bidirectional long shortterm memory (BiLSTM) (Graves and Schmidhuber, 2005).
16	6	The attention weight of an enabled domain e is formulated as follows: ae = σ (u · ve) , where u is the utterance vector, ve is the enablement vector of enabled domain e, and σ is sigmoid function.
19	22	The utterance vector and the weighted sum of the domain enablement vectors are concatenated to represent the utterance and the domain enablement as a single vector.
20	2	Given the concatenated vector, a feed-forward neural network with a single hidden layer3 is used to predict the confidence score by logistic sigmoid function for each domain.
21	3	One issue of the proposed architecture is that the domain enablement can be trained to be a very strong signal, where one of the enabled domains would be the predicted domains regardless of the relevancy of the utterances to the predicted domains in many cases.
23	20	During inference, we always use the correct domain enablements of the given utterances.
26	97	We hypothesize that if the ground-truth domain is one of the enabled domains, the attention weight for the ground-truth domain should be high and vice versa.
27	22	To apply this hypothesis in the model training as a supervised attention method, we formulate an auxiliary loss function as follows: La = − ∑ e∈E ye log ae + (1− ye) log (1− ae) , where E is a set of enabled domains and ae is the attention weight for the enabled domain e.
28	91	One issue of supervised attention in Section 2.1 is that enabled domains that are not ground-truth domains are encouraged to have lower attention weights regardless of their relevancies to the input utterances and the ground-truth domains.
29	43	Distillation methods utilize not only the ground-truth but also all the output activations of a source model so that all the prediction information from the source model can be utilized for more effective knowledge transfer between the source model and the target model (Hinton et al., 2014).
30	6	Selfdistillation, which trains a model leveraging the outputs of the source model with the same architecture or capacity, has been shown to improve the target model’s performance with a distillation method (Furlanello et al., 2018).
31	41	We use a variant of self-distillation methods, where the model outputs at the previous epoch with the best dev set performance are used as the soft targets for the distillation,4 so that the enabled domains that are not ground-truths can also be used for the supervised attention.
32	26	While conventional distillation methods utilize softmax activations as the target values, we show that distillation on top of sigmoid activations is also effective without loss of generality.
33	28	The loss function for the self-distillation on the attention weights is formulated as follows: Ld = − ∑ e∈E ãe log ae + (1− ãe) log (1− ae) , where ãe is the attention weight of the model showing the dev set performance in the previous epochs.
