0,1,label2,summary_sentences
Deep learning has significantly advanced our ability to address a wide range of difficult machine learning and signal processing problems.,1. Introduction,[0],[0]
"Today’s machine learning landscape is dominated by deep (neural) networks (DNs), which are compositions of a large number of simple parameterized linear and nonlinear transforms.",1. Introduction,[0],[0]
"An all-too-common story of late is that of plugging a deep network into an application as a black box, training it on copious training data,
1ECE Department, Rice University, Houston, TX, USA.",1. Introduction,[0],[0]
"Correspondence to: Randall B. <randallbalestriero@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"and then significantly improving performance over classical approaches.
",1. Introduction,[0],[0]
"Despite this empirical progress, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field.",1. Introduction,[0],[0]
"Ongoing attempts to build a rigorous mathematical framework fall roughly into five camps: (i) probing and measuring DNs to visualize their inner workings (Zeiler & Fergus, 2014); (ii) analyzing their properties such as expressive power (Cohen et al., 2016), loss surface geometry (Lu & Kawaguchi, 2017; Soudry & Hoffer, 2017), nuisance management (Soatto & Chiuso, 2016), sparsification (Papyan et al., 2017), and generalization abilities; (iii) new mathematical frameworks that share some (but not all) common features with DNs (Bruna & Mallat, 2013); (iv) probabilistic generative models from which specific DNs can be derived (Arora et al., 2013; Patel et al., 2016); and (v) information theoretic bounds (Tishby & Zaslavsky, 2015).
",1. Introduction,[0],[0]
"In this paper, we build a rigorous bridge between DNs and approximation theory via spline functions and operators.",1. Introduction,[0],[0]
"We prove that a large class of DNs — including convolutional neural networks (CNNs) (LeCun, 1998), residual networks (ResNets) (He et al., 2016; Targ et al., 2016), skip connection networks (Srivastava et al., 2015), fully connected networks (Pal & Mitra, 1992), recurrent neural networks (RNNs) (Graves, 2013), and beyond — can be written as spline operators.",1. Introduction,[0],[0]
"In particular, when these networks employ current standard-practice piecewise-affine, convex nonlinearities (e.g., ReLU, max-pooling, etc.)",1. Introduction,[0],[0]
"they can be written as the composition of max-affine spline operators (MASOs) (Magnani & Boyd, 2009; Hannah & Dunson, 2013).",1. Introduction,[0],[0]
"We focus on such nonlinearities here but note that our framework applies also to non-piecewise-affine nonlinearities through a standard approximation argument.
",1. Introduction,[0],[0]
The max-affine spline connection provides a powerful portal through which to view and analyze the inner workings of a DN using tools from approximation theory and functional analysis.,1. Introduction,[0],[0]
"Here is a summary of our key contributions:
[C1] We prove that a large class of DNs can be written as a composition of MASOs, from which it follows immediately that, conditioned on the input signal, the output of a DN is a simple affine transformation of the input.",1. Introduction,[0],[0]
"We illustrate in Section 4 by deriving a closed-form expression for the
input/output mapping of a CNN.
",1. Introduction,[0],[0]
"[C2] The affine mapping formula enables us to interpret a MASO DN as constructing a set of signal-dependent, classspecific templates against which the signal is compared via a simple inner product.",1. Introduction,[0],[0]
"In Section 5 we relate DNs directly to the classical theory of optimal classification via matched filters and provide insights into the effects of data memorization (Zhang et al., 2016).
",1. Introduction,[0],[0]
[C3] We propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal to each other.,1. Introduction,[0],[0]
"In Section 6, we show that this leads to significantly improved classification performance and reduced overfitting on standard test data sets like CIFAR100 with no change to the DN architecture.
",1. Introduction,[0],[0]
"[C4] The partition of the input space induced by a MASO links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up a new geometric avenue to study how DNs cluster and organize signals in a hierarchical fashion.",1. Introduction,[0],[0]
"Section 7 studies the properties of the MASO partition.
",1. Introduction,[0],[0]
"[C5] Leveraging the fact that a DN considers two signals to be similar if they lie in the same MASO partition region, we develop a new signal distance in Section 7.3 that measures the difference between their partition encodings.",1. Introduction,[0],[0]
"The distance is easily computed via backpropagation.
",1. Introduction,[0],[0]
A number of appendices in the Supplementary Material (SM) contain the mathematical setup and proofs.,1. Introduction,[0],[0]
"A significantly extended account of these events with numerous new results is available in (Balestriero & Baraniuk, 2018).",1. Introduction,[0],[0]
A deep network (DN) is an operator fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC that maps an input signal1 x ∈ RD to an output prediction ŷ ∈ RC as fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC .,2. Background on Deep Networks,[0],[0]
"All current DNs can be written as a composition of L intermediate mappings called layers
fΘ(x) =",2. Background on Deep Networks,[0],[0]
( f (L) θ(L),2. Background on Deep Networks,[0],[0]
◦ · · · ◦,2. Background on Deep Networks,[0],[0]
f (1) θ(1) ),2. Background on Deep Networks,[0],[0]
"(x), (1)
where Θ = { θ(1), . . .",2. Background on Deep Networks,[0],[0]
", θ(L) } is the collection of the network’s parameters from each layer.",2. Background on Deep Networks,[0],[0]
"This composition of mappings is nonlinear and non-commutative, in general.
",2. Background on Deep Networks,[0],[0]
A DN layer at level ` is an operator f (`) θ(`) that takes as input the vector-valued signal z(`−1)(x) ∈ RD(`−1) and produces the vector-valued output z(`)(x) ∈ RD(`) .,2. Background on Deep Networks,[0],[0]
We will assume that x and z(`) are column vectors.,2. Background on Deep Networks,[0],[0]
We initialize with z(0)(x) =,2. Background on Deep Networks,[0],[0]
"x and denote z(L)(x) =: z for convenience.
1",2. Background on Deep Networks,[0],[0]
"For concreteness, we focus here on processing K-channel images x, such as color digital photographs.",2. Background on Deep Networks,[0],[0]
"But our analysis and techniques apply to signals of any index-dimensionality, including speech and audio signals, video signals, etc.
",2. Background on Deep Networks,[0],[0]
"The signals z(`)(x) are typically called feature maps; it is easy to see that
z(`)(x) =",2. Background on Deep Networks,[0],[0]
"( f (`)
θ(`) ◦ · · · ◦ f (1) θ(1)
) (x), ` ∈ {1, . . .",2. Background on Deep Networks,[0],[0]
", L}.",2. Background on Deep Networks,[0],[0]
"(2)
We briefly overview the basic DN operators and layers we consider in this paper; more details and additional layers are provided in (Goodfellow et al., 2016) and (Balestriero & Baraniuk, 2018).",2. Background on Deep Networks,[0],[0]
"A fully connected operator performs an arbitrary affine transformation by multiplying its input by the dense matrix W (`) ∈ RD(`)×D(`−1) and adding the arbitrary bias vector b(`)W ∈ RD (`) , as in f (`)W ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= W (`)z(`−1)(x) + b (`) W .,2. Background on Deep Networks,[0],[0]
"A convolution operator reduces the number of parameters in the affine transformation by replacing the unconstrained W (`) with a multichannel convolution matrix, as in f (`)C ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= C(`)z(`−1)(x) +,2. Background on Deep Networks,[0],[0]
"b (`) C .
",2. Background on Deep Networks,[0],[0]
"An activation operator applies a scalar nonlinear activation function σ independently to each entry of its input, as in[ f (`) σ",2. Background on Deep Networks,[0],[0]
( z(`−1)(x) ),2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
k := σ,2. Background on Deep Networks,[0],[0]
"( [z(`−1)(x)]k ) , k = 1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"Nonlinearities are crucial to DNs, since otherwise the entire network would collapse to a single global affine transform.",2. Background on Deep Networks,[0],[0]
Three popular activation functions are the rectified linear unit (ReLU) σReLU(u),2. Background on Deep Networks,[0],[0]
":= max(u, 0), the leaky ReLU σLReLU(u)",2. Background on Deep Networks,[0],[0]
":= max(ηu, u), η > 0, and the absolute value σabs(u) := |u|.",2. Background on Deep Networks,[0],[0]
These three functions are both piecewise affine and convex.,2. Background on Deep Networks,[0],[0]
Other popular activation functions include the sigmoid σsig(u) := 11+e−u and hyperbolic tangent σtanh(u) := 2σsig(2u)−1.,2. Background on Deep Networks,[0],[0]
"These two functions are neither piecewise affine nor convex.
",2. Background on Deep Networks,[0],[0]
"A pooling operator subsamples its input to reduce its dimensionality according to a sub-sampling policy ρ applied over a collection of input indices {Rk}K (`)
k=1 (typically a small patch), e.g., max pooling[ f (`) ρ ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
"k
:= max d∈R(`)k
[ z(`−1)(x) ]",2. Background on Deep Networks,[0],[0]
"d , k =
1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"See (Balestriero & Baraniuk, 2018) for the definitions of average pooling, channel pooling, skip connections, and recurrent layers.
",2. Background on Deep Networks,[0],[0]
Definition 1.,2. Background on Deep Networks,[0],[0]
"A DN layer f (`) θ(`)
comprises a single nonlinear DN operator (non-affine to be precise) composed with any preceding affine operators lying between it and the preceding nonlinear operator.
",2. Background on Deep Networks,[0],[0]
"This definition yields a single, unique layer decomposition for any DN, and the complete DN is then the composition of its layers per (1).",2. Background on Deep Networks,[0],[0]
"For example, in a standard CNN, there are two different layers types: i) convolution-activation and ii) max-pooling.
",2. Background on Deep Networks,[0],[0]
We form the prediction ŷ by feeding fΘ(x) through a final nonlinearity g : RD(L) → RD(L) as in ŷ = g(fΘ(x)).,2. Background on Deep Networks,[0],[0]
"In classification, g is typically the softmax nonlinearity, which arises naturally from posing the classification inference as a
multinomial logistic regression problem (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"In regression, typically no g is applied.
",2. Background on Deep Networks,[0],[0]
"We learn the DN parameters Θ for a particular prediction task in a supervised setting using a labeled data set D = (xn,yn) N n=1, a loss function, and a learning policy to update the parameters Θ in the predictor fΘ(x).",2. Background on Deep Networks,[0],[0]
"For classification problems, the loss function is typically the negative cross-entropy LCE(x,y) (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"For regression problems, the loss function is typically is the squared error.",2. Background on Deep Networks,[0],[0]
"Since the layer-by-layer operations in a DN are differentiable almost everywhere with respect to their parameters and inputs, we can use some flavor of first-order optimization such as gradient descent to optimize the parameters Θ with respect to the loss function.",2. Background on Deep Networks,[0],[0]
"Moreover, the gradients for all internal parameters can be computed efficiently by backpropagation (Hecht-Nielsen, 1992), which follows from the chain rule of calculus.",2. Background on Deep Networks,[0],[0]
"Approximation theory is the study of how and how well functions can best be approximated using simpler functions (Powell, 1981).",3. Background on Spline Operators,[0],[0]
"A classical example of a simpler function is a spline s : RD → R (Schmidhuber, 1994).",3. Background on Spline Operators,[0],[0]
"For concreteness, we will work exclusively with affine splines in this paper (aka “linear splines”), but our ideas generalize naturally to higher-order splines.
",3. Background on Spline Operators,[0],[0]
Multivariate Affine Splines.,3. Background on Spline Operators,[0],[0]
"Consider a partition of a domain RD into a set of regions Ω = {ω1, . . .",3. Background on Spline Operators,[0],[0]
", ωR}",3. Background on Spline Operators,[0],[0]
"and a set of local mappings Φ = {φ1, . . .",3. Background on Spline Operators,[0],[0]
", φR} that map each region in the partition to R via φr(x) := 〈[α]r,·,x〉+ [β]r for x ∈ ωr.2",3. Background on Spline Operators,[0],[0]
"The parameters are: α ∈ RR×D, a matrix of hyperplane “slopes,” and β ∈ RR, a vector of hyperplane “offsets” or “biases”.",3. Background on Spline Operators,[0],[0]
We will use the terms offset and bias interchangeably in the sequel.,3. Background on Spline Operators,[0],[0]
"The notation [α]r,· denotes the column vector formed from the rth row of α.
",3. Background on Spline Operators,[0],[0]
"With this setup, the multivariate affine spline is defined as
s[α, β,Ω](x) = R∑ r=1 (〈[α]r,·,x〉+ [β]r)1(x ∈ ωr)
",3. Background on Spline Operators,[0],[0]
"=: 〈α[x],x〉+ β[x], (3)
where 1(x ∈ ωr) is the indicator function.",3. Background on Spline Operators,[0],[0]
The second line of (3) introduces the streamlined notation α[x] =,3. Background on Spline Operators,[0],[0]
"[α]r,· when x ∈ ωr; the definition for β[x] is similar.",3. Background on Spline Operators,[0],[0]
Such a spline is piecewise affine and hence piecewise convex.,3. Background on Spline Operators,[0],[0]
"However, in general, it is neither globally affine nor globally convex unless R = 1, a case we denote as a degenerate spline, since it corresponds simply to an affine mapping.
",3. Background on Spline Operators,[0],[0]
"2 To make the connection between splines and DNs more immediately obvious, here x is interpreted as a point in RD , which plays the rôle of the space of signals in the other sections.
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Functions.,3. Background on Spline Operators,[0],[0]
"A major complication of function approximation with splines in general is the need to jointly optimize both the spline parameters α, β and the input domain partition Ω (the “knots” for a 1D spline) (Bennett & Botkin, 1985).",3. Background on Spline Operators,[0],[0]
"However, if a multivariate affine spline is constrained to be globally convex, then it can always be rewritten as a max-affine spline (Magnani & Boyd, 2009; Hannah & Dunson, 2013)
s[α, β,Ω](x) = max r=1,...,R
〈[α]r,·,x〉+ [β]r .",3. Background on Spline Operators,[0],[0]
"(4)
An extremely useful feature of such a spline is that it is completely determined by its parameters α and β without needing to specify the partition Ω.",3. Background on Spline Operators,[0],[0]
"As such, we denote a max-affine spline simply as s[α, β].",3. Background on Spline Operators,[0],[0]
"Changes in the parameters α, β of a max-affine spline automatically induce changes in the partition Ω, meaning that they are adaptive partitioning splines (Magnani & Boyd, 2009).
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Operators.,3. Background on Spline Operators,[0],[0]
"A natural extension of an affine spline function is an affine spline operator (ASO) S[A,B,ΩS ] that produces a multivariate output.",3. Background on Spline Operators,[0],[0]
It is obtained simply by concatenating K affine spline functions from (3).,3. Background on Spline Operators,[0],[0]
"The details and a more general development are provided in the SM and (Balestriero & Baraniuk, 2018).
",3. Background on Spline Operators,[0],[0]
"We are particularly interested in the max-affine spline operator (MASO) S[A,B] :",3. Background on Spline Operators,[0],[0]
RD → RK formed by concatenating K independent max-affine spline functions from (4).,3. Background on Spline Operators,[0],[0]
"A MASO with slope parameters A ∈ RK×R×D and offset parameters B ∈ RK×R is defined as
S[A,B](x) =  maxr=1,...,R〈[A]1,r,·,x〉+ [B]1,r... maxr=1,...,R〈[A]K,r,·,x〉+ [B]K,r  =: A[x]x",3. Background on Spline Operators,[0],[0]
+B[x].,3. Background on Spline Operators,[0],[0]
"(5)
The second line of (5) introduces the streamlined notation in terms of the signal-dependent matrix A[x] and signal-dependent vector B[x], where [A[x]]k,· := [A]k,rk(x),· and [B[x]]k := [B]k,rk(x) with rk(x) = arg maxr〈[A]k,r,·,x〉+ [B]k,r.
Max-affine spline functions and operators are always piecewise affine and globally convex (and hence also continuous) with respect to each output dimension.",3. Background on Spline Operators,[0],[0]
"Conversely, any piecewise affine and globally convex function/operator can be written as a max-affine spline.",3. Background on Spline Operators,[0],[0]
"Moverover, using standard approximation arguments, it is easy to show that a MASO can approximate arbitrarily closely any (nonlinear) operator that is convex in each output dimension.",3. Background on Spline Operators,[0],[0]
"While a MASO is appropriate only for approximating convex functions/operators, we now show that virtually all of
today’s DNs can be written as a composition of MASOs, one for each layer.",4. DNs are Compositions of Spline Operators,[0],[0]
"Such a composition is, in general, nonconvex and hence can approximate a much larger class of functions/operators.",4. DNs are Compositions of Spline Operators,[0],[0]
"Interestingly, under certain broad conditions, the composition remains a piecewise affine spline operator, which enables a variety of insights into DNs.",4. DNs are Compositions of Spline Operators,[0],[0]
"We now state our main theoretical results, which are proved in the SM and elaborated in (Balestriero & Baraniuk, 2018).
",4.1. DN Operators are MASOs,[0],[0]
Proposition 1.,4.1. DN Operators are MASOs,[0],[0]
An arbitrary fully connected operator f (`)W is an affine mapping and hence a degenerate MASO S,4.1. DN Operators are MASOs,[0],[0]
"[ A
(`) W , B (`) W ] , with R = 1, [A(`)W ]k,1,· = [ W (`) ]",4.1. DN Operators are MASOs,[0],[0]
"k,· and
[B (`) W ]k,1 =
[ b
(`) W ] k , leading to W (`)z(`−1)(x) + b(`)W",4.1. DN Operators are MASOs,[0],[0]
"=
A (`)",4.1. DN Operators are MASOs,[0],[0]
W,4.1. DN Operators are MASOs,[0],[0]
[x]z (`−1)(x) +B (`) W,4.1. DN Operators are MASOs,[0],[0]
[x].,4.1. DN Operators are MASOs,[0],[0]
"The same is true of a convolution operator with W (`), b(`)W replaced by C (`), b (`) C .",4.1. DN Operators are MASOs,[0],[0]
Proposition 2.,4.1. DN Operators are MASOs,[0],[0]
"Any activation operator f (`)σ using a piecewise affine and convex activation function is a MASO S [ A (`) σ ,B (`) σ ] with R = 2, [ B (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1 = [ B (`) σ ] k,2 =
0 ∀k, and for ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = 0, [ A (`) σ ] k,2,· =
ek ∀k; for leaky ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = νek, [ A (`) σ ] k,2,· =
ek ∀k, ν > 0; and for absolute value [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,·
= −ek,[ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,2,· = ek ∀k, where ek represents the kth canonical basis element of RD(`) .
",4.1. DN Operators are MASOs,[0],[0]
Proposition 3.,4.1. DN Operators are MASOs,[0],[0]
"Any pooling operator f (`)ρ that is piecewise affine and convex is a MASO S [ A (`) ρ ,B (`) ρ ]",4.1. DN Operators are MASOs,[0],[0]
.3,4.1. DN Operators are MASOs,[0],[0]
"Max-
pooling has R = #Rk (typically a constant over all output dimensions k), [ A (`) ρ ] k,·,·
= {ei, i ∈ Rk}, and[ B (`) ρ ] k,r = 0 ∀k, r. Average-pooling is a degenerate
MASO with R = 1, [ A (`) ρ ] k,1,· = 1#(Rk) ∑ i∈Rk ei, and[
B (`) ρ ] k,1 = 0 ∀k.
",4.1. DN Operators are MASOs,[0],[0]
Proposition 4.,4.1. DN Operators are MASOs,[0],[0]
"A DN layer constructed from an arbitrary composition of fully connected/convolution operators followed by one activation or pooling operator is a MASO S[A(`), B(`)] such that
f (`)(z(`−1)(x))",4.1. DN Operators are MASOs,[0],[0]
= A(`)[x]z(`−1)(x) +B(`)[x].,4.1. DN Operators are MASOs,[0],[0]
"(6)
Consequently, a large class of DNs boil down to a composition of MASOs.",4.1. DN Operators are MASOs,[0],[0]
"We prove the following in the SM and in (Balestriero & Baraniuk, 2018) for CNNs, ResNets, skip connection nets, fully connected nets, and RNNs.
3",4.1. DN Operators are MASOs,[0],[0]
"This result is agnostic to the pooling type (spatial or channel).
",4.1. DN Operators are MASOs,[0],[0]
Theorem 1.,4.1. DN Operators are MASOs,[0],[0]
"A DN constructed from an arbitrary composition of fully connected/convolution, activation, and pooling operators of the types in Propositions 1–3 is a composition of MASOs that is equivalent to a global affine spline operator.
",4.1. DN Operators are MASOs,[0],[0]
"Note carefully that, while the layers of each of the DNs stated in Theorem 1 are MASOs, the composition of several layers is not necessarily a MASO.",4.1. DN Operators are MASOs,[0],[0]
"Indeed, a composition of MASOs remains a MASO if and only if all of its component operators (except the first) are non-decreasing with respect to each of their output dimensions (Boyd & Vandenberghe, 2004).",4.1. DN Operators are MASOs,[0],[0]
"Interestingly, ReLU and max-pooling are both nondecreasing, while leaky ReLU is strictly increasing.",4.1. DN Operators are MASOs,[0],[0]
"The culprits causing non-convexity of the composition of layers are negative entries in the fully connected or convolution operators, which destroy the required non-increasing property.",4.1. DN Operators are MASOs,[0],[0]
"A DN where these culprits are thwarted is an interesting special case, because it is convex with respect to its input (Amos et al., 2016) and multiconvex (Xu & Yin, 2013) with respect to its parameters.
",4.1. DN Operators are MASOs,[0],[0]
Theorem 2.,4.1. DN Operators are MASOs,[0],[0]
"A DN whose layers ` = 2, . . .",4.1. DN Operators are MASOs,[0],[0]
", L consist of an arbitrary composition of fully connected and convolution operators with nonnegative weights, i.e., W (`)k,j ≥ 0, C (`) k,j ≥ 0; non-decreasing, piecewise-affine, and convex activation operators; and non-decreasing, piecewise-affine, and convex pooling operators is globally a MASO and thus also globally convex with respect to each of its output dimensions.
",4.1. DN Operators are MASOs,[0],[0]
"The above results pertain to DNs using convex, affine operators.",4.1. DN Operators are MASOs,[0],[0]
"Other popular non-convex DN operators (e.g., the sigmoid and arctan activation functions) can be approximated arbitrarily closely by an affine spline operator but not by a MASO.
DNs are Signal-Dependent Affine Transformations.",4.1. DN Operators are MASOs,[0],[0]
"A common theme of the above results is that, for DNs constructed from fully connected/convolution, activation, and pooling operators from Propositions 1–3, the operator/layer outputs z(`)(x) are always a signal-dependent affine function of the input x (recall (5)).",4.1. DN Operators are MASOs,[0],[0]
The particular affine mapping applied to x depends on which partition of the spline it falls in RD.,4.1. DN Operators are MASOs,[0],[0]
"More on this in Section 7 below.
",4.1. DN Operators are MASOs,[0],[0]
DN Learning and MASO Parameters.,4.1. DN Operators are MASOs,[0],[0]
"Given labeled training data (xn,yn)Nn=1, learning in a DN that meets the conditions of Theorem 1 (i.e., optimizing its parameters Θ) is equivalent to optimally approximating the mapping from input x to output ŷ =",4.1. DN Operators are MASOs,[0],[0]
g ( z(L)(x) ),4.1. DN Operators are MASOs,[0],[0]
"using an appropriate cost function (e.g., cross-entropy for classification or squared error for regression) by learning the parameters θ(`) of the layers.",4.1. DN Operators are MASOs,[0],[0]
"In general the overall optimization problem is nonconvex (it is actually piecewise multi-convex in general (Rister, 2016)).
",4.1. DN Operators are MASOs,[0],[0]
"z (L) CNN(x) = W (L)
( 1∏
`=L−1
A(`)ρ",4.1. DN Operators are MASOs,[0],[0]
[x]A (`) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (`) ) ︸,4.1. DN Operators are MASOs,[0],[0]
"︷︷ ︸
ACNN[x]
x + W (L) L−1∑",4.1. DN Operators are MASOs,[0],[0]
`=1  `+1∏ j=L−1 A(j)ρ [x]A (j) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (j) (A(`)ρ [x]A(`)σ [x]b(`)C )︸ ︷︷ ︸,4.1. DN Operators are MASOs,[0],[0]
"BCNN[x] +b (L) W
(7)",4.1. DN Operators are MASOs,[0],[0]
"Combining Propositions 1–3 and Theorem 1 and substituting (5) into (2), we can write an explicit formula for the output of any layer z(`)(x) of a DN in terms of the input x for a variety of different architectures.",4.2. Application: DN Affine Mapping Formula,[0],[0]
"The formula for a standard CNN (using ReLU activation and max-pooling) is given in (7) above; we derive this formula and analogous formulas for ResNets and RNNs in (Balestriero & Baraniuk, 2018).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"In (7), A(`)σ [x] are the signal-dependent matrices corresponding to the ReLU activations,",4.2. Application: DN Affine Mapping Formula,[0],[0]
A(`)ρ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] are the signal-dependent matrices corresponding to maxpooling, and the biases b(L)W , b (`) C arise directly from the fully connected and convolution operators.",4.2. Application: DN Affine Mapping Formula,[0],[0]
The absence of B (`) σ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x], B (`) ρ",4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] is due to the absence of bias in the ReLU (recall (2)) and max-pooling operators (recall (3)).
",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Inspection of (7) reveals the exact form of the signaldependent, piecewise affine mapping linking x to z(L)CNN(x).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Moreover, this formula can be collapsed into
z (L) CNN(x) = W (L) ( ACNN[x]x +BCNN[x] )",4.2. Application: DN Affine Mapping Formula,[0],[0]
"+ b (L) W (8)
from which we can recognize
z",4.2. Application: DN Affine Mapping Formula,[0],[0]
(L−1) CNN (x) = ACNN[x]x,4.2. Application: DN Affine Mapping Formula,[0],[0]
"+BCNN[x] (9)
as an explicit, signal-dependent, affine formula for the featurization process that aims to convert x into a set of (hopefully) linearly separable features that are then input to the linear classifier in layer ` = L with parameters W (L) and b
(L) W .",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Of course, the final prediction ŷ is formed by running z (L) CNN(x) through a softmax nonlinearity g, but this merely rescales its entries to create a probability distribution.",4.2. Application: DN Affine Mapping Formula,[0],[0]
We now dig deeper into (8) in order to bridge DNs and classical optimal classification theory.,5. DNs are Template Matching Machines,[0],[0]
"While we focus on CNNs and classification for concreteness, our analysis holds for any DN meeting the conditions of Theorem 1.",5. DNs are Template Matching Machines,[0],[0]
An alternate interpretation of (8) is that z(L)CNN(x) is the output of a bank of linear matched filters (plus a set of biases).,5.1. Template Matching,[0],[0]
"That is, the cth element of z(L)(x) equals the inner product between the signal x and the matched filter for the cth class, which is contained in the cth row of the matrix
W (L)A[x].",5.1. Template Matching,[0],[0]
"The bias W (L)B[x] + b(L)W can be used to account for the fact that some classes might be more likely than others (i.e., the prior probability over the classes).",5.1. Template Matching,[0],[0]
"It is well-known that a matched filterbank is the optimal classifier for deterministic signals in additive white Gaussian noise (Rabiner & Gold, 1975).",5.1. Template Matching,[0],[0]
"Given an input x, the class decision is simply the index of the largest element of z(L)(x).4
Yet another interpretation of (8) is that z(L)(x) is computed not in a single matched filter calculation but hierarchically as the signal propagates through the DN layers.",5.1. Template Matching,[0],[0]
Abstracting (5) to write the per-layer maximization process as z(`)(x) = maxr(`),5.1. Template Matching,[0],[0]
A (`) r(`) z(`−1)(x),5.1. Template Matching,[0],[0]
"+B (`) r(`) and cascading, we obtain a formula for the end-to-end DN mapping
z(L)(x) =",5.1. Template Matching,[0],[0]
"W (L) max r(L−1)
",5.1. Template Matching,[0],[0]
"( A
(L−1) r(L−1)
max r(2)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .
",5.1. Template Matching,[0],[0]
"max r(1)
",5.1. Template Matching,[0],[0]
"( A (1)
r(1) x + B (1) r(1)
) + B (2)
r(2)
) · · ·+ B(L−1)
r(L−1)
) + b
(L) W .
",5.1. Template Matching,[0],[0]
"(10)
",5.1. Template Matching,[0],[0]
"This formula elucidates that a DN performs a hierarchical, greedy template matching on its input, a computationally efficient yet sub-optimal template matching technique.",5.1. Template Matching,[0],[0]
Such a procedure is globally optimal when the DN is globally convex.,5.1. Template Matching,[0],[0]
Corollary 1.,5.1. Template Matching,[0],[0]
"For a DN abiding by the requirements of Theorem 2, the computation (10) collapses to the following globally optimal template matching
z(L−1)(x)",5.1. Template Matching,[0],[0]
"= W (L) max r(L−1),r(2),...,r(1)
( A
(L−1) r(L−1)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .",5.1. Template Matching,[0],[0]
"(
A (1)
r(1) x + B
(1) r(1)
) +",5.1. Template Matching,[0],[0]
"B (2)
r(2)
) · · ·+ B(L−1
r(L−1)
)",5.1. Template Matching,[0],[0]
"+ b
(L) W .
(11)",5.1. Template Matching,[0],[0]
"Since the complete DN mapping (up to the final softmax) can be expressed as in (8), given a signal x, we can compute the signal-dependent template for class c via A[x]c =",5.2. Template Visualization Examples,[0],[0]
"d[z(L)(x)]c dx , which can be efficiently computed via backpropogation (Hecht-Nielsen, 1992).5",5.2. Template Visualization Examples,[0],[0]
"Once the template A[x]c has been computed, the bias term b[x]c can be computed via b[x]c = z(L)(x)c − 〈A[x]c,·,x〉.",5.2. Template Visualization Examples,[0],[0]
"Figure 1 plots
4Again, since the softmax merely rescales the entries of z(L)(x) into a probability distribution, it does not affect the location of its largest element.
",5.2. Template Visualization Examples,[0],[0]
"5In fact, we can use the same backpropagation procedure used for computing the gradient with respect to a fully connected or
various signal-dependent templates for two CNNs trained on the MNIST and CIFAR10 datasets.",5.2. Template Visualization Examples,[0],[0]
"Under the matched filterbank interpretation of a DN developed in Section 5.1, the optimal template for an image x of class c is a scaled version of x itself.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
But what are the optimal templates for the other (incorrect) classes?,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In an idealized setting, we can answer this question.
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
Proposition 5.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Consider an idealized DN consisting of a composition of MASOs that has sufficient approximation power to span arbitrary MASO matrices A[xn] from (9) for any input xn from the training set.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"Train the DN to classify among C classes using the training data D = (xn, yn)Nn=1 with normalized inputs ‖xn‖2 = 1 ∀n and the cross-entropy loss LCE(yn, fΘ(xn)) with the addition of the regularization constraint that ∑ c ‖A[xn]c,·‖2",5.3. Collinear Templates and Data Set Memorization,[0],[0]
< α with α > 0.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"At the global minimum of this constrained optimization problem, the rows of A?[xn] (the optimal templates) have the form:
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"[A?[xn]]c,· =  + √ (C−1)α C xn, c = yn − √
α C(C−1) xn, c 6=",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"yn
(12)
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In short, the idealized CNN in the proposition will memorize a set of collinear templates whose bimodal outputs force
convolution weight but instead with the input x.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"This procedure is becoming increasingly popular in the study of adversarial examples (Szegedy et al., 2013).
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
the softmax output to a Dirac delta function (aka 1-hot representation) that peaks at the correct class.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Figure 2 confirms this bimodal behavior on the MNIST and CIFAR10 datasets.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"While a DN’s signal-dependent matched filterbank (8) is optimized for classifying signals immersed in additive white Gaussian noise, such a statistical model is overly simplistic for most machine learning problems of interest.",6. New DNs with Orthogonal Templates,[0],[0]
"In practice, errors will arise not just from random noise but also from nuisance variations in the inputs such as arbitrary rotations, positions, and modalities of the objects of interest.",6. New DNs with Orthogonal Templates,[0],[0]
The effects of these nuisances are only poorly approximated as Gaussian random errors.,6. New DNs with Orthogonal Templates,[0],[0]
"Limited work has been done on filterbanks for classification in nonGaussian noise; one promising direction involves using not matched but rather orthogonal templates (Eldar & Oppenheim, 2001).
",6. New DNs with Orthogonal Templates,[0],[0]
"For a MASO DN’s templates to be orthogonal for all inputs, it is necessary that the rows of the matrix W (L) in the final linear classifier layer be orthogonal.",6. New DNs with Orthogonal Templates,[0],[0]
"This weak constraint on the DN still enables the earlier layers to create a high-performance, class-agnostic, featurized representation (recall the discussion just below (9)).",6. New DNs with Orthogonal Templates,[0],[0]
"To create orthogonal templates during learning, we simply add to the standard (potentially regularized) cross-entropy loss function LCE a term that penalizes non-zero off-diagonal entries in the matrix W (L)(W (L))T leading to the new loss with the additional penalty
LCE + λ ∑ c1 6=c2 ∣∣∣〈[W",6. New DNs with Orthogonal Templates,[0],[0]
"(L)] c1,· , [ W (L) ]",6. New DNs with Orthogonal Templates,[0],[0]
"c2,· 〉∣∣∣2 .",6. New DNs with Orthogonal Templates,[0],[0]
"(13) The parameter λ controls the tradeoff between cross-entropy
minimization and orthogonality preservation.",6. New DNs with Orthogonal Templates,[0],[0]
"Conveniently, when minimizing (13) via backpropagation, the orthogonal rows of W (L) induce orthogonal backpropagation updates for the various classes.
",6. New DNs with Orthogonal Templates,[0],[0]
We now empirically demonstrate that orthogonal templates lead to significantly improved classification performance.,6. New DNs with Orthogonal Templates,[0],[0]
"We conducted a range of experiments with three different conventional DN architectures – smallCNN, largeCNN, and ResNet4-4 – trained on three different datasets – SVHN, CIFAR10, and CIFAR100.",6. New DNs with Orthogonal Templates,[0],[0]
"Each DN employed bias units, ReLU activations, and max-pooling as well as batchnormalization prior each ReLU.",6. New DNs with Orthogonal Templates,[0],[0]
"The full experimental details are given in the (Balestriero & Baraniuk, 2018).",6. New DNs with Orthogonal Templates,[0],[0]
"For learning, we used the Adam optimizer with an exponential learning rate decay.",6. New DNs with Orthogonal Templates,[0],[0]
All inputs were centered to zero mean and scaled to a maximum value of one.,6. New DNs with Orthogonal Templates,[0],[0]
"No further preprocessing was performed, such as ZCA whitening (Nam et al., 2014).",6. New DNs with Orthogonal Templates,[0],[0]
We assessed how the classification performance of a given DN would change as we varied the orthogonality penalty λ in (13).,6. New DNs with Orthogonal Templates,[0],[0]
"For each configuration of DN architecture, training dataset, learning rate, and penalty λ, we averaged over 15 runs to estimate the average performance and standard deviation.
",6. New DNs with Orthogonal Templates,[0],[0]
We report here on only the CIFAR100 with largeCNN experiments.,6. New DNs with Orthogonal Templates,[0],[0]
"(See (Balestriero & Baraniuk, 2018) for detailed results for all three datasets and the other architectures.",6. New DNs with Orthogonal Templates,[0],[0]
The trends for all three datasets are similar and are independent of the learning rate.),6. New DNs with Orthogonal Templates,[0],[0]
The results for CIFAR100 in Figure 3 indicate that the benefits of the orthogonality penalty emerge distinctly as soon as λ > 0.,6. New DNs with Orthogonal Templates,[0],[0]
"In addition to improved final accuracy and generalization performance, we see that template orthogonality reduces the temptation of the DN to overfit.",6. New DNs with Orthogonal Templates,[0],[0]
"(This is is especially visible in the examples in (Balestriero & Baraniuk, 2018).)",6. New DNs with Orthogonal Templates,[0],[0]
One explanation is that the orthogonal weights W (L) positively impact not only the prediction but also the backpropagation via orthogonal gradient updates with respect to each output dimension’s partial derivatives.,6. New DNs with Orthogonal Templates,[0],[0]
"Like any spline, it is the interplay between the (affine) spline mappings and the input space partition that work the magic in a MASO DN.",7. DN’s Intrinsic Multiscale Partition,[0],[0]
Recall from Section 3 that a MASO has the attractive property that it implicitly partitions its input space as a function of its slope and offset parameters.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
The induced partition Ω opens up a new geometric avenue to study how a DN clusters and organizes signals in a hierarchical fashion.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
"A DN operator at level ` directly influences the partitioning of its input space RD(`−1) and indirectly influences the partitioning of the overall signal space RD.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
A ReLU activation operator splits each of its input dimensions into two half-planes depending on the sign of the input in each dimension.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"This partitions RD(`−1) into a combinatorially large number (up to 2D (`)
) of regions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Following a fully connected or convolution operator with a ReLU simply rotates the partition in RD(`−1) .,7.1. Effect of the DN Operators on the Partition,[0],[0]
"A max-pooling operator also partitions RD(`−1) into a combinatorially large number (up to #RD (`)
) of regions, where #R is the size of the pooling region.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
This per-MASO partitioning of each layer’s input space constructs an overall partitioning of the input signal space RD.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"As each MASO is applied, it subdivides the input space RD into finer and finer partitions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"The final partition corresponds to the intersection of all of the intermediate partitions, and hence we can encode the input in terms of the ordered collection of per-layer partition regions into which it falls.",7.1. Effect of the DN Operators on the Partition,[0],[0]
This overall process can be interpreted as a hierarchical vector quantization (VQ) of the training input signals xn.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"There are thus many potential connections between DNs and optimal quantization, information theory, and clustering that we leave for future research.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"See (Balestriero & Baraniuk, 2018) for some early results.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Unfortunately there is no simple formula for the partition of the signal space.,7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"However, once can obtain the set of inputs signals xn that fall into the same partition region at each layer of a DN.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"At layer `, denote the index of the region selected by the input x (recall (6)) by[ t(`)(x) ]",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"k
= arg max r
〈",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[A(`)]k,r,·, z (`−1)(x) 〉 +",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[B(`)]k,r.
(14)
Thus, [t(`)]k ∈ {1, . . .",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
", R(`)}, with R(`) the number of partition regions in the layer’s input space.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Encoding the partition as an ordered collection of integers designating the activate hyperplane parameters from (4), we can now visualize which inputs fall into the same or nearby partitions.
",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Due to the very large number of possible regions (up to 2D (`) for a ReLU at layer `) and the limited amount of training data, in general, many partitions will be empty or contain only a single training data point.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"To validate the utility of the hierarchical intrinsic clustering induced by a DN, we define a new distance function between the signals x1 and x2 that quantifies the similarity of their position encodings t`(x1) and t`(x2) at layer ` via
d ( t(`)(x1), t (`)(x2) )
",7.3. A New Image Distance based on the DN Partition,[0],[0]
"= 1− ∑D(`) k=1 1 ( [t(`)(x1)]k = [t (`)(x2)]k )
D(`) .",7.3. A New Image Distance based on the DN Partition,[0],[0]
"(15)
For a ReLU MASO, this corresponds simply to counting how many entries of the layer inputs for x1 and x2 are positive or negative at the same positions.",7.3. A New Image Distance based on the DN Partition,[0],[0]
"For a max-pooling
MASO, this corresponds to counting how many argmax positions are the same in each patch for x1 and x2.
",7.3. A New Image Distance based on the DN Partition,[0],[0]
Figure 4 provides a visualization of the nearest neighbors of a test image under this partition-based distance measure.,7.3. A New Image Distance based on the DN Partition,[0],[0]
"Visual inspection of the figures highlights that, as we progress through the layers of the DN, similar images become closer in the new distance but further in Euclidean distance.",7.3. A New Image Distance based on the DN Partition,[0],[0]
We have used the theory of splines to build a rigorous bridge between deep networks (DNs) and approximation theory.,8. Conclusions,[0],[0]
"Our key finding is that, conditioned on the input signal, the output of a DN can be written as a simple affine transformation of the input.",8. Conclusions,[0],[0]
"This links DNs directly to the classical theory of optimal classification via matched filters and provides insights into the positive effects of data memorization.
",8. Conclusions,[0],[0]
"There are many avenues for future work, including a more in-depth analysis of the hierarchical MASO partitioning, particularly from the viewpoint of vector quantization and K-means clustering, which are unsupervised learning techniques, and information theory.",8. Conclusions,[0],[0]
The spline viewpoint also could inspire the creation of new DN layers that have certain attractive partitioning or approximation capabilities.,8. Conclusions,[0],[0]
"We have begun exploring some of these directions in (Balestriero & Baraniuk, 2018).6
6 This work was partially supported by ARO grant W911NF-151-0316, AFOSR grant FA9550-14-1-0088, ONR grants N0001417-1-2551 and N00014-18-12571, DARPA grant G001534-7500, and a DOD Vannevar Bush Faculty Fellowship (NSSEFF) grant N00014-18-1-2047.",8. Conclusions,[0],[0]
We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators.,abstractText,[0],[0]
"Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings.",abstractText,[0],[0]
"For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input.",abstractText,[0],[0]
"This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization.",abstractText,[0],[0]
"Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture.",abstractText,[0],[0]
The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion.,abstractText,[0],[0]
"As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.",abstractText,[0],[0]
A Spline Theory of Deep Networks,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis.",1 Introduction,[0],[0]
"It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006;
∗Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041).
",1 Introduction,[0],[0]
"Yoshida et al., 2014).",1 Introduction,[0],[0]
"Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016).",1 Introduction,[0],[0]
"In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014).",1 Introduction,[0],[0]
"This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs.
",1 Introduction,[0],[0]
"Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015).",1 Introduction,[0],[0]
"Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014).",1 Introduction,[0],[0]
"However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance.",1 Introduction,[0],[0]
"Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014).",1 Introduction,[0],[0]
"They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and
2263
Zhao, 2016), also including discourse parsing.",1 Introduction,[0],[0]
Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics.,1 Introduction,[0],[0]
Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance.,1 Introduction,[0],[0]
"Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving.
",1 Introduction,[0],[0]
"The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives.",1 Introduction,[0],[0]
Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments.,1 Introduction,[0],[0]
"The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments.",1 Introduction,[0],[0]
"To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015).",1 Introduction,[0],[0]
"Our method will be evaluated on the benchmark dataset against state-of-the-art methods.
",1 Introduction,[0],[0]
"The rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, and Section 4 concludes this paper.",1 Introduction,[0],[0]
"The architecture of the model, as shown in Figure 1, is straightforward.",2 Method,[0],[0]
It can be divided into three parts: 1) CNN for modeling arguments; 2) CGNN unit for feature transformation; 3) a conventional softmax layer for the final classification.,2 Method,[0],[0]
"CNN is used to obtain the vector representations for the sentences, CGNN further captures and transforms the features for the final classification.",2 Method,[0],[0]
"As CNN has been broadly adopted for modeling sentences, we will explain it in brevity.",2.1 Convolutional Neural Network,[0],[0]
"For two arguments, typical sentence modeling process
will be applied: sentence embedding (including embeddings for words and part-of-speech (POS) tags) through projection layer, convolution operations (with multiple groups of filters) through the convolution layer, obtaining the sentence representation through one-max-pooling.",2.1 Convolutional Neural Network,[0],[0]
"The two arguments will get their sentence vectors independently without any interfering, and the convolution operation will be the same by sharing parameters.",2.1 Convolutional Neural Network,[0],[0]
The final argument-pair representation will be the vector v which is concatenated from two sentence vectors and this vector will be used as the input of the CGNN unit.,2.1 Convolutional Neural Network,[0],[0]
"For implicit sense classification, the key is how to effectively capture the interactions between the two arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
"The interactions could be word pairs, phrase pairs or even the latent meaning of the two full arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
Pitler et al. (2009) has shown that word pair features are helpful.,2.2 Collaborative Gated Neural Network,[0],[0]
"To model these interactions, we have to make a full use of the sentence vectors obtained from CNN.",2.2 Collaborative Gated Neural Network,[0],[0]
"However, common neural hidden layers might be insufficient to deal with the challenge.",2.2 Collaborative Gated Neural Network,[0],[0]
"We need to seek more powerful neural models, i.e., gated neural network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"In recent years, gated mechanism has gained popularity in neural models.",2.2 Collaborative Gated Neural Network,[0],[0]
"Although it is first introduced in the cells of recurrent neural networks, like Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014), traditional feed-forward neural models such as the Highway Network could also benefit from it (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"The existing studies show that the gated mechanism in highway network serves not only a means for easier training, but also a tool to route information in a trained network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Motivated by the idea of highway network, we propose a collaborative gated neural network (CGNN) for this task.",2.2 Collaborative Gated Neural Network,[0],[0]
"The architecture of CGNN is illustrated in Figure 1, and it contains a sequence of transformations.",2.2 Collaborative Gated Neural Network,[0],[0]
"First, the inner-cell ĉ is obtained through linear transformation and non-linear activation on the input v, and this process is exactly the operation of an ordinary neural layer.
",2.2 Collaborative Gated Neural Network,[0],[0]
"ĉ = tanh(Wc · v + bc)
",2.2 Collaborative Gated Neural Network,[0],[0]
"Meanwhile, the two gates gi and go are calculated independently because they are only influenced by
the original input through different parameters:
gi = σ(W i · v + bi) go = σ(W o · v + bo)
where the σ denotes sigmoid function which guarantees the values in the gates are in [0,1].",2.2 Collaborative Gated Neural Network,[0],[0]
"Two gated operations are applied sequentially, where a gated operation indicates the element-wise multiplication of an inner-cell and a gate.",2.2 Collaborative Gated Neural Network,[0],[0]
"Between the two gated operations, a non-linear activation operation is applied.",2.2 Collaborative Gated Neural Network,[0],[0]
"The procedure could be formulated as follows:
c = ĉ gi h = tanh(c) go
where denotes element-wise multiplication, c is the second inner-cell and h is the output of CGNN unit.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Although the two gates are generated independently, they will work collaboratively because they control the information flow of the inner-cells sequentially which resembles logical AND operation in a probabilistic version.",2.2 Collaborative Gated Neural Network,[0],[0]
"In fact, the transformations after ĉ will concern only element-wise operations which might give finer controls for each dimension, and the information can only flow on the dimensions where both gates are “open”.",2.2 Collaborative Gated Neural Network,[0],[0]
"This procedure will help select the most crucial features.
",2.2 Collaborative Gated Neural Network,[0],[0]
The gates in this model are mainly used for routing information from sentence-pairs vectors.,2.2 Collaborative Gated Neural Network,[0],[0]
"When there is only one gate in our network, the model works similar to the highway network (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"After the transformation of the CGNN unit, the transformed vector h will be sent to a conventional softmax for classification.
",2.3 Output and Training,[0],[0]
"The training object J will be the cross-entropy error E with L2 regularization:
E(ŷ, y) =",2.3 Output and Training,[0],[0]
"− l∑
j
yj × log(Pr(ŷj))
",2.3 Output and Training,[0],[0]
J(θ),2.3 Output and Training,[0],[0]
"= 1
m
m∑
k
E(ŷ(k), y(k))",2.3 Output and Training,[0],[0]
+,2.3 Output and Training,[0],[0]
"λ
2 ‖θ‖2
where yj is the gold label and ŷj is the predicted one.",2.3 Output and Training,[0],[0]
"We adopt the diagonal variant of AdaGrad (Duchi et al., 2011) for the optimization process.",2.3 Output and Training,[0],[0]
"As for the benchmark dataset, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) corpus1 is used for evaluation.",3.1 Setting,[0],[0]
"In the PDTB, each discourse relation is annotated between two argument spans.
",3.1 Setting,[0],[0]
"To be consistent with the setups of prior works, we formulate the implicit relation classification task as four one-versus-other binary classification problems only using the four top level classes: COMPARISON (COMP.), CONTINGENCY (CONT.), EXPANSION (EXP.) and TEMPORAL (TEMP.).",3.1 Setting,[0],[0]
"While different works include different relations of varying specificities, all of them include these four core relations (Pitler et al., 2009).",3.1 Setting,[0],[0]
"Following dataset splitting convention of the previous works, we use sections 2-20 for training, sections 21-22 for testing and sections 0-1 for development set.",3.1 Setting,[0],[0]
"The proposed model is possible to be extended for multi-class classification of discourse parsing, but for the comparisons with most of previous works, we will follow them and focus on the binary classification problems.
",3.1 Setting,[0],[0]
"For other hyper-parameters of the model and training process, we fix the lengths of both the input arguments to be 80, and apply truncating or zero-padding when necessary.",3.1 Setting,[0],[0]
"The dimensions for word embeddings and POS embeddings are respectively 300 and 50, and the embedding layer adopts a dropout of 0.2.",3.1 Setting,[0],[0]
"The word embeddings are initialized with pre-trained word vectors using word2vec 2 (Mikolov et al., 2013) and other parameters are randomly initialized including POS embeddings.",3.1 Setting,[0],[0]
"We
1http://www.seas.upenn.edu/˜pdtb/ 2http://www.code.google.com/p/word2vec
set the starting learning rate to 0.001.",3.1 Setting,[0],[0]
"For CNN model, we utilize three groups of filters with window widths of (2, 2, 2) and their filter numbers are all set to 1024.",3.1 Setting,[0],[0]
The hyper-parameters are the same for all models and we do not tune them individually.,3.1 Setting,[0],[0]
"For transformation of sentence vectors, a simple Multilayer Perceptron (MLP) layer could be a straightforward choice, while more complex neural modules, such as LSTM and highway network, could also be considered.",3.2 Model Analysis,[0],[0]
Our model utilizes a CGNN unit with refined gated mechanism for the transformation.,3.2 Model Analysis,[0],[0]
Will the proposed CGNN really bring about further performance improvement?,3.2 Model Analysis,[0],[0]
"We now answer this question empirically.
",3.2 Model Analysis,[0],[0]
"As shown in Table 1, CNN model usually performs well on its own.",3.2 Model Analysis,[0],[0]
"Utilizing an MLP layer or a Highway layer could improve the accuracies on CONTINGENCY, EXPANSION, TEMPORARY except for COMPARISON.",3.2 Model Analysis,[0],[0]
"Though the primary motivation of Highway is to ease gradient-based training of highly deep networks through utilizing gated units, it works merely as an ordinary MLP in the proposed model, which explains the reason that it performs like MLP.",3.2 Model Analysis,[0],[0]
"Despite one of four classes, COMPARISON, not receiving performance improvement, introducing a non-linear transformation layer lets the classification benefit as a whole.",3.2 Model Analysis,[0],[0]
"“CNN+LSTM” denotes the method of using LSTM to read the convolution sequence (without pooling operation), and it even does not perform better than MLP.
",3.2 Model Analysis,[0],[0]
The CGNN achieves the best performance on all classes including COMPARISON.,3.2 Model Analysis,[0],[0]
It gains 3.97% imrovement on average F1 score using CNN only model.,3.2 Model Analysis,[0],[0]
"We assume that CGNN is well-suited to work with CNN, adaptively transforming and combining local features detected by the individual filters.",3.2 Model Analysis,[0],[0]
We show the main results in Tables 2 and 3.,3.3 Results,[0],[0]
"The metrics include precision (P), recall (R), accuracy (Acc) and F1 score.",3.3 Results,[0],[0]
"Since not all of these metrics are reported in previous work, the comparisons are correspondingly in Table 2 and 3.",3.3 Results,[0],[0]
"Some previous work merges Entrel with Expansion, which is also explored in our study and noted as EXP.+.
",3.3 Results,[0],[0]
We compare with best-performed or competitive models including both traditional linear methods and recent neural methods.,3.3 Results,[0],[0]
"For traditional methods: Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features; Zhou et al. (2010) improve the performance through predicting connective words as features; Park and Cardie (2012) propose a locallyoptimal feature set and further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up; Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the learning; Rutherford and Xue (2014) employ Brown cluster pair and coreference patterns for performance enhancement.",3.3 Results,[0],[0]
"Several neural methods have also been included for comparison: Zhang et al. (2015) propose a simplified neural network which has only
three different pooling operations (max, min, average); Ji and Eisenstein (2015) compute distributed semantics representation by composition up the syntactic parse tree through recursive neural network; Braud and Denis (2015) consider shallow lexical features and word embeddings.",3.3 Results,[0],[0]
Chen et al. (2016) replace the original words by word embeddings to overcome the data sparsity problem and they also utilize gated relevance network to capture the semantic interaction between word pairs.,3.3 Results,[0],[0]
"The gated network is different from ours but also works well.
",3.3 Results,[0],[0]
"Our model achieves F-measure improvements of 1.85% on COMPARISON, 1.56% on CONTINGENCY, 1.27% on EXPANSION, 0.94% on EXPANSION+, 4.89% on TEMPORAL, against the state-ofthe-art of each class.",3.3 Results,[0],[0]
We improve by 4.73% on average F1 score when not including ENTREL in EXPANSION as reported in Table 2 and 3.19% on average F1 score otherwise as reported in Table 3.,3.3 Results,[0],[0]
The results show that our model achieves the best performance and especially makes the most remarkable progress on TEMPORAL.,3.3 Results,[0],[0]
"In this paper, we propose a stacking gated neural architecture for implicit discourse relation classification.",4 Conclusion,[0],[0]
Our model includes convolution and collaborative gated neural network.,4 Conclusion,[0],[0]
The analysis and experiments show that CNN performs well on its own and combining CGNN provides further gains.,4 Conclusion,[0],[0]
Our evaluation on PTDB shows that the proposed model outperforms previous state-of-the-art systems.,4 Conclusion,[0],[0]
Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks.,abstractText,[0],[0]
Implicit discourse relation classification is the bottleneck for discourse parsing.,abstractText,[0],[0]
"Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred.",abstractText,[0],[0]
This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation.,abstractText,[0],[0]
Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems.,abstractText,[0],[0]
A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030–1040, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
A verb plays a primary role in conveying the meaning of a sentence.,1 Introduction,[0],[0]
"Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP.
",1 Introduction,[0],[0]
Verb classes are one such lexical resource.,1 Introduction,[0],[0]
"Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior.",1 Introduction,[0],[0]
"Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009).
",1 Introduction,[0],[0]
"There have also been many attempts to automatically acquire verb classes with the goal of ei-
ther adding frequency information to an existing resource or of inducing similar verb classes for other languages.",1 Introduction,[0],[0]
"Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013).",1 Introduction,[0],[0]
"This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses.",1 Introduction,[0],[0]
"Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008).
",1 Introduction,[0],[0]
"In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy.",1 Introduction,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,1 Introduction,[0],[0]
"By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy.
",1 Introduction,[0],[0]
"Our novel contributions are summarized as follows:
• induce both semantic frames and verb classes from a massive amount of verb uses by a scalable method,
• explicitly deal with verb polysemy, • discover effective features for each of the
clustering steps, and
• quantitatively evaluate a soft clustering of verbs.
1030",1 Introduction,[0],[0]
"As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous.",2 Related Work,[0],[0]
"A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points.",2 Related Work,[0],[0]
"As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance.
",2 Related Work,[0],[0]
"Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods.",2 Related Work,[0],[0]
Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions.,2 Related Work,[0],[0]
"They evaluated their result with a gold-standard test set, where a single class is assigned to a verb.",2 Related Work,[0],[0]
Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features.,2 Related Work,[0],[0]
Parisien and Stevenson (2011) extended their model by adding semantic features.,2 Related Work,[0],[0]
They tried to account for verb learning by children and did not evaluate the resultant verb classes.,2 Related Work,[0],[0]
"Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985).",2 Related Work,[0],[0]
"All of the above methods considered verbs to be monosemous and did not deal with verb polysemy.
",2 Related Work,[0],[0]
"Our approach also uses Bayesian methods, but is designed to capture verb polysemy.
",2 Related Work,[0],[0]
"We summarize a few studies that consider polysemy of verbs in the rest of this section.
",2 Related Work,[0],[0]
Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy.,2 Related Work,[0],[0]
"Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007).",2 Related Work,[0],[0]
"Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives.
",2 Related Work,[0],[0]
"The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs.",2 Related Work,[0],[0]
They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.,2 Related Work,[0],[0]
"In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999).",2 Related Work,[0],[0]
"However, the verb itself is still represented as a single data point.",2 Related Work,[0],[0]
"After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering.",2 Related Work,[0],[0]
They considered multiple classes only in the gold-standard data used for their evaluations.,2 Related Work,[0],[0]
"We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993).
",2 Related Work,[0],[0]
Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb.,2 Related Work,[0],[0]
"These models are approximated to condition not
on verbs but on subcategorization frames.",2 Related Work,[0],[0]
"As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy.",2 Related Work,[0],[0]
"They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models.",2 Related Work,[0],[0]
Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet.,2 Related Work,[0],[0]
This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle.,2 Related Work,[0],[0]
"Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure.
",2 Related Work,[0],[0]
"Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013).",2 Related Work,[0],[0]
LDA-frames are probabilistic semantic frames automatically induced from a raw corpus.,2 Related Work,[0],[0]
"He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles.",2 Related Work,[0],[0]
Both of these are represented as a probabilistic distribution of words across verbs.,2 Related Work,[0],[0]
"He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012).",2 Related Work,[0],[0]
"He did not evaluate the resulting frames as verb classes.
",2 Related Work,[0],[0]
"In sum, there have been no studies that quantitatively evaluate polysemous verb classes automatically induced by unsupervised methods.",2 Related Work,[0],[0]
Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches.,3.1 Overview,[0],[0]
"Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost.",3.1 Overview,[0],[0]
"For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011).",3.1 Overview,[0],[0]
"Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013).",3.1 Overview,[0],[0]
"However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models.
",3.1 Overview,[0],[0]
"In this paper, we propose a two-step approach for inducing semantic frames and verb classes.",3.1 Overview,[0],[0]
"First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)).",3.1 Overview,[0],[0]
"To do that, we induce verb-specific semantic frames by clustering verb uses.",3.1 Overview,[0],[0]
"Then, we induce verb classes by clustering these verbspecific semantic frames across verbs.",3.1 Overview,[0],[0]
"An interesting point here is that we can use exactly the same method for these two clustering steps.
",3.1 Overview,[0],[0]
"Our procedure to automatically induce verb classes from verb uses is summarized as follows:
1.",3.1 Overview,[0],[0]
"induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and
2.",3.1 Overview,[0],[0]
"induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1.
",3.1 Overview,[0],[0]
Each of these two steps is described in the following sections in detail.,3.1 Overview,[0],[0]
We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014).,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"Our semantic frames consist of case slots, each of which consists of word instances that can be filled.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"The procedure for inducing these semantic frames is as follows:
1.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses,
2.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and
1In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"To reach 1,000 iterations, which are reported to be optimum, it would take three months.
3.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames.
",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
These three steps are briefly described below.,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
We apply dependency parsing to a large raw corpus.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Then, we extract predicate-argument structures from the dependency parses.",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Dependents that have the following dependency relations to a verb are extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗
In this process, the verb and arguments are lemmatized, and only the head of an argument is preserved for compound nouns.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
Predicate-argument structures are collected for each verb and the subsequent processes are applied to the predicate-argument structures of each verb.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
These initial frames are the input of the subsequent clustering process.,3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We select an argument in the following order by considering the degree of effect on the verb sense:3
dobj, ccomp, nsubj, prep ∗, iobj.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"After this process, we discard minor initial frames that occur fewer than 10 times.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance.
",3.2.3 Clustering Method,[0],[0]
"We calculate the posterior probability of a cluster cj given an initial frame fi as follows:
P (cj |fi) ∝",3.2.3 Clustering Method,[0],[0]
"{ n(cj) N+α · P (fi|cj) cj ̸= new
α N+α · P (fi|cj) cj",3.2.3 Clustering Method,[0],[0]
"= new,
(1)
where N is the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj .",3.2.3 Clustering Method,[0],[0]
α is a hyperparameter that determines how likely it is for a new cluster to be created.,3.2.3 Clustering Method,[0],[0]
"In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi.
P (fi|cj) is defined based on the DirichletMultinomial distribution as follows:
P (fi|cj) = ∏ w∈V",3.2.3 Clustering Method,[0],[0]
"P (w|cj)count(fi,w), (2)
where V is the vocabulary in all case slots cooccurring with the verb and count(fi, w) is the number of w in the initial frame fi.",3.2.3 Clustering Method,[0],[0]
"The original method in Kawahara et al. (2014) defined w as pairs of slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not consider slot-only features, e.g., “nsubj” and “dobj,” which ignore lexical information.",3.2.3 Clustering Method,[0],[0]
"Here we experiment with both representations and compare the results.
",3.2.3 Clustering Method,[0],[0]
"P (w|cj) is defined as follows:
P (w|cj) =",3.2.3 Clustering Method,[0],[0]
"count(cj , w) + β∑ t∈V",3.2.3 Clustering Method,[0],[0]
"count(cj , t) + |V",3.2.3 Clustering Method,[0],[0]
"| · β , (3)
where count(cj , w) is the current number of w in the cluster cj , and β is a hyper-parameter of Dirichlet distribution.",3.2.3 Clustering Method,[0],[0]
"For a new cluster, this probability is uniform (1/|V |).
",3.2.3 Clustering Method,[0],[0]
"We regard each output cluster as a semantic frame, by merging the initial frames in a cluster into a semantic frame.",3.2.3 Clustering Method,[0],[0]
"In this way, semantic frames for each verb are acquired.
",3.2.3 Clustering Method,[0],[0]
We use Gibbs sampling to realize this clustering.,3.2.3 Clustering Method,[0],[0]
"To induce verb classes across verbs, we apply clustering to the induced verb-specific semantic
frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"This is because an initial frame has the same structure as a semantic frame, which is produced by merging initial frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"We regard each output cluster as a verb class this time.
",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"For the features, w, in equation (2), we try the two representations again: slot-only features and slot-word pair features.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The representation using only slots corresponds to the consideration of only syntactic argument patterns.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The other representation using the slot-word pairs means that semantic similarity based on word overlap is naturally considered by looking at lexical information.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes.,4 Experiments and Evaluations,[0],[0]
"Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations.",4 Experiments and Evaluations,[0],[0]
These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation.,4 Experiments and Evaluations,[0],[0]
"Finally, we discuss the results of our full experiments.",4 Experiments and Evaluations,[0],[0]
"We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information.",4.1 Experimental Settings,[0],[0]
"Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences.",4.1 Experimental Settings,[0],[0]
"From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words.",4.1 Experimental Settings,[0],[0]
"We focused on verbs whose frequency in the web corpus was more than 1,000.",4.1 Experimental Settings,[0],[0]
"There were 19,649 verbs, including phrasal verbs, and separating passive and active constructions.",4.1 Experimental Settings,[0],[0]
"We extracted 2,032,774,982 predicate-argument structures.
",4.1 Experimental Settings,[0],[0]
We also used the English Gigaword corpus (LDC2011T07; English Gigaword Fifth Edition).,4.1 Experimental Settings,[0],[0]
"This corpus consists of approximately 180 million sentences, which totaling four billion words.
",4.1 Experimental Settings,[0],[0]
"There were 7,356 verbs after applying the same frequency threshold as the web corpus.",4.1 Experimental Settings,[0],[0]
"We extracted 423,778,278 predicate-argument structures from this corpus.
",4.1 Experimental Settings,[0],[0]
We set the hyper-parameters α in (1) and β in (3) to 1.0.,4.1 Experimental Settings,[0],[0]
The cluster assignments for all the components were initialized randomly.,4.1 Experimental Settings,[0],[0]
We took 100 samples for each input frame and selected the cluster assignment that has the highest probability.,4.1 Experimental Settings,[0],[0]
"To measure the precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)).",4.2 Evaluation Metrics,[0],[0]
"However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4",4.2 Evaluation Metrics,[0],[0]
We propose a normalized version of modified purity and inverse purity.,4.2 Evaluation Metrics,[0],[0]
"This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013).
",4.2 Evaluation Metrics,[0],[0]
"To measure the precision of a clustering, a normalized version of modified purity is defined as follows.",4.2 Evaluation Metrics,[0],[0]
Suppose K is the set of automatically induced clusters and G is the set of gold classes.,4.2 Evaluation Metrics,[0],[0]
Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class.,4.2 Evaluation Metrics,[0],[0]
"Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb.",4.2 Evaluation Metrics,[0],[0]
"Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s classes.",4.2 Evaluation Metrics,[0],[0]
The core idea of purity is that each cluster Ki is associated with its most prevalent gold class.,4.2 Evaluation Metrics,[0],[0]
"In addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity.",4.2 Evaluation Metrics,[0],[0]
"The normalized modified purity (nmPU) can then be written as follows:
nmPU = 1 N ∑ i s.t. |Ki|>1",4.2 Evaluation Metrics,[0],[0]
"max j δKi(Ki ∩ Gj), (4)
δKi(Ki ∩ Gj) = ∑
v∈Ki∩Gj civ, (5)
",4.2 Evaluation Metrics,[0],[0]
4Korhonen et al. (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb.,4.2 Evaluation Metrics,[0],[0]
"They reported only precision measures including modified purity, and avoided extending the evaluation metrics for soft clusterings.
where N denotes the total number of verbs, |Ki| denotes the number of positive components in Ki, and civ denotes the v-th component of Ki.",4.2 Evaluation Metrics,[0],[0]
"δKi(Ki ∩ Gj) means the total mass of the set of verbs in Ki ∩Gj , given by summing up the values in Ki.",4.2 Evaluation Metrics,[0],[0]
"In case of evaluating a hard clustering, this is equal to |Ki ∩",4.2 Evaluation Metrics,[0],[0]
"Gj | because all the values of civ are equal to 1.
",4.2 Evaluation Metrics,[0],[0]
"As usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering:
niPU = 1 N ∑",4.2 Evaluation Metrics,[0],[0]
j max i δGj (Ki ∩ Gj).,4.2 Evaluation Metrics,[0],[0]
"(6)
Finally, we use the harmonic mean (F1) of nmPU and niPU as a single measure of clustering quality.",4.2 Evaluation Metrics,[0],[0]
"We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997).",4.3 Type-level Multi-class Evaluations,[0],[0]
"It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous.",4.3 Type-level Multi-class Evaluations,[0],[0]
The average number of verb classes per verb is 2.24.,4.3 Type-level Multi-class Evaluations,[0],[0]
"An excerpt from this data is shown in Table 1.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"As our baselines, we adopt two previously proposed methods.",4.3 Type-level Multi-class Evaluations,[0],[0]
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003).,4.3 Type-level Multi-class Evaluations,[0],[0]
They used the information bottleneck (IB) method for assigning probabilities of classes to each verb.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Note that Korhonen et al. (2003) actually hardened the clusterings and left
the evaluations of soft clusterings for their future work.",4.3 Type-level Multi-class Evaluations,[0],[0]
"For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies.",4.3 Type-level Multi-class Evaluations,[0],[0]
"To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities.",4.3 Type-level Multi-class Evaluations,[0],[0]
"That is, classes that have a higher class attribute probability than the threshold are output for each verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"We report the results of the following threshold values: 0.01, 0.02, 0.05 and 0.10.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"The other baseline is LDA-frames (Materna, 2012).",4.3 Type-level Multi-class Evaluations,[0],[0]
"We use the induced LDA-frames that are
5http://ilexir.co.uk/applications/valex/
available on the web site.6 This frame data was induced from the BNC and consists of 1,200 frames and 400 semantic roles.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Again, we set a threshold for frame attribute probabilities.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We report results using our methods with four feature combinations (slot-only (S) and slot-word pair (SW) features each used for both the framegeneration and verb-class clustering steps) for both the Gigaword and web corpora.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Table 2 lists evaluation results for the baseline methods and our methods.7 The results of the IB baseline and our methods are obtained by averaging five runs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We can see that “web/SW-S” achieved the best performance and obtained a higher F1 than the baselines by more than nine points.,4.3 Type-level Multi-class Evaluations,[0],[0]
“Web/SWS” uses the combination of slot-word pair features for clustering verb-specific frames and slotonly features for clustering across verbs.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Interestingly, this result indicates that slot distributions are more effective than lexical information in slotword pairs for inducing verb classes similar to the gold standard.",4.3 Type-level Multi-class Evaluations,[0],[0]
"This result is consistent with expectations, given a gold standard based on Levin’s verb classes, which are organized according to the syntactic behavior of verbs.",4.3 Type-level Multi-class Evaluations,[0],[0]
"The use of slot-word pairs for verb class induction generally merged too many frames into each class, apparently due to accidental word overlaps across verbs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
The verb classes induced from the web corpus achieved a higher F1 than those from the Gigaword corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
This can be attributed to the larger size of the web corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
"The employment of this kind of huge corpus is enabled by our scalable method.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"6http://nlp.fi.muni.cz/projekty/lda-frames/ 7Although we do not think that the classes with very small attribute probabilities are meaningful, the F1 scores for lower thresholds than 0.01 converged to about 66 in the case of LDA-frames.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Since we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"However, we wish to compare our method with previous work on the induction of a predominant (monosemous) class for each verb.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Then, we apply clustering to these frames across verbs.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For clustering features, we again compare two representations: slot-only features (S) and slot-word pair features (SW).
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al. (2003).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This data contains 110 verbs and 33 classes.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"As we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"When we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al. (2003), and LDA-frames proposed by Materna (2012).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"The
clusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To harden the clusterings of the IB method and the LDA-frames, the class with the highest probability is selected for each verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This hardening process is exactly the same as Korhonen et al. (2003).,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8
Table 3 lists accuracies of baseline methods and our methods.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
Our proposed method using the web corpus achieved comparable performance with the baseline methods on the predominant class evaluation and outperformed them on the multiple class evaluation.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"More sophisticated methods for predominant class induction, such as the method of Sun and Korhonen (2009) using selectional preferences, could produce better single-class outputs, but have difficulty in producing polysemy-aware verb classes.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"From the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F1 than those based on slot-word pair features in many cases.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This result is different from that of multi-class evaluations in Section 4.3.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of semantic frames.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We conduct token-level multi-class evaluations using 119 verbs, which appear 100 or more times in sections 02-21 of the SemLink WSJ corpus.",4.5 Token-level Multi-class Evaluations,[0],[0]
"These 119 verbs cover 102 VerbNet classes, and 48 of them are polysemous in the sense of being in more than one VerbNet class.",4.5 Token-level Multi-class Evaluations,[0],[0]
Each instance of these 119 verbs in this corpus belongs to one of 102 VerbNet classes.,4.5 Token-level Multi-class Evaluations,[0],[0]
We first add these instances to the instances from a raw corpus and apply the twostep clustering to these merged instances.,4.5 Token-level Multi-class Evaluations,[0],[0]
"Then, we compare the induced verb classes of the SemLink instances with their gold-standard VerbNet classes.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1).",4.5 Token-level Multi-class Evaluations,[0],[0]
"It is not necessary to normalize these metrics because the clustering of these instances is hard.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"8Korhonen et al. (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"For clustering features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3).",4.5 Token-level Multi-class Evaluations,[0],[0]
The results of these methods are obtained by averaging five runs.,4.5 Token-level Multi-class Evaluations,[0],[0]
"For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs.",4.5 Token-level Multi-class Evaluations,[0],[0]
Table 4 lists accuracies of these methods for the two corpora.,4.5 Token-level Multi-class Evaluations,[0],[0]
"We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”).
",4.5 Token-level Multi-class Evaluations,[0],[0]
Modi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus.,4.5 Token-level Multi-class Evaluations,[0],[0]
We also conducted the above evaluation against FrameNet frames for 75 verbs.9,4.5 Token-level Multi-class Evaluations,[0],[0]
"We achieved an F1 of 62.79% (66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F1 of 60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is difficult to directly compare these results with Modi et al. (2012), but our induced verb classes seem to have higher F1 accuracy.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We finally induce verb classes from the semantic frames of 1,667 verbs, which appear at least once in sections 02-21 of the WSJ corpus.",4.6 Full Experiments and Discussions,[0],[0]
"Based on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slotonly features.",4.6 Full Experiments and Discussions,[0],[0]
"We ended with 38,481 semantic frames and 699 verb classes from the Gigaword
9Since FrameNet frames are not assigned to all verbs of SemLink, the number of verbs is different from the evaluations against VerbNet classes.
corpus, and 61,903 semantic frames and 840 verb classes from the web corpus.",4.6 Full Experiments and Discussions,[0],[0]
"It took two days to induce verb classes from the Gigaword corpus and three days from the web corpus.
",4.6 Full Experiments and Discussions,[0],[0]
Examples of verb classes and semantic frames induced from the web corpus are shown in Table 5 and Table 6.,4.6 Full Experiments and Discussions,[0],[0]
"While there are many classes with consistent meanings, such as “Class 4” and “Class 16,” some classes have mixed meanings.",4.6 Full Experiments and Discussions,[0],[0]
"For instance, “Class 2” consists of the semantic frames “need:2” and “say:2.”",4.6 Full Experiments and Discussions,[0],[0]
"These frames were merged due to the high syntactic similarity of constituting slot distributions, which are comprised of a subject and a sentential complement.",4.6 Full Experiments and Discussions,[0],[0]
"To improve the quality of verb classes, it is necessary to develop a clustering model that can consider syntactic and lexical similarity in a balanced way.",4.6 Full Experiments and Discussions,[0],[0]
We presented a step-wise unsupervised method for inducing verb classes from instances in gigaword corpora.,5 Conclusion,[0],[0]
This method first clusters predicateargument structures to induce verb-specific semantic frames and then clusters these semantic frames across verbs to induce verb classes.,5 Conclusion,[0],[0]
"Both clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process.",5 Conclusion,[0],[0]
"The resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.10
10http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
From the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points.",5 Conclusion,[0],[0]
"This indicates that slot distributions are more effective than lexical information in slotword pairs for the induction of verb classes, when Levin-style classes are used for evaluation.",5 Conclusion,[0],[0]
"This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs.
",5 Conclusion,[0],[0]
"As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation.",5 Conclusion,[0],[0]
"For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus.",5 Conclusion,[0],[0]
"It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011).",5 Conclusion,[0],[0]
This work was supported by Kyoto University John Mung Program and JST CREST.,Acknowledgments,[0],[0]
"We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing.",Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgments,[0],[0]
We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora.,abstractText,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,abstractText,[0],[0]
"By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering.",abstractText,[0],[0]
"In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words.",abstractText,[0],[0]
The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.,abstractText,[0],[0]
A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1243–1252 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1243",text,[0],[0]
Neural architectures have taken the field of machine translation by storm and are in the process of replacing phrase-based systems.,1 Introduction,[0],[0]
"Based on the encoder-decoder framework (Sutskever et al., 2014) increasingly complex neural systems are being developed at the moment.",1 Introduction,[0],[0]
"These systems find new ways of extracting information from the source sentence and the target sentence prefix for example by using convolutions (Gehring et al., 2017) or stacked self-attention layers (Vaswani et al., 2017).",1 Introduction,[0],[0]
"These architectural changes have led to great performance improvements over classical RNN-based neural translation systems (Bahdanau et al., 2014).
∗Code and a workflow that reproduces the experiments are available at https://github.com/philschulz/ stochastic-decoder.
†Work done prior to joining Amazon.
",1 Introduction,[0],[0]
"Surprisingly, there have been almost no efforts to change the probabilistic model wich is used to train the neural architectures.",1 Introduction,[0],[0]
A notable exception is the work of Zhang et al. (2016) who introduce a sentence-level latent Gaussian variable.,1 Introduction,[0],[0]
"In this work, we propose a more expressive latent variable model that extends the attentionbased architecture of Bahdanau et al. (2014).",1 Introduction,[0],[0]
"Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.).",1 Introduction,[0],[0]
"Neural machine translation (NMT) models are incapable of capturing this variation, however.",1 Introduction,[0],[0]
"This is because their likelihood function incorporates the statistical assumption that there is one (and only one) output1 for a given source sentence, i.e.,
P (yn1 |xm1 ) = n∏
i=1
P (yi|xm1 , y<i) .",1 Introduction,[0],[0]
"(1)
Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data.",1 Introduction,[0],[0]
The noise sources are modelled as Gaussian random variables.,1 Introduction,[0],[0]
"The contributions of this work are: • The introduction of an NMT system that is capable of capturing word-level variation in translation data.
",1 Introduction,[0],[0]
• A thorough discussions of issues encountered when training this model.,1 Introduction,[0],[0]
"In particular, we motivate the use of KL scaling as introduced by Bowman et al. (2016) theoretically.
",1 Introduction,[0],[0]
1Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence.,1 Introduction,[0],[0]
"The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system.
",1 Introduction,[0],[0]
• An empirical demonstration of the improvements achievable with the proposed model.,1 Introduction,[0],[0]
The NMT system upon which we base our experiments is based on the work of Bahdanau et al. (2014).,2 Neural Machine Translation,[0],[0]
The likelihood of the model is given in Equation (1).,2 Neural Machine Translation,[0],[0]
We briefly describe its architecture.,2 Neural Machine Translation,[0],[0]
"Let xm1 = (x1, . . .",2 Neural Machine Translation,[0],[0]
", xm) be the source sentence and yn1 the target sentence.",2 Neural Machine Translation,[0],[0]
Let RNN (·) be any function computed by a recurrent neural network (we use a bi-LSTM for the encoder and an LSTM for the decoder).,2 Neural Machine Translation,[0],[0]
We call the decoder state at the ith target position ti; 1 ≤ i ≤,2 Neural Machine Translation,[0],[0]
n.,2 Neural Machine Translation,[0],[0]
The computation performed by the baseline system is summarised below.,2 Neural Machine Translation,[0],[0]
"[
h1, . . .",2 Neural Machine Translation,[0],[0]
", hm ] = RNN (xm1 ) (2a)
t̃i = RNN (ti−1, yi−1) (2b)
eij = v ⊤ a tanh ( Wa[t̃i, hj ] ⊤ + ba ) (2c) αij = exp (eij)∑m j=1 exp (eij) (2d)
ci = m∑ j=1 αijhj (2e) ti = Wt[t̃i, ci] ⊤ + bt (2f)
ϕi = softmax(Woti + bo) (2g)
",2 Neural Machine Translation,[0],[0]
"The parameters {Wa,Wt,Wo, ba, bt, bo, va} ⊆ θ are learned during training.",2 Neural Machine Translation,[0],[0]
The model is trained usingmaximum likelihood estimation.,2 Neural Machine Translation,[0],[0]
Thismeans that we employ a cross-entropy loss whose input is the probability vector returned by the softmax.,2 Neural Machine Translation,[0],[0]
This section introduces our stochastic decoder model for capturing word-level variation in translation data.,3 Stochastic Decoder,[0],[0]
Imagine an idealised translator whose translations are always perfectly accurate and fluent.,3.1 Motivation,[0],[0]
"If an MT systemwas providedwith training data from such a translator, it would still encounter variation in that data.",3.1 Motivation,[0],[0]
"After all, there are several perfectly accurate and fluent translations for each source sentence.",3.1 Motivation,[0],[0]
"These can be highly different in both their lexical as well as their syntactic realisations.
",3.1 Motivation,[0],[0]
"In practice, of course, human translators’ performance varies according to their level of education, their experience on the job, their familiarity with the textual domain and myriads of other factors.",3.1 Motivation,[0],[0]
"Even within a single translator variation may occur due to level of stress, tiredness or status of health.",3.1 Motivation,[0],[0]
"That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing onemachinegenerated translation against several human translations (see e.g. Papineni et al., 2002).
",3.1 Motivation,[0],[0]
"Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable.",3.1 Motivation,[0],[0]
"Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations.",3.1 Motivation,[0],[0]
"A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions.",3.1 Motivation,[0],[0]
"Multimodal modelling of these variations is thus called for—and our results confirm this intuition.
",3.1 Motivation,[0],[0]
An example of variation comes from free word order and agreement phenomena in morphologically rich languages.,3.1 Motivation,[0],[0]
An English sentence with rigid word order may be translated into several orderings in German.,3.1 Motivation,[0],[0]
"However, all orderings need to respect the agreement relationship between the main verb and the subject (indicated by underlining) as well as the dative case of the direct object (dashes) and the accusative of the indirect object (dots).",3.1 Motivation,[0],[0]
"The agreement requirements are fixed and independent of word order.
",3.1 Motivation,[0],[0]
1.,3.1 Motivation,[0],[0]
I can’t imagine you naked.,3.1 Motivation,[0],[0]
(a) Ich kann mir . . .,3.1 Motivation,[0],[0]
.dich,3.1 Motivation,[0],[0]
nicht nackt vorstellen.,3.1 Motivation,[0],[0]
(b) Ich kann . . . .,3.1 Motivation,[0],[0]
.dich,3.1 Motivation,[0],[0]
mir nicht nackt vorstellen.,3.1 Motivation,[0],[0]
(c) . . . .,3.1 Motivation,[0],[0]
.Dich,3.1 Motivation,[0],[0]
"kann ichmir nicht nackt vorstellen.
",3.1 Motivation,[0],[0]
"Stochastically encoding the word order variation allows the model to learn the same agreement phenomenon from different translation variants as it does not need to encode the word order and agreement relationships jointly in the decoder state.
",3.1 Motivation,[0],[0]
"Further examples of VP and NP variation from an actual translation corpus are shown in Figure 1.
",3.1 Motivation,[0],[0]
We aim to address these word-level variation phenomena with a stochastic decoder model.,3.1 Motivation,[0],[0]
The model contains a latent Gaussian variable for each target position.,3.2 Model formulation,[0],[0]
This variable depends on the previous latent states and the decoder state.,3.2 Model formulation,[0],[0]
"Through the use of recurrent networks, the conditioning context does not need to be restricted and the likelihood factorises exactly.
",3.2 Model formulation,[0],[0]
"P (yn1 |xm1 ) = ∫
dzn0 p(z0|xm1 )× n∏
i=1
p(zi|z<i, y<i, xm1 )P (yi|zi1, y<i, xm1 ) (3)
As can be seen from Equation (3), the model also contains a 0th latent variable that is meant to initialise the chain of latent variables based solely on the source sentence.",3.2 Model formulation,[0],[0]
Contrast this with the model of Zhang et al. (2016) which uses only that 0th variable.,3.2 Model formulation,[0],[0]
A graphical representation of the stochastic decoder model is given in Figure 2a.,3.2 Model formulation,[0],[0]
"Its generative story is as follows
Z0|xm1 ∼ N (µ0, σ20) (4a) Zi|z",3.2 Model formulation,[0],[0]
<,3.2 Model formulation,[0],[0]
"i, y<i, xm1 ∼ N (µi, σ2i ) (4b) Yi|zi0, y<i, xm1 ∼ Cat(ϕi) (4c)
where i = 1, . . .",3.2 Model formulation,[0],[0]
", n and both the Gaussian and the Categorical parameters are predicted by neural network architectures whose inputs vary per time step.",3.2 Model formulation,[0],[0]
This probabilistic formulation can be implemented with a multitude of different architectures.,3.2 Model formulation,[0],[0]
We present ours in the next section.,3.2 Model formulation,[0],[0]
"Since the model contains latent variables and is parametrised by a neural network, it falls into the class of deep generative models (DGMs).",3.3 Neural Architecture,[0],[0]
"We use a reparametrisation of the Gaussian variables (Kingma and Welling, 2014; Rezende et al., 2014; Titsias and Lázaro-Gredilla, 2014) to enable backpropagation inside a stochastic computation graph (Schulman et al., 2015).",3.3 Neural Architecture,[0],[0]
In order to sample ddimensional Gaussian variable z ∈,3.3 Neural Architecture,[0],[0]
Rd,3.3 Neural Architecture,[0],[0]
"with mean µ and variance σ2, we first sample from a standard Gaussian distribution and then transform the sample,
z = µ+ σ ⊙ ϵ ϵ",3.3 Neural Architecture,[0],[0]
"∼ N (0, I) .",3.3 Neural Architecture,[0],[0]
"(5)
Here µ, σ ∈ Rd and ⊙ denotes element-wise multiplication (also known as Hadamard product).",3.3 Neural Architecture,[0],[0]
See the supplement for details on the Gaussian reparametrisation.,3.3 Neural Architecture,[0],[0]
We use neural networks with one hidden layer with a tanh activation to compute the mean and standard deviation of each Gaussian distribution.,3.3 Neural Architecture,[0],[0]
A softplus transformation is applied to the output of the standard deviation’s network to ensure positivity.,3.3 Neural Architecture,[0],[0]
Let us denote the functions that these networks compute by f .,3.3 Neural Architecture,[0],[0]
"For the initial latent state z0 we compute the mean and standard deviation as
µ0 = fµ0 (hm) σ0",3.3 Neural Architecture,[0],[0]
= fσ0 (hm) .,3.3 Neural Architecture,[0],[0]
"(6)
The parameters of all other latent distributions are computed by functions fµ and fσ whose inputs vary per target position.
",3.3 Neural Architecture,[0],[0]
"µi = fµ (ti−1, zi−1) σi",3.3 Neural Architecture,[0],[0]
"= fσ (ti−1, zi−1) (7)
Using these values, each latent variable is sampled according to Equation (5).",3.3 Neural Architecture,[0],[0]
The sampled latent variables are then used to modify the update of the decoder hidden state (Equation (2b)),3.3 Neural Architecture,[0],[0]
"as follows:
t̃i = RNN (ti−1, yi−1, zi) (8)
The remaining computations stay unchanged.",3.3 Neural Architecture,[0],[0]
Notice that the latent values are used directly in updating the decoder state.,3.3 Neural Architecture,[0],[0]
This makes the decoder state a function of a random variable and thus the decoder state is itself random.,3.3 Neural Architecture,[0],[0]
"Applying this argument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic.",3.3 Neural Architecture,[0],[0]
"We use variational inference (see e.g. Blei et al., 2017) to train the model.",4 Inference and Training,[0],[0]
"In variational inference, we employ a variational distribution q(z) that approximates the true posterior p(z|x) over the latent variables.",4 Inference and Training,[0],[0]
The distribution q(z) has its own set of parameters λ that is disjoint from the set of model parameters θ.,4 Inference and Training,[0],[0]
It is used to maximise the evidence lower bound (ELBO) which is a lower bound on the marginal likelihood p(x).,4 Inference and Training,[0],[0]
The ELBO is maximised with respect to both the model parameters θ and the variational parameters λ.,4 Inference and Training,[0],[0]
"Most NLP models that use DGMs only use one latent variable (e.g. Bowman et al., 2016).",4 Inference and Training,[0],[0]
"Models
that use several variables usually employ a mean field approximation under which all latent variables are independent.",4 Inference and Training,[0],[0]
"This turns the ELBO into a sum of expectations (e.g. Zhou and Neubig, 2017).",4 Inference and Training,[0],[0]
"For our stochastic decoder we design a more flexible approximation posterior family which respects the dependencies between the latent variables,
q(zn0 )",4 Inference and Training,[0],[0]
= q(z0) n∏ i=1,4 Inference and Training,[0],[0]
q(zi|z<i) .,4 Inference and Training,[0],[0]
"(9)
Our stochastic decoder can be viewed as a stack of conditional DGMs (Sohn et al., 2015) in which the latent variables depend on one another.",4 Inference and Training,[0],[0]
"The ELBO thus consists of nested positional ELBOs,
ELBO0 + Eq(z0)[ELBO1 +Eq(z1)[ELBO2 + . . .",4 Inference and Training,[0],[0]
"]] ,
(10)
where for a given target position i the ELBO is
ELBOi = Eq(zi)",4 Inference and Training,[0],[0]
"[log p(yi|x m 1 , y<i, z<i, zi)]
−KL (q(zi) ||",4 Inference and Training,[0],[0]
"p(zi|xm1 , y<i, z<i)) .",4 Inference and Training,[0],[0]
"(11)
",4 Inference and Training,[0],[0]
The first term is often called reconstruction or likelihood term whereas the second term is called the KL term.,4 Inference and Training,[0],[0]
"Since the KL term is a function of two Gaussian distributions, and the Gaussian is an exponential family, we can compute it analytically (Michalowicz et al., 2014), without the need for sampling.",4 Inference and Training,[0],[0]
This is very similar to the hierarchical latent variable model of Rezende et al. (2014).,4 Inference and Training,[0],[0]
"Following common practice in DGM research, we employ a neural network to compute the variational distributions.",4 Inference and Training,[0],[0]
"To discriminate it from the
generative model, we call this neural net the inference model.",4 Inference and Training,[0],[0]
At training time both the source and target sentence are observed.,4 Inference and Training,[0],[0]
We exploit this by endowing our inference model with a “lookahead” mechanism.,4 Inference and Training,[0],[0]
"Concretely, samples from the inference network condition on the information available to the generation network (Section 3.3) and also on the target words that are yet to be processed by the generative decoder.",4 Inference and Training,[0],[0]
This allows the latent distribution to not only encode information about the currently modelled word but also about the target words that follow it.,4 Inference and Training,[0],[0]
The conditioning of the inference network is illustrated graphically in Figure 2b.,4 Inference and Training,[0],[0]
The inference network produces additional representations of the target sentence.,4 Inference and Training,[0],[0]
"One representation encodes the target sentence bidirectionally (12a), in analogy to the source sentence encoding.",4 Inference and Training,[0],[0]
The second representation is built by encoding the target sentence in reverse (12b).,4 Inference and Training,[0],[0]
This reverse encoding can be used to provide information about future context to the decoder.,4 Inference and Training,[0],[0]
"We use the symbols b and r for the bidirectional and reverse target encodings, respectively.",4 Inference and Training,[0],[0]
"In our experiments, we again use LSTMs to compute these encodings.",4 Inference and Training,[0],[0]
"[
b1, . . .",4 Inference and Training,[0],[0]
", bn ] = RNN (yn1 ) (12a)[
r1, . . .",4 Inference and Training,[0],[0]
", rn ] = RNN (yn1 ) (12b)
",4 Inference and Training,[0],[0]
"In analogy to the generativemodel (Section 3.3), the inference network uses single hidden layer networks to compute the mean and standard deviations of the latent variable distributions.",4 Inference and Training,[0],[0]
"We denote these functions g and again employ different functions for the initial latent state and all other latent states.
µ0 = gµ0 (hm, bn) (13a) σ0 = gσ0",4 Inference and Training,[0],[0]
"(hm, bn) (13b) µi = gµ (ti−1, zi−1, ri, yi) (13c)",4 Inference and Training,[0],[0]
σi,4 Inference and Training,[0],[0]
"= gσ (ti−1, zi−1, ri, yi) (13d)
",4 Inference and Training,[0],[0]
"As before, we use Equation (5) to sample from the variational distribution.",4 Inference and Training,[0],[0]
"During training, all samples are obtained from the inference network.",4 Inference and Training,[0],[0]
Only at test time do we sample from the generator.,4 Inference and Training,[0],[0]
"Notice that since the inference network conditions on representations produced by the generator network, a naïve application of backpropagation would update parts of the generator network with gradients computed for
the inference network.",4 Inference and Training,[0],[0]
We prevent this by blocking gradient flow from the inference net into the generator.,4 Inference and Training,[0],[0]
The training procedure as outlined above does not work well empirically.,4.1 Analysis of the Training Procedure,[0],[0]
This is because our model uses a strong generator.,4.1 Analysis of the Training Procedure,[0],[0]
By this we mean that the generation model (that is the baseline NMT model) is a very good density model in and by itself and does not need to rely on latent information to achieve acceptable likelihood values during training.,4.1 Analysis of the Training Procedure,[0],[0]
"DGMs with strong generators have a tendency to not make use of latent information (Bowman et al., 2016).",4.1 Analysis of the Training Procedure,[0],[0]
"This problem went initially unnoticed because early DGMs (Kingma and Welling, 2014; Rezende et al., 2014) used weak generators2, i.e. models that made very strong independence assumptions and were not able to capture contextual information without making use of the information encoded by the latent variable.",4.1 Analysis of the Training Procedure,[0],[0]
WhyDGMswould ignore the latent information can be understood by considering the KL-term of the ELBO.,4.1 Analysis of the Training Procedure,[0],[0]
"In order for the latent variable to be informative about the observed data, we need them to have high mutual information I(Z;Y ).
I(Z;Y ) =",4.1 Analysis of the Training Procedure,[0],[0]
"Ep(z,y) [ log p(Z, Y )
p(Z)p(Y )
]",4.1 Analysis of the Training Procedure,[0],[0]
"(14)
Observe that we can rewrite the mutual information as an expected KL divergence by applying the definition of conditional probability.
I(Z;Y ) = Ep(y)",4.1 Analysis of the Training Procedure,[0],[0]
[KL (p(Z|Y ) ||,4.1 Analysis of the Training Procedure,[0],[0]
p(Z)),4.1 Analysis of the Training Procedure,[0],[0]
"] (15)
Since we cannot compute the posterior p(z|y) exactly, we approximate it with the variational distribution q(z|y) (the joint is approximated by q(z|y)p(y) where the latter factor is the data distribution).",4.1 Analysis of the Training Procedure,[0],[0]
"To the extent that the variational distribution recovers the true posterior, the mutual information can be computed this way.",4.1 Analysis of the Training Procedure,[0],[0]
"In fact, if we take the learned prior p(z) to be an approximation of themarginal ∫ q(z|y)p(y)dy it can easily be shown that the thus computed KL term is an upper bound on mutual information (Alemi et al., 2017).",4.1 Analysis of the Training Procedure,[0],[0]
"The trouble is that the ELBO (Equation (11)) can be trivially maximised by setting the KL-term to 0 and maximising only the reconstruction term.
2The term weak generator has first been coined by Alemi et al. (2017).
",4.1 Analysis of the Training Procedure,[0],[0]
This is especially likely at the beginning of training when the variational approximation does not yet encode much useful information.,4.1 Analysis of the Training Procedure,[0],[0]
We can only hope to learn a useful variational distribution if a) the variational approximation is allowed to move away from the prior and b) the resulting increase in the reconstruction term is higher than the increase in the KL-term (i.e. the ELBO increases overall).,4.1 Analysis of the Training Procedure,[0],[0]
"Several schemes have been proposed to enable better learning of the variational distribution (Bowman et al., 2016; Kingma et al., 2016; Alemi et al., 2017).",4.1 Analysis of the Training Procedure,[0],[0]
Here we use KL scaling and increase the scale gradually until the original objective is recovered.,4.1 Analysis of the Training Procedure,[0],[0]
"This has the following effect: during the initial learning stage, the KL-term barely contributes to the objective and thus the updates to the variational parameters are driven by the signal from the reconstruction term and hardly restricted by the prior.",4.1 Analysis of the Training Procedure,[0],[0]
Once the scale factor approaches 1 the variational distribution will be highly informative to the generator (assuming sufficiently slow increase of the scale factor).,4.1 Analysis of the Training Procedure,[0],[0]
The KL-term can now be minimised by matching the prior to the variational distribution.,4.1 Analysis of the Training Procedure,[0],[0]
"Notice that up to this point, the prior has hardly been updated.",4.1 Analysis of the Training Procedure,[0],[0]
Thus moving the variational approximation back to the prior would likely reduce the reconstruction term since the standard normal prior is not useful for inference purposes.,4.1 Analysis of the Training Procedure,[0],[0]
This is in stark contrast to Bowman et al. (2016) whose prior was a fixed standard normal distribution.,4.1 Analysis of the Training Procedure,[0],[0]
"Although they used KL scaling, the KL term could only be decreased by moving the variational approximation back to the fixed prior.",4.1 Analysis of the Training Procedure,[0],[0]
This problem disappears in our model where priors are learned.,4.1 Analysis of the Training Procedure,[0],[0]
Moving the prior towards the variational approximation has another desirable effect.,4.1 Analysis of the Training Procedure,[0],[0]
The prior can now learn to emulate the variational “lookahead” mechanism without having access to future contexts itself (recall that the inference model has access to future target tokens).,4.1 Analysis of the Training Procedure,[0],[0]
At test time we can thus hope to have learned latent variable distributions that encode information not only about the output at the current position but about future outputs as well.,4.1 Analysis of the Training Procedure,[0],[0]
We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations.,5 Experiments,[0],[0]
"We trained models to
translate from English into Arabic, Czech, French and German.",5 Experiments,[0],[0]
The number of sentences for each language after preprocessing is shown in Table 1.,5 Experiments,[0],[0]
"The vocabulary was split into 50,000 subword units using Google’s sentence piece3 software in its standard settings.",5 Experiments,[0],[0]
"As our baseline NMT systems we use Sockeye (Hieber et al., 2017)4.",5 Experiments,[0],[0]
Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2.,5 Experiments,[0],[0]
"We report baselines with and without dropout (Srivastava et al., 2014).",5 Experiments,[0],[0]
For dropout a retention probability of 0.5 was used.,5 Experiments,[0],[0]
As a second baseline we use our own implementation of the model of Zhang et al. (2016) which contains a single sentence-level Gaussian latent variable (SENT).,5 Experiments,[0],[0]
Our implementation differs from theirs in three aspects.,5 Experiments,[0],[0]
"First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the inference network (Zhang et al. (2016) use the average of all states).",5 Experiments,[0],[0]
"Second, the latent variable is smaller in size than the one used by (Zhang et al., 2016).5",5 Experiments,[0],[0]
This was done to make their model and the stochastic decoder proposed here as similar as possible.,5 Experiments,[0],[0]
"Finally, their implementation was based on groundhog whereas ours builds on Sockeye.",5 Experiments,[0],[0]
Our stochastic decoder model (SDEC) is also built on top of the basic Sockeyemodel.,5 Experiments,[0],[0]
It adds the components described in Sections 3 and 4.,5 Experiments,[0],[0]
Recall that the functions that compute the means and standard deviations are implemented by neural nets with a single hidden layer with tanh activation.,5 Experiments,[0],[0]
The width of that layer is twice the size of the latent variable.,5 Experiments,[0],[0]
In our experiments we tested different latent variable sizes and used KL scaling (see Section 4.1).,5 Experiments,[0],[0]
"The scale started from 0 and was increased by 1/20,000 after each mini-batch.",5 Experiments,[0],[0]
"Thus, at iteration t the scale is min(t/20,000, 1).",5 Experiments,[0],[0]
"All models use 1028 units for the LSTM hid3https://github.com/google/sentencepiece 4https://github.com/awslabs/sockeye 5We did, however, find that increasing the latent variable size actually hurt performance in our implementation.
",5 Experiments,[0],[0]
den state (or 512 for each direction in the bidirectional LSTMs) and 256 for the attention mechansim.,5 Experiments,[0],[0]
"Training is done with Adam (Kingma and Ba, 2015).",5 Experiments,[0],[0]
In decoding we use a beam of size 5 and output the most likely word at each position.,5 Experiments,[0],[0]
We deterministically set all latent variables to their mean values during decoding.,5 Experiments,[0],[0]
"Monte Carlo decoding (Gal, 2016) is difficult to apply to our setting as it would require sampling entire translations.
",5 Experiments,[0],[0]
Results We show the BLEU scores for all models that we tested on the IWSLT data set in Table 2.,5 Experiments,[0],[0]
"The stochastic decoder dominates the Sockeye baseline across all 4 languages, and outperforms SENT on most languages.",5 Experiments,[0],[0]
"Except on German, there is a trend towards smaller latent variable sizes being more helpful.",5 Experiments,[0],[0]
This is in line with findings by Chung et al. (2015) and Fraccaro et al. (2016) who also used relatively small latent variables.,5 Experiments,[0],[0]
This observation also implies that our model does not improve simply because it has more parameters than the baseline.,5 Experiments,[0],[0]
That the margin between the SDEC and SENT models is not large was to be expected for two reasons.,5 Experiments,[0],[0]
"First, Chung et al. (2015) and Fraccaro et al. (2016) have shown that stochastic RNNs lead to enormous improvements in modelling continuous sequences but only modest increases in performance for discrete sequences (such as natural language).",5 Experiments,[0],[0]
"Second, translation performance is measured in BLEU score.",5 Experiments,[0],[0]
We observed that SDEC often reached better ELBO values than SENT indicating a better model fit.,5 Experiments,[0],[0]
"How to fully leverage the better modelling ability of stochastic RNNs when producing discrete outputs is a matter of future research.
",5 Experiments,[0],[0]
"Qualitative Analysis Finally, we would like to demonstrate that our model does indeed capture variation in translation.",5 Experiments,[0],[0]
"To this end, we randomly picked sentences from the IWSLT test set and had our model translate them several times, however, the values of the latent variables were sampled instead of fixed.",5 Experiments,[0],[0]
"Contrary to the BLEU-based evaluation, beam search was not used in this evaluation in order to avoid interaction between different latent variable samples.",5 Experiments,[0],[0]
See Figure 3 for examples of syntactic and lexical variation.,5 Experiments,[0],[0]
It is important to note that we do not sample from the categorical output distribution.,5 Experiments,[0],[0]
For each target position we pick the most likely word.,5 Experiments,[0],[0]
"A non-stochastic NMT system would always yield the same translation in
this scenario.",5 Experiments,[0],[0]
"Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system.",5 Experiments,[0],[0]
"This supports our initial point that the SENT model is likely insensitive to local variation, a problem that our model was designed to address.",5 Experiments,[0],[0]
"Like the model of Bowman et al. (2016), SENT presumably tends to ignore the latent variable.",5 Experiments,[0],[0]
The stochastic decoder is strongly influenced by previous work on stochastic RNNs.,6 Related Work,[0],[0]
The first such proposal was made by Bayer and Osendorfer (2015) who introduced i.i.d.,6 Related Work,[0],[0]
Gaussian latent variables at each output position.,6 Related Work,[0],[0]
"Since their model neglects any sequential dependence of the noise sources, it underperformed on several sequence modeling tasks.",6 Related Work,[0],[0]
Chung et al. (2015) made the latent variables depend on previous information by feeding the previous decoder state into the latent variable sampler.,6 Related Work,[0],[0]
Their inference model did not make use of future elements in the sequence.,6 Related Work,[0],[0]
Using a “look-ahead” mechanism in the inference net was proposed by Fraccaro et al. (2016) who had a separate stochastic and deterministic RNN layer which both influence the output.,6 Related Work,[0],[0]
"Since the stochastic layer in their model depends on the deterministic layer but not vice versa, they could first run the deterministic layer at inference time and then condition the inference net’s encoding of the future on the thus obtained features.",6 Related Work,[0],[0]
"Like us, they used KL scaling during training.",6 Related Work,[0],[0]
"More recently, Goyal et al. (2017) proposed an auxiliary loss that has the inference net predict future feature representations.",6 Related Work,[0],[0]
This approach yields state-of-the-art results but is still in need of a theoretical justification.,6 Related Work,[0],[0]
"Within translation, Zhang et al. (2016) were the first to incorporate Gaussian variables into an NMT model.",6 Related Work,[0],[0]
Their approach only uses one sentence-level latent variable (corresponding to our z0) and can thus not deal with word-level variation directly.,6 Related Work,[0],[0]
"Concurrently to our work, Su et al. (2018) have also proposed a recurrent latent variable model for NMT.",6 Related Work,[0],[0]
Their approach differs from ours in that they do not use a 0th latent variable nor a look-ahead mechanism during inference time.,6 Related Work,[0],[0]
"Furthermore, their underlying recurrent model is a GRU.",6 Related Work,[0],[0]
"In the wider field of NLP, deep generative mod-
els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016).",6 Related Work,[0],[0]
Wehave presented a recurrent decoder formachine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora.,7 Conclusion and Future Work,[0],[0]
Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation.,7 Conclusion and Future Work,[0],[0]
"The proposed model consistently outperforms strong baselines
on several language pairs.",7 Conclusion and Future Work,[0],[0]
"As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future.",7 Conclusion and Future Work,[0],[0]
"Here, we list the three which we believe to be most promising.
",7 Conclusion and Future Work,[0],[0]
• Latent factor models: our model only contains one source of variation per word.,7 Conclusion and Future Work,[0],[0]
"A latent factor model such as DARN (Gregor et al., 2014) would consider several sources simultaneously.",7 Conclusion and Future Work,[0],[0]
This would also allow us to perform a better analysis of the model behaviour as we could correlate the factors with observed linguistic phenomena.,7 Conclusion and Future Work,[0],[0]
•,7 Conclusion and Future Work,[0],[0]
"Richer prior and variational distributions: The diagonal Gaussian is likely too simple a
distribution to appropriately model the variation in our data.",7 Conclusion and Future Work,[0],[0]
"Richer distributions computed by normalising flows (Rezende and Mohamed, 2015; Kingma et al., 2016) will likely improve our model.",7 Conclusion and Future Work,[0],[0]
"• Extension to other architectures: Introducing latent variables into non-autoregressive translation models such as the transformer (Vaswani et al., 2017) should increase their translation ability further.",7 Conclusion and Future Work,[0],[0]
Philip Schulz and Wilker Aziz were supported by the Dutch Organisation for Scientific Research (NWO) VICI Grant nr. 277-89-002.,8 Acknowledgements,[0],[0]
Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105).,8 Acknowledgements,[0],[0]
"The process of translation is ambiguous, in that there are typically many valid translations for a given sentence.",abstractText,[0],[0]
"This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process.",abstractText,[0],[0]
"To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora.",abstractText,[0],[0]
We provide an indepth analysis of the pitfalls encountered in variational inference for training deep generative models.,abstractText,[0],[0]
Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.,abstractText,[0],[0]
A Stochastic Decoder for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4810–4815 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4810",text,[0],[0]
"Community question-answer fora are great resources, collecting answers to frequently and lessfrequently asked questions on specific topics, but these are often not moderated and contain many irrelevant answers.",1 Introduction,[0],[0]
"Community Question Answering (CQA), cast as a question relevancy ranking problem, was the topic of two shared tasks at SemEval 2016-17.",1 Introduction,[0],[0]
"This is a non-trivial retrieval task, typically evaluated using mean average precision (MAP).",1 Introduction,[0],[0]
"We present a strong baseline for this task, on par with or surpassing state-of-the-art systems.
",1 Introduction,[0],[0]
"The English subtasks of the SemEval CQA (Nakov et al., 2015, 2017) consist of QuestionQuestion Similarity, Question-Comment Similarity, and Question-External Comment Similarity.",1 Introduction,[0],[0]
"In this study, we focus on the core subtask of Question-Question similarity, defined as follows:",1 Introduction,[0],[0]
"Given a question, rank other relevant questions by their relevancy to that question.",1 Introduction,[0],[0]
This proved to be a difficult task in both SemEval-16 and SemEval17 as it is the one with the least amount of data available.,1 Introduction,[0],[0]
"The baseline was the ranking retrieved
by performing a Google search, which proved to be a strong baseline beating a large portion of the systems submitted.
",1 Introduction,[0],[0]
Contribution Our baseline is a simple multitask feed-forward neural network taking distance measures between pairs of questions as input.,1 Introduction,[0],[0]
We use a question-answer dataset as auxiliary task; but we also experiment with datasets for pairwise classification tasks such as natural language inference and fake news detection.,1 Introduction,[0],[0]
"This simple, easy-totrain model is on par or better than state-of-theart systems for question relevancy ranking.",1 Introduction,[0],[0]
We also show that this simple model outperforms a more complex model based on recurrent neural networks.,1 Introduction,[0],[0]
We present a simple baseline model for question relevancy ranking.1,2 Our Model,[0],[0]
It is a deep feed-forward network with a hidden layer that is shared with an auxiliary task model.,2 Our Model,[0],[0]
The input to the network is extremely simple and consists of five distance measures of the input question-question pair.,2 Our Model,[0],[0]
"§2.1 discusses these distance measures, and how they relate.",2 Our Model,[0],[0]
§2.2 introduces the multi-task learning architecture that we propose.,2 Our Model,[0],[0]
"We use four similarity metrics and three sentence representations (averaged word embeddings, binary unigram vectors, and trigram vectors).",2.1 Features,[0],[0]
"The cosine distance between the sentence representations of query x and query y is∑
i xiyi√∑",2.1 Features,[0],[0]
"i x 2 + √∑ i y 2
1Code available at http://anavaleriagonzalez/FAQ rank.
",2.1 Features,[0],[0]
The Manhattan distance is∑,2.1 Features,[0],[0]
i |xi,2.1 Features,[0],[0]
"− yi|
The Bhattacharya distance is
− ln( ∑ i √ xiyi)
and is a measure of divergence, and the Euclidean distance is √∑
i
(xi − yi)2
Note that the squared Euclidean distance is proportional to cosine distance and Manhattan distance.",2.1 Features,[0],[0]
"The Bhattacharya and Jaccard metrics, on the other hand, are sensitive to the number of types in the input (the `1 norm of the vector encodings).",2.1 Features,[0],[0]
"So, for example, only the cosine, Euclidean, and Manhattan distances will be the same for
x = 〈1, 1, 0, 0, 1, 0, 1, 1, 0, 1〉,y = 〈0, 0, 1, 0, 1, 0, 0, 0, 1, 1〉
and
x = 〈0, 0, 0, 0, 0, 1, 0, 0, 1, 1〉,y = 〈1, 1, 1, 1, 0, 0, 0, 0, 0, 1〉
The Jaccard index is the only metric that can only be applied to two of our representations, unigrams and n-grams: It is defined over mdimensional binary (indicator) vectors and therefore not applicable to averaged embeddings.",2.1 Features,[0],[0]
"It is defined as
x · y m
We represent each query pair by these 14 numerical features.",2.1 Features,[0],[0]
"Our architecture is a simple feed-forward, multitask learning (MTL) architecture.",2.2 MTL Architecture,[0],[0]
Our architecture is presented in Figure 1 and is a Multi-Layer Perceptron (MLP) that takes a pair of sequences as input.,2.2 MTL Architecture,[0],[0]
The sequences can be sampled from the main task or the auxiliary task.,2.2 MTL Architecture,[0],[0]
"The MLP has one shared hidden layer, a task-specific hidden layer and, finally, a task-specific classification layer for each output.",2.2 MTL Architecture,[0],[0]
"The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2.",2.2 MTL Architecture,[0],[0]
"We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model.",2.3 LSTM baseline,[0],[0]
"It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively.",2.3 LSTM baseline,[0],[0]
"The outputs are then concatenated, to enable representing the differences between the two sequences.",2.3 LSTM baseline,[0],[0]
"Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above.",2.3 LSTM baseline,[0],[0]
"For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs.",3 Datasets,[0],[0]
Below we present the datasets used for our main and auxiliary tasks.,3 Datasets,[0],[0]
"We provide some summary statistics for each dataset in Table 3.
SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related queries.",3 Datasets,[0],[0]
"As an auxiliary task, we use the data from subtask A, which is a questionrelated comment ranking task.
",3 Datasets,[0],[0]
"Natural Language Inference Natural Language Inference (NLI), consists in predicting ENTAILMENT, CONTRADICTION or NEUTRAL, given a hypothesis and a premise.",3 Datasets,[0],[0]
"We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres.",3 Datasets,[0],[0]
"Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task.
",3 Datasets,[0],[0]
Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online.,3 Datasets,[0],[0]
"This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018).",3 Datasets,[0],[0]
"Formally, the FNC task consists in, given a headline and the body of
2http://www.fakenewschallenge.org/
text which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline.",3 Datasets,[0],[0]
"There are four labels:
• AGREES:",3 Datasets,[0],[0]
The body of the article is in agreement with the headline • DISAGREES:,3 Datasets,[0],[0]
"The body of the article is in dis-
agreement with the headline • DISCUSSES:",3 Datasets,[0],[0]
"The body of the article does not
take a position • UNRELATED: the body of the article dis-
cusses a different topic
We include fake news detection as a weak auxiliary signal that can lead to better generalization of our question-question ranking model.",3 Datasets,[0],[0]
"We evaluate our performance on the main task of question relevancy ranking using the official
SemEval-2017 Task 3 evaluation scripts (Nakov et al., 2017).",3.1 Evaluation,[0],[0]
"The scripts provide a variety of metrics; however, in accordance with the shared task, we report Mean Average Precision (MAP) (the official metric for the SemEval 2016 and 2017 shared tasks); Mean Reciprocal Rank (MRR), which has being thoroughly used for IR and QA; Average Recall; and, finally, the accuracy of predicting relevant documents.",3.1 Evaluation,[0],[0]
The results from our experiments are shown in Table 1.,4 Results,[0],[0]
"We present the official metric from the SemEval task, as well as other common metrics.",4 Results,[0],[0]
"For the SemEval-16 data, our multitask MLP architecture with a question-answer auxiliary task performed best on all metrics, except accuracy, where the multi-task MLP using all auxiliary tasks performed best.",4 Results,[0],[0]
We outperform the winning systems of both the SemEval 2016 and 2017 campaigns.,4 Results,[0],[0]
"In addition, our improvements from single-task to multi-task are significant (p < 0.01).",4 Results,[0],[0]
We also outperform the official IR baseline used in the SemEval 2016 and 2017 shared tasks.,4 Results,[0],[0]
We discuss the STL-LSTM-SIM results in §5.,4 Results,[0],[0]
"Furthermore, in Table 2, we show the performance of our models when training on feature combinations, while in Table 3, we present an ablation test where we remove one feature at a time.
",4 Results,[0],[0]
"Learning curve In Figure 4, we also present our learning curves for the development set when incrementally increasing the training set size.",4 Results,[0],[0]
"We observe that when using an auxiliary task, the learning is more stable across training set size.",4 Results,[0],[0]
"For the SemEval shared tasks on CQA, several authors used complex recurrent and convolutional neural network architectures (Severyn and Moschitti, 2015; Barrón-Cedeno et al., 2016).",5 Discussion,[0],[0]
"For example, Barrón-Cedeno et al. used a convolutional neural network in combination with feature vectors representing lexical, syntactic, and semantic similarity as well as tree kernels.",5 Discussion,[0],[0]
Their performance was slightly lower than the best system (SemEval-Best for 2016 in Table 1).,5 Discussion,[0],[0]
"The best system used lexical and semantic similarity measures in combination with a ranking model based on support vector machines (SVMs) (Filice et al., 2016; Franco-Salvador et al., 2016).",5 Discussion,[0],[0]
Both systems are harder to implement and train than the model we propose here.,5 Discussion,[0],[0]
"For SemEval-17, FrancoSalvador et al. (2016), the winning team used
distributed representations of words, knowledge graphs and frames from FrameNet (Baker et al., 1998) as some of their features, and used SVMs for ranking.
",5 Discussion,[0],[0]
"For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose.",5 Discussion,[0],[0]
"This architecture is based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997).",5 Discussion,[0],[0]
"For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query.",5 Discussion,[0],[0]
We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer.,5 Discussion,[0],[0]
"This way we can evaluate the usefulness of the flexible, expressive LSTM network directly (as our MTL model becomes an ablation instance of the full, more complex architecture).",5 Discussion,[0],[0]
We use the same dropout regularization and SGD values as for the MLP.,5 Discussion,[0],[0]
"Tuning all parameters on the development data, we do not manage to outperform our proposed model, however.",5 Discussion,[0],[0]
See lines MTL-LSTM-SIM in Table 1 for results.,5 Discussion,[0],[0]
"We show that simple feature engineering, combined with an auxiliary task and a simple feedfor-
ward neural architecture is appropriate for a small dataset and manages to beat the baseline and the best performing systems for the Semeval task of question relevancy ranking.",6 Conclusion,[0],[0]
We observe that introducing pairwise classification tasks leads to significant improvements in performance and a more stable model.,6 Conclusion,[0],[0]
"Overall, our simple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data.",6 Conclusion,[0],[0]
The first author of this paper is funded by a BotXO PhD Award;3 the last author by an ERC Starting Grant.,Acknowledgments,[0],[0]
We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.,Acknowledgments,[0],[0]
The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks – a task that amounts to question relevancy ranking – involve complex pipelines and manual feature engineering.,abstractText,[0],[0]
"Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google’s search engine.",abstractText,[0],[0]
We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair.,abstractText,[0],[0]
"This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.",abstractText,[0],[0]
A strong baseline for question relevancy ranking,title,[0],[0]
"Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention.",1 Introduction,[0],[0]
"Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing.
",1 Introduction,[0],[0]
"The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction
and normalization and 2) temporal relation (also known as TLINKs (Pustejovsky et al., 2003a)) extraction.",1 Introduction,[0.9999999336490296],"['The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction and normalization and 2) temporal relation (also known as TLINKs (Pustejovsky et al., 2003a)) extraction.']"
"While the first task has now been well handled by the state-of-the-art systems (HeidelTime (Strötgen and Gertz, 2010), SUTime (Chang and Manning, 2012), IllinoisTime (Zhao et al., 2012), NavyTime (Chambers, 2013), UWTime (Lee et al., 2014), etc.) with end-to-end F1 scores being around 80%, the second task has long been a challenging one; even the top systems only achieved F1 scores of around 35% in the TE workshops.
",1 Introduction,[0],[0]
"The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e., events or timexes) and edges represent the TLINKs between them.",1 Introduction,[0],[0]
"The task is challenging because it often requires global considerations – considering the entire graph, the TLINK annotation is quadratic in the number of nodes and thus very expensive, and an overwhelming fraction of the temporal relations are missing in human annotation.",1 Introduction,[1.0],"['The task is challenging because it often requires global considerations – considering the entire graph, the TLINK annotation is quadratic in the number of nodes and thus very expensive, and an overwhelming fraction of the temporal relations are missing in human annotation.']"
"In this paper, we propose a structured learning approach to temporal relation extraction, where local models are updated based on feedback from global inferences.",1 Introduction,[0],[0]
"The structured approach also gives rise to a semisupervised method, making it possible to take advantage of the readily available unlabeled data.",1 Introduction,[0],[0]
"As a byproduct, this approach further provides a new, effective perspective on handling those missing relations.
",1 Introduction,[0],[0]
"In the common formulations, temporal relations are categorized into three types: the E-E TLINKs (those between a pair of events), the T-T TLINKs (those between a pair of timexes), and the E-T TLINKs (those between an event and a timex).",1 Introduction,[1.0],"['In the common formulations, temporal relations are categorized into three types: the E-E TLINKs (those between a pair of events), the T-T TLINKs (those between a pair of timexes), and the E-T TLINKs (those between an event and a timex).']"
"While the proposed approach can be generally applied to all three types, this paper focuses on the majority type, i.e., the E-E TLINKs.",1 Introduction,[0],[0]
"For example, consider the following snippet taken from the
ar X
iv :1
90 6.
04 94
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 Ju
n 20
19
training set provided in the TE3 workshop.",1 Introduction,[0],[0]
"We want to construct a temporal graph as in Fig. 1 for the events in boldface in Ex1.
Ex1 . . .",1 Introduction,[0],[0]
"tons of earth cascaded down a hillside, ripping two houses from their foundations.",1 Introduction,[1.0],"['tons of earth cascaded down a hillside, ripping two houses from their foundations.']"
"No one was hurt, but firefighters ordered the evacuation of nearby homes and said they’ll monitor the shifting ground.. . .
",1 Introduction,[0.9978397116158628],"['No one was hurt, but firefighters ordered the evacuation of nearby homes and said they’ll monitor the shifting ground.. .']"
"As discussed in existing work (Verhagen, 2004; Bramsen et al., 2006; Mani et al., 2006; Chambers and Jurafsky, 2008), the structure of a temporal graph is constrained by some rather simple rules:
1.",1 Introduction,[0],[0]
Symmetry.,1 Introduction,[0],[0]
"For example, if A is before B, then B must be after A.
2.",1 Introduction,[0],[0]
Transitivity.,1 Introduction,[0],[0]
"For example, if A is before B and B is before C, then A must be before C.
This particular structure of a temporal graph (especially the transitivity structure) makes its nodes highly interrelated, as can be seen from Fig. 1.",1 Introduction,[0],[0]
"It is thus very challenging to identify the TLINKs between them, even for human annotators: The inter-annotator agreement on TLINKs is usually about 50%-60% (Mani et al., 2006).",1 Introduction,[0],[0]
Fig. 2 shows the actual human annotations provided by TE3.,1 Introduction,[0],[0]
"Among all the ten possible pairs of nodes, only three TLINKs were annotated.",1 Introduction,[0],[0]
"Even if we only look at main events in consecutive sentences and at events in the same sentence, there are still quite a few missing TLINKs, e.g., the one between hurt and cascaded and the one between monitor and ordered.
",1 Introduction,[1.0000000212384301],"['Even if we only look at main events in consecutive sentences and at events in the same sentence, there are still quite a few missing TLINKs, e.g., the one between hurt and cascaded and the one between monitor and ordered.']"
Early attempts by Mani et al. (2006); Chambers et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) studied local methods – learning models that make pairwise decisions between each pair of events.,1 Introduction,[0],[0]
"State-of-the-art local methods, including ClearTK (Bethard, 2013), UTTime
(Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results.",1 Introduction,[0],[0]
"However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph).",1 Introduction,[0],[0]
"Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs.",1 Introduction,[1.0],"['Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs.']"
"Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005).",1 Introduction,[1.0],"['Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005).']"
"In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline.",1 Introduction,[0],[0]
The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve.,1 Introduction,[0],[0]
"This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008).
",1 Introduction,[0],[0]
"Although L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning).",1 Introduction,[0],[0]
"In parallel to the work presented here, Leeuwenberg and Moens (2017) also proposed a structured learning approach to extracting the temporal relations.",1 Introduction,[0],[0]
"Their work focuses on a domain-specific dataset from Clinical TempEval (Bethard et al., 2016), so their work does not need to address some of the difficulties of the general problem that our work addresses.",1 Introduction,[0],[0]
"More importantly, they compared structured learning to local baselines, while we find that the comparison between structured learning and L+I is more interesting and important for
understanding the effect of global considerations in the learning phase.",1 Introduction,[0],[0]
"In difference from existing methods, we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way.",1 Introduction,[0],[0]
"Our solution targets on these issues and, as we show, achieves significant improvements on two commonly used evaluation sets.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
"Section 2 clarifies the temporal relation types and the evaluation metric of a temporal graph used in this paper, Section 3 explains the structured learning approach in detail, and Section 4 discusses the practical issue of missing relations.",1 Introduction,[0],[0]
We provide experiments and discussion in Section 5 and conclusion in Section 6.,1 Introduction,[0],[0]
"Existing corpora for temporal processing often follows the interval representation of events proposed in Allen (1984), and makes use of 13 relation types in total.",2.1 Temporal Relation Types,[1.0],"['Existing corpora for temporal processing often follows the interval representation of events proposed in Allen (1984), and makes use of 13 relation types in total.']"
"In many systems, vague or none is also included as another relation type when a TLINK is not clear or missing.",2.1 Temporal Relation Types,[0],[0]
"However, current systems usually use a reduced set of relation types, mainly due to the following reasons.
1.",2.1 Temporal Relation Types,[0],[0]
The non-uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others (see Table 1 in Mani et al. (2006)).,2.1 Temporal Relation Types,[0],[0]
"For example, relations such as immediately before or immediately after barely exist in a corpus compared to before and after.
2.",2.1 Temporal Relation Types,[0],[0]
"Due to the ambiguity in natural language, determining relations like before and immediately before can be a difficult task itself (Chambers et al., 2014).
",2.1 Temporal Relation Types,[0],[0]
"In this work, we follow the reduced set of temporal relation types used in CAEVO (Chambers et al., 2014): before, after, includes, is included, equal, and vague.",2.1 Temporal Relation Types,[0],[0]
"The most recent evaluation metric in TE3, i.e., the temporal awareness (UzZaman and Allen, 2011), is adopted in this work.",2.2 Quality of A Temporal Graph,[0],[0]
"Specifically, let Gsys and Gtrue be two temporal graphs from the system prediction and the ground truth, respectively.",2.2 Quality of A Temporal Graph,[0],[0]
"The
precision and recall of temporal awareness are defined as follows.
",2.2 Quality of A Temporal Graph,[0],[0]
"P = |G−sys ∩G+true| |G−sys| , R = |G−true ∩G+sys| |G−true|
where G+ is the closure of graph G, G− is the reduction of G, “∩” is the intersection between TLINKs in two graphs, and |G| is the number of TLINKs in G. The temporal awareness metric better captures how “useful” a temporal graph is.",2.2 Quality of A Temporal Graph,[0],[0]
"For example, if system 1 produces ripping is before hurt and hurt is before monitor, and system 2 adds ripping is before monitor on top of system 1.",2.2 Quality of A Temporal Graph,[1.0],"['For example, if system 1 produces ripping is before hurt and hurt is before monitor, and system 2 adds ripping is before monitor on top of system 1.']"
"Since system 2 is simply a transitive closure of system 1, they would have the same evaluation scores.",2.2 Quality of A Temporal Graph,[0],[0]
Note that vague relations are usually considered as non-existing TLINKs and are not counted during evaluation.,2.2 Quality of A Temporal Graph,[0],[0]
"As shown in Fig. 1, the learning problem in temporal relation extraction is global in nature.",3 A Structured Training Approach,[0],[0]
"Even the top local method in TE3, UTTime (Laokulrat et al., 2013), only achieved F1=56.5 when presented with a pair of temporal entities (Task C– relation only (UzZaman et al., 2013)).",3 A Structured Training Approach,[0],[0]
"Since the success of an L+I method strongly relies on the quality of the local classifiers, a poor local classifier is obviously a roadblock for L+I methods.",3 A Structured Training Approach,[0],[0]
"Following the insights from Punyakanok et al. (2005), we propose to use a structured learning approach (also called “Inference Based Training” (IBT)).
",3 A Structured Training Approach,[0],[0]
"Unlike the current L+I approach, where local classifiers are trained independently beforehand without knowledge of the predictions on neighboring pairs, we train local classifiers with feedback that accounts for other relations, by performing global inference in each round of the learning process.",3 A Structured Training Approach,[0],[0]
"In order to introduce the structured learning algorithm, we first explain its most important component, the global inference step.",3 A Structured Training Approach,[0],[0]
"In a document with n pairs of events, let φi ∈ X ⊆ Rd be the extracted d-dimensional feature and yi ∈ Y be the temporal relation for the i-th pair of events, i = 1, 2, . . .",3.1 Inference,[0.9969300100290811],"['In a document with n pairs of events, let φi ∈ X ⊆ Rd be the extracted d-dimensional feature and yi ∈ Y be the temporal relation for the i-th pair of events, i = 1, 2, .']"
", n, where Y = {rj}6j=1 is the label set for the six temporal relations we use.",3.1 Inference,[0],[0]
"Moreover, let x = {φ1, . . .",3.1 Inference,[0],[0]
", φn} ∈ X n",3.1 Inference,[0],[0]
"and y = {y1, . . .",3.1 Inference,[0],[0]
", yn} ∈ Yn be more compact representations of all the features and labels in this
document.",3.1 Inference,[0],[0]
"Given the weight vector wr of a linear classifier trained for relation r ∈ Y (i.e., using the one-vs-all scheme), the global inference step is to solve the following constrained optimization problem:
ŷ = arg max y∈C(Yn) f(x,y), (1)
where C(Yn) ⊆ Yn constrains the temporal graph to be symmetrically and transitively consistent, and f(x,y) is the scoring function:
f(x,y) = n∑ i=1",3.1 Inference,[0],[0]
fyi(φi),3.1 Inference,[0],[0]
= n∑ i=1,3.1 Inference,[0],[0]
ew T yi φi∑,3.1 Inference,[0],[0]
"r∈Y e wTr φi .
",3.1 Inference,[0],[0]
"Specifically, fyi(φi) is the probability of the i-th event pair having relation yi.",3.1 Inference,[0],[0]
"f(x, y) is simply the sum of these probabilities over all the event pairs in a document, which we think of as the confidence of assigning y = {y1, ..., yn} to this document and therefore, it needs to be maximized in Eq.",3.1 Inference,[0],[0]
"(1).
Note that when C(Yn) = Yn, Eq. (1) can be solved for each ŷi independently, which is what the so-called local methods do, but the resulting ŷ may not satisfy global consistency in this way.",3.1 Inference,[0],[0]
When C(Yn) 6=,3.1 Inference,[0],[0]
"Yn, Eq.",3.1 Inference,[0],[0]
"(1) cannot be decoupled for each ŷi and is usually formulated as an ILP problem (Roth and Yih, 2004; Chambers and Jurafsky, 2008; Do et al., 2012).",3.1 Inference,[0],[0]
"Specifically, let Ir(ij) ∈ {0, 1} be the indicator function of relation r for event i and event j and fr(ij) ∈",3.1 Inference,[0],[0]
"[0, 1] be the corresponding soft-max score.",3.1 Inference,[0],[0]
"Then the ILP objective for global inference is formulated as follows.
",3.1 Inference,[0],[0]
Î,3.1 Inference,[0],[0]
=,3.1 Inference,[0],[0]
argmax,3.1 Inference,[0],[0]
"I
∑ ij∈E ∑ r∈Y fr(ij)Ir(ij) (2)
s.t. ΣrIr(ij) = 1 (uniqueness) , Ir(ij) = Ir̄(ji), (symmetry)
Ir1(ij)",3.1 Inference,[0],[0]
"+ Ir2(jk)− ΣNm=1Irm3 (ik) ≤ 1, (transitivity)
for all distinct events i, j, and k, where E = {ij | sentence dist(i, j)≤ 1}, r̄ is the reverse of r, and N is the number of possible relations for r3 when r1 and r2 are true.
",3.1 Inference,[0],[0]
Our formulation in Eq.,3.1 Inference,[0],[0]
"(2) is different from previous work (Chambers and Jurafsky, 2008; Do et al., 2012) in two aspects: 1) We restrict our event pairs ij to a smaller set E = {ij | sentence dist(i, j)≤ 1} where pairs that are
more than one sentence away are deleted for computational efficiency and (usually) for better performance.",3.1 Inference,[0],[0]
"In fact, to make better use of global constraints, we should have allowed more event pairs in Eq.",3.1 Inference,[0],[0]
(2).,3.1 Inference,[0],[0]
"However, fr(ij) is usually more reliable when i and j are closer in text.",3.1 Inference,[0],[0]
"Many participating systems in TE3 (UzZaman et al., 2013) have used this pre-filtering strategy to balance the trade-off between confidence in fr(ij) and global constraints.",3.1 Inference,[1.0],"['Many participating systems in TE3 (UzZaman et al., 2013) have used this pre-filtering strategy to balance the trade-off between confidence in fr(ij) and global constraints.']"
"We observe that the strategy fits very well to the existing datasets: As shown in Fig. 3, annotated TLINKs barely exist if two events are two sentences away.",3.1 Inference,[0],[0]
"2) Previously, transitivity constraints were formulated as Ir1(ij) + Ir2(jk)",3.1 Inference,[0],[0]
"− Ir3(ik) ≤ 1, which is a special case when N = 1 and can be understood as “r1 and r2 determine a single r3”.",3.1 Inference,[0],[0]
"However, it was overlooked that, although some r1 and r2 cannot uniquely determine r3, they can still constrain the set of labels r3 can take.",3.1 Inference,[0],[0]
"For example, as shown in Fig. 4, when r1=before and r2=is included, r3 is not determined",3.1 Inference,[0],[0]
"but we know that r3 ∈ {before, is included}1.",3.1 Inference,[0],[0]
"This information can be easily exploited by allowing N > 1.
",3.1 Inference,[0],[0]
"With these two differences, the optimization problem (2) can still be efficiently solved using off-the-shelf ILP packages such as GUROBI
1The transitivity table in Allen (1983) shows two more possible relations, overlap and immediately before, which are not in our label set.
",3.1 Inference,[0],[0]
"(Gurobi Optimization, Inc., 2012).",3.1 Inference,[0],[0]
"With the inference solver defined above, we propose to use the structured perceptron (Collins, 2002) as a representative for the inference based training (IBT) algorithm to learn those weight vectors wr.",3.2 Learning,[0],[0]
"Specifically, let L = {xk,yk}Kk=1 be the labeled training set of K instances (usually documents).",3.2 Learning,[0],[0]
The structured perceptron training algorithm for this problem is shown in Algorithm 1.,3.2 Learning,[0],[0]
"The Illinois-SL package (Chang et al., 2010) was used in our experiments for its structured perceptron component.",3.2 Learning,[0],[0]
"In terms of the features used in this work, we adopt the same set of features designed for E-E TLINKs in Sec. 3.1 of Do et al. (2012).
",3.2 Learning,[0],[0]
"In Algorithm 1, Line 6 is the inference step as in Eq.",3.2 Learning,[0],[0]
"(1) or (2), which is augmented with a closure operation on ŷ in the following line.",3.2 Learning,[0],[0]
"In the case in which there is only one pair of events in each instance (thus no structure to take advantage of), Algorithm 1 reduces to the conventional perceptron algorithm and Line 6 simply chooses the top scoring label.",3.2 Learning,[0],[0]
"With a structured instance instead, Line 6 becomes slower to solve, but it can provide valuable information so that the perceptron learner is able to look further at other labels rather than an isolated pair.",3.2 Learning,[0],[0]
"For example in Ex1 and Fig. 1, the fact that (ripping,ordered)=before is established through two other relations: 1) ripping is an adverbial participle and thus included in cascaded and 2) cascaded is before ordered.",3.2 Learning,[0],[0]
"If (ripping,ordered)=before is presented to a local learning algorithm without knowing its predictions on (ripping,cascaded) and (cascaded,ordered), then the model either cannot support it or overfits it.",3.2 Learning,[0],[0]
"In IBT, however, if the classifier was correct in deciding (ripping,cascaded) and (cascaded,ordered), then (ripping,ordered) would be correct automatically and would not contribute to updating the classifier.",3.2 Learning,[0],[0]
The scarcity of training data and the difficulty in annotation have long been a bottleneck for temporal processing systems.,3.3 Semi-supervised Structured Learning,[0],[0]
"Given the inherent global constraints in temporal graphs, we propose to perform semi-supervised structured learning using the constraint-driven learning (CoDL) algorithm (Chang et al., 2007, 2012), as shown in Algorithm 2, where the function “Learn” in Lines 2 and 9 represents any standard learning algorithm
Algorithm 1: Structured perceptron algorithm for temporal relations
Input: Training set L = {xk,yk}Kk=1, learning rate λ
1 Perform graph closure on each yk 2",3.3 Semi-supervised Structured Learning,[0],[0]
"Initialize wr = 0, ∀r ∈ Y 3 while convergence criteria not satisfied do 4 Shuffle the examples in L 5 foreach (x,y) ∈ L do 6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 if ŷ",3.3 Semi-supervised Structured Learning,[0],[0]
6=,3.3 Semi-supervised Structured Learning,[0],[0]
"y then 9 wr = wr + λ( ∑ i:yi=r
φi−∑",3.3 Semi-supervised Structured Learning,[0],[0]
"i:ŷi=r φi), ∀r ∈ Y
10 return {wr}r∈Y
(e.g., perceptron, SVM, or even structured perceptron",3.3 Semi-supervised Structured Learning,[0],[0]
"; here we used the averaged perceptron (Freund and Schapire, 1998)) and subscript “r” means selecting the learned weight vector for relation r ∈ Y .",3.3 Semi-supervised Structured Learning,[0],[0]
"CoDL improves the model learned from a small amount of labeled data by repeatedly generating feedback through labeling unlabeled examples, which is in fact a semi-supervised version of IBT.",3.3 Semi-supervised Structured Learning,[1.0],"['CoDL improves the model learned from a small amount of labeled data by repeatedly generating feedback through labeling unlabeled examples, which is in fact a semi-supervised version of IBT.']"
"Experiments show that this scheme is indeed helpful in this problem.
",3.3 Semi-supervised Structured Learning,[0],[0]
Algorithm 2: Constraint-driven learning algorithm,3.3 Semi-supervised Structured Learning,[0],[0]
"Input: Labeled set L, unlabeled set U ,
weighting coefficient γ 1",3.3 Semi-supervised Structured Learning,[0],[0]
"Perform closure on each graph in L 2 Initialize wr = Learn(L)r, ∀ r ∈",3.3 Semi-supervised Structured Learning,[0],[0]
Y 3 while convergence criteria not satisfied do 4 T = ∅ 5 foreach x ∈ U do,3.3 Semi-supervised Structured Learning,[0],[0]
"6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 T = T ∪ {(x, ŷ)} 9 wr = γwr + (1− γ)Learn(T )",3.3 Semi-supervised Structured Learning,[0],[0]
"r,∀ r ∈ Y
10 return {wr}r∈Y",3.3 Semi-supervised Structured Learning,[0],[0]
"Since even human annotators find it difficult to annotate temporal graphs, many of the TLINKs are left unspecified by annotators (compare Fig. 2 to Fig. 1).",4 Missing Annotations,[0],[0]
"While some of these missing TLINKs can be inferred from existing ones, the vast majority still remain unknown as shown in Table 1.",4 Missing Annotations,[1.0],"['While some of these missing TLINKs can be inferred from existing ones, the vast majority still remain unknown as shown in Table 1.']"
"De-
spite the existence of denser annotation schemes (e.g., Cassidy et al. (2014)), the TLINK annotation task is quadratic in the number of nodes, and it is practically infeasible to annotate complete graphs.",4 Missing Annotations,[0],[0]
"Therefore, the problem of identifying these unknown relations in training and test is a major issue that dramatically hurts existing methods.
",4 Missing Annotations,[0],[0]
We could simply use these unknown pairs (or some filtered version of them) to design rules or train classifiers to identify whether a TLINK is vague or not.,4 Missing Annotations,[1.0],['We could simply use these unknown pairs (or some filtered version of them) to design rules or train classifiers to identify whether a TLINK is vague or not.']
"However, we propose to exclude both the unknown pairs and the vague classifier from the training process – by changing the structured loss function to ignore the inference feedback on vague TLINKs (see Line 9 in Algorithm 1 and Line 9 in Algorithm 2).",4 Missing Annotations,[0],[0]
"The reasons are discussed below.
",4 Missing Annotations,[0],[0]
"First, it is believed that a lot of the unknown pairs are not really vague but rather pairs that the annotators failed to look at (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014).",4 Missing Annotations,[1.0],"['First, it is believed that a lot of the unknown pairs are not really vague but rather pairs that the annotators failed to look at (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014).']"
"For example, (cascaded, monitor) should be annotated as before but is missing in Fig. 2.",4 Missing Annotations,[0],[0]
It is hard to exclude this noise in the data during training.,4 Missing Annotations,[1.0],['It is hard to exclude this noise in the data during training.']
"Second, compared to the overwhelmingly large number of unknown TLINKs (89.5% as shown in Table 1), the scarcity of non-vague TLINKs makes it hard to learn a good vague classifier.",4 Missing Annotations,[1.0],"['Second, compared to the overwhelmingly large number of unknown TLINKs (89.5% as shown in Table 1), the scarcity of non-vague TLINKs makes it hard to learn a good vague classifier.']"
"Third, vague is fundamentally different from the other relation types.",4 Missing Annotations,[0],[0]
"For example, if a before TLINK can be established given a sentence, then it always holds as before regardless of other events around it, but if a TLINK is vague given a sentence, it may still change to other types afterwards if a connection can later be established through other nodes from the context.",4 Missing Annotations,[0],[0]
"This distinction emphasizes that vague is a consequence of lack of background/contextual information, rather than a concrete relation type to be trained on.",4 Missing Annotations,[0],[0]
"Fourth, without the vague classifier, the predicted temporal graph tends to become more densely connected, thus the global transitivity constraints can be more effective in correcting local mistakes (Chambers
and Jurafsky, 2008).",4 Missing Annotations,[1.0000000109481755],"['Fourth, without the vague classifier, the predicted temporal graph tends to become more densely connected, thus the global transitivity constraints can be more effective in correcting local mistakes (Chambers and Jurafsky, 2008).']"
"However, excluding the local classifier for vague TLINKs would undesirably assign nonvague TLINKs to every pair of events.",4 Missing Annotations,[0],[0]
"To handle this, we take a closer look at the vague TLINKs.",4 Missing Annotations,[1.0],"['To handle this, we take a closer look at the vague TLINKs.']"
We note that a vague TLINK could arise in two situations if the annotators did not fail to look at it.,4 Missing Annotations,[0],[0]
"One is that an annotator looks at this pair of events and decides that multiple relations can exist, and the other one is that two annotators disagree on the relation (similar arguments were also made in Cassidy et al. (2014)).",4 Missing Annotations,[1.0],"['One is that an annotator looks at this pair of events and decides that multiple relations can exist, and the other one is that two annotators disagree on the relation (similar arguments were also made in Cassidy et al. (2014)).']"
"In both situations, the annotators first try to assign all possible relations to a TLINK, and then change the relation to vague if more than one can be assigned.",4 Missing Annotations,[1.0],"['In both situations, the annotators first try to assign all possible relations to a TLINK, and then change the relation to vague if more than one can be assigned.']"
"This human annotation process for vague is different from many existing methods, which either identify the existence of a TLINK first (using rules or machinelearned classifiers) and then classify, or directly include vague as a classification label along with other non-vague relations.
",4 Missing Annotations,[0],[0]
"In this work, however, we propose to mimic this mental process by a post-filtering method2.",4 Missing Annotations,[0],[0]
"Specifically, we take each TLINK produced by ILP and determine whether it is vague using its relative entropy (the Kullback-Leibler divergence) to the uniform distribution.",4 Missing Annotations,[1.0],"['Specifically, we take each TLINK produced by ILP and determine whether it is vague using its relative entropy (the Kullback-Leibler divergence) to the uniform distribution.']"
"Let {rm}Mm=1 be the set of relations that the i-th pair of events can take, we filter the i-th TLINK given by ILP by:
δi = M∑ m=1 frm(φi) log (Mfrm(φi)),
where frm(φi) is the soft-max score of rm, obtained by the local classifier for rm.",4 Missing Annotations,[1.0000001066027246],"['Let {rm}Mm=1 be the set of relations that the i-th pair of events can take, we filter the i-th TLINK given by ILP by: δi = M∑ m=1 frm(φi) log (Mfrm(φi)), where frm(φi) is the soft-max score of rm, obtained by the local classifier for rm.']"
"We then compare δi to a fixed threshold τ to determine the vagueness of this TLINK; we accept its originally predicted label if δi > τ , or change it to vague otherwise.",4 Missing Annotations,[1.0],"['We then compare δi to a fixed threshold τ to determine the vagueness of this TLINK; we accept its originally predicted label if δi > τ , or change it to vague otherwise.']"
Using relative entropy here is intuitively appealing and empirically useful as shown in the experiments section; better metrics are of course yet to be designed.,4 Missing Annotations,[1.0],['Using relative entropy here is intuitively appealing and empirically useful as shown in the experiments section; better metrics are of course yet to be designed.']
"The TempEval3 (TE3) workshop (UzZaman et al., 2013) provided the TimeBank (TB) (Pustejovsky et al., 2003b), AQUAINT (AQ) (Graff, 2002), Silver (TE3-SV), and Platinum (TE3-PT) datasets,
2Some systems (e.g., TARSQI (Verhagen and Pustejovsky, 2008)) employed a similar idea from a different standpoint, by thresholding TLINKs based on confidence scores.
where TB and AQ are usually for training, and TE3-PT is usually for testing.",5.1 Datasets,[0],[0]
"The TE3-SV dataset is a much larger, machine-annotated and automatically-merged dataset based on multiple systems, with the intention to see if these “silver” standard data can help when included in training (although almost all participating systems saw performance drop with TE3-SV included in training).
",5.1 Datasets,[0.9999999773870705],"['The TE3-SV dataset is a much larger, machine-annotated and automatically-merged dataset based on multiple systems, with the intention to see if these “silver” standard data can help when included in training (although almost all participating systems saw performance drop with TE3-SV included in training).']"
Two popular augmentations on TB are the VerbClause temporal relation dataset (VC) and TimebankDense dataset (TD).,5.1 Datasets,[0],[0]
"The VC dataset has specially annotated event pairs that follow the socalled Verb-Clause structure (Bethard et al., 2007), which is usually beneficial to be included in training (UzZaman et al., 2013).",5.1 Datasets,[0],[0]
The TD dataset contains 36 documents from TB which were reannotated using the dense event ordering framework proposed in Cassidy et al. (2014).,5.1 Datasets,[0],[0]
The experiments included in this paper will involve the TE3 datasets as well as these augmentations.,5.1 Datasets,[0],[0]
"Therefore, some statistics on them are shown in Table 2 for the readers’ information.",5.1 Datasets,[0],[0]
"In addition to the state-of-the-art systems, another two baseline methods were also implemented for a better understanding of the proposed ones.",5.2 Baseline Methods,[0],[0]
"The first is the regularized averaged perceptron (AP) (Freund and Schapire, 1998) implemented in the LBJava package (Rizzolo and Roth, 2010) and is a local method.",5.2 Baseline Methods,[0],[0]
"On top of the first baseline, we performed global inference in Eq.(2), referred to as the L+I baseline (AP+ILP).",5.2 Baseline Methods,[0],[0]
"Both of them used the same feature set (i.e., as designed in Do et al. (2012))",5.2 Baseline Methods,[0],[0]
as in the proposed structured perceptron (SP) and CoDL for fair comparisons.,5.2 Baseline Methods,[0],[0]
"To clarify,
SP and CoDL are training algorithms and their immediate outputs are the weight vectors {wr}r∈Y for local classifiers.",5.2 Baseline Methods,[0],[0]
"An ILP inference was performed on top of them to yield the final output, and we refer to it as “S+I” (i.e., structured learning+inference) methods.",5.2 Baseline Methods,[0],[0]
"To show the benefit of using structured learning, we first tested one scenario where the gold pairs of events that have a non-vague TLINK were known priori.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This setup was a standard task presented in TE3, so that the difficulty of detecting vague TLINKs was ruled out.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This setup also helps circumvent the issue that TE3 penalizes systems which assign extra labels that do not exist in the annotated graph, while these extra labels may be actually correct because the annotation itself might be incomplete.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"UTTime (Laokulrat et al., 2013) was the top system in this task in TE3.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"Since UTTime is not available to us, and its performance was reported in TE3 in terms of both E-E and E-T TLINKs together, we locally trained an E-T classifier based on Do et al. (2012) and included its prediction only for fair comparison.
",5.3.1 TE3 Task C - Relation Only,[0],[0]
UTTime is a local method and was trained on TB+AQ and tested on TE3-PT.,5.3.1 TE3 Task C - Relation Only,[0],[0]
We used the same datasets for our local baseline and its performance is shown in Table 3 under the name “AP-1”.,5.3.1 TE3 Task C - Relation Only,[0],[0]
Note that the reported numbers below are the temporal awareness scores obtained from the official evaluation script provided in TE3.,5.3.1 TE3 Task C - Relation Only,[0],[0]
"We can see that UTTime is about 3% better than AP-1 in the absolute value of F1, which is expected since UTTime included more advanced features derived from syntactic parse trees.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"By adding the VC and TD datasets into the training set, we retrained our local baseline and achieved comparable performance to
UTTime (“AP-2” in Table 3).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"On top of AP-2, a global inference step enforcing symmetry and transitivity constraints (“AP+ILP”) can further improve the F1 score by 9.3%, which is consistent with previous observations (Chambers and Jurafsky, 2008; Do et al., 2012).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"SP+ILP further improved the performance in precision, recall, and F1 significantly (per the McNemar’s test (Everitt, 1992; Dietterich, 1998) with p <0.0005), reaching an F1 score of 67.2%.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This meets our expectation that structured learning can be better when the local problem is difficult (Punyakanok et al., 2005).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"In the first scenario, we knew in advance which TLINKs existed or not, so the “pre-filtering” (i.e., ignoring distant pairs as mentioned in Sec. 3.1 and “post-filtering” methods were not used when generating the results in Table 3.",5.3.2 TE3 Task C,[0],[0]
"We then tested a more practical scenario, where we only knew the events, but did not know which ones are related.",5.3.2 TE3 Task C,[0],[0]
"This setup was Task C in TE3 and the top system was ClearTK (Bethard, 2013).",5.3.2 TE3 Task C,[0],[0]
"Again, for fair comparison, we simply added the E-T TLINKs predicted by ClearTK.",5.3.2 TE3 Task C,[0],[0]
"Moreover, 10% of the training data was held out for development.",5.3.2 TE3 Task C,[0],[0]
"Corresponding results on the TE3-PT testset are shown in Table 4.
",5.3.2 TE3 Task C,[0],[0]
"From lines 2-4, all systems see significant drops in performance if compared with the same entries in Table 3.",5.3.2 TE3 Task C,[0],[0]
It confirms our assertion that how to handle vague TLINKs is a major issue for this temporal relation extraction problem.,5.3.2 TE3 Task C,[0],[0]
"The improvement of SP+ILP (line 4) over AP (line 2) was small and AP+ILP (line 3) was even worse than AP, which necessitates the use of a better approach
towards vague TLINKs.",5.3.2 TE3 Task C,[0],[0]
"By applying the postfiltering method proposed in Sec. 4, we were able to achieve better performances using SP+ILP (line 5), which shows the effectiveness of this strategy.",5.3.2 TE3 Task C,[0],[0]
"Finally, by setting U in Algorithm 2 to be the TE3-SV dataset, CoDL+ILP (line 6) achieved the best F1 score with a relative improvement over ClearTK being 14.8%.",5.3.2 TE3 Task C,[0],[0]
"Note that when using TE3SV in this paper, we did not use its annotations on TLINKs because of its well-known large noise (UzZaman et al., 2013).
",5.3.2 TE3 Task C,[0],[0]
"In UzZaman et al. (2013), we notice that the best performance of ClearTK was achieved when trained on TB+VC (line 7 is higher than its reported values in TE3 because of later changes in ClearTK), so we retrained the proposed systems on the same training set and results are shown on lines 8-9.",5.3.2 TE3 Task C,[0],[0]
"In this case, the improvement of S+I over Local was small, which may be due to the lack of training data.",5.3.2 TE3 Task C,[0],[0]
"Note that line 8 was still significantly different to line 7 per the McNemar’s test, although there was only 0.2% absolute difference in F1, which can be explained from their large differences in precision and recall.",5.3.2 TE3 Task C,[0],[0]
"The proposed structured learning approach was further compared to a recent system, a CAscading EVent Ordering architecture (CAEVO) proposed in Chambers et al. (2014) (lines 10-13).",5.3.3 Comparison with CAEVO,[0],[0]
We used the same training set and test set as CAEVO in the S+I systems.,5.3.3 Comparison with CAEVO,[0],[0]
"Again, we added the E-T TLINKs predicted by CAEVO to both S+I systems.",5.3.3 Comparison with CAEVO,[0],[0]
"In Chambers et al. (2014), CAEVO was reported on the straightforward evaluation metric including the vague TLINKs, but the temporal awareness scores
were used here, which explains the difference between line 11 in Table 4 and what was reported in Chambers et al. (2014).
ClearTK was reported to be outperformed by CAEVO on TD-Test (Chambers et al., 2014), but we observe that ClearTK on line 10 was much worse even than itself on line 7 (trained on TB+VC) and on line 1 (trained on TB+AQ+VC+TD) due to the annotation scheme difference between TD and TB/AQ/VC. ClearTK was designed mainly for TE3, aiming for high precision, which is reflected by its high precision on line 10, but it does not have enough flexibility to cope with two very different annotation schemes.",5.3.3 Comparison with CAEVO,[0],[0]
"Therefore, we have chosen CAEVO as the baseline system to evaluate the significance of the proposed ones.",5.3.3 Comparison with CAEVO,[0],[0]
"On the TD-Test dataset, all systems other than ClearTK had better F1 scores compared to their performances on TE3-PT.",5.3.3 Comparison with CAEVO,[0],[0]
"This notable difference (i.e., 48.53 vs 40.3) indicates the better quality of the dense annotation scheme that was used to create TD (Cassidy et al., 2014).",5.3.3 Comparison with CAEVO,[0],[0]
"SP+ILP outperformed CAEVO and if additional unlabeled dataset TE3-SV was used, CoDL+ILP achieved the best score with a relative improvement in F1 score being 6.3%.
",5.3.3 Comparison with CAEVO,[0],[0]
"We notice that the proposed systems often have higher recall than precision, and that this is less an issue on a densely annotated testset (TD-Test), so their low precision on TE3-PT possibly came from the missing annotations on TE3-PT.",5.3.3 Comparison with CAEVO,[0],[0]
It is still under investigation how to control precision and recall in real applications.,5.3.3 Comparison with CAEVO,[0],[0]
We develop a structured learning approach to identifying temporal relations in natural language text and show that it captures the global nature of this problem better than state-of-the-art systems do.,6 Conclusion,[0],[0]
A new perspective towards vague relations is also proved to gain from fully taking advantage of the structured approach.,6 Conclusion,[0],[0]
"In addition, the global nature of this problem gives rise to a better way of making use of the readily available unlabeled data, which further improves the proposed method.",6 Conclusion,[0],[0]
"The improved performance on both TE3-PT and TDTest, two differently annotated datasets, clearly shows the advantage of the proposed method over existing methods.",6 Conclusion,[0],[0]
We plan to build on the notable improvements shown here and expand this study to deal with additional temporal reasoning problems in natural language text.,6 Conclusion,[0],[0]
We thank all the reviewers for providing useful comments.,Acknowledgements,[0],[0]
This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizon Network; by the US Defense Advanced Research Projects Agency (DARPA) under contract FA8750-13-2-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053.,Acknowledgements,[0],[0]
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies of the U.S. Government.,Acknowledgements,[0],[0]
Identifying temporal relations between events is an essential step towards natural language understanding.,abstractText,[0],[0]
"However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events.",abstractText,[0],[0]
"Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators.",abstractText,[0],[0]
This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge.,abstractText,[0],[0]
"As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods.",abstractText,[0],[0]
"As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem.",abstractText,[0],[0]
A Structured Learning Approach to Temporal Relation Extraction,title,[0],[0]
