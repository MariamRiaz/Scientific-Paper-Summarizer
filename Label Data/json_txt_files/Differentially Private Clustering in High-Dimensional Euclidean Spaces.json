{"sections": [{"text": "\u221a dOPT+\npoly(log n, dd, kd). We also study the case where the data points are s-sparse and show that the clustering loss can scale logarithmically with d, i.e., log3(n)OPT + poly(log n, log d, k, s). Experiments on both synthetic and real datasets verify the effectiveness of the proposed method."}, {"heading": "1. Introduction", "text": "In this work, we consider the problem of clustering sensitive data while preserving the privacy of individuals represented in the dataset. In particular, we consider k-means and k-median clustering under the constraint of differential privacy, which is a popular information-theoretic notion of privacy (Dwork et al., 2014; 2006) that roughly requires the output of algorithm to be insensitive to changes in an individual\u2019s data. Clustering is an important building block for many data processing tasks with applications in recommendation systems (McSherry & Mironov, 2009), database systems (Ester et al., 1996), image processing (Zhang et al., 2014; 2015; Pappas, 1992), and data mining (Berkhin, 2006). Improved privacy-preserving clustering algorithms have the potential to significantly improve\n1Carnegie Mellon University, Pittsburgh, PA, USA 2Princeton University, Princeton, NJ, USA 3Peking University, Beijing, China. Correspondence to: Wenlong Mou <mouwenlong@pku.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe quality of many different areas of private data analysis.\nFormally, we consider the following problem: Given a set of points x1, . . . , xn in Rd, privately find a set of k centers z1, . . . , zk in Rd that approximately minimize one of the following clustering objectives:\nn\u2211 i=1\nmin j \u2016xi \u2212 zj\u20162\ufe38 \ufe37\ufe37 \ufe38\nk-means objective\nor n\u2211 i=1\nmin j \u2016xi \u2212 zj\u2016\ufe38 \ufe37\ufe37 \ufe38 k-median objective . (1)\nMinimizing these objectives exactly is NP-hard (Dasgupta, 2008), so we instead seek to find approximate solutions whose clustering objective is at most \u03b1\u00d7OPT+ \u03b2, where OPT denotes the optimal objective. Unlike the non-private setting, any differentially private clustering algorithm must have \u03b2 > 0 (Bassily et al., 2014).\nDespite a large amount of work on private clustering, many fundamental problems remain open. One of the longstanding challenges is to design polynomial-time private k-means and k-median clustering algorithms with small clustering loss for the high-dimensional, big-data setting. There is significant evidence to indicate that even without the requirement on privacy, exact optimization of these objective function in Euclidean spaces might be computationally infeasible (Dasgupta, 2008; Aloise et al., 2009): The best-known result in this line of research is (9 + )\u00d7OPT by the local search algorithm (Kanungo et al., 2002). When the space is discrete, there exists a private algorithm that provides slightly better loss guarantee 6 \u00d7 OPT (Gupta et al., 2010). However, this problem becomes notoriously hard in the context of differential privacy in Euclidean spaces, as one typically needs to preserve privacy for each point in the much larger Euclidean space. While there exist algorithms in this setting with clustering loss polylog(k) \u00d7 OPT + O(n/ log2 n) (Nock et al., 2016), or\u221a d \u00d7 OPT + poly(dd, kd, log n) when d is a small constant (Feldman et al., 2009) (See Table 1), the problem is left unresolved in the big-data (large n), high-dimensional (d = \u2126(polylogn)) scenarios if we require both \u03b1 and \u03b2 to be as small as polylog(n). Note that running the algorithm of Feldman et al. (2009) after projecting to O(log n) dimensional space gives O(log(n)log(n)) error, and the algorithm of Nock et al. (2016) has O\u0303(n) additive loss. Moreover, brute-force discretization in O(log n) dimensional space\ndoes not give a polynomial time algorithm.\nGiven the difficulty of the general form of private clustering, many positive results in this line of research have focused on the assumptions that the data points are wellseparated. A set of data points are called well-separated if all near-optimal clusterings of data induce similar data partitions. We can exploit such a structural assumption in many aspects. For example, these well-separated datasets are amenable to slightly perturbed initialization (Ostrovsky et al., 2012). This is the main insight behind prior work for private k-means clustering (Nissim et al., 2007; Wang et al., 2015). However, these observations and techniques break down in the general case.\nAnother interesting setting is when the data is extremely high dimensional, but each example is s-sparse. In this case, we might hope to improve the additive error \u03b2 to be as small as poly(log d, k, log n, s). When the entries of data points are dense, the dependence of \u03b2 = poly(d) is in general inevitable even if the number of centers is 1, according to the lower bound for private empirical risk minimization (Bassily et al., 2014). While some prior works have explored the possibility of non-private clustering in the context of sparse data (Barger & Feldman, 2016), much remains unknown in the private setting."}, {"heading": "1.1. Our Contributions", "text": "Our work tackles the problem of private clustering in high-dimensional Euclidean spaces, specifically in the case when we have plentiful high-dimensional data. We advance the state-of-the-art in several aspects: \u2022 We design and analyze a computationally efficient algo-\nrithm for private k-means clustering with clustering loss at most log3(n) \u00d7 OPT + poly(log n, d, k) (See Corollary 1). In contrast to Nock et al. (2016) and Nissim et al. (2007), our algorithm achieves small clustering loss without additional assumptions on data. Furthermore, our clustering loss bound is also competitive even under their assumptions.\n\u2022 We extend our algorithm to the problem of k-median clustering. The clustering loss is at most log3/2(n) \u00d7\nOPT+poly(log n, d, k) (See Theorem 12). Our guarantee advances the state-of-the-art results of Feldman et al. (2009) in the high-dimensional space.\n\u2022 In the case of s-sparse data, we further improve the additive error term \u03b2 to be at most poly(log n, log d, s, k) for private k-means clustering (See Corollary 2). To the best of knowledge, this is the first result concerning the computationally efficient, differentially private clustering algorithm for high-dimensional s-sparse data.\n\u2022 We propose an approach for privately constructing a candidate set of centers with approximation guarantee (See Theorem 5). The candidate set can be potentially applied to other problems and is of independent interest.\n\u2022 We empirically compare our algorithm with the nonprivate k-means++ algorithm and four strong private baseilnes. Across all datasets, our algorithm is competitive with k-means++ and significantly outperforms the private baselines, especially for large dimensional data."}, {"heading": "1.2. Our Techniques", "text": "Our algorithm has two main steps: First, we use the Johnson-Lindenstrauss (JL) transform to project the data into O(log n)-dimensional space and use a novel technique to privately construct a small set of good candidate centers in the projected space; Then we apply a discrete clustering algorithm to privately find k good centers from the candidate set. Centers in the original space are recovered by noisy averaging.\nPrivate Candidate Set: Our algorithm uses a novel private technique that recursively subdivides low-dimensional Euclidean spaces to construct a set of candidate points containing good centers. The algorithm proceeds in rounds, recursively subdividing the space into multiple cubes until there are few points in each cube. Finally, the algorithm outputs the centers of all cubes as a candidate set of centers.\nIn order to make the above procedure work well, we need to achieve three goals: (a) The output preserves privacy; (b) The algorithm is computationally efficient, i.e., the size\nof candidate set is polynomial in n and k; and (c) The candidate set has high (\u03b1, \u03b2)-approximation rate, namely, it contains a subset of size k on which the clustering loss is at most \u03b1 \u00d7 OPT + \u03b2 with \u03b1, \u03b2 small. To achieve goal (a), our algorithm randomizes the decision of whether to subdivide a cube or not. To achieve goal (b), we make the probability of empty cube being further divided negligible, and the size of candidate set is upper bounded by the size of a simple partition tree, which is poly(n, k). For goal (c), it suffices to ensure each of the cluster centers to be captured within distance at scale of its own radius. As a lot of data points will be gathered around an optimal center, we can put candidate centers at each cube containing many points during partition. Random shift and repetition are used to avoid the worst cases.\nFrom Candidate Set to Private Clustering: Our private clustering algorithm then follows the technique of local swapping on the discrete set of candidate centers, inspired by the work of Gupta et al. (2010) for k-median clustering. Their algorithm maintains a set of k centers and greedily replaces one center with a better one from the candidate set. However, Gupta et al. (2010)\u2019s algorithm only works for the k-median problem where the loss obeys the triangle inequality. To extend the analysis to the k-means problem, we adopt the techniques of Kanungo et al. (2002). In particular, we construct k swap pairs of points, take the average of gains of these swap pairs, and relate it to the optimal loss OPT. Finally, we recover the centers in the original high-dimensional space privately. Given the centers in the projected space, the centers in the original space has low sensitivity. We can thus take the noisy mean of a cluster to obtain a private center for each cluster."}, {"heading": "2. Related Work", "text": "The problem of private clustering in the Euclidean spaces was investigated by Blum et al. (2005), who proposed a private version of Lloyd iteration, which we refer to as SuLQ k-means. However, the algorithm suffers from the absence of uniform guarantee on the clustering loss. Given the sensitive and non-convex nature of clustering objective, Nissim et al. (2007) and Wang et al. (2015) applied the sampleand-aggregate framework to address the problem of private clustering by making strong assumptions on the input data. When no assumption is made on the structure of data, Feldman et al. (2009) provided an information-theoretic upper bound OPT + poly(k, d, log n) for the clustering loss according to the brute-force discretization of whole space and the exponential mechanism. However, no computationally efficient algorithm is available in the high-dimensional Euclidean spaces with clustering loss even close to this bound: Feldman et al. (2009) proposed an efficient algorithm for the bi-criteria approximation in the constantdimensional spaces, but their additive loss term \u03b2 actu-\nally exponentially depends on d; Gupta et al. (2010) designed an algorithm with a constant-factor approximation ratio and poly(k, log |V |) additive loss term for clustering in the finite-data space V , but the algorithm does not work in the Euclidean spaces. Recently, Nock et al. (2016) proposed a private version of the k-means++ algorithm. However, the additive loss term \u03b2 therein is almost as high as the data size n."}, {"heading": "3. Preliminaries", "text": "We define some notation and clarify our problem setup.\nNotation: We will use capital letters to represent matrices or datasets and lower-case letter to represent vectors or single data points. For a vector v, v[i] denotes the ith entry of v. We denote by M(X) the output of an algorithm with input dataset X . We will frequently use d to indicate the dimension of input space, p to indicate the dimension of space after projection, and \u039b to indicate the radius of input data. For vector norms, we will denote by \u2016 \u00b7 \u2016 the `2 norm, \u2016 \u00b7 \u20160 the number of non-zero entries, and \u2016 \u00b7 \u2016\u221e the maximum of absolute value among entries. Define B(x,\u039b) = {y : \u2016x \u2212 y\u2016 \u2264 \u039b}. We denote by U([\u2212\u039b,\u039b]p) the uniform distribution in the p-dimensional cube [\u2212\u039b,\u039b]p. For any set V , we denote by |V | the cardinality of the set. We will frequently denote the clustering loss in problem (1) on the centers z1, z2, ..., zk by L(z1, z2, ..., zk). A sample drawn from one-dimensional Laplace distribution with density f(x) = 12b exp ( \u2212 |x|b ) is denoted by Lap(b).\nProblem Setup: We use the following definition of differential privacy:\nDefinition 1 ( -Differential Privacy). A randomized algorithmM with output range O is -differentially private if for any given set S \u2286 O and two datasets X \u2208 Rd\u00d7n and Y = [X; z] for any z \u2208 Rd, we have e\u2212 Pr[M(X) \u2208 S] \u2264 Pr[M(Y ) \u2208 S] \u2264 e Pr[M(X) \u2208 S].\nIn this paper, we study the problem of private clustering in high-dimensional Euclidean spaces without making any assumptions on the data structure. Formally, we define our problem as follows.\nProblem 1 (Private Clustering in High-Dimensional Euclidean Spaces). Suppose d = \u2126(polylog(n)) is the dimension of Euclidean space. Given bounded data points x1, x2, ..., xn \u2208 Rd as input, how can we efficiently output k centers z1, z2, ..., zk such that the algorithm is -differentially private and the clustering loss is at most polylog(n) \u00d7 OPT + poly(d, log n, k, 1 )? In the case of sparse data where \u2016xi\u20160 \u2264 s for each i, can we improve the clustering loss to polylog(n) \u00d7 OPT + poly(log d, log n, k, s, 1 )?\nAlgorithm 1 private partition({xi}ni=1, , \u03b4,Q). input X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] \u2286 B(0,\u039b) \u2286 Rp, parameters , \u03b4, initial cube Q s.t. {xi}ni=1 \u2286 Q. output Private grid C \u2286 Rp. Initialize depth a = 0, active set of cubesA = {Q}, and set C = \u2205. while a \u2264 log n and A 6= \u2205 do a = a+ 1. C = C \u222a (\u22c3 Qi\u2208A center(Qi) ) .\nfor Qi \u2208 A do Remove Qi from A. PartitionQi evenly in each dimension and obtain 2p\ncubes {Q(l)i }2 p\nl=1. for l \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 2p} do Add Q(l)i to A with probability f ( |Q(l)i \u2229X| ) ,\nwhere\nf(m) =\n{ 1 2 exp(\u2212\n\u2032(\u03b3 \u2212m)), m \u2264 \u03b3, 1\u2212 12 exp( \u2032(\u03b3 \u2212m)), otherwise,\n\u2032 = 2 logn and \u03b3 = 20 \u2032 log n \u03b4 .\nend for end for\nend while"}, {"heading": "4. Private Candidate Set", "text": "In this section, we present an efficient algorithm that constructs a polynomial-sized candidate set of centers privately in the low-dimensional space Rp with dimension p = 8 log n. This algorithm will serve as the building block for the private clustering. Our algorithm works by repeatedly applying a recursive discretization of the space with random shifts. It is worth noticing that direct extension of previous methods such as (Matous\u030cek, 2000) lead to arbitrarily bad quality. The random shift is thus essential to our proof, which will be further explained in Appendix."}, {"heading": "4.1. Private Discretization Routine", "text": "We first describe our subroutine of private discretization, a private recursive division procedure: We start with a cube containing all the data points, privately decide whether to partition the current cubes based on number of data points they contain, and stop when there are few points in each cube. Our algorithm is a variation of the hierarchical partitioning in (Matous\u030cek, 2000), while setting appropriate stopping probabilities preserves privacy for our algorithm (See Algorithm 1).\nTo make the algorithm computationally efficient, we need to show that the number of candidate centers generated by Algorithm 1 is as small as poly(n). This is based on the fact that, by design, no empty cube is subdivided by our\nalgorithm with high probability. A cube is called active if the algorithm will divide the cube into multiple subcubes in the next round. We have the following theorem on the size of candidate set.\nTheorem 1. The set C generated by Algorithm 1 satisfies |C| \u2264 n log n, with probability 1\u2212 \u03b4.\nThough we have generated n log n candidate centers, they are well-aligned and depend only slightly on the data. This alignment makes it possible for us to perform composition argument by the number of recursion instead of by number of points. We have the following theorem on the privacy.\nTheorem 2. Algorithm 1 preserves -differential privacy.\nThe following theorem uses tail bounds for the exponential distribution and the union bound to upper bound the number of points in each cube not subdivided by Algorithm 1.\nTheorem 3. With probability at least 1 \u2212 \u03b4, in Algorithm 1, when a cube Qi is removed from A and its subdivided cubes are not inserted to A, then we have either |Qi \u2229X| \u2264 O ( \u03b3 log n\u03b4 ) , or the edge length of Qi is at most \u039bn ."}, {"heading": "4.2. Private Construction of Candidate Set", "text": "We now give an algorithm that constructs a polynomialsized candidate set with -differential privacy by applying the above procedure of private discretization as a subroutine. A good candidate set should contain k potential centers with small clustering loss relative to OPT. We formalize such a criterion as follows.\nDefinition 2 ((\u03b1, \u03b2)-Approximate Candidate Set). Given a set of points S = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} \u2286 Rp, a set of points C \u2286 Rp is called an (\u03b1, \u03b2)-approximate candidate set of centers, if \u2203z1, z2, \u00b7 \u00b7 \u00b7 , zk \u2208 C such that the clustering loss (1) on these points is at most \u03b1\u00d7 OPT + \u03b2.\nAs an example, the dataset S is itself a (2, 0)-approximate candidate set, although is not private. One may also use an\nAlgorithm 2 candidate({xi}ni=1, , \u03b4). input X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] \u2286 B(0,\u039b) \u2286 Rp, parameters , \u03b4. output Candidate center set C. Initialize C = \u2205. for t = 1, 2, \u00b7 \u00b7 \u00b7 , T = 27k log n\u03b4 do\nSample shift vector v \u223c U([\u2212\u039b,\u039b]p). Let Qv = [\u2212\u039b,\u039b]d + v. C = C \u222a private partition({xi}ni=1, T , \u03b4 T , Qv).\nend for\n\u039b n -cover of B(0,\u039b) to construct an (1, O(k\u039b\u03b3 log n \u03b4 )\u039b 2)- approximate candidate set with privacy. However, this brute-force discretization results in a set of size n\u2126(p) in Rp, which depends on n exponentially even if p = \u0398(log n). In contrast, our following Algorithm 2 efficiently constructs an (O(log3 n),O(kpolylog(n)))-approximate candidate set of size polynomial in n.\nSince Algorithm 2 only sees the private data through repeated application of Algorithm 1, we obtain the following privacy guarantee using standard composition theorems.\nTheorem 4. Algorithm 2 preserves -differential privacy.\nThe remaining key argument is to show the approximation rate of candidate set constructed by Algorithm 2. The randomness and repetition is critical for the algorithm: They make it possible for us to \u201cguess\u201d the position of optimal centers and avoid the worst cases. Figure 1 depicts how each optimal cluster may be captured by a cube of the appropriate scale, provided that the center of the cluster is not near the boundary of a cube. Our proof techniques are partly inspired by random grids for near neighbor reporting (Aiger et al., 2014) and locality sensitive hashing (Andoni & Indyk, 2006). We give a short sketch of our proof in the following, and readers may refer to the Appendix for complete proofs.\nTheorem 5. With probability at least 1 \u2212 \u03b4, Algorithm 2 outputs an ( O(log3 n),O(k\u03b3( T ) log n \u03b4 ) ) -appriximate candidate set of centers, where \u03b3(c) = 40c log n \u03b4 log n, and"}, {"heading": "T = k log n\u03b4 .", "text": "Proof Sketch. There exists a set of fixed but unknown optimal centers u\u22171, u \u2217 2, \u00b7 \u00b7 \u00b7 , u\u2217k, and corresponding optimal clusters S\u22171 , S \u2217 2 , \u00b7 \u00b7 \u00b7 , S\u2217k . We say u\u2217i is captured by C\nwith factor L, if B ( u\u2217j , Lr \u2217 j + O( 1 n ) ) \u2229 C 6= \u2205, where\nr\u2217j = \u221a\n1 |S\u2217j | \u2211 i\u2208S\u2217j \u2016xi \u2212 ul\u20162 is the average loss. We\nwill show that any optimal center is captured with factor O(log3/2 n), unless the size of corresponding cluster is too small.\nFor each u\u2217i , we can guarantee the number of points around it, using Markov Inequality: \u2223\u2223B(u\u2217j , 2r\u2217j ) \u2229 S\u2217j \u2223\u2223 \u2265 12 \u2223\u2223S\u2217j \u2223\u2223. Consider the tree induced by hierarchical partition, as\nAlgorithm 3 localswap({xi}ni=1, C, , \u03b4). input Private dataset {xi}ni=1 \u2286 Rp with \u2016xi\u2016 \u2264 \u039b, pa-\nrameters , \u03b4, candidate set C. output Clustering centers Z = [z1, z2, \u00b7 \u00b7 \u00b7 , zk] \u2286 C.\nUniformly sample k centers i.i.d. fromC and formZ(0). T \u2190 100k log n\u03b4 . for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do\nChoose ( x \u2208 Z(t\u22121), y \u2208 C \\ Z(t\u22121) ) with proba-\nbility in proposition to exp ( \u2212 L(Z\n\u2032)\u2212L(Z(t\u22121)) 8\u039b2(T+1)\n) ,\nwhere Z \u2032 = Z(t\u22121) \u2212 {x}+ {y}. Z(t) \u2190 Z(t\u22121) \u2212 {x}+ {y}.\nend for Choose t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , T} with probability in proportion\nto exp ( \u2212 L(Z\n(t)) 8(T+1)\u039b2\n) .\nOutput Z(t).\nshown in Theorem 3, it doesn\u2019t stop being divided until either there\u2019re only \u03b3( T ) log n \u03b4 data points within it, or it\u2019s edge length is less than 1n . Since the center of a cube Ql can capture points within this cube with factor \u221a p, we only need to show the partition tree is activated at a level with edge length pr\u2217j .\nIf the ball around u\u2217j is completely contained in Ql, we\u2019ve already capture this center with O(log3 n) factor. But actually the ball can be divided into several cubes, making it hard to activate this cube. That\u2019s why we turn to the random shift. Using geometric arguments we can show that, B(u\u2217j , 2r\u2217j ) \u2286 Ql with constant probability. Several repetitions are then used to boost the probability of success, and to make it uniformly hold for k centers.\nTherefore, we can guarantee that each optimal cluster with size at least \u2126 ( k\u03b3 log n\u03b4 ) will be captured. Smaller clusters can be ignored safely, as its contribution to the total clustering loss goes to the \u03c3 = O ( k log\n3 n \u03b4\n) term."}, {"heading": "5. From Candidate Set to Private Clustering", "text": "In this section, we develop an efficient algorithm of private clustering based on the candidate set of centers that we construct in the low-dimensional spaces. Technically, our approach is a two-step procedure of private discrete clustering in the low-dimensional space and private recovery in the original high-dimensional space. In particular, the step of private discrete clustering extends the work of Gupta et al. (2010) to the k-means problem on the candidate set of centers, and the step of private recovery outputs k centers in the input space.\n5.1. Private Discrete Clustering\nIn this section, we propose a differentially private k-means algorithm in the discrete spaces. Inspired from the previous work on k-median problem (Gupta et al., 2010), our algorithm builds upon the local swap heuristics for k-means clustering (Kanungo et al., 2002): In each round, the algorithm maintains a set greedily by replacing one point therein with a better one outside (See Algorithm 3). We first prove that such an algorithm is differentially private.\nTheorem 6. Algorithm 3 preserves -differential privacy.\nProof. The privacy guarantee is straightforward using the basic composition theorem over T rounds of the algorithm, and an additional exponential mechanism that selects the best one. It is easy to verify the sensitivity of loss increments L(Z \u2212{x}+ {y})\u2212L(Z) is 8\u039b2, the privacy guarantee of exponential mechanism in each round follows.\nThe analysis of clustering loss of Algorithm 3 is based on a lower bound on the total gains of k swap pairs (Gupta et al., 2010). However, for the k-means problem, the triangle inequality does not hold for the quadratic `2 loss. To resolve this issue, we apply the inequality relaxation techniques for swap pairs developed by Kanungo et al. (2002). We have the following theorem on the clustering loss.\nTheorem 7. With probability at least 1 \u2212 \u03b4, the output of Algorithm 3 obeys L(Z) \u2264 30OPT+O ( k2\u039b2 log 2 n|C|\n\u03b4\n) ."}, {"heading": "5.2. Private Recovery of Centers in Original Space", "text": "We now propose Algorithm 4 for approximately recovering k centers in the original high-dimensional space. This algorithm is basically built on Algorithms 2 and 3 as subroutines: Algorithm 2 receives a set of points in the lowdimensional projected space as input, and outputs a small set of points that contains k centers with good clustering loss; Algorithm 3 privately outputs a set of clustering centers from a given candidate set.\nThe following parallel composition lemma (McSherry, 2009) guarantees that if we have an -differentially private algorithm for recovering the center of one cluster, then we can use it to output the centers of all k centers while still preserving differential privacy. This result follows from the fact that the clusters are disjoint.\nLemma 1 (McSherry (2009)). Let C1, . . . , Ck be any partition of the points x1, . . . , xn in Rd and suppose thatA(S) is an differentially private algorithm that operates on sets of points in Rd. Outputting (A(C1), . . . ,A(Ck)) also preserves differential privacy.\nNow we are ready to prove the privacy of Algorithm 4.\nTheorem 8. Assume that candidate ({xi}ni=1, , \u03b4) (Algorithm 2) preserves -differential privacy for {xi}ni=1, and that given any candidate set of centers C,\nAlgorithm 4 Private Clustering. input x1, x2, \u00b7 \u00b7 \u00b7 , xn \u2208 B(0,\u039b), parameters k, , \u03b4. output Clustering centres z1, z2, \u00b7 \u00b7 \u00b7 , zk \u2208 Rd.\nSet dimension p = 8 log n, number of trials T = 2 log 1\u03b4 . for t = 1, 2, \u00b7 \u00b7 \u00b7T do\nSample G \u223c N (0, 1)p\u00d7d. [y1, y2, \u00b7 \u00b7 \u00b7 , yn] = 1\u221adG[x1, x2, \u00b7 \u00b7 \u00b7 , xn]. C = candidate ( {yi}ni=1, 6T , \u03b4 ) .\n{u1, u2, \u00b7 \u00b7 \u00b7 , uk} = localswap ( {yi}ni=1, C, 6T , \u03b4 ) . Sj = {i : j = argminl \u2016yi \u2212 ul\u2016}, j = 1, 2, \u00b7 \u00b7 \u00b7 , k. sj = max { |Sj |+ Lap ( 24T ) , 1 } .\nz (t) j = 1 |sj | \u2211 xi\u2208Sj xi + Lap ( 24T\u039b sj )d , \u2200j.\nend for Choose Z from Z(1), Z(2), \u00b7 \u00b7 \u00b7 , Z(T ) with probability in proportion to exp ( \u2212 L(Z\n(t)) 24\u039b2\n) .\nlocalswap ({xi}ni=1, C, , \u03b4) (Algorithm 3) preserves - differential privacy for {xi}ni=1. Then Algorithm 4 preserves -differential privacy. Putting everything together, we have the following theorem on the clustering loss of Algorithm 4. The key technique in our proof is to convert the argument of preservation of pairwise distance in the JL Lemma to the bound on the clustering loss. This is due to a simple observation that the optimal loss in any cluster only depends on the pairwise distances among its data points. Theorem 9. Assume that candidate ({xi}ni=1, , \u03b4) (Algorithm 2) outputs an (\u03b1, \u03c31( ))-approximate candidate set with probability at least 23 , and that with probability at least 23 , localswap ({xi} n i=1, C, , \u03b4) (Algorithm 3) achieves clustering loss at most cOPTC + \u03c32( ), where OPTC is the optimal clustering centers in the candidate set of centers C. Then with probability at least 1 \u2212 \u03b4, the output of Algorithm 4 has k-means clustering loss at most 3c\u03b1OPT + 3c\u03c3\u20321 + 3\u03c3 \u2032 2 + O ( d\u039b2 log3 1\u03b4 2 ) , where\n\u03c3\u2032i = \u03c3i\n(\n2 log 1/\u03b4\n) for i = 1, 2.\nTheorem 9, together with Theorems 5 and 7, leads to the following guarantees on the clustering loss of Algorithm 4. Corollary 1. There is an -differential private algorithm that runs in poly(k, d, n) time, and releases a set of centers z\u03031, z\u03032, \u00b7 \u00b7 \u00b7 , z\u0303k such that with probability at least 1\u2212 \u03b4, L ( {z\u0303j}kj=1 ) \u2264 O(log3 n)OPT + O ( k2 +d 2 \u039b 2 log5 n\u03b4 ) ."}, {"heading": "6. Extensions", "text": "In this section, we present two extensions of our algorithms: a) private k-means clustering with highdimensional sparse data; b) private k-median clustering.\n6.1. High-Dimensional Sparse Data\nAlgorithm 5 Privately Recover Centers for Sparse Dataset. input Private data set {xi}ni=1 \u2286 Rd with \u2016xi\u2016\u221e \u2264\n\u039b, \u2016xi\u20160 \u2264 s, parameters , \u03b4, accuracy \u03b7. output v \u2208 Rd.\nCompute \u00b5 = 1n \u2211n i=1 xi \u2208 Rd. Initialize I = {1, 2, \u00b7 \u00b7 \u00b7 , d}, v = 0 \u2208 Rd. for j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , d 2s\u03b7 e} do\nSample r \u2208 I with probability in proportion to exp { \u03b7n 4\u039bs |\u00b5[r]| } .\nI = I \\ {r}, v[r] = v[r] + \u00b5[r] + Lap (\n4\u039bs \u03b7n\n) .\nend for\nFor the case of high-dimensional sparse data where \u2016xi\u20160 \u2264 s, our goal is to improve the additive loss term \u03b2 to be as small as poly(k, s, log d, log n) by small modifications of Algorithm 4. The steps of discretization routine, construction of candidate set, and clustering in the discrete space all remain the same as in the general case. The only difference is the step of private recovery of centers in the high-dimensional original space. In the non-sparse setting, we simply take a noisy mean of points that belong to cluster i and output the center for cluster i, resulting in \u2126(d) additive loss. However, such a procedure does not exploit the sparse nature of data points. The challenge is that a high-dimensional vector has too many entries to hide for differential privacy, usually resulting in large error. To improve the clustering loss, we force the output vector to be sparse, by choosing coordinates with large absolute values, while zeroing out others (See Algorithm 5). Both the choice of non-zero coordinates and the estimation of their values need to preserve privacy, for which we use both the exponential and Laplacian mechanisms. By a composition argument, we have the privacy of Algorithm 5.\nTheorem 10. Algorithm 5 preserves -differential privacy. The following theorem guarantees that for highdimensional sparse data, the clustering loss has logarithmic dependence on the dimension.\nTheorem 11. With probability at least 1 \u2212 \u03b4, the output of Algorithm 5 obeys \u2211n i=1 \u2016xi \u2212 v\u20162 \u2264 1 1\u2212\u03b7OPT +\nO\n( \u039b2s2 log ds\u03b7\u03b4\n\u03b72\n) .\nThe intuition of Theorem 11 is based on the following observation: If the mean is approximately sparse, we can truncate it safely with small additional loss; If not, the mean must spread across a large set of entries, so the support of data vectors must be very different from each other, making the variance large. In both cases, the loss of truncation can be bounded by the variance of data points, and we can put such a loss of truncation to the multiplicative factor.\nBy the privacy argument in Lemma 1, as well as Theorems 5 and 7, we have the following result.\nCorollary 2. For x1, x2, \u00b7 \u00b7 \u00b7 , xn \u2208 Rd with \u2016xi\u20160 \u2264 s and \u2016xi\u2016\u221e \u2264 C, there is an -differentially private algorithm that runs in poly(k, d, n) time, and releases a set of centers z\u03031, z\u03032, \u00b7 \u00b7 \u00b7 , z\u0303k such that with probability at least 1\u2212 \u03b4, L ( {z\u0303j}kj=1 ) \u2264 O(log3 n)OPT+O ( sk2+s2 log d\u03b4 log 2 n \u03b4 ) .\nAn important implication of Corollary 2 is that private clustering for high-dimensional sparse data is as easy as private clustering in O(log d) dimensions. The approximation factor we can achieve in the high-dimensional sparse case is roughly the same as low-dimensional case.\n6.2. k-Median Clustering We can also easily modify our algorithms to adapt to kmedian problem. Note that Theorem 5 is independent of form of loss function, since it is based on capturing the optimal centers. Therefore, the candidate set constructed in Algorithm 2 guarantees an ( O(log\n3 2 n),O(k\u039b\u03b3 log n\u03b4 )\n) -\napproximation rate. Since the discrete clustering algorithm proposed by Gupta et al. (2010) is designed for k-median, it only remains to develop a private recovery procedure in the original space Rd. According to Lemma 1, a private 1-median algorithm suffices to recover the centers. We can achieve good approximation rate via log-concave sampling (Bassily et al., 2014). Lemma 2 (Error Bound for Exponential Mechanism (Bassily et al., 2014)). For x1, x2, \u00b7 \u00b7 \u00b7 , xn \u2208 Rd, there is a polynomial-time algorithm that releases a center z and preserves -differential privacy, such that with probability at least 1 \u2212 \u03b4, we have \u2211n i=1 \u2016xi \u2212 z\u2016 \u2212 minp \u2016xi \u2212 p\u2016 \u2264 O(d\u039b log 2 1 \u03b4 ). Incorporating log-concave sampling into the step of private recovery in Algorithm 4, we derive a private k-median algorithm. The privacy guarantee follows directly from Lemma 1 and the composition argument. As for the kmedian objective, the optimal clustering loss is no longer a function of pairwise distances. Fortunately, observe that the original dataset is (2, 0)-approximate candidate set for k-median loss. By this, we have the following guarantee.\nTheorem 12. For k-median problem, there is an - differentially private algorithms that runs in poly(k, d, n) time, and releases a set of centers z\u03031, z\u03032, \u00b7 \u00b7 \u00b7 , z\u0303k such that with probability at least 1 \u2212 \u03b4, L ( {z\u0303j}kj=1 ) \u2264\nO(log3/2 n)OPT + O ( (k2+d)\u039b log\n3 n \u03b4\n) ."}, {"heading": "7. Experiments", "text": "In this section, we present an empirical evaluation of our proposed clustering algorithm and several strong baselines on real-world image and synthetic datasets. We compare against non-private k-means++ of Arthur & Vassilvitskii (2007), SuLQ k-means of Blum et al. (2005), the sample and aggregate clustering algorithm of Nissim et al. (2007),\nthe k-variates++ algorithm of Nock et al. (2016), and the griding algorithm of Su et al. (2016). The k-variates++ algorithm can only run when k is small, and the griding algorithm has time and space complexity exponential in the dimension, so we are only able to compare against these two baselines with small k or small d. We postpone detailed comparisons against these two algorithms to supplementary material. For all other datasets with higher dimensions and all values of k, our algorithm is competitive with non-private k-means++ and is always better than SuLQ and sample and aggregate. Moreover, in agreement with our theory, the gap between the performance of our algorithm and the other private baselines grows drastically as the dimension of the dataset increases.\nThe implementation of our algorithm projects to a space of dimension p = log(n)/2, rather than 8 log(n) and repeats the candidate set construction routine only k times. Finally, we perform 8 iterations of the SuLQ k-means algorithm to further improve the quality of the resulting centers. These modifications do not affect the privacy guarantee of the algorithm, but gave improved empirical performance. Our implementation of the SuLQ k-means algorithm runs for 20 iterations and uses the Gaussian mechanism to approximate the sum of points in each cluster, since this allowed us to add less noise. The SuLQ algorithm initializes its centers to be k randomly chosen points from the bounding box of the data. Unless otherwise stated, we set = 1.0.\nResults: We first compared our algorithm and all baselines on a small synthetic dataset in 3 dimensions with k = 3. Su et al.\u2019s griding algorithm achieves the best objective, while sample and aggregate, SuLQ, and our method all perform comparably, and k-variates++ is an order of magnitude worse. Details of the comparison are given in the supplementary material. The griding algorithm and k-variates++ were not able to run in the rest of our experiments.\nNext, we ran the non-private k-means++, SuLQ k-means, and sample and aggregate algorithms on the following datasets for each value of k in {2, 4, 8, 16, 32, 64}. A more detailed description is given in the supplementary material.\nMNIST: The raw pixels of MNIST (LeCun et al., 1998). It has 70k examples and 784 features.\nCIFAR-10: 100k randomly sampled examples from the CIFAR10 dataset (Krizhevsky, 2009) with 160 features extracted from layer in3c of a Google Inception (Szegedy et al., 2015) network.\nSynthetic: A synthetic dataset of 100k samples drawn from a mixture of 64 Gaussians in R100.\nFigure 2 (a-c) shows the objective values obtained by each algorithm averaged over 5 independent runs. The sample and aggregate algorithm\u2019s results have been omitted, since its objective values are orders of magnitude worse than the other algorithms. Across all values of k and all datasets, our algorithm is competitive with non-private k-means++ and always outperforms SuLQ k-means. As the dimensionality of the datasets increases, our algorithm remains competitive with k-means++, while SuLQ becomes less competitive for large dimensions.\nFinally, figure 2 (d) shows the effect of the privacy parameter on the objective values for each algorithm on MNIST with k = 10. Our algorithm is competitive with the nonprivate k-means algorithm even for small , while the SuLQ algorithm objective deteriorates quickly."}, {"heading": "8. Conclusions", "text": "In this paper, we propose efficient algorithms for -private k-means and k-median clustering in Rd that achieves clustering loss at most O ( log3 n ) OPT + poly ( k, d, log n, 1\n) and O ( log 3 2 n ) OPT + poly ( k, d, log n, 1 ) , respectively.\nWe also study the scenario where the data points are s-sparse and show that the k-means clustering loss can be even smaller, namely, O ( log3 n ) OPT +\npoly ( k, s, log d, log n, 1 ) . Results of this type advance the state-of-the-art approaches in the high-dimensional Euclidean spaces. Our method of constructing candidate set can be potentially applied to other problems, which might be of independent interest more broadly."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Liwei Wang and Colin White for helpful discussions. Parts of this work was done when W.M. was visiting CMU. This work was done when Y.L. was visiting Simons Institute. This work was supported in part by grants NSF IIS-1618714, NSF CCF-1535967, NSF CCF-1422910, NSF CCF-1451177, a Sloan Fellowship, a Microsoft Research Fellowship, NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, ONRN00014-16-12329, and the Chinese MOE Training Plan for Top-Notch Students in Basic Discipline."}], "year": 2017, "references": [{"title": "Reporting neighbors in high-dimensional euclidean space", "authors": ["Aiger", "Dror", "Kaplan", "Haim", "Sharir", "Micha"], "venue": "SIAM Journal on Computing,", "year": 2014}, {"title": "Np-hardness of euclidean sum-of-squares clustering", "authors": ["Aloise", "Daniel", "Deshpande", "Amit", "Hansen", "Pierre", "Popat", "Preyas"], "venue": "Machine learning,", "year": 2009}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "authors": ["Andoni", "Alexandr", "Indyk", "Piotr"], "venue": "In IEEE Symposium on Foundations of Computer Science,", "year": 2006}, {"title": "k-means++: The advantages of careful seeding", "authors": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms,", "year": 2007}, {"title": "k-means for streaming and distributed big sparse data", "authors": ["Barger", "Artem", "Feldman", "Dan"], "venue": "In SIAM International Conference on Data Mining,", "year": 2016}, {"title": "Differentially private empirical risk minimization: Efficient algorithms and tight error bounds", "authors": ["Bassily", "Raef", "Smith", "Adam", "Thakurta", "Abhradeep"], "venue": "arXiv preprint arXiv:1405.7085,", "year": 2014}, {"title": "A survey of clustering data mining techniques", "authors": ["Berkhin", "Pavel"], "venue": "In Grouping Multidimensional Data,", "year": 2006}, {"title": "Practical privacy: the SuLQ framework", "authors": ["Blum", "Avrim", "Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi"], "venue": "In ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,", "year": 2005}, {"title": "The hardness of k-means clustering", "authors": ["Dasgupta", "Sanjoy"], "venue": "Department of Computer Science and Engineering,", "year": 2008}, {"title": "Calibrating noise to sensitivity in private data analysis", "authors": ["Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"], "venue": "In Theory of Cryptography Conference,", "year": 2006}, {"title": "The algorithmic foundations of differential privacy", "authors": ["Dwork", "Cynthia", "Roth", "Aaron"], "venue": "Foundations and Trends in Theoretical Computer Science,", "year": 2014}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "authors": ["Ester", "Martin", "Kriegel", "Hans-Peter", "Sander", "J\u00f6rg", "Xu", "Xiaowei"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "year": 1996}, {"title": "Private coresets", "authors": ["Feldman", "Dan", "Fiat", "Amos", "Kaplan", "Haim", "Nissim", "Kobbi"], "venue": "In ACM Symposium on Theory of Computing,", "year": 2009}, {"title": "Differentially private combinatorial optimization", "authors": ["Gupta", "Anupam", "Ligett", "Katrina", "McSherry", "Frank", "Roth", "Aaron", "Talwar", "Kunal"], "venue": "In ACM-SIAM symposium on Discrete Algorithms,", "year": 2010}, {"title": "A local search approximation algorithm for k-means clustering", "authors": ["Kanungo", "Tapas", "Mount", "David M", "Netanyahu", "Nathan S", "Piatko", "Christine D", "Silverman", "Ruth", "Wu", "Angela Y"], "venue": "In Annual Symposium on Computational Geometry,", "year": 2002}, {"title": "Learning multiple layers of features from tiny images", "authors": ["Krizhevsky", "Alex"], "venue": "Technical report, University of Toronto,", "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "year": 1998}, {"title": "On approximate geometric k-clustering", "authors": ["Matou\u0161ek", "Jir\u0131"], "venue": "Discrete & Computational Geometry,", "year": 2000}, {"title": "Differentially private recommender systems: building privacy into the net", "authors": ["McSherry", "Frank", "Mironov", "Ilya"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "year": 2009}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "authors": ["McSherry", "Frank D"], "venue": "In ACM SIGMOD International Conference on Management of Data,", "year": 2009}, {"title": "Smooth sensitivity and sampling in private data analysis", "authors": ["Nissim", "Kobbi", "Raskhodnikova", "Sofya", "Smith", "Adam"], "venue": "In ACM Symposium on Theory of Computing,", "year": 2007}, {"title": "k-variates++: more pluses in the kmeans++", "authors": ["Nock", "Richard", "Canyasse", "Rapha\u00ebl", "Boreli", "Roksana", "Nielsen", "Frank"], "venue": "arXiv preprint arXiv:1602.01198,", "year": 2016}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "authors": ["Ostrovsky", "Rafail", "Rabani", "Yuval", "Schulman", "Leonard J", "Swamy", "Chaitanya"], "venue": "Journal of the ACM,", "year": 2012}, {"title": "An adaptive clustering algorithm for image segmentation", "authors": ["Pappas", "Thrasyvoulos N"], "venue": "IEEE Transactions on Signal Processing,", "year": 1992}, {"title": "Differentially private k-means clustering", "authors": ["Su", "Dong", "Cao", "Jianneng", "Li", "Ninghui", "Bertino", "Elisa", "Jin", "Hongxia"], "venue": "In Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy,", "year": 2016}, {"title": "Going deeper with convolutions", "authors": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2015}, {"title": "Differentially private subspace clustering", "authors": ["Wang", "Yining", "Yu-Xiang", "Singh", "Aarti"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Robust latent low rank representation for subspace clustering", "authors": ["Zhang", "Hongyang", "Lin", "Zhouchen", "Chao", "Gao", "Junbin"], "year": 2014}, {"title": "Relations among some low-rank subspace recovery models", "authors": ["Zhang", "Hongyang", "Lin", "Zhouchen", "Chao", "Gao", "Junbin"], "venue": "Neural computation,", "year": 2015}], "id": "SP:38fa24bb25ca1592e61764d0a6603e40b6dcb190", "authors": [{"name": "Maria-Florina Balcan", "affiliations": []}, {"name": "Travis Dick", "affiliations": []}, {"name": "Yingyu Liang", "affiliations": []}, {"name": "Wenlong Mou", "affiliations": []}, {"name": "Hongyang Zhang", "affiliations": []}], "abstractText": "We study the problem of clustering sensitive data while preserving the privacy of individuals represented in the dataset, which has broad applications in practical machine learning and data analysis tasks. Although the problem has been widely studied in the context of lowdimensional, discrete spaces, much remains unknown concerning private clustering in highdimensional Euclidean spaces R. In this work, we give differentially private and efficient algorithms achieving strong guarantees for k-means and k-median clustering when d = \u03a9(polylog(n)). Our algorithm achieves clustering loss at most log(n)OPT+poly(log n, d, k), advancing the state-of-the-art result of \u221a dOPT+ poly(log n, d, k). We also study the case where the data points are s-sparse and show that the clustering loss can scale logarithmically with d, i.e., log(n)OPT + poly(log n, log d, k, s). Experiments on both synthetic and real datasets verify the effectiveness of the proposed method.", "title": "Differentially Private Clustering in High-Dimensional Euclidean Spaces"}