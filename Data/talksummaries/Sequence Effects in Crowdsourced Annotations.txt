0	10	NLP research relies heavily on annotated datasets for training and evaluation.
8	159	Since all workers see the instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds).
9	25	Possible explanations for sequence effects include: Gambler’s fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution.
10	65	In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True).
12	62	Chen et al. (2016) showed evidence for the gambler’s fallacy in decisions of loan officers, asylum judges, and baseball umpires.
13	36	Sequential contrast effects: A high quality item may raise the bar for the next item.
14	40	On the other hand, a bad item may make the next item seem better in comparison (Kenrick and Gutierres, 1980; Hartzmark and Shue, to appear) Assimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item.
18	63	We use linear regression for continuous data and logistic regression for binary data.2 If there is no dependence between consecutive instances, and annotators assign labels/scores based only on the aspects of the current instance, then the data can be explained from the gold score (learning a positive β2 value) and bias term (β0), with β1 set to zero.
20	15	A positive value of β1 can be explained by priming or anchoring, and a negative value with sequential contrast effects or the gambler’s fallacy.
21	9	Accordingly, we test the statistical significance of 2η is not included in the case of logistic regression the β1 6= 0 to determine whether sequencing effects are present in crowdsourced text corpora.
22	41	We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation.
24	14	In the RTE task, annotators are presented with two sentences, and are asked to judge whether the second text can be inferred from the first.
27	21	On MTurk, each RTE HIT contains 20 instances, and each TEMPORAL HIT contains 10 instances, which the workers see in sequential order.
32	14	As shown in Table 1, over all workers (“All”), we find a small negative autocorrelation for both the RTE and TEMPORAL tasks.
40	10	We also look separately at datasets of good and bad workers, based on whether the correlation with the expert annotations is greater than 0.5.
53	21	Good workers are assumed to give high scores to the references, similar scores to the pair of repeats, and high scores to the MT system translations when compared to corresponding degraded translations.
60	12	Results As this is a (practically) continuous output, we use a linear regression model, whereby the current score is predicted based on the previous score, with the mean of all worker scores as control.
64	14	An interesting question is whether the bias changes as workers annotate more data, which could be ascribed to learning through the task, calibrating their internal scales, or becoming fatigued on a monotonous task.
65	9	Each HIT consists of 100 sentences, and we divide the dataset into 3 equal groups based on the position of sentence in the HIT.
70	19	We discretize scores into low, middle and high based on equal-frequency binning, and divide the dataset into 3 groups based on the score assigned to the previous sentence.
71	66	As shown in Table 5 we can see that the sentences in the “low” partition and the “high” partition have a difference of 0.18, which is highly significant;3 moreover, this difference is likely to be sufficiently large to alter the rankings of systems in an evaluation.
74	56	Thus, it is theoretically possible to exploit sequence bias to artificially deflate (or inflate) a specific system’s computed score by ordering a HIT such that the system’s output is seen consistently immediately after a bad (or good) output.
76	16	The negative autocorrelation can be attributed either to sequential contrast effects or the gambler’s fallacy.
77	129	These effects were not significant for the AFFECTIVE dataset, perhaps due to the nature of the annotation task, whereby annotations of one emotion are separated by six other annotations, thus limiting the potential for sequencing effects.
78	10	It is also possible that the dataset is too small to obtain statistical significance.
79	38	MT judgements are subjective, and when people are asked to rate them on a continuous scale, they need time to calibrate their scale.
80	14	We show that the sequential bias decreases for better workers as they annotate more sentences in the HIT, indicating a learning effect.
84	44	Sequence problems can be easily addressed by adequate randomisation — providing each individual worker with a separate dataset that has been randomised, such that no two workers see the same ordered data.
87	49	We limited our scope to binary and continuous responses, however it is likely that sequence effects are prevalent for multinomial and structured outputs, e.g., in discourse and parsing, where priming is known to have a significant effect (Reitter et al., 2006).
88	45	Another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers.
