1	27	In the Budget Allocation Problem, one is given a bipartite influence graph between channels S and people T , and the task is to assign a budget y(s) to each channel s in S with the goal of maximizing the expected number of influenced people I(y).
2	51	Each edge (s, t) 2 E between channel s and person t is weighted with a probability pst that, e.g., an advertisement on radio station s will influence person t to buy some product.
3	24	The budget y(s) controls how many independent attempts are made via the channel s to influence the people in T .
4	21	The probability that a customer t is influenced when the advertising budget is y is It(y) = 1 Y (s,t)2E [1 pst]y(s), (1) and hence the expected number of influenced people is I(y) = P t2T It(y).
7	29	Since its introduction by Alon et al. (2012), several works have extended the formulation of Budget Allocation and provided algorithms (Bian et al., 2017; Hatano et al., 2015; Maehara et al., 2015; Soma et al., 2014; Soma & Yoshida, 2015).
11	45	If y is continuous, the problem is a concave maximization problem.
19	54	In this work, we revisit Budget Allocation under uncertainty from the perspective of robust optimization (Bertsimas et al., 2011; Ben-Tal et al., 2009).
20	68	We maximize the worst-case influence – not approximation ratio – for p in a confidence set centered around the “best guess” (e.g., posterior mean).
21	86	This avoids pitfalls of the approximation ratio formulation (which can be misled to return poor worst-case budgets, as demonstrated in Appendix A), while also allowing us to formulate the problem as a max-min game: max y2Y min p2P I(y; p), (2) where an “adversary” can arbitrarily manipulate p within the confidence set P .
22	80	With p fixed, I(y; p) is concave in y.
23	121	However, the influence function I(y; p) is not convex, and not even quasiconvex, in the adversary’s variables pst.
24	40	The new, key insight we exploit in this work is that I(y; p) has the property of continuous submodularity in p – in contrast to previously exploited submodular maximization in y – and can hence be minimized by generalizing techniques from discrete submodular optimization (Bach, 2015).
25	27	The techniques in (Bach, 2015), however, are restricted to box constraints, and do not directly apply to our confidence sets.
26	27	In fact, general constrained submodular minimization is hard (Svitkina & Fleischer, 2011; Goel et al., 2009; Iwata & Nagano, 2009).
27	51	We make the following contributions: 1.
28	43	We present an algorithm with optimality bounds for Robust Budget Allocation in the nonconvex adversarial scenario (2).
29	116	We provide the first results for continuous submodular minimization with box constraints and one more “nice” constraint, and conditions under which the algorithm is guaranteed to return a global optimum.
32	35	R defined on subsets S ✓ V of a ground set V is submodular if for all sets S, T ✓ V , it holds that F (S) + F (T ) F (S \ T ) + F (S[T ).
35	50	Submodularity has also been considered on continuous domains X ⇢ Rd, where, if f is also twicedifferentiable, the property of submodularity means that all off-diagonal entries of the the Hessian are nonpositive, i.e., @f(x) @xi@xj  0 for all i 6= j (Topkis, 1978, Theorem 3.2).
36	34	These functions may be convex, concave, or neither.
37	35	Submodular functions on lattices can be minimized by a reduction to set functions, more precisely, ring families (Birkhoff, 1937).
39	24	More recently, Bach (2015) extended results based on the convex Lovász extension, by building on connections to optimal transport.
45	35	In fact, it also applies to minimization: Proposition 1.1.
49	11	The influence function is still monotone submodular and amenable to the greedy algorithm (Kempe et al., 2003), but it cannot be evaluated explicitly and requires approximation (Chen et al., 2010).
54	53	Gen- eral signomial optimization is NP-hard (Chiang, 2005), but certain subclasses are tractable: posynomials with all nonnegative coefficients can be minimized via Geometric Programming (Boyd et al., 2007), and signomials with a single negative coefficient admit sum of squares-like relaxations (Chandrasekaran & Shah, 2016).
61	17	In this paper we are principally concerned with robust maximization of the continuous influence function I(y), but mention some results for the discrete case.
74	23	Merely maximizing expectation does not explicitly account for volatility and hence risk.
75	21	One option is to include variance (Ben-Tal & Nemirovski, 2000; Bertsimas et al., 2011; Atamtürk & Narayanan, 2008): min y2Y E[I(y;X)] + " p Var(I(y;X)), (7) but in our case this CVaR formulation seems difficult: Fact 2.1.
76	23	For y in the nonnegative orthant, the term p Var(I(y;X)) need not be convex or concave, and need not be submodular or supermodular.
77	17	This observation does not rule out a solution, but the apparent difficulties further motivate a robust formulation that, as we will see, is amenable to optimization.
