1	26	Since languages of the world exhibit an astonishing diversity, the sample of languages used in a typical typological study is selected from a diverse set of language families and from various geographical regions.
4	90	They have also tried to uncover deep historical relations between languages (Nichols, 1992).
5	19	One of the main developments in diachronic typology in the last decade has been the application of powerful statistical tools borrowed from the field of evolutionary biology (Dediu, 2010; Greenhill et al., 2010; Dunn et al., 2011; Maurits and Griffiths, 2014; Greenhill et al., 2017).
6	32	As illustrated in Figure 1, the key idea is that if a phylogenetic tree is given, we can infer the ancestral states with varying degrees of confidence, and by extension, can induce diachronic universals of change.
7	56	To perform statistical inference, we assume that each feature evolves along the branches of the tree according to a continuous-time Markov chain (CTMC) model, which is controlled by a transition rate matrix (TRM).
9	12	One problem in previous studies is that they do not adequately model a characteristic of typological features that has been central to linguistic typology, that is, the fact that these features are not independent but depend on each other (Greenberg, 1963; Daumé III and Campbell, 2007).
14	16	Despite the long-standing interest in interfeature dependencies, most statistical models assume independence between features (Daumé III, 2009; Dediu, 2010; Greenhill et al., 2010, 2017; Murawaki, 2016; Murawaki and Yamauchi, 2018).
15	21	A rare exception is Dunn et al. (2011), who extended Greenberg’s idea by applying a phylogenetic model of correlated evolution (Pagel and Meade, 2006).
17	14	Typological features have two or more possible values in general, and more importantly, the dependencies between features are not limited to a pair (Itoh and Ueda, 2004).
20	7	Figure 2 shows an overview of our framework.
21	30	Follow- ing Murawaki (2017), we assume that a sequence of discrete surface features that represents a language is generated from a sequence of binary latent variables called parameters (Step 1).
22	6	Parameters are, by assumption, independent of each other and switching one parameter entails multiple changes of surface features in general.
23	10	Thus, by performing phylogenetic inference on the latent space, we can handle the dependencies of all available features in an implicit manner (Step 2).
24	17	The latent parameter representation can be projected back to the surface feature representation when needed for analysis.
25	7	Like Maurits and Griffiths (2014), we run simulation experiments to interpret the estimated model parameters (Step 3).
26	14	What we propose is a general framework with which we can analyze any discrete feature, but as a proof-of-concept demonstration, we follow Maurits and Griffiths (2014) in focusing on the order of subject, object and verb (hereafter simply referred to as basic word order or BWO).1 In the dataset we use, the BWO feature has 7 possible values, 6 logically possible orders plus the special value No dominant order (Dryer, 2013b), meaning that it cannot be analyzed directly with Dunn et al.’s model.
27	37	We show that languages sharing the same word order are not a coherent group but exhibit varying degrees of diachronic stability depending on other features.
66	55	Central to our framework of diachronic analysis are the latent representations of languages (Murawaki, 2017).
67	19	Each language l is represented as a sequence of N discrete features xl,∗ = (xl,1, · · · , xl,N ) ∈ NN0 .
68	16	xl,n can take a binary value (xl,n ∈ {0, 1}) or categorical value (xl,n ∈ {1, 2, · · · , Fn}, where Fn is the number of distinct values).
69	10	We assume that xl,∗ is stochastically generated from its latent representation, zl,∗ = (zl,1, · · · , zl,K) ∈ {0, 1}K , where K is the number of binary parameters, which is given a priori.
70	44	Dependencies between surface features are captured by weight matrix W ∈ RK×M .
72	6	We then obtain model parameter vector θl,∗ ∈ (0, 1)M by normalizing θ̃l,∗ for each feature type n. We use the sigmoid function for binary features, θl,f(n,1) = 1 1 + exp(−θ̃l,f(n,1)) , (1) and the softmax function for categorical features, θl,f(n,i) = exp(θ̃l,f(n,i))∑Fn i′=1 exp(θ̃l,f(n,i′)) .
73	8	(2) Note that while a binary feature corresponds to one model parameter, categorical feature n is tied to Fn model parameters.
76	26	To gain an insight into how W captures interfeature dependencies, suppose that for parameter k, a certain group of languages take zl,k = 1.
77	38	If two categorical feature values (n1, i1) and (n2, i2) have large positive weights (i.e., wk,f(n1,i1) > 0 and wk,f(n2,i2) > 0), then the pair must often cooccur in these languages because W raises both θl,f(n1,i1) and θl,f(n2,i2).
79	25	The remaining question is how zl,k is generated.
80	34	We draw z∗,k = (z1,k, · · · , zL,k) from an autologistic model (Besag, 1974) that incorporates the observation that phylogenetically or areally close languages tend to take the same value.
81	37	To complete the generative story, letX andZ be the matrices of languages in the surface and latent representations, respectively, and let A be a set of latent variables controllingK autologistic models.
84	9	Even if less than 30% of the items of X are present, this model has been demonstrated to recover missing values reasonably well.
