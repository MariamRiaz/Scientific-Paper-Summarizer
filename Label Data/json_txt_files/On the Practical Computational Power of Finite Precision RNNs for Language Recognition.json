{"sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Network (RNNs) emerge as very strong learners of sequential data. A famous result by Siegelmann and Sontag (1992; 1994), and its extension in (Siegelmann, 1999), demonstrates that an Elman-RNN (Elman, 1990) with a sigmoid activation function, rational weights and infinite precision states can simulate a Turingmachine in real-time, making RNNs Turingcomplete. Recently, Chen et al (2017) extended the result to the ReLU activation function. However, these constructions (a) assume reading the entire input into the RNN state and only then performing the computation, using unbounded time; and (b) rely on having infinite precision in the network states. As argued by Chen et al (2017), this is not the model of RNN computation used in NLP applications. Instead, RNNs are often used by feeding an input sequence into the RNN one item at a time, each immediately returning a state-\nvector that corresponds to a prefix of the sequence and which can be passed as input for a subsequent feed-forward prediction network operating in constant time. The amount of tape used by a Turing machine under this restriction is linear in the input length, reducing its power to recognition of context-sensitive language. More importantly, computation is often performed on GPUs with 32bit floating point computation, and there is increasing evidence that competitive performance can be achieved also for quantized networks with 4-bit weights or fixed-point arithmetics (Hubara et al., 2016). The construction of (Siegelmann, 1999) implements pushing 0 into a binary stack by the operation g \u2190 g/4 + 1/4. This allows pushing roughly 15 zeros before reaching the limit of the 32bit floating point precision. Finally, RNN solutions that rely on carefully orchestrated mathematical constructions are unlikely to be found using backpropagation-based training.\nIn this work we restrict ourselves to inputbound recurrent neural networks with finiteprecision states (IBFP-RNN), trained using backpropagation. This class of networks is likely to coincide with the networks one can expect to obtain when training RNNs for NLP applications. An IBFP Elman-RNN is finite state. But what about other RNN variants? In particular, we consider the Elman RNN (SRNN) (Elman, 1990) with squashing and with ReLU activations, the Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014).\nThe common wisdom is that the LSTM and GRU introduce additional gating components that handle the vanishing gradients problem of training SRNNs, thus stabilizing training and making it more robust. The LSTM and GRU are often considered as almost equivalent variants of each other.\nar X\niv :1\n80 5.\n04 90\n8v 1\n[ cs\n.L G\n] 1\n3 M\nay 2\n01 8\nWe show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.\nThese results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using back-propagation, while using the hypothesized control structure. Figure 1 shows the activations of 10-d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.1\n1Is the ability to perform unbounded counting relevant to \u201creal world\u201d NLP tasks? In some cases it might be. For example, processing linearized parse trees (Vinyals et al., 2015; Choe and Charniak, 2016; Aharoni and Goldberg, 2017) requires counting brackets and nesting levels. Indeed, previous works that process linearized parse trees report using LSTMs and not GRUs for this purpose. Our work here suggests that this may not be a coincidence."}, {"heading": "2 The RNN Models", "text": "An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht\u22121 and returns a state vector ht:\nht = R(xt, ht\u22121) (1)\nThe RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let \u03a3 be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1- hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L\u2286 \u03a3\u2217 if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn \u2208 L.\nElman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:\nht = tanh(Wxt + Uht\u22121 + b) (2)\nElman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can be replaced by any other squashing function without sacrificing computational power.\nIRNN The IRNN model, explored by (Le et al., 2015), replaces the tanh activation with a nonsquashing ReLU:\nht = max(0, (Wxt + Uht\u22121 + b)) (3)\nThe computational power of such RNNs (given infinite precision) is explored in (Chen et al., 2017).\nGated Recurrent Unit (GRU) In the GRU (Cho et al., 2014), the function R incorporates a gating mechanism, taking the form:\nzt = \u03c3(W zxt + U zht\u22121 + b z) (4) rt = \u03c3(W rxt + U rht\u22121 + b r) (5) h\u0303t = tanh(W hxt + U\nh(rt \u25e6 ht\u22121) + bh)(6) ht = zt \u25e6 ht\u22121 + (1\u2212 zt) \u25e6 h\u0303t (7)\nWhere \u03c3 is the sigmoid function and \u25e6 is the Hadamard product (element-wise product).\nLong Short Term Memory (LSTM) In the LSTM (Hochreiter and Schmidhuber, 1997), R uses a different gating component configuration:\nft = \u03c3(W fxt + U fht\u22121 + b f ) (8) it = \u03c3(W ixt + U iht\u22121 + b i) (9) ot = \u03c3(W oxt + U oht\u22121 + b o) (10) c\u0303t = tanh(W cxt + U cht\u22121 + b c) (11) ct = ft \u25e6 ct\u22121 + it \u25e6 c\u0303t (12) ht = ot \u25e6 g(ct) (13)\nwhere g can be either tanh or the identity.\nEquivalences The GRU and LSTM are at least as strong as the SRNN: by setting the gates of the GRU to zt = 0 and rt = 1 we obtain the SRNN computation. Similarly by setting the LSTM gates to it = 1,ot = 1, and ft = 0. This is easily achieved by setting the matrices W and U to 0, and the biases b to the (constant) desired gate values.\nThus, all the above RNNs can recognize finitestate languages."}, {"heading": "3 Power of Counting", "text": "Power beyond finite state can be obtained by introducing counters. Counting languages and kcounter machines are discussed in depth in (Fischer et al., 1968). When unbounded computation is allowed, a 2-counter machine has Turing power. However, for computation bound by input length (real-time) there is a more interesting hierarchy. In particular, real-time counting languages cut across the traditional Chomsky hierarchy: real-time k-counter machines can recognize at least one context-free language (anbn), and at least one context-sensitive one (anbncn). However, they cannot recognize the context free language given by the grammar S \u2192 x|aSa|bSb (palindromes).\nSKCM For our purposes, we consider a simplified variant of k-counter machines (SKCM). A counter is a device which can be incremented by a fixed amount (INC), decremented by a fixed amount (DEC) or compared to 0 (COMP0). Informally,2 an SKCM is a finite-state automaton extended with k counters, where at each step of the computation each counter can be incremented, decremented or ignored in an input-dependent way, and state-transitions and accept/reject decisions can inspect the counters\u2019 states using COMP0. The results for the three languages discussed above hold for the SKCM variant as well, with proofs provided in Appendix A."}, {"heading": "4 RNNs as SKCMs", "text": "In what follows, we consider the effect on the state-update equations on a single dimension, ht[j]. We omit the index [j] for readability.\nLSTM The LSTM acts as an SKCM by designating k dimensions of the memory cell ct as counters. In non-counting steps, set it = 0, ft = 1 through equations (8-9). In counting steps, the counter direction (+1 or -1) is set in c\u0303t (equation 11) based on the input xt and state ht\u22121. The counting itself is performed in equation (12), after setting it = ft = 1. The counter can be reset to 0 by setting it = ft = 0.\nFinally, the counter values are exposed through ht = otg(ct), making it trivial to compare the counter\u2019s value to 0.3\n2Formal definition is given in Appendix A. 3Some further remarks on the LSTM: LSTM supports both increment and decrement in a single dimension. The counting dimensions in ct are exposed through a function\nWe note that this implementation of the SKCM operations is achieved by saturating the activations to their boundaries, making it relatively easy to reach and maintain in practice.\nSRNN The finite-precision SRNN cannot designate unbounded counting dimensions.\nThe SRNN update equation is:\nht = tanh(Wx+ Uht\u22121 + b)\nht[i] = tanh( dx\u2211 j=1 Wijx[j]+ dh\u2211 j=1 Uijht\u22121[j]+b[i])\nBy properly setting U and W, one can get certain dimensions of h to update according to the value of x, by ht[i] = tanh(ht\u22121[i] +wix+ b[i]). However, this counting behavior is within a tanh activation. Theoretically, this means unbounded counting cannot be achieved without infinite precision. Practically, this makes the counting behavior inherently unstable, and bounded to a relatively narrow region. While the network could adapt to set w to be small enough such that counting works for the needed range seen in training without overflowing the tanh, attempting to count to larger n will quickly leave this safe region and diverge.\nIRNN Finite-precision IRNNs can perform unbounded counting conditioned on input symbols. This requires representing each counter as two dimensions, and implementing INC as incrementing one dimension, DEC as incrementing the other, and COMP0 as comparing their difference to 0. Indeed, Appendix A in (Chen et al., 2017) provides concrete IRNNs for recognizing the languages anbn and anbncn. This makes IBFP-RNN with ReLU activation more powerful than IBFP-RNN with a squashing activation. Practically, ReLUactivated RNNs are known to be notoriously hard\ng. For both g(x) = x and g(x) = tanh(x), it is trivial to do compare 0. Another operation of interest is comparing two counters (for example, checking the difference between them). This cannot be reliably achieved with g(x) = tanh(x), due to the non-linearity and saturation properties of the tanh function, but is possible in the g(x) = x case. LSTM can also easily set the value of a counter to 0 in one step. The ability to set the counter to 0 gives slightly more power for real-time recognition, as discussed by Fischer et al. (1968).\nRelation to known architectural variants: Adding peephole connections (Gers and Schmidhuber, 2000) essentially sets g(x) = x and allows comparing counters in a stable way. Coupling the input and the forget gates (it = 1 \u2212 ft) (Greff et al., 2017) removes the single-dimension unbounded counting ability, as discussed for the GRU.\nto train because of the exploding gradient problem.\nGRU Finite-precision GRUs cannot implement unbounded counting on a given dimension. The tanh in equation (6) combined with the interpolation (tying zt and 1 \u2212 zt) in equation (7) restricts the range of values in h to between -1 and 1, precluding unbounded counting with finite precision. Practically, the GRU can learn to count up to some bound m seen in training, but will not generalize well beyond that.4 Moreover, simulating forms of counting behavior in equation (7) require consistently setting the gates zt, rt and the proposal h\u0303t to precise, non-saturated values, making it much harder to find and maintain stable solutions.\nSummary We show that LSTM and IRNN can implement unbounded counting in dedicated counting dimensions, while the GRU and SRNN cannot. This makes the LSTM and IRNN at least as strong as SKCMs, and strictly stronger than the SRNN and the GRU.5"}, {"heading": "5 Experimental Results", "text": "Can the LSTM indeed learn to behave as a kcounter machine when trained using backpropagation? We show empirically that:\n1. LSTMs can be trained to recognize anbn and anbncn.\n2. These LSTMs generalize to much higher n than seen in the training set (though not infinitely so).\n3. The trained LSTM learn to use the perdimension counting mechanism.\n4. The GRU can also be trained to recognize anbn and anbncn, but they do not have clear counting dimensions, and they generalize to much smaller n than the LSTMs, often failing to generalize correctly even for n within their training domain.\n4One such mechanism could be to divide a given dimension by k > 1 at each symbol encounter, by setting zt = 1/k and h\u0303t = 0. Note that the inverse operation would not be implementable, and counting down would have to be realized with a second counter.\n5One can argue that other counting mechanisms\u2014 involving several dimensions\u2014are also possible. Intuitively, such mechanisms cannot be trained to perform unbounded counting based on a finite sample as the model has no means of generalizing the counting behavior to dimensions beyond those seen in training. We discuss this more in depth in Appendix B, where we also prove that an SRNN cannot represent a binary counter.\n5. Trained LSTM networks outperform trained GRU networks on random test sets for the languages anbn and anbncn.\nSimilar empirical observations regarding the ability of the LSTM to learn to recognize anbn and anbncn are described also in (Gers and Schmidhuber, 2001).\nWe train 10-dimension, 1-layer LSTM and GRU networks to recognize anbn and anbncn. For anbn the training samples went up to n = 100 and for anbncn up to n = 50.6\nResults On anbn, the LSTM generalizes well up to n = 256, after which it accumulates a deviation making it reject anbn but recognize anbn+1 for a while, until the deviation grows.7 The GRU does not capture the desired concept even within its training domain: accepting anbn+1 for n > 38, and also accepting anbn+2 for n > 97. It stops accepting anbn for n > 198.\nOn anbncn the LSTM recognizes well until n = 100. It then starts accepting also anbn+1cn. At n > 120 it stops accepting anbncn and switches to accepting anbn+1cn, until at some point the deviation grows. The GRU accepts already a9b10c12, and stops accepting anbncn for n > 63.\nFigure 1a plots the activations of the 10 dimensions of the anbn-LSTM for the input a1000b1000. While the LSTM misclassifies this example, the use of the counting mechanism is clear. Figure 1b plots the activation for the anbncn LSTM on a100b100c100. Here, again, the two counting dimensions are clearly identified\u2014indicating the LSTM learned the canonical 2-counter solution\u2014 although the slightly-imprecise counting also starts to show. In contrast, Figures 1c and 1d show the state values of the GRU-networks. The GRU behavior is much less interpretable than the LSTM. In the anbn case, some dimensions may be performing counting within a bounded range, but move to erratic behavior at around t = 1750 (the\n6Implementation in DyNet, using the SGD Optimizer. Positive examples are generated by sampling n in the desired range. For negative examples we sample 2 or 3 n values independently, and ensuring at least one of them differs from the others. We dedicate a portion of the examples as the dev set, and train up to 100% dev set accuracy.\n7These fluctuations occur as the networks do not fully saturate their gates, meaning the LSTM implements an imperfect counter that accumulates small deviations during computation, e.g.: increasing the counting dimension by 0.99 but decreasing only by 0.98. Despite this, we see that the its solution remains much more robust than that found by the GRU \u2014 the LSTM has learned the essence of the counting based solution, but its implementation is imprecise.\nnetwork starts to misclassify on sequences much shorter than that). The anbncn state dynamics are even less interpretable.\nFinally, we created 1000-sample test sets for each of the languages. For anbn we used words with the form an+ibn+j where n \u2208 rand(0, 200) and i, j \u2208 rand(\u22122, 2), and for anbncn we use words of the form an+ibn+jcn+k where n \u2208 rand(0, 150) and i, j, k \u2208 rand(\u22122, 2). The LSTM\u2019s accuracy was 100% and 98.6% on anbn and anbncn respectively, as opposed to the GRU\u2019s 87.0% and 86.9%, also respectively.\nAll of this empirically supports our result, showing that IBFP-LSTMs can not only theoretically implement \u201cunbounded\u201d counters, but also learn to do so in practice (although not perfectly), while IBFP-GRUs do not manage to learn proper counting behavior, even when allowing floating point computations."}, {"heading": "6 Conclusions", "text": "We show that the IBFP-LSTM can model a realtime SKCM, both in theory and in practice. This makes it more powerful than the IBFP-SRNN and the IBFP-GRU, which cannot implement unbounded counting and are hence restricted to recognizing regular languages. The IBFP-IRNN can also perform input-dependent counting, and is thus more powerful than the IBFP-SRNN.\nWe note that in addition to theoretical distinctions between architectures, it is important to consider also the practicality of different solutions: how easy it is for a given architecture to discover and maintain a stable behavior in practice. We leave further exploration of this question for future work."}, {"heading": "Acknowledgments", "text": "The research leading to the results presented in this paper is supported by the European Union\u2019s Seventh Framework Programme (FP7) under grant agreement no. 615688 (PRIME), The Israeli Science Foundation (grant number 1555/15), and The Allen Institute for Artificial Intelligence.\nAppendix"}, {"heading": "A Simplified K-Counter Machines", "text": "We use a simplified variant of the k-counter machines (SKCM) defined in (Fischer et al., 1968), which has no autonomous states and makes classification decisions based on a combination of its current state and counter values. This variant consumes input sequences on a symbol by symbol basis, updating at each step its state and its counters, the latter of which may be manipulated by increment, decrement, zero, or no-ops alone, and observed only by checking equivalence to zero. To define the transitions of this model its accepting configurations, we will introduce the following notations:\nNotations We define z : Zk \u2192 {0, 1}k as follows: for every n \u2208 Zk, for every 1 \u2264 i \u2264 k, z(n)i = 0 iff ni = 0 (this function masks a set of integers such that only their zero-ness is observed). For a vector of operations, o \u2208 {\u22121,+1,\u00d70,\u00d71}k, we denote by o(n) the pointwise application of the operations to the vector n \u2208 Zk, e.g. for o = (+1,\u00d70,\u00d71), o((5, 2, 3)) = (6, 0, 3).\nWe now define the model. An SKCM is a tuple M = \u3008\u03a3, Q, qo, k, \u03b4, u, F \u3009 containing:\n1. A finite input alphabet \u03a3 2. A finite state set Q 3. An initial state q0 \u2208 Q 4. k \u2208 N, the number of counters 5. A state transition function\n\u03b4 : Q\u00d7 \u03a3\u00d7 {0, 1}k \u2192 Q\n6. A counter update function8\nu : \u03a3\u2192 {\u22121,+1,\u00d70,\u00d71}k\n7. A set of accepting masked9 configurations\nF \u2286 Q\u00d7 {0, 1}k\nThe set of configurations of an SKCM is the set C = Q\u00d7 Zk, and the initial configuration is c0 = (q0, 0\u0304) (i.e., the counters are initiated to zero). The\n8 We note that in this definition, the counter update function depends only on the input symbol. In practice we see that the LSTM is not limited in this way, and can also update according to some state-input combinations \u2014 as can be seen when it it is taught, for instance, the language anban We do not explore this here however, leaving a more complete characterization of the learnable models to future work.\n9i.e., counters are observed only by zero-ness.\ntransitions of an SKCM are as follows: given a configuration ct = (q, n) (n \u2208 Zk) and inputwt \u2208 \u03a3, the next configuration of the SKCM is ct+1 = (\u03b4(q, wt, z(n)), u(wt)(n)).\nThe language recognized by a k-counter machine is the set of words w for which the machine reaches an accepting configuration \u2014 a configuration c = (q, n) for which (q, z(n)) \u2208 F .\nNote that while the counters can and are increased to various non-zero values, the transition function \u03b4 and the accept/reject classification of the configurations observe only their zero-ness.\nA.1 Computational Power of SKCMs We show that the SKCM model can recognize the context-free and context-sensitive languages anbn and anbncn, but not the context free language of palindromes, meaning its computational power differs from the language classes defined in the Chomsky hierarchy. Similar proofs appear in (Fischer et al., 1968) for their variant of the kcounter machine.\nanbn: We define the following SKCM over the alphabet {a, b}:\n1. Q = {qa, qb, qr} 2. q0 = qa 3. k = 1 4. u(a) = +1, u(b) = \u22121 5. for any z \u2208 {0, 1}: \u03b4(qa, a, z) = qa, \u03b4(qa, b, z) = qb, \u03b4(qb, a, z) = qr, \u03b4(qb, b, z) = qb \u03b4(qr, a, z) = qr, \u03b4(qr, b, z) = qr 6. C = {(qb, 0)}\nThe state qr is a rejecting sink state, and the states qa and qb keep track of whether the sequence is currently in the \u201ca\u201d or \u201cb\u201d phase. If an a is seen after moving to the b phase, the machine moves to (and stays in) the rejecting state. The counter is increased on input a and decreased on input b, and the machine accepts only sequences that reach the state qb with counter value zero, i.e., that have increased and decreased the counter an equal number of times, without switching from b to a. It follows easily that this machine recognizes exactly the language anbn.\nanbncn: We define the following SKCM over the alphabet {a, b}. As its state transition function ignores the counter values, we use the shorthand \u03b4(q, \u03c3) for \u03b4(q, \u03c3, z), for all z \u2208 {0, 1}2.\n1. Q = {qa, qb, qc, qr}\n2. q0 = qa 3. k = 2 4. u(a) = (+1, \u2205), u(b) = (\u22121,+1), u(c) = (\u2205,\u22121) 5. for any z \u2208 {0, 1}: \u03b4(qa, a) = qa, \u03b4(qa, b) = qb, \u03b4(qa, c) = qr, \u03b4(qb, a) = qr, \u03b4(qb, b) = qb, \u03b4(qb, c) = qc, \u03b4(qc, a) = qr, \u03b4(qc, b) = qr, \u03b4(qc, c) = qc, \u03b4(qr, a) = qr, \u03b4(qr, b) = qr, \u03b4(qr, c) = qr 6. C = {(qc, 0, 0)}\nBy similar reasoning as that for anbn, we see that this machine recognizes exactly the language anbncn. We note that this construction can be extended to build an SKCM for any language of the sort an1a n 2 ...a n m, using k = m \u2212 1 counters and k + 1 states.\nPalindromes: We prove that no SKCM can recognize the language of palindromes defined over the alphabet {a, b, x} by the grammar S \u2192 x|aSa|bSb. The intuition is that in order to correctly recognize this language in an one-way setting, one must be able to reach a unique configuration for every possible input sequence over {a, b} (requiring an exponential number of reachable configurations), whereas for any SKCM, the number of reachable configurations is always polynomial in the input length.10\nLet M be an SKCM with k counters. As its counters are only manipulated by steps of 1 or resets, the maximum and minimum values that each counter can attain on any input w \u2208 \u03a3\u2217 are +|w| and \u2212|w|, and in particular the total number of possible values a counter could reach at the end of input w is 2|w| + 1. This means that the total number of possible configurations M could reach on input of length n is c(n) = |Q| \u00b7 (2n+ 1)k. c(n) is polynomial in n, and so there exists a value m for which the number of input sequences of length m over {a, b} \u2014 2m \u2014 is greater than c(m). It follows by the pigeonhole principle that there exist two input sequences w1 6= w2 \u2208 {a, b}m for which M reaches the same configuration. This means that for any suffix w \u2208 \u03a3\u2217, and in particular for w = x \u00b7 w\u221211 where w \u22121 1 is the reverse of w1, M classifies w1 \u00b7 w and w2 \u00b7 w 10This will hold even if the counter update function can rely on any state-input combination.\nidentically\u2014despite the fact that w1 \u00b7 x \u00b7w\u221211 is in the language and w2 \u00b7 x \u00b7 w\u221211 is not. This means that M necessarily does not recognize this palindrome language, and ultimately that no such M exists.\nNote that this proof can be easily generalized to any palindrome grammar over 2 or more characters, with or without a clear \u2018midpoint\u2019 marker.\nB Impossibility of Counting in Binary\nWhile we have seen that the SRNN and GRU cannot allocate individual counting dimensions, the question remains whether they can count using a more elaborate mechanism, perhaps over several dimensions. We show here that one such mechanism \u2014 a binary counter \u2014 is not implementable in the SRNN.\nFor the purposes of this discussion, we first define a binary counter in an RNN.\nBinary Interpretation In an RNN with hidden state values in the range (\u22121, 1), the binary interpretation of a sequence of dimensions d1, ..., dn of its hidden state is the binary number obtained by replacing each positive hidden value in the sequence with a \u20181\u2019 and each negative value with a \u20180\u2019. For instance: the binary interpretation of the dimensions 3,0,1 in the hidden state vector (0.5,\u22120.1, 0.3, 0.8) is 110, i.e., 6.\nBinary Counting We say that the dimensions d1, d2, ..., dn in an RNN\u2019s hidden state implement a binary counter in the RNN if, in every transition, their binary interpretation either increases, decreases, resets to 0, or doesn\u2019t change.11\nA similar pair of definitions can be made for state values in the range (0, 1).\nWe first note intuitively that an SRNN would not generalize binary counting to a counter with dimensions beyond those seen in training \u2014 as it would have no reason to learn the \u2018carry\u2019 behavior between the untrained dimensions. We prove further that we cannot reasonably implement such counters regardless.\nWe now present a proof sketch that a singlelayer SRNN with hidden size n \u2265 3 cannot implement an n-dimensional binary counter that will consistently increase on one of its input symbols. After this, we will prove that even with helper\n11We note that the SKCMs presented here are more restricted in their relation between counter action and transition, but prefer here to give a general definition. Our proof will be relevant even within the restrictions.\ndimensions, we cannot implement a counter that will consistently increase on one input token and decrease on another \u2014 as we might want in order to classify the language of all words w for which #a(w) = #b(w).12\nConsistently Increasing Counter: The proof relies on the linearity of the affine transform Wx+ Uh + b, and the fact that \u2018carry\u2019 is a non-linear operation. We work with state values in the range (\u22121, 1), but the proof can easily be adapted to (0, 1) by rewriting h as h\u2032 + 0.5, where h\u2032 = h \u2212 0.5 is a vector with values in the range (\u22120.5, 0.5).\nSuppose we have a single-layer SRNN with hidden size n = 3, such that its entire hidden state represents a binary counter that increases every time it receives the input symbol a. We denote by xa the embedding of a, and assume w.l.o.g. that the hidden state dimensions are ordered from MSB to LSB, e.g. the hidden state vector (1, 1,\u22121) represents the number 110=6.\nRecall that the binary interpretation of the hidden state relies only on the signs of its values. We use p and n to denote \u2018some\u2019 positive or negative value, respectively. Then the number 6 can be represented by any state vector (p, p, n).\nRecall also that the SRNN state transition is\nht = tanh(Wxt + Uht\u22121 + b)\nand consider the state vectors (\u22121, 1, 1) and (1,\u22121,\u22121), which represent 3 and 4 respectively. Denoting b\u0303 = Wxa + b, we find that the constants U and b\u0303 must satisfy:\ntanh(U(\u22121, 1, 1) + b\u0303) = (p, n, n) tanh(U(1,\u22121,\u22121) + b\u0303) = (p, n, p)\nAs tanh is sign-preserving, this simplifies to:\nU(\u22121, 1, 1) = (p, n, n)\u2212 b\u0303 U(1,\u22121,\u22121) = (p, n, p)\u2212 b\u0303\nNoting the linearity of matrix multiplication and that (1,\u22121,\u22121) = \u2212(\u22121, 1, 1), we obtain:\nU(\u22121, 1, 1) = U(\u2212(1,\u22121,\u22121)) = \u2212U(1,\u22121,\u22121) 12Of course a counter could also be \u2018decreased\u2019 by incrementing a parallel, \u2018negative\u2019 counter, and implementing compare-to-zero as a comparison between these two. As intuitively no RNN could generalize binary counting behavior to dimensions not used in training, this approach could quickly find both counters outside of their learned range even on a sequence where the difference between them is never larger than in training.\n(p, n, n)\u2212 b\u0303 = b\u0303\u2212 (p, n, p) i.e. for some assignment to each p and n, 2b\u0303 = (p, n, n) + (p, n, p), and in particular b\u0303[1] < 0.\nSimilarly, for (\u22121,\u22121, 1) and (1, 1,\u22121), we obtain\nU(\u22121,\u22121, 1) = (n, p, n)\u2212 b\u0303 U(1, 1,\u22121) = (p, p, p)\u2212 b\u0303\ni.e. (n, p, n)\u2212 b\u0303 = b\u0303\u2212 (p, p, p)\nor 2b\u0303 = (p, p, p) + (n, p, n), and in particular that b\u0303[1] > 0, leading to a contradiction and proving that such an SRNN cannot exist. The argument trivially extends to n > 3 (by padding from the MSB).\nWe note that this proof does not extend to the case where additional, non counting dimensions are added to the RNN \u2014 at least not without further assumptions, such as the assumption that the counter behave correctly for all values of these dimensions, reachable and unreachable. One may argue then that, with enough dimensions, it could be possible to implement a consistently increasing binary counter on a subset of the SRNN\u2019s state.13 We now show a counting mechanism that cannot be implemented even with such \u2018helper\u2019 dimensions.\nBi-Directional Counter: We show that for n \u2265 3, no SRNN can implement an n-dimensional binary counter that increases for one token, \u03c3up, and decreases for another, \u03c3down. As before, we show the proof explicitly for n = 3, and note that it can be simply expanded to any n > 3 by padding.\nAssume by contradiction we have such an SRNN, with m \u2265 3 dimensions, and assume w.l.o.g. that a counter is encoded along the first 3 of these. We use the shorthand (v1, v2, v3)c to show the values of the counter dimensions explicitly while abstracting the remaining state dimensions, e.g. we write the hidden state (\u22120.5, 0.1, 1, 1, 1) as (\u22120.5, 0.1, 1)c where c = (1, 1).\nLet xup and xdown be the embeddings of \u03c3up and \u03c3down, and as before denote bup = Wxup + b and bdown = Wxdown + b. Then for some reachable state h1 \u2208 R where the counter value is 1 (e.g., the state reached on the input sequence \u03c3up\n14)), we find that the constants U, bdown, and 13(By storing processing information on the additional, \u2018helper\u2019 dimensions) 14(Or whichever appropriate sequence if the counter is not initiated to zero.)\nbup must satisfy:\ntanh(Uh1 + bup) = (n, p, n)c1\ntanh(Uh1 + bdown) = (n, n, n)c2\n(i.e., \u03c3up increases the counter and updates the additional dimensions to the values c1, while \u03c3down decreases and updates to c2.) Removing the signpreserving function tanh we obtain the constraints\nUh1 + bup = (n, p, n)sign(c1)\nUh1 + bdown = (n, n, n)sign(c2)\ni.e. (bup \u2212 bdown)[0 : 2] = (n, p, n) \u2212 (n, n, n), and in particular (bup \u2212 bdown)[1] > 0. Now consider a reachable state h3 for which the counter value is 3. Similarly to before, we now obtain\nUh3 + bup = (p, n, n)sign(c3)\nUh3 + bdown = (n, p, n)sign(c4)\nfrom which we get (bup \u2212 bdown)[0 : 2] = (p, n, n) \u2212 (n, p, n), and in particular (bup \u2212 bdown)[1] < 0, a contradiction to the previous statement. Again we conclude that no such SRNN can exist."}], "year": 2018, "references": [{"title": "Towards string-to-tree neural machine translation", "authors": ["Roee Aharoni", "Yoav Goldberg."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 132\u2013140, Vancouver, Canada. Asso-", "year": 2017}, {"title": "Recurrent neural networks as weighted language recognizers", "authors": ["Yining Chen", "Sorcha Gilroy", "Kevin Knight", "Jonathan May."], "venue": "CoRR, abs/1711.05408.", "year": 2017}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "year": 2014}, {"title": "Parsing as language modeling", "authors": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331\u20132336, Austin, Texas. Association for Computational Linguistics.", "year": 2016}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv:1412.3555 [cs].", "year": 2014}, {"title": "Finding Structure in Time", "authors": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211.", "year": 1990}, {"title": "Counter machines and counter languages", "authors": ["Patrick C. Fischer", "Albert R. Meyer", "Arnold L. Rosenberg."], "venue": "Mathematical systems theory, 2(3):265\u2013 283.", "year": 1968}, {"title": "Lstm recurrent networks learn simple context-free and contextsensitive languages", "authors": ["F.A. Gers", "E. Schmidhuber."], "venue": "Transactions on Neural Networks, 12(6):1333\u20131340.", "year": 2001}, {"title": "Recurrent nets that time and count", "authors": ["F.A. Gers", "J. Schmidhuber."], "venue": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New", "year": 2000}, {"title": "Lstm: A search space odyssey", "authors": ["K. Greff", "R.K. Srivastava", "J. Koutnk", "B.R. Steunebrink", "J. Schmidhuber."], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 28(10):2222\u20132232.", "year": 2017}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "year": 1997}, {"title": "Binarized neural networks", "authors": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio."], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems", "year": 2016}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "authors": ["Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton."], "venue": "arXiv:1504.00941", "year": 2015}, {"title": "Neural Networks and Analog Computation: Beyond the Turing Limit, 1 edition", "authors": ["Hava Siegelmann."], "venue": "Birkh\u00e4user Basel.", "year": 1999}, {"title": "Recurrent neural networks and finite automata", "authors": ["Hava T. Siegelmann."], "venue": "Computational Intelligence, 12:567\u2013574.", "year": 1996}, {"title": "On the computational power of neural nets", "authors": ["Hava T. Siegelmann", "Eduardo D. Sontag."], "venue": "Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT 1992, Pittsburgh, PA, USA, July 27-29, 1992., pages 440\u2013449.", "year": 1992}, {"title": "Analog computation via neural networks", "authors": ["Hava T. Siegelmann", "Eduardo D. Sontag."], "venue": "Theor. Comput. Sci., 131(2):331\u2013360.", "year": 1994}, {"title": "Grammar as a foreign language", "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201915, pages", "year": 2015}], "id": "SP:8d17daebcf740b3c25ab44ce74ada36675e963d3", "authors": [{"name": "Gail Weiss", "affiliations": []}, {"name": "Yoav Goldberg", "affiliations": []}, {"name": "Eran Yahav", "affiliations": []}], "abstractText": "While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.", "title": "On the Practical Computational Power of Finite Precision RNNs for Language Recognition"}